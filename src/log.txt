file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
Weights loaded from src/weights_.npz
batch_rate :  0.001
  LR: 0.9
  LR: 0.9, Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0, 0.0]
  LR: 0.9, Hidden Size: 784, Sample: 1/1
  LR: 0.9, Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
      exponential decayed learning_rate :              0.9
      batch rate adapted learning_rate :              0.0009000000000000001
      fit adapted learning_rate :              0.0009000000000000001
    epoch : 0 ; learning_rate : 0.0009000000000000001 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 0.7990018389620848
    Epoch 0, Loss: 1.3250906522935977, fit: 0.5166666666666667
    Epoch 0, Loss: 1.3250906522935977
      Weights saved to src/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.9145691605781531_fit_0.51666666666666672024-01-02170720
  self.fit : 0.5166666666666667
  self.loss : 0.9145691605781531
  current_accuracy : 0.4766
  LR: 0.9, Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.001
      exponential decayed learning_rate :              0.9
      batch rate adapted learning_rate :              0.0009000000000000001
      fit adapted learning_rate :              2.680504184676138e-06
    epoch : 0 ; learning_rate : 2.680504184676138e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.6352921711507652
    Epoch 0, Loss: 1.0065801660479994, fit: 0.48333333333333334
    Epoch 0, Loss: 1.0065801660479994
      exponential decayed learning_rate :              0.8900553503645301
      batch rate adapted learning_rate :              0.0008900553503645301
      fit adapted learning_rate :              4.5196058572454425e-06
    epoch : 1 ; learning_rate : 4.5196058572454425e-06 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.6338202237185573
    Epoch 1, Loss: 1.0049481237974445, fit: 0.4
      exponential decayed learning_rate :              0.8802205852361404
      batch rate adapted learning_rate :              0.0008802205852361405
      fit adapted learning_rate :              1.4784325784919849e-05
    epoch : 2 ; learning_rate : 1.4784325784919849e-05 ; fit : 0.4
    batch_size :          60
    best_batch_loss : 0.5698927985098209
    Epoch 2, Loss: 1.0006864202960701, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.8704944904338053
      batch rate adapted learning_rate :              0.0008704944904338053
      fit adapted learning_rate :              5.698438298676083e-06
    epoch : 3 ; learning_rate : 5.698438298676083e-06 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.5702861464369489
    Epoch 3, Loss: 0.9963236096596072, fit: 0.36666666666666664
      exponential decayed learning_rate :              0.8608758651927264
      batch rate adapted learning_rate :              0.0008608758651927265
      fit adapted learning_rate :              2.228431569421747e-05
    epoch : 4 ; learning_rate : 2.228431569421747e-05 ; fit : 0.36666666666666664
    batch_size :          60
    best_batch_loss : 0.5948343471157254
    Epoch 4, Loss: 0.9904549610845245, fit: 0.5833333333333334
      exponential decayed learning_rate :              0.8513635220160889
      batch rate adapted learning_rate :              0.0008513635220160889
      fit adapted learning_rate :              7.73437285543277e-07
    epoch : 5 ; learning_rate : 7.73437285543277e-07 ; fit : 0.5833333333333334
    batch_size :          60
    best_batch_loss : 0.6106664155510598
    Epoch 5, Loss: 0.9857157325788847, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.841956286528456
      batch rate adapted learning_rate :              0.0008419562865284561
      fit adapted learning_rate :              1.8938422089454287e-06
    epoch : 6 ; learning_rate : 1.8938422089454287e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.639556978505571
    Epoch 6, Loss: 0.9851826437559861, fit: 0.4166666666666667
      exponential decayed learning_rate :              0.8326529973307818
      batch rate adapted learning_rate :              0.0008326529973307818
      fit adapted learning_rate :              1.1163449226604947e-05
    epoch : 7 ; learning_rate : 1.1163449226604947e-05 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.5265371286642941
    Epoch 7, Loss: 0.982603496644073, fit: 0.43333333333333335
      exponential decayed learning_rate :              0.8234525058570279
      batch rate adapted learning_rate :              0.0008234525058570279
      fit adapted learning_rate :              8.75507536205191e-06
    epoch : 8 ; learning_rate : 8.75507536205191e-06 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.6160241459449247
    Epoch 8, Loss: 0.9787114256586198, fit: 0.5333333333333333
      Weights saved to src/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.8937479157941765_fit_0.53333333333333332024-01-02170823
  self.fit : 0.5333333333333333
  self.loss : 0.8937479157941765
  current_accuracy : 0.4915
  LR: 0.9, Epochs: 100
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          89
    learning_rate :          0.9
    batch_rate :          0.001
      exponential decayed learning_rate :              0.9
      batch rate adapted learning_rate :              0.0009000000000000001
      fit adapted learning_rate :              2.0244019972565163e-06
    epoch : 0 ; learning_rate : 2.0244019972565163e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.558118964046926
    Epoch 0, Loss: 0.9766090205275447, fit: 0.38333333333333336
    Epoch 0, Loss: 0.9766090205275447
      exponential decayed learning_rate :              0.8989893319423155
      batch rate adapted learning_rate :              0.0008989893319423154
      fit adapted learning_rate :              1.880002070557526e-05
    epoch : 1 ; learning_rate : 1.880002070557526e-05 ; fit : 0.38333333333333336
    batch_size :          60
    best_batch_loss : 0.6480970982899069
    Epoch 1, Loss: 0.9726297541321697, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8979797988289894
      batch rate adapted learning_rate :              0.0008979797988289894
      fit adapted learning_rate :              2.019857886939345e-06
    epoch : 2 ; learning_rate : 2.019857886939345e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.5404300478310325
    Epoch 2, Loss: 0.9687765588133203, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.8969713993855201
      batch rate adapted learning_rate :              0.0008969713993855201
      fit adapted learning_rate :              5.871761661039723e-06
    epoch : 3 ; learning_rate : 5.871761661039723e-06 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.6295907759821839
    Epoch 3, Loss: 0.9673405879587398, fit: 0.45
      exponential decayed learning_rate :              0.895964132338836
      batch rate adapted learning_rate :              0.000895964132338836
      fit adapted learning_rate :              7.502260500948787e-06
    epoch : 4 ; learning_rate : 7.502260500948787e-06 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.5617422303812591
    Epoch 4, Loss: 0.9648607747354959, fit: 0.4166666666666667
      exponential decayed learning_rate :              0.8949579964172959
      batch rate adapted learning_rate :              0.0008949579964172959
      fit adapted learning_rate :              1.199877762402337e-05
    epoch : 5 ; learning_rate : 1.199877762402337e-05 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.5489964466015392
    Epoch 5, Loss: 0.9613450466024899, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8939529903506866
      batch rate adapted learning_rate :              0.0008939529903506867
      fit adapted learning_rate :              2.0108002434659613e-06
    epoch : 6 ; learning_rate : 2.0108002434659613e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.5565565744276617
    Epoch 6, Loss: 0.958866861295347, fit: 0.45
      exponential decayed learning_rate :              0.8929491128702209
      batch rate adapted learning_rate :              0.000892949112870221
      fit adapted learning_rate :              7.477014555656383e-06
    epoch : 7 ; learning_rate : 7.477014555656383e-06 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.6069308822135857
    Epoch 7, Loss: 0.9571877064142015, fit: 0.5
      exponential decayed learning_rate :              0.8919463627085368
      batch rate adapted learning_rate :              0.0008919463627085368
      fit adapted learning_rate :              3.484165479330222e-06
    epoch : 8 ; learning_rate : 3.484165479330222e-06 ; fit : 0.5
    batch_size :          60
    best_batch_loss : 0.6046845151913037
    Epoch 8, Loss: 0.955235778024106, fit: 0.48333333333333334
      exponential decayed learning_rate :              0.8909447385996951
      batch rate adapted learning_rate :              0.0008909447385996951
      fit adapted learning_rate :              4.524122075563068e-06
    epoch : 9 ; learning_rate : 4.524122075563068e-06 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.5210115882859931
    Epoch 9, Loss: 0.9537991213576785, fit: 0.5833333333333334
      exponential decayed learning_rate :              0.8899442392791785
      batch rate adapted learning_rate :              0.0008899442392791785
      fit adapted learning_rate :              8.084866674613723e-07
    epoch : 10 ; learning_rate : 8.084866674613723e-07 ; fit : 0.5833333333333334
    batch_size :          60
    best_batch_loss : 0.5499309549710101
    Epoch 10, Loss: 0.9528660834874343, fit: 0.43333333333333335
    Epoch 10, Loss: 0.9528660834874343
      exponential decayed learning_rate :              0.8889448634838898
      batch rate adapted learning_rate :              0.0008889448634838898
      fit adapted learning_rate :              9.451400314108325e-06
    epoch : 11 ; learning_rate : 9.451400314108325e-06 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.5668127698834647
    Epoch 11, Loss: 0.9510453685128144, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8879466099521498
      batch rate adapted learning_rate :              0.0008879466099521498
      fit adapted learning_rate :              1.9972898784936494e-06
    epoch : 12 ; learning_rate : 1.9972898784936494e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.6038993116481833
    Epoch 12, Loss: 0.949054214392666, fit: 0.36666666666666664
      exponential decayed learning_rate :              0.8869494774236966
      batch rate adapted learning_rate :              0.0008869494774236965
      fit adapted learning_rate :              2.2959247620800717e-05
    epoch : 13 ; learning_rate : 2.2959247620800717e-05 ; fit : 0.36666666666666664
    batch_size :          60
    best_batch_loss : 0.5968828816090469
    Epoch 13, Loss: 0.9448380883375321, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.8859534646396829
      batch rate adapted learning_rate :              0.0008859534646396829
      fit adapted learning_rate :              5.79963596464766e-06
    epoch : 14 ; learning_rate : 5.79963596464766e-06 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.5168128936757165
    Epoch 14, Loss: 0.9398388388343649, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8849585703426759
      batch rate adapted learning_rate :              0.0008849585703426759
      fit adapted learning_rate :              1.99056877476776e-06
    epoch : 15 ; learning_rate : 1.99056877476776e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.5414556936717752
    Epoch 15, Loss: 0.938486916609266, fit: 0.43333333333333335
      exponential decayed learning_rate :              0.883964793276654
      batch rate adapted learning_rate :              0.000883964793276654
      fit adapted learning_rate :              9.398451431613542e-06
    epoch : 16 ; learning_rate : 9.398451431613542e-06 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.50583945587249
    Epoch 16, Loss: 0.9366117409707747, fit: 0.48333333333333334
      exponential decayed learning_rate :              0.8829721321870068
      batch rate adapted learning_rate :              0.0008829721321870068
      fit adapted learning_rate :              4.483638033053194e-06
    epoch : 17 ; learning_rate : 4.483638033053194e-06 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.5179139682251497
    Epoch 17, Loss: 0.9342997138094034, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8819805858205323
      batch rate adapted learning_rate :              0.0008819805858205323
      fit adapted learning_rate :              1.983870288307286e-06
    epoch : 18 ; learning_rate : 1.983870288307286e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.6007759494818947
    Epoch 18, Loss: 0.9332361577844693, fit: 0.55
      exponential decayed learning_rate :              0.8809901529254359
      batch rate adapted learning_rate :              0.000880990152925436
      fit adapted learning_rate :              1.4813959889347089e-06
    epoch : 19 ; learning_rate : 1.4813959889347089e-06 ; fit : 0.55
    batch_size :          60
    best_batch_loss : 0.5803436343583811
    Epoch 19, Loss: 0.9326631076367449, fit: 0.6833333333333333
      exponential decayed learning_rate :              0.8800008322513287
      batch rate adapted learning_rate :              0.0008800008322513288
      fit adapted learning_rate :              8.898194355538946e-08
    epoch : 20 ; learning_rate : 8.898194355538946e-08 ; fit : 0.6833333333333333
    batch_size :          60
    best_batch_loss : 0.5533704525924179
    Epoch 20, Loss: 0.9324121397311802, fit: 0.4166666666666667
    Epoch 20, Loss: 0.9324121397311802
      exponential decayed learning_rate :              0.8790126225492262
      batch rate adapted learning_rate :              0.0008790126225492262
      fit adapted learning_rate :              1.178499664665818e-05
    epoch : 21 ; learning_rate : 1.178499664665818e-05 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.5278928146913935
    Epoch 21, Loss: 0.9305091491366161, fit: 0.55
      exponential decayed learning_rate :              0.8780255225715461
      batch rate adapted learning_rate :              0.0008780255225715461
      fit adapted learning_rate :              1.4764109258209578e-06
    epoch : 22 ; learning_rate : 1.4764109258209578e-06 ; fit : 0.55
    batch_size :          60
    best_batch_loss : 0.5588616263993255
    Epoch 22, Loss: 0.9283609320160485, fit: 0.5666666666666667
      exponential decayed learning_rate :              0.8770395310721073
      batch rate adapted learning_rate :              0.0008770395310721073
      fit adapted learning_rate :              1.0904253757460025e-06
    epoch : 23 ; learning_rate : 1.0904253757460025e-06 ; fit : 0.5666666666666667
    batch_size :          60
    best_batch_loss : 0.5617147323943648
    Epoch 23, Loss: 0.9279461517962695, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8760546468061281
      batch rate adapted learning_rate :              0.0008760546468061281
      fit adapted learning_rate :              1.970540863000197e-06
    epoch : 24 ; learning_rate : 1.970540863000197e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.5700405151603545
    Epoch 24, Loss: 0.9274542164138441, fit: 0.55
      exponential decayed learning_rate :              0.8750708685302246
      batch rate adapted learning_rate :              0.0008750708685302247
      fit adapted learning_rate :              1.471442638001884e-06
    epoch : 25 ; learning_rate : 1.471442638001884e-06 ; fit : 0.55
    batch_size :          60
    best_batch_loss : 0.5481121241901692
    Epoch 25, Loss: 0.9269070713898266, fit: 0.5666666666666667
      exponential decayed learning_rate :              0.8740881950024092
      batch rate adapted learning_rate :              0.0008740881950024093
      fit adapted learning_rate :              1.086755972453748e-06
    epoch : 26 ; learning_rate : 1.086755972453748e-06 ; fit : 0.5666666666666667
    batch_size :          60
    best_batch_loss : 0.5215744226640123
    Epoch 26, Loss: 0.9265018467006143, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.873106624982089
      batch rate adapted learning_rate :              0.000873106624982089
      fit adapted learning_rate :              2.6004066243699433e-06
    epoch : 27 ; learning_rate : 2.6004066243699433e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.5106787786239362
    Epoch 27, Loss: 0.9259285657721924, fit: 0.43333333333333335
      exponential decayed learning_rate :              0.8721261572300644
      batch rate adapted learning_rate :              0.0008721261572300643
      fit adapted learning_rate :              9.272581208334638e-06
    epoch : 28 ; learning_rate : 9.272581208334638e-06 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.5254807962684637
    Epoch 28, Loss: 0.9240480037349996, fit: 0.6333333333333333
      exponential decayed learning_rate :              0.8711467905085268
      batch rate adapted learning_rate :              0.0008711467905085269
      fit adapted learning_rate :              2.846182764824712e-07
    epoch : 29 ; learning_rate : 2.846182764824712e-07 ; fit : 0.6333333333333333
    batch_size :          60
    best_batch_loss : 0.5455977829210835
    Epoch 29, Loss: 0.922547135498891, fit: 0.55
      exponential decayed learning_rate :              0.8701685235810587
      batch rate adapted learning_rate :              0.0008701685235810587
      fit adapted learning_rate :              1.4631992834990518e-06
    epoch : 30 ; learning_rate : 1.4631992834990518e-06 ; fit : 0.55
    batch_size :          60
    best_batch_loss : 0.48123508955127847
    Epoch 30, Loss: 0.9222780504069857, fit: 0.5
    Epoch 30, Loss: 0.9222780504069857
      exponential decayed learning_rate :              0.8691913552126297
      batch rate adapted learning_rate :              0.0008691913552126297
      fit adapted learning_rate :              3.3952787312993346e-06
    epoch : 31 ; learning_rate : 3.3952787312993346e-06 ; fit : 0.5
    batch_size :          60
    best_batch_loss : 0.5257020736973631
    Epoch 31, Loss: 0.9215294796017992, fit: 0.5666666666666667
      exponential decayed learning_rate :              0.8682152841695976
      batch rate adapted learning_rate :              0.0008682152841695976
      fit adapted learning_rate :              1.079454168173885e-06
    epoch : 32 ; learning_rate : 1.079454168173885e-06 ; fit : 0.5666666666666667
    batch_size :          60
    best_batch_loss : 0.39536691282064934
    Epoch 32, Loss: 0.920820665044714, fit: 0.43333333333333335
      exponential decayed learning_rate :              0.8672403092197045
      batch rate adapted learning_rate :              0.0008672403092197046
      fit adapted learning_rate :              9.220634110919819e-06
    epoch : 33 ; learning_rate : 9.220634110919819e-06 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.5304451316263378
    Epoch 33, Loss: 0.9192420762750536, fit: 0.5833333333333334
      exponential decayed learning_rate :              0.8662664291320767
      batch rate adapted learning_rate :              0.0008662664291320767
      fit adapted learning_rate :              7.869761132337067e-07
    epoch : 34 ; learning_rate : 7.869761132337067e-07 ; fit : 0.5833333333333334
    batch_size :          60
    best_batch_loss : 0.48719255297716135
    Epoch 34, Loss: 0.9176909172089462, fit: 0.5833333333333334
      exponential decayed learning_rate :              0.8652936426772231
      batch rate adapted learning_rate :              0.0008652936426772231
      fit adapted learning_rate :              7.860923669894778e-07
    epoch : 35 ; learning_rate : 7.860923669894778e-07 ; fit : 0.5833333333333334
    batch_size :          60
    best_batch_loss : 0.5123285889090873
    Epoch 35, Loss: 0.9174455102511122, fit: 0.45
      exponential decayed learning_rate :              0.8643219486270326
      batch rate adapted learning_rate :              0.0008643219486270326
      fit adapted learning_rate :              7.237308036383997e-06
    epoch : 36 ; learning_rate : 7.237308036383997e-06 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.5333919249746032
    Epoch 36, Loss: 0.916239257660092, fit: 0.5666666666666667
      exponential decayed learning_rate :              0.8633513457547737
      batch rate adapted learning_rate :              0.0008633513457547737
      fit adapted learning_rate :              1.073406821747999e-06
    epoch : 37 ; learning_rate : 1.073406821747999e-06 ; fit : 0.5666666666666667
    batch_size :          60
    best_batch_loss : 0.4660323100171369
    Epoch 37, Loss: 0.9149627503706145, fit: 0.5
      exponential decayed learning_rate :              0.8623818328350921
      batch rate adapted learning_rate :              0.0008623818328350921
      fit adapted learning_rate :              3.3686790345120784e-06
    epoch : 38 ; learning_rate : 3.3686790345120784e-06 ; fit : 0.5
    batch_size :          60
    best_batch_loss : 0.5145755715776039
    Epoch 38, Loss: 0.9142928728954094, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.8614134086440098
      batch rate adapted learning_rate :              0.0008614134086440099
      fit adapted learning_rate :              2.565580274007116e-06
    epoch : 39 ; learning_rate : 2.565580274007116e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.5730930188542162
    Epoch 39, Loss: 0.9134109486650069, fit: 0.6166666666666667
      exponential decayed learning_rate :              0.8604460719589235
      batch rate adapted learning_rate :              0.0008604460719589236
      fit adapted learning_rate :              4.011772909776371e-07
    epoch : 40 ; learning_rate : 4.011772909776371e-07 ; fit : 0.6166666666666667
    batch_size :          60
    best_batch_loss : 0.5345495677079043
    Epoch 40, Loss: 0.9129485331475016, fit: 0.5
    Epoch 40, Loss: 0.9129485331475016
      exponential decayed learning_rate :              0.8594798215586023
      batch rate adapted learning_rate :              0.0008594798215586023
      fit adapted learning_rate :              3.3573430529632903e-06
    epoch : 41 ; learning_rate : 3.3573430529632903e-06 ; fit : 0.5
    batch_size :          60
    best_batch_loss : 0.5960214079963108
    Epoch 41, Loss: 0.9123962547731871, fit: 0.55
      exponential decayed learning_rate :              0.858514656223187
      batch rate adapted learning_rate :              0.000858514656223187
      fit adapted learning_rate :              1.4436031594082194e-06
    epoch : 42 ; learning_rate : 1.4436031594082194e-06 ; fit : 0.55
    batch_size :          60
    best_batch_loss : 0.49873111272358994
    Epoch 42, Loss: 0.9116621888082502, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8575505747341883
      batch rate adapted learning_rate :              0.0008575505747341883
      fit adapted learning_rate :              1.9289189958226264e-06
    epoch : 43 ; learning_rate : 1.9289189958226264e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.5117207546089995
    Epoch 43, Loss: 0.9111536344764141, fit: 0.6166666666666667
      exponential decayed learning_rate :              0.856587575874485
      batch rate adapted learning_rate :              0.0008565875758744849
      fit adapted learning_rate :              3.993782926942481e-07
    epoch : 44 ; learning_rate : 3.993782926942481e-07 ; fit : 0.6166666666666667
    batch_size :          60
    best_batch_loss : 0.5355198341250241
    Epoch 44, Loss: 0.9108109552118366, fit: 0.5
      exponential decayed learning_rate :              0.8556256584283229
      batch rate adapted learning_rate :              0.000855625658428323
      fit adapted learning_rate :              3.3422877282356366e-06
    epoch : 45 ; learning_rate : 3.3422877282356366e-06 ; fit : 0.5
    batch_size :          60
    best_batch_loss : 0.488793589630977
    Epoch 45, Loss: 0.9102584149367023, fit: 0.43333333333333335
      exponential decayed learning_rate :              0.8546648211813132
      batch rate adapted learning_rate :              0.0008546648211813131
      fit adapted learning_rate :              9.086929562439381e-06
    epoch : 46 ; learning_rate : 9.086929562439381e-06 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.5298344726916526
    Epoch 46, Loss: 0.9084329461633502, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.85370506292043
      batch rate adapted learning_rate :              0.00085370506292043
      fit adapted learning_rate :              2.542622215152687e-06
    epoch : 47 ; learning_rate : 2.542622215152687e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.5042625902248469
    Epoch 47, Loss: 0.9066840703483395, fit: 0.48333333333333334
      exponential decayed learning_rate :              0.8527463824340109
      batch rate adapted learning_rate :              0.0008527463824340109
      fit adapted learning_rate :              4.330154909147106e-06
    epoch : 48 ; learning_rate : 4.330154909147106e-06 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.46353540523807274
    Epoch 48, Loss: 0.905656852758531, fit: 0.5666666666666667
      exponential decayed learning_rate :              0.8517887785117528
      batch rate adapted learning_rate :              0.0008517887785117528
      fit adapted learning_rate :              1.0590310538562743e-06
    epoch : 49 ; learning_rate : 1.0590310538562743e-06 ; fit : 0.5666666666666667
    batch_size :          60
    best_batch_loss : 0.5540995801047939
    Epoch 49, Loss: 0.9048713887827845, fit: 0.43333333333333335
      exponential decayed learning_rate :              0.8508322499447127
      batch rate adapted learning_rate :              0.0008508322499447128
      fit adapted learning_rate :              9.046181067816796e-06
    epoch : 50 ; learning_rate : 9.046181067816796e-06 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.5227496725587856
    Epoch 50, Loss: 0.9034095673297141, fit: 0.4666666666666667
    Epoch 50, Loss: 0.9034095673297141
      exponential decayed learning_rate :              0.849876795525305
      batch rate adapted learning_rate :              0.0008498767955253051
      fit adapted learning_rate :              5.563470572184825e-06
    epoch : 51 ; learning_rate : 5.563470572184825e-06 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.5489217074085708
    Epoch 51, Loss: 0.9012494604755407, fit: 0.55
      exponential decayed learning_rate :              0.8489224140472996
      batch rate adapted learning_rate :              0.0008489224140472996
      fit adapted learning_rate :              1.4274736839117406e-06
    epoch : 52 ; learning_rate : 1.4274736839117406e-06 ; fit : 0.55
    batch_size :          60
    best_batch_loss : 0.5310841626582903
    Epoch 52, Loss: 0.9002294482598847, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.8479691043058217
      batch rate adapted learning_rate :              0.0008479691043058218
      fit adapted learning_rate :              2.5255385917420353e-06
    epoch : 53 ; learning_rate : 2.5255385917420353e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.46770968666273693
    Epoch 53, Loss: 0.8996655255293076, fit: 0.48333333333333334
      exponential decayed learning_rate :              0.8470168650973491
      batch rate adapted learning_rate :              0.0008470168650973491
      fit adapted learning_rate :              4.30106103301529e-06
    epoch : 54 ; learning_rate : 4.30106103301529e-06 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.49098409288904954
    Epoch 54, Loss: 0.8986637273665096, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8460656952197113
      batch rate adapted learning_rate :              0.0008460656952197113
      fit adapted learning_rate :              1.9030856480144515e-06
    epoch : 55 ; learning_rate : 1.9030856480144515e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.49639417580299344
    Epoch 55, Loss: 0.8977763636427526, fit: 0.6166666666666667
      exponential decayed learning_rate :              0.8451155934720876
      batch rate adapted learning_rate :              0.0008451155934720877
      fit adapted learning_rate :              3.9402955676259453e-07
    epoch : 56 ; learning_rate : 3.9402955676259453e-07 ; fit : 0.6166666666666667
    batch_size :          60
    best_batch_loss : 0.5574389959451813
    Epoch 56, Loss: 0.8974418559351159, fit: 0.6
      exponential decayed learning_rate :              0.844166558655006
      batch rate adapted learning_rate :              0.000844166558655006
      fit adapted learning_rate :              5.53232995880145e-07
    epoch : 57 ; learning_rate : 5.53232995880145e-07 ; fit : 0.6
    batch_size :          60
    best_batch_loss : 0.4163939963513368
    Epoch 57, Loss: 0.8973052120037974, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.8432185895703413
      batch rate adapted learning_rate :              0.0008432185895703414
      fit adapted learning_rate :              5.519884568791133e-06
    epoch : 58 ; learning_rate : 5.519884568791133e-06 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.502720946956318
    Epoch 58, Loss: 0.8964126509703931, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.842271685021314
      batch rate adapted learning_rate :              0.000842271685021314
      fit adapted learning_rate :              1.8945516459886214e-06
    epoch : 59 ; learning_rate : 1.8945516459886214e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.4452815479378572
    Epoch 59, Loss: 0.8953766299941874, fit: 0.6
      exponential decayed learning_rate :              0.8413258438124882
      batch rate adapted learning_rate :              0.0008413258438124882
      fit adapted learning_rate :              5.513713050009525e-07
    epoch : 60 ; learning_rate : 5.513713050009525e-07 ; fit : 0.6
    batch_size :          60
    best_batch_loss : 0.5252604722053207
    Epoch 60, Loss: 0.8950202727576718, fit: 0.4666666666666667
    Epoch 60, Loss: 0.8950202727576718
      exponential decayed learning_rate :              0.8403810647497706
      batch rate adapted learning_rate :              0.0008403810647497706
      fit adapted learning_rate :              5.501309540127911e-06
    epoch : 61 ; learning_rate : 5.501309540127911e-06 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.5432923667901772
    Epoch 61, Loss: 0.8941707812141936, fit: 0.6
      exponential decayed learning_rate :              0.8394373466404088
      batch rate adapted learning_rate :              0.0008394373466404088
      fit adapted learning_rate :              5.501336594942586e-07
    epoch : 62 ; learning_rate : 5.501336594942586e-07 ; fit : 0.6
    batch_size :          60
    best_batch_loss : 0.49244680800031065
    Epoch 62, Loss: 0.8932934624436594, fit: 0.35
      exponential decayed learning_rate :              0.83849468829299
      batch rate adapted learning_rate :              0.00083849468829299
      fit adapted learning_rate :              2.6718198306090283e-05
    epoch : 63 ; learning_rate : 2.6718198306090283e-05 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.5235218567313106
    Epoch 63, Loss: 0.8894724534333014, fit: 0.6
      exponential decayed learning_rate :              0.8375530885174388
      batch rate adapted learning_rate :              0.0008375530885174388
      fit adapted learning_rate :              5.48898792090789e-07
    epoch : 64 ; learning_rate : 5.48898792090789e-07 ; fit : 0.6
    batch_size :          60
    best_batch_loss : 0.46112549904583905
    Epoch 64, Loss: 0.8855878049058391, fit: 0.5666666666666667
      exponential decayed learning_rate :              0.836612546125017
      batch rate adapted learning_rate :              0.000836612546125017
      fit adapted learning_rate :              1.0401624073284653e-06
    epoch : 65 ; learning_rate : 1.0401624073284653e-06 ; fit : 0.5666666666666667
    batch_size :          60
    best_batch_loss : 0.5407824724739603
    Epoch 65, Loss: 0.8853687945110954, fit: 0.48333333333333334
      exponential decayed learning_rate :              0.8356730599283205
      batch rate adapted learning_rate :              0.0008356730599283205
      fit adapted learning_rate :              4.24345840384802e-06
    epoch : 66 ; learning_rate : 4.24345840384802e-06 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.5186145523668161
    Epoch 66, Loss: 0.8846176535431228, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.8347346287412793
      batch rate adapted learning_rate :              0.0008347346287412794
      fit adapted learning_rate :              5.464346793602341e-06
    epoch : 67 ; learning_rate : 5.464346793602341e-06 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.4785043631613185
    Epoch 67, Loss: 0.8832699036845838, fit: 0.48333333333333334
      exponential decayed learning_rate :              0.8337972513791548
      batch rate adapted learning_rate :              0.0008337972513791549
      fit adapted learning_rate :              4.233933248696256e-06
    epoch : 68 ; learning_rate : 4.233933248696256e-06 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.46756477335843855
    Epoch 68, Loss: 0.8819349479278779, fit: 0.6333333333333333
      exponential decayed learning_rate :              0.8328609266585392
      batch rate adapted learning_rate :              0.0008328609266585392
      fit adapted learning_rate :              2.7210964223006806e-07
    epoch : 69 ; learning_rate : 2.7210964223006806e-07 ; fit : 0.6333333333333333
    batch_size :          60
    best_batch_loss : 0.5102189168260469
    Epoch 69, Loss: 0.8813089872775091, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.8319256533973531
      batch rate adapted learning_rate :              0.0008319256533973531
      fit adapted learning_rate :              2.4777557725233726e-06
    epoch : 70 ; learning_rate : 2.4777557725233726e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.5591282217119328
    Epoch 70, Loss: 0.8809177787069922, fit: 0.5333333333333333
    Epoch 70, Loss: 0.8809177787069922
      exponential decayed learning_rate :              0.8309914304148452
      batch rate adapted learning_rate :              0.0008309914304148453
      fit adapted learning_rate :              1.8691785682609575e-06
    epoch : 71 ; learning_rate : 1.8691785682609575e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.4946899641139492
    Epoch 71, Loss: 0.8803376841484957, fit: 0.5833333333333334
      exponential decayed learning_rate :              0.8300582565315897
      batch rate adapted learning_rate :              0.0008300582565315898
      fit adapted learning_rate :              7.540821143643567e-07
    epoch : 72 ; learning_rate : 7.540821143643567e-07 ; fit : 0.5833333333333334
    batch_size :          60
    best_batch_loss : 0.44654739882239836
    Epoch 72, Loss: 0.8799744334858823, fit: 0.5
      exponential decayed learning_rate :              0.8291261305694854
      batch rate adapted learning_rate :              0.0008291261305694854
      fit adapted learning_rate :              3.238773947537052e-06
    epoch : 73 ; learning_rate : 3.238773947537052e-06 ; fit : 0.5
    batch_size :          60
    best_batch_loss : 0.5182351505063875
    Epoch 73, Loss: 0.879437917027355, fit: 0.48333333333333334
      exponential decayed learning_rate :              0.8281950513517541
      batch rate adapted learning_rate :              0.0008281950513517541
      fit adapted learning_rate :              4.20548587624135e-06
    epoch : 74 ; learning_rate : 4.20548587624135e-06 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.5505410126066013
    Epoch 74, Loss: 0.8783903097385406, fit: 0.5
      exponential decayed learning_rate :              0.8272650177029388
      batch rate adapted learning_rate :              0.0008272650177029388
      fit adapted learning_rate :              3.2315039754021048e-06
    epoch : 75 ; learning_rate : 3.2315039754021048e-06 ; fit : 0.5
    batch_size :          60
    best_batch_loss : 0.42278986725734147
    Epoch 75, Loss: 0.8773961119057638, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.826336028448903
      batch rate adapted learning_rate :              0.000826336028448903
      fit adapted learning_rate :              2.4611079802288274e-06
    epoch : 76 ; learning_rate : 2.4611079802288274e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.4986928424769397
    Epoch 76, Loss: 0.876628050799929, fit: 0.6
      exponential decayed learning_rate :              0.8254080824168282
      batch rate adapted learning_rate :              0.0008254080824168282
      fit adapted learning_rate :              5.409394408926928e-07
    epoch : 77 ; learning_rate : 5.409394408926928e-07 ; fit : 0.6
    batch_size :          60
    best_batch_loss : 0.4835914526179324
    Epoch 77, Loss: 0.8762246889545556, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.8244811784352133
      batch rate adapted learning_rate :              0.0008244811784352133
      fit adapted learning_rate :              2.4555836099803364e-06
    epoch : 78 ; learning_rate : 2.4555836099803364e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.5058123504074926
    Epoch 78, Loss: 0.8758225635697744, fit: 0.6
      exponential decayed learning_rate :              0.8235553153338726
      batch rate adapted learning_rate :              0.0008235553153338726
      fit adapted learning_rate :              5.397252114572071e-07
    epoch : 79 ; learning_rate : 5.397252114572071e-07 ; fit : 0.6
    batch_size :          60
    best_batch_loss : 0.502082369553772
    Epoch 79, Loss: 0.8754201363378921, fit: 0.6
      exponential decayed learning_rate :              0.8226304919439344
      batch rate adapted learning_rate :              0.0008226304919439345
      fit adapted learning_rate :              5.391191192003771e-07
    epoch : 80 ; learning_rate : 5.391191192003771e-07 ; fit : 0.6
    batch_size :          60
    best_batch_loss : 0.48502375325575625
    Epoch 80, Loss: 0.8752732150825235, fit: 0.5166666666666667
    Epoch 80, Loss: 0.8752732150825235
      exponential decayed learning_rate :              0.8217067070978399
      batch rate adapted learning_rate :              0.0008217067070978399
      fit adapted learning_rate :              2.447320296613566e-06
    epoch : 81 ; learning_rate : 2.447320296613566e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.46714370401253114
    Epoch 81, Loss: 0.8748814734386772, fit: 0.6333333333333333
      exponential decayed learning_rate :              0.8207839596293409
      batch rate adapted learning_rate :              0.0008207839596293409
      fit adapted learning_rate :              2.6816389441989756e-07
    epoch : 82 ; learning_rate : 2.6816389441989756e-07 ; fit : 0.6333333333333333
    batch_size :          60
    best_batch_loss : 0.5329854646723573
    Epoch 82, Loss: 0.8745094202866859, fit: 0.6166666666666667
      exponential decayed learning_rate :              0.8198622483734995
      batch rate adapted learning_rate :              0.0008198622483734996
      fit adapted learning_rate :              3.82255351609086e-07
    epoch : 83 ; learning_rate : 3.82255351609086e-07 ; fit : 0.6166666666666667
    batch_size :          60
    best_batch_loss : 0.5167756036551265
    Epoch 83, Loss: 0.8744222482845146, fit: 0.5666666666666667
      exponential decayed learning_rate :              0.8189415721666855
      batch rate adapted learning_rate :              0.0008189415721666856
      fit adapted learning_rate :              1.0181920425551047e-06
    epoch : 84 ; learning_rate : 1.0181920425551047e-06 ; fit : 0.5666666666666667
    batch_size :          60
    best_batch_loss : 0.45286078468119856
    Epoch 84, Loss: 0.8742346254126757, fit: 0.4166666666666667
      exponential decayed learning_rate :              0.8180219298465756
      batch rate adapted learning_rate :              0.0008180219298465757
      fit adapted learning_rate :              1.0967289266195798e-05
    epoch : 85 ; learning_rate : 1.0967289266195798e-05 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.5118967628204707
    Epoch 85, Loss: 0.8726263399627942, fit: 0.6
      exponential decayed learning_rate :              0.8171033202521518
      batch rate adapted learning_rate :              0.0008171033202521519
      fit adapted learning_rate :              5.354968319604505e-07
    epoch : 86 ; learning_rate : 5.354968319604505e-07 ; fit : 0.6
    batch_size :          60
    best_batch_loss : 0.456464560728799
    Epoch 86, Loss: 0.8711141956125966, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.8161857422236997
      batch rate adapted learning_rate :              0.0008161857422236997
      fit adapted learning_rate :              2.430876997226252e-06
    epoch : 87 ; learning_rate : 2.430876997226252e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.5136472655846417
    Epoch 87, Loss: 0.8707247582488793, fit: 0.5666666666666667
      exponential decayed learning_rate :              0.8152691946028073
      batch rate adapted learning_rate :              0.0008152691946028074
      fit adapted learning_rate :              1.0136261666246571e-06
    epoch : 88 ; learning_rate : 1.0136261666246571e-06 ; fit : 0.5666666666666667
    batch_size :          60
    best_batch_loss : 0.5051851590830235
    Epoch 88, Loss: 0.8702674375835326, fit: 0.4666666666666667
      Weights saved to src/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.9980433955256809_fit_0.46666666666666672024-01-02171858
  self.fit : 0.4666666666666667
  self.loss : 0.9980433955256809
  current_accuracy : 0.5504
  LR: 0.9, Epochs: 1, Hidden Size: 784, Samples: 1, Accuracy mean: 0.4766
  LR: 0.9, Epochs: 10, Hidden Size: 784, Samples: 1, Accuracy mean: 0.4915
  LR: 0.9, Epochs: 100, Hidden Size: 784, Samples: 1, Accuracy mean: 0.5504
  LR: 0.1
  LR: 0.1, Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0, 0.0]
  LR: 0.1, Hidden Size: 784, Sample: 1/1
  LR: 0.1, Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.001
      exponential decayed learning_rate :              0.1
      batch rate adapted learning_rate :              0.0001
      fit adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4286856554502099
    Epoch 0, Loss: 1.7138725106476305, fit: 0.11666666666666667
    Epoch 0, Loss: 1.7138725106476305
      Weights saved to src/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6634480377574283_fit_0.116666666666666672024-01-02171906
  self.fit : 0.11666666666666667
  self.loss : 1.6634480377574283
  current_accuracy : 0.1424
  LR: 0.1, Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.001
      exponential decayed learning_rate :              0.1
      batch rate adapted learning_rate :              0.0001
      fit adapted learning_rate :              3.70678121733545e-05
    epoch : 0 ; learning_rate : 3.70678121733545e-05 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.3519359083431277
    Epoch 0, Loss: 1.646859041499858, fit: 0.11666666666666667
    Epoch 0, Loss: 1.646859041499858
      exponential decayed learning_rate :              0.09889503892939223
      batch rate adapted learning_rate :              9.889503892939224e-05
      fit adapted learning_rate :              3.665822727911293e-05
    epoch : 1 ; learning_rate : 3.665822727911293e-05 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.2931786010874902
    Epoch 1, Loss: 1.608533348986554, fit: 0.15
      exponential decayed learning_rate :              0.09780228724846006
      batch rate adapted learning_rate :              9.780228724846007e-05
      fit adapted learning_rate :              2.6650196602354087e-05
    epoch : 2 ; learning_rate : 2.6650196602354087e-05 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.2419094008823033
    Epoch 2, Loss: 1.573230420338443, fit: 0.16666666666666666
      exponential decayed learning_rate :              0.0967216100482006
      batch rate adapted learning_rate :              9.672161004820059e-05
      fit adapted learning_rate :              2.2494355212785762e-05
    epoch : 3 ; learning_rate : 2.2494355212785762e-05 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.1487748778608662
    Epoch 3, Loss: 1.5445565660151497, fit: 0.15
      exponential decayed learning_rate :              0.09565287391030293
      batch rate adapted learning_rate :              9.565287391030293e-05
      fit adapted learning_rate :              2.6064501833313684e-05
    epoch : 4 ; learning_rate : 2.6064501833313684e-05 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.221996698767272
    Epoch 4, Loss: 1.5165663741451312, fit: 0.3
      exponential decayed learning_rate :              0.09459594689067655
      batch rate adapted learning_rate :              9.459594689067654e-05
      fit adapted learning_rate :              5.453268092313188e-06
    epoch : 5 ; learning_rate : 5.453268092313188e-06 ; fit : 0.3
    batch_size :          60
    best_batch_loss : 1.1883117349792367
    Epoch 5, Loss: 1.4985609500095112, fit: 0.2833333333333333
      exponential decayed learning_rate :              0.09355069850316178
      batch rate adapted learning_rate :              9.355069850316178e-05
      fit adapted learning_rate :              6.510055275815562e-06
    epoch : 6 ; learning_rate : 6.510055275815562e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 1.1296624309226695
    Epoch 6, Loss: 1.491769208526765, fit: 0.23333333333333334
      exponential decayed learning_rate :              0.0925169997034202
      batch rate adapted learning_rate :              9.25169997034202e-05
      fit adapted learning_rate :              1.1042672461540643e-05
    epoch : 7 ; learning_rate : 1.1042672461540643e-05 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.1806974595211686
    Epoch 7, Loss: 1.4818405829023549, fit: 0.2
      exponential decayed learning_rate :              0.0914947228730031
      batch rate adapted learning_rate :              9.14947228730031e-05
      fit adapted learning_rate :              1.5350267285005144e-05
    epoch : 8 ; learning_rate : 1.5350267285005144e-05 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.151037122781065
    Epoch 8, Loss: 1.467199639858141, fit: 0.16666666666666666
      Weights saved to src/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6012157499879887_fit_0.166666666666666662024-01-02172012
  self.fit : 0.16666666666666666
  self.loss : 1.6012157499879887
  current_accuracy : 0.2474
  LR: 0.1, Epochs: 100
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          89
    learning_rate :          0.1
    batch_rate :          0.001
      exponential decayed learning_rate :              0.1
      batch rate adapted learning_rate :              0.0001
      fit adapted learning_rate :              2.3256803936137793e-05
    epoch : 0 ; learning_rate : 2.3256803936137793e-05 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.0382840859517897
    Epoch 0, Loss: 1.44564701151898, fit: 0.3
    Epoch 0, Loss: 1.44564701151898
      exponential decayed learning_rate :              0.09988770354914617
      batch rate adapted learning_rate :              9.988770354914618e-05
      fit adapted learning_rate :              5.758327333078211e-06
    epoch : 1 ; learning_rate : 5.758327333078211e-06 ; fit : 0.3
    batch_size :          60
    best_batch_loss : 1.0624061993097789
    Epoch 1, Loss: 1.429319356785065, fit: 0.16666666666666666
      exponential decayed learning_rate :              0.09977553320322105
      batch rate adapted learning_rate :              9.977553320322105e-05
      fit adapted learning_rate :              2.3204600133309184e-05
    epoch : 2 ; learning_rate : 2.3204600133309184e-05 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.0029823812269614
    Epoch 2, Loss: 1.413121177666635, fit: 0.26666666666666666
      exponential decayed learning_rate :              0.09966348882061334
      batch rate adapted learning_rate :              9.966348882061334e-05
      fit adapted learning_rate :              8.335804006518108e-06
    epoch : 3 ; learning_rate : 8.335804006518108e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 1.0377230273782496
    Epoch 3, Loss: 1.396022397039059, fit: 0.2
      exponential decayed learning_rate :              0.09955157025987066
      batch rate adapted learning_rate :              9.955157025987067e-05
      fit adapted learning_rate :              1.670198197389027e-05
    epoch : 4 ; learning_rate : 1.670198197389027e-05 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.03146431525538
    Epoch 4, Loss: 1.382517915831635, fit: 0.25
      exponential decayed learning_rate :              0.09943977737969956
      batch rate adapted learning_rate :              9.943977737969956e-05
      fit adapted learning_rate :              9.955205984317152e-06
    epoch : 5 ; learning_rate : 9.955205984317152e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 0.9056240821490189
    Epoch 5, Loss: 1.3680208135606822, fit: 0.36666666666666664
      exponential decayed learning_rate :              0.09932811003896519
      batch rate adapted learning_rate :              9.932811003896519e-05
      fit adapted learning_rate :              2.5711708864352226e-06
    epoch : 6 ; learning_rate : 2.5711708864352226e-06 ; fit : 0.36666666666666664
    batch_size :          60
    best_batch_loss : 0.9867311428815245
    Epoch 6, Loss: 1.3613415309139498, fit: 0.25
      exponential decayed learning_rate :              0.09921656809669122
      batch rate adapted learning_rate :              9.921656809669122e-05
      fit adapted learning_rate :              9.932859852331408e-06
    epoch : 7 ; learning_rate : 9.932859852331408e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.0334752893886292
    Epoch 7, Loss: 1.3546289167373269, fit: 0.21666666666666667
      exponential decayed learning_rate :              0.09910515141205965
      batch rate adapted learning_rate :              9.910515141205966e-05
      fit adapted learning_rate :              1.4049765957991471e-05
    epoch : 8 ; learning_rate : 1.4049765957991471e-05 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 0.9034317096026769
    Epoch 8, Loss: 1.3420418559891056, fit: 0.2833333333333333
      exponential decayed learning_rate :              0.09899385984441057
      batch rate adapted learning_rate :              9.899385984441058e-05
      fit adapted learning_rate :              6.8888368538656115e-06
    epoch : 9 ; learning_rate : 6.8888368538656115e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 0.9098092077918563
    Epoch 9, Loss: 1.3312284981216878, fit: 0.3
      exponential decayed learning_rate :              0.09888269325324206
      batch rate adapted learning_rate :              9.888269325324206e-05
      fit adapted learning_rate :              5.700390489489828e-06
    epoch : 10 ; learning_rate : 5.700390489489828e-06 ; fit : 0.3
    batch_size :          60
    best_batch_loss : 0.9556233076394882
    Epoch 10, Loss: 1.3248814974095822, fit: 0.35
    Epoch 10, Loss: 1.3248814974095822
      exponential decayed learning_rate :              0.09877165149820999
      batch rate adapted learning_rate :              9.877165149820999e-05
      fit adapted learning_rate :              3.147307441054515e-06
    epoch : 11 ; learning_rate : 3.147307441054515e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.9612949259776412
    Epoch 11, Loss: 1.3205351086822452, fit: 0.35
      exponential decayed learning_rate :              0.09866073443912776
      batch rate adapted learning_rate :              9.866073443912777e-05
      fit adapted learning_rate :              3.143773126500752e-06
    epoch : 12 ; learning_rate : 3.143773126500752e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.9793103500892532
    Epoch 12, Loss: 1.3174808368587991, fit: 0.26666666666666666
      exponential decayed learning_rate :              0.09854994193596628
      batch rate adapted learning_rate :              9.854994193596629e-05
      fit adapted learning_rate :              8.242667505956764e-06
    epoch : 13 ; learning_rate : 8.242667505956764e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 0.9592192128218472
    Epoch 13, Loss: 1.3120590308361706, fit: 0.2833333333333333
      exponential decayed learning_rate :              0.09843927384885366
      batch rate adapted learning_rate :              9.843927384885367e-05
      fit adapted learning_rate :              6.850244031534663e-06
    epoch : 14 ; learning_rate : 6.850244031534663e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 0.8844738368202464
    Epoch 14, Loss: 1.3049282377956695, fit: 0.23333333333333334
      exponential decayed learning_rate :              0.0983287300380751
      batch rate adapted learning_rate :              9.83287300380751e-05
      fit adapted learning_rate :              1.1736350755542017e-05
    epoch : 15 ; learning_rate : 1.1736350755542017e-05 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 0.8778528551230874
    Epoch 15, Loss: 1.2963067887266682, fit: 0.26666666666666666
      exponential decayed learning_rate :              0.09821831036407268
      batch rate adapted learning_rate :              9.821831036407269e-05
      fit adapted learning_rate :              8.214930008319546e-06
    epoch : 16 ; learning_rate : 8.214930008319546e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 0.8998605783811378
    Epoch 16, Loss: 1.287329413770152, fit: 0.3333333333333333
      exponential decayed learning_rate :              0.0981080146874452
      batch rate adapted learning_rate :              9.81080146874452e-05
      fit adapted learning_rate :              3.828021911291876e-06
    epoch : 17 ; learning_rate : 3.828021911291876e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 0.8599267239854943
    Epoch 17, Loss: 1.2819346393308264, fit: 0.35
      exponential decayed learning_rate :              0.09799784286894803
      batch rate adapted learning_rate :              9.799784286894803e-05
      fit adapted learning_rate :              3.1226504304660826e-06
    epoch : 18 ; learning_rate : 3.1226504304660826e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.9118270986781217
    Epoch 18, Loss: 1.2788427558533184, fit: 0.2833333333333333
      exponential decayed learning_rate :              0.09788779476949289
      batch rate adapted learning_rate :              9.788779476949289e-05
      fit adapted learning_rate :              6.8118674149242244e-06
    epoch : 19 ; learning_rate : 6.8118674149242244e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 0.9073700013043886
    Epoch 19, Loss: 1.2744803640151476, fit: 0.35
      exponential decayed learning_rate :              0.09777787025014764
      batch rate adapted learning_rate :              9.777787025014764e-05
      fit adapted learning_rate :              3.1156411170702112e-06
    epoch : 20 ; learning_rate : 3.1156411170702112e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.8617913078064922
    Epoch 20, Loss: 1.2701383193219473, fit: 0.35
    Epoch 20, Loss: 1.2701383193219473
      exponential decayed learning_rate :              0.09766806917213625
      batch rate adapted learning_rate :              9.766806917213625e-05
      fit adapted learning_rate :              3.1121423626743987e-06
    epoch : 21 ; learning_rate : 3.1121423626743987e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.8713109306254517
    Epoch 21, Loss: 1.2674392812573105, fit: 0.38333333333333336
      exponential decayed learning_rate :              0.09755839139683846
      batch rate adapted learning_rate :              9.755839139683846e-05
      fit adapted learning_rate :              2.040179691899687e-06
    epoch : 22 ; learning_rate : 2.040179691899687e-06 ; fit : 0.38333333333333336
    batch_size :          60
    best_batch_loss : 0.9142626943124534
    Epoch 22, Loss: 1.265204498153999, fit: 0.4
      exponential decayed learning_rate :              0.0974488367857897
      batch rate adapted learning_rate :              9.74488367857897e-05
      fit adapted learning_rate :              1.636766254468009e-06
    epoch : 23 ; learning_rate : 1.636766254468009e-06 ; fit : 0.4
    batch_size :          60
    best_batch_loss : 0.8405427692675062
    Epoch 23, Loss: 1.2635918680687157, fit: 0.4166666666666667
      exponential decayed learning_rate :              0.0973394052006809
      batch rate adapted learning_rate :              9.733940520068091e-05
      fit adapted learning_rate :              1.3050376461613146e-06
    epoch : 24 ; learning_rate : 1.3050376461613146e-06 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.7849734825189258
    Epoch 24, Loss: 1.2623507940335645, fit: 0.4
      exponential decayed learning_rate :              0.09723009650335829
      batch rate adapted learning_rate :              9.723009650335829e-05
      fit adapted learning_rate :              1.6330922576858458e-06
    epoch : 25 ; learning_rate : 1.6330922576858458e-06 ; fit : 0.4
    batch_size :          60
    best_batch_loss : 0.8364494869321244
    Epoch 25, Loss: 1.2610954929635534, fit: 0.2833333333333333
      exponential decayed learning_rate :              0.09712091055582324
      batch rate adapted learning_rate :              9.712091055582324e-05
      fit adapted learning_rate :              6.758501072384612e-06
    epoch : 26 ; learning_rate : 6.758501072384612e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 0.8036824310117358
    Epoch 26, Loss: 1.2575225089933304, fit: 0.3333333333333333
      exponential decayed learning_rate :              0.09701184722023211
      batch rate adapted learning_rate :              9.701184722023212e-05
      fit adapted learning_rate :              3.7852511642096397e-06
    epoch : 27 ; learning_rate : 3.7852511642096397e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 0.8696624003495378
    Epoch 27, Loss: 1.2530569161437726, fit: 0.31666666666666665
      exponential decayed learning_rate :              0.09690290635889603
      batch rate adapted learning_rate :              9.690290635889603e-05
      fit adapted learning_rate :              4.6067819177734605e-06
    epoch : 28 ; learning_rate : 4.6067819177734605e-06 ; fit : 0.31666666666666665
    batch_size :          60
    best_batch_loss : 0.8941867950590524
    Epoch 28, Loss: 1.2495389452908066, fit: 0.3333333333333333
      exponential decayed learning_rate :              0.09679408783428077
      batch rate adapted learning_rate :              9.679408783428078e-05
      fit adapted learning_rate :              3.7767545321713004e-06
    epoch : 29 ; learning_rate : 3.7767545321713004e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 0.800098553498628
    Epoch 29, Loss: 1.246048243983695, fit: 0.35
      exponential decayed learning_rate :              0.09668539150900651
      batch rate adapted learning_rate :              9.668539150900652e-05
      fit adapted learning_rate :              3.0808298486644216e-06
    epoch : 30 ; learning_rate : 3.0808298486644216e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.8988736721973568
    Epoch 30, Loss: 1.2432718421801225, fit: 0.26666666666666666
    Epoch 30, Loss: 1.2432718421801225
      exponential decayed learning_rate :              0.09657681724584775
      batch rate adapted learning_rate :              9.657681724584776e-05
      fit adapted learning_rate :              8.077636350697346e-06
    epoch : 31 ; learning_rate : 8.077636350697346e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 0.8272839647719301
    Epoch 31, Loss: 1.2387504335308528, fit: 0.3333333333333333
      exponential decayed learning_rate :              0.09646836490773307
      batch rate adapted learning_rate :              9.646836490773307e-05
      fit adapted learning_rate :              3.7640453309525514e-06
    epoch : 32 ; learning_rate : 3.7640453309525514e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 0.769721129312748
    Epoch 32, Loss: 1.2340658022673958, fit: 0.4
      exponential decayed learning_rate :              0.09636003435774494
      batch rate adapted learning_rate :              9.636003435774494e-05
      fit adapted learning_rate :              1.6184785546781807e-06
    epoch : 33 ; learning_rate : 1.6184785546781807e-06 ; fit : 0.4
    batch_size :          60
    best_batch_loss : 0.8331573441185175
    Epoch 33, Loss: 1.2319377742678468, fit: 0.38333333333333336
      exponential decayed learning_rate :              0.09625182545911964
      batch rate adapted learning_rate :              9.625182545911965e-05
      fit adapted learning_rate :              2.012856267906165e-06
    epoch : 34 ; learning_rate : 2.012856267906165e-06 ; fit : 0.38333333333333336
    batch_size :          60
    best_batch_loss : 0.8584860978037999
    Epoch 34, Loss: 1.2305113118428903, fit: 0.3333333333333333
      exponential decayed learning_rate :              0.09614373807524701
      batch rate adapted learning_rate :              9.614373807524702e-05
      fit adapted learning_rate :              3.751378897616714e-06
    epoch : 35 ; learning_rate : 3.751378897616714e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 0.8698938419108598
    Epoch 35, Loss: 1.2282526038094592, fit: 0.2833333333333333
      exponential decayed learning_rate :              0.09603577206967029
      batch rate adapted learning_rate :              9.603577206967029e-05
      fit adapted learning_rate :              6.682987883923151e-06
    epoch : 36 ; learning_rate : 6.682987883923151e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 0.8162593044524231
    Epoch 36, Loss: 1.2243081339514916, fit: 0.3
      exponential decayed learning_rate :              0.09592792730608596
      batch rate adapted learning_rate :              9.592792730608595e-05
      fit adapted learning_rate :              5.530054112620514e-06
    epoch : 37 ; learning_rate : 5.530054112620514e-06 ; fit : 0.3
    batch_size :          60
    best_batch_loss : 0.7581785257958731
    Epoch 37, Loss: 1.2196387485869602, fit: 0.4166666666666667
      exponential decayed learning_rate :              0.09582020364834357
      batch rate adapted learning_rate :              9.582020364834356e-05
      fit adapted learning_rate :              1.2846695823353701e-06
    epoch : 38 ; learning_rate : 1.2846695823353701e-06 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.7726407966987412
    Epoch 38, Loss: 1.2171205965973897, fit: 0.4166666666666667
      exponential decayed learning_rate :              0.09571260096044554
      batch rate adapted learning_rate :              9.571260096044555e-05
      fit adapted learning_rate :              1.2832269439892086e-06
    epoch : 39 ; learning_rate : 1.2832269439892086e-06 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.8613193904056248
    Epoch 39, Loss: 1.2161593051234998, fit: 0.3333333333333333
      exponential decayed learning_rate :              0.09560511910654707
      batch rate adapted learning_rate :              9.560511910654707e-05
      fit adapted learning_rate :              3.7303628244590873e-06
    epoch : 40 ; learning_rate : 3.7303628244590873e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 0.8192961903692025
    Epoch 40, Loss: 1.2143305903735773, fit: 0.3
    Epoch 40, Loss: 1.2143305903735773
      exponential decayed learning_rate :              0.09549775795095582
      batch rate adapted learning_rate :              9.549775795095582e-05
      fit adapted learning_rate :              5.505255705334278e-06
    epoch : 41 ; learning_rate : 5.505255705334278e-06 ; fit : 0.3
    batch_size :          60
    best_batch_loss : 0.8466320047773604
    Epoch 41, Loss: 1.2109101047960111, fit: 0.4166666666666667
      exponential decayed learning_rate :              0.0953905173581319
      batch rate adapted learning_rate :              9.53905173581319e-05
      fit adapted learning_rate :              1.278908741865783e-06
    epoch : 42 ; learning_rate : 1.278908741865783e-06 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.8130272733835148
    Epoch 42, Loss: 1.2084262420200405, fit: 0.5
      exponential decayed learning_rate :              0.09528339719268758
      batch rate adapted learning_rate :              9.528339719268758e-05
      fit adapted learning_rate :              3.7220077028393587e-07
    epoch : 43 ; learning_rate : 3.7220077028393587e-07 ; fit : 0.5
    batch_size :          60
    best_batch_loss : 0.8728780221909267
    Epoch 43, Loss: 1.2078191182994569, fit: 0.31666666666666665
      exponential decayed learning_rate :              0.09517639731938722
      batch rate adapted learning_rate :              9.517639731938723e-05
      fit adapted learning_rate :              4.524703361794717e-06
    epoch : 44 ; learning_rate : 4.524703361794717e-06 ; fit : 0.31666666666666665
    batch_size :          60
    best_batch_loss : 0.7240934752666651
    Epoch 44, Loss: 1.206000948342873, fit: 0.36666666666666664
      exponential decayed learning_rate :              0.095069517603147
      batch rate adapted learning_rate :              9.5069517603147e-05
      fit adapted learning_rate :              2.460934530544895e-06
    epoch : 45 ; learning_rate : 2.460934530544895e-06 ; fit : 0.36666666666666664
    batch_size :          60
    best_batch_loss : 0.8024612420686653
    Epoch 45, Loss: 1.2035073358125956, fit: 0.3
      exponential decayed learning_rate :              0.0949627579090348
      batch rate adapted learning_rate :              9.49627579090348e-05
      fit adapted learning_rate :              5.4744140175676145e-06
    epoch : 46 ; learning_rate : 5.4744140175676145e-06 ; fit : 0.3
    batch_size :          60
    best_batch_loss : 0.7435067429716521
    Epoch 46, Loss: 1.2006347646597337, fit: 0.43333333333333335
      exponential decayed learning_rate :              0.09485611810227002
      batch rate adapted learning_rate :              9.485611810227002e-05
      fit adapted learning_rate :              1.0085250292276859e-06
    epoch : 47 ; learning_rate : 1.0085250292276859e-06 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.8145076532026596
    Epoch 47, Loss: 1.19825945299907, fit: 0.3
      exponential decayed learning_rate :              0.09474959804822343
      batch rate adapted learning_rate :              9.474959804822344e-05
      fit adapted learning_rate :              5.462125775779962e-06
    epoch : 48 ; learning_rate : 5.462125775779962e-06 ; fit : 0.3
    batch_size :          60
    best_batch_loss : 0.849055708209813
    Epoch 48, Loss: 1.195890108049054, fit: 0.43333333333333335
      exponential decayed learning_rate :              0.09464319761241699
      batch rate adapted learning_rate :              9.4643197612417e-05
      fit adapted learning_rate :              1.0062612253998646e-06
    epoch : 49 ; learning_rate : 1.0062612253998646e-06 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.8187410094017745
    Epoch 49, Loss: 1.1936052238932515, fit: 0.38333333333333336
      exponential decayed learning_rate :              0.09453691666052365
      batch rate adapted learning_rate :              9.453691666052365e-05
      fit adapted learning_rate :              1.976993416394767e-06
    epoch : 50 ; learning_rate : 1.976993416394767e-06 ; fit : 0.38333333333333336
    batch_size :          60
    best_batch_loss : 0.7870610183441243
    Epoch 50, Loss: 1.1925153532464365, fit: 0.3333333333333333
    Epoch 50, Loss: 1.1925153532464365
      exponential decayed learning_rate :              0.09443075505836723
      batch rate adapted learning_rate :              9.443075505836723e-05
      fit adapted learning_rate :              3.684540968593512e-06
    epoch : 51 ; learning_rate : 3.684540968593512e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 0.7404295627966389
    Epoch 51, Loss: 1.1904763214828797, fit: 0.2833333333333333
      exponential decayed learning_rate :              0.09432471267192219
      batch rate adapted learning_rate :              9.43247126719222e-05
      fit adapted learning_rate :              6.563917781425014e-06
    epoch : 52 ; learning_rate : 6.563917781425014e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 0.8060369905291005
    Epoch 52, Loss: 1.1868253748075095, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.09421878936731354
      batch rate adapted learning_rate :              9.421878936731354e-05
      fit adapted learning_rate :              6.167758253335224e-07
    epoch : 53 ; learning_rate : 6.167758253335224e-07 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.7539151529123769
    Epoch 53, Loss: 1.1842697778078073, fit: 0.4166666666666667
      exponential decayed learning_rate :              0.09411298501081657
      batch rate adapted learning_rate :              9.411298501081657e-05
      fit adapted learning_rate :              1.26178075753099e-06
    epoch : 54 ; learning_rate : 1.26178075753099e-06 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.7493593674914177
    Epoch 54, Loss: 1.1836063751897237, fit: 0.3333333333333333
      exponential decayed learning_rate :              0.09400729946885682
      batch rate adapted learning_rate :              9.400729946885681e-05
      fit adapted learning_rate :              3.6680183911030888e-06
    epoch : 55 ; learning_rate : 3.6680183911030888e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 0.7471786287290124
    Epoch 55, Loss: 1.1818615826548078, fit: 0.3333333333333333
      exponential decayed learning_rate :              0.09390173260800974
      batch rate adapted learning_rate :              9.390173260800975e-05
      fit adapted learning_rate :              3.663899336633214e-06
    epoch : 56 ; learning_rate : 3.663899336633214e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 0.8189951608406763
    Epoch 56, Loss: 1.1792777666306329, fit: 0.45
      exponential decayed learning_rate :              0.09379628429500067
      batch rate adapted learning_rate :              9.379628429500068e-05
      fit adapted learning_rate :              7.853932243528998e-07
    epoch : 57 ; learning_rate : 7.853932243528998e-07 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.75651102192426
    Epoch 57, Loss: 1.1777128266878438, fit: 0.4
      exponential decayed learning_rate :              0.0936909543967046
      batch rate adapted learning_rate :              9.36909543967046e-05
      fit adapted learning_rate :              1.5736482605997533e-06
    epoch : 58 ; learning_rate : 1.5736482605997533e-06 ; fit : 0.4
    batch_size :          60
    best_batch_loss : 0.8185596566774717
    Epoch 58, Loss: 1.176901109154235, fit: 0.35
      exponential decayed learning_rate :              0.093585742780146
      batch rate adapted learning_rate :              9.3585742780146e-05
      fit adapted learning_rate :              2.982061149740979e-06
    epoch : 59 ; learning_rate : 2.982061149740979e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.7469669344770735
    Epoch 59, Loss: 1.175293928405599, fit: 0.38333333333333336
      exponential decayed learning_rate :              0.09348064931249869
      batch rate adapted learning_rate :              9.34806493124987e-05
      fit adapted learning_rate :              1.9549043355704283e-06
    epoch : 60 ; learning_rate : 1.9549043355704283e-06 ; fit : 0.38333333333333336
    batch_size :          60
    best_batch_loss : 0.7725060405978373
    Epoch 60, Loss: 1.173587548745361, fit: 0.35
    Epoch 60, Loss: 1.173587548745361
      exponential decayed learning_rate :              0.09337567386108563
      batch rate adapted learning_rate :              9.337567386108563e-05
      fit adapted learning_rate :              2.9753674126001663e-06
    epoch : 61 ; learning_rate : 2.9753674126001663e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.7141034382345908
    Epoch 61, Loss: 1.1718832514956996, fit: 0.38333333333333336
      exponential decayed learning_rate :              0.09327081629337876
      batch rate adapted learning_rate :              9.327081629337876e-05
      fit adapted learning_rate :              1.9505162244282804e-06
    epoch : 62 ; learning_rate : 1.9505162244282804e-06 ; fit : 0.38333333333333336
    batch_size :          60
    best_batch_loss : 0.7158616876430265
    Epoch 62, Loss: 1.1701814821077268, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.09316607647699889
      batch rate adapted learning_rate :              9.316607647699888e-05
      fit adapted learning_rate :              6.09884547424699e-07
    epoch : 63 ; learning_rate : 6.09884547424699e-07 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.7370050265705537
    Epoch 63, Loss: 1.1692830865249622, fit: 0.4166666666666667
      exponential decayed learning_rate :              0.09306145427971543
      batch rate adapted learning_rate :              9.306145427971543e-05
      fit adapted learning_rate :              1.2476827959978028e-06
    epoch : 64 ; learning_rate : 1.2476827959978028e-06 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.7990537739306522
    Epoch 64, Loss: 1.1686510266595411, fit: 0.45
      exponential decayed learning_rate :              0.09295694956944633
      batch rate adapted learning_rate :              9.295694956944633e-05
      fit adapted learning_rate :              7.783651441749985e-07
    epoch : 65 ; learning_rate : 7.783651441749985e-07 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.631751968871331
    Epoch 65, Loss: 1.1679364482787815, fit: 0.35
      exponential decayed learning_rate :              0.09285256221425785
      batch rate adapted learning_rate :              9.285256221425785e-05
      fit adapted learning_rate :              2.958698731317732e-06
    epoch : 66 ; learning_rate : 2.958698731317732e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.7404752175148545
    Epoch 66, Loss: 1.1666515834694482, fit: 0.43333333333333335
      exponential decayed learning_rate :              0.09274829208236438
      batch rate adapted learning_rate :              9.274829208236437e-05
      fit adapted learning_rate :              9.8611429451851e-07
    epoch : 67 ; learning_rate : 9.8611429451851e-07 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.7818055826839118
    Epoch 67, Loss: 1.1652849718296348, fit: 0.2833333333333333
      exponential decayed learning_rate :              0.09264413904212832
      batch rate adapted learning_rate :              9.264413904212832e-05
      fit adapted learning_rate :              6.446969138602573e-06
    epoch : 68 ; learning_rate : 6.446969138602573e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 0.7891259202821133
    Epoch 68, Loss: 1.1627513755771721, fit: 0.4166666666666667
      exponential decayed learning_rate :              0.0925401029620599
      batch rate adapted learning_rate :              9.25401029620599e-05
      fit adapted learning_rate :              1.240692994745026e-06
    epoch : 69 ; learning_rate : 1.240692994745026e-06 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.817818262024809
    Epoch 69, Loss: 1.160083850515578, fit: 0.4
      exponential decayed learning_rate :              0.09243618371081702
      batch rate adapted learning_rate :              9.243618371081702e-05
      fit adapted learning_rate :              1.5525729313962758e-06
    epoch : 70 ; learning_rate : 1.5525729313962758e-06 ; fit : 0.4
    batch_size :          60
    best_batch_loss : 0.7484323484815086
    Epoch 70, Loss: 1.1591301908862919, fit: 0.4166666666666667
    Epoch 70, Loss: 1.1591301908862919
      exponential decayed learning_rate :              0.09233238115720503
      batch rate adapted learning_rate :              9.233238115720504e-05
      fit adapted learning_rate :              1.2379080509218611e-06
    epoch : 71 ; learning_rate : 1.2379080509218611e-06 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.7169635761733758
    Epoch 71, Loss: 1.1581635624397553, fit: 0.35
      exponential decayed learning_rate :              0.09222869517017664
      batch rate adapted learning_rate :              9.222869517017664e-05
      fit adapted learning_rate :              2.9388195315647435e-06
    epoch : 72 ; learning_rate : 2.9388195315647435e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.707514985100026
    Epoch 72, Loss: 1.1567620867224728, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.09212512561883171
      batch rate adapted learning_rate :              9.212512561883172e-05
      fit adapted learning_rate :              6.030702662288888e-07
    epoch : 73 ; learning_rate : 6.030702662288888e-07 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.7501744779192959
    Epoch 73, Loss: 1.1555264070826503, fit: 0.26666666666666666
      exponential decayed learning_rate :              0.09202167237241712
      batch rate adapted learning_rate :              9.202167237241712e-05
      fit adapted learning_rate :              7.696646327815867e-06
    epoch : 74 ; learning_rate : 7.696646327815867e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 0.7226524062602
    Epoch 74, Loss: 1.152698952137889, fit: 0.43333333333333335
      exponential decayed learning_rate :              0.09191833530032655
      batch rate adapted learning_rate :              9.191833530032655e-05
      fit adapted learning_rate :              9.772900646785334e-07
    epoch : 75 ; learning_rate : 9.772900646785334e-07 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.7595192343944279
    Epoch 75, Loss: 1.149769512004502, fit: 0.38333333333333336
      exponential decayed learning_rate :              0.09181511427210033
      batch rate adapted learning_rate :              9.181511427210033e-05
      fit adapted learning_rate :              1.9200740076312758e-06
    epoch : 76 ; learning_rate : 1.9200740076312758e-06 ; fit : 0.38333333333333336
    batch_size :          60
    best_batch_loss : 0.6961256429476574
    Epoch 76, Loss: 1.1488010925209824, fit: 0.36666666666666664
      exponential decayed learning_rate :              0.09171200915742536
      batch rate adapted learning_rate :              9.171200915742537e-05
      fit adapted learning_rate :              2.374023303066457e-06
    epoch : 77 ; learning_rate : 2.374023303066457e-06 ; fit : 0.36666666666666664
    batch_size :          60
    best_batch_loss : 0.7040149373792599
    Epoch 77, Loss: 1.1473251044574728, fit: 0.3333333333333333
      exponential decayed learning_rate :              0.09160901982613481
      batch rate adapted learning_rate :              9.160901982613482e-05
      fit adapted learning_rate :              3.5744412552187982e-06
    epoch : 78 ; learning_rate : 3.5744412552187982e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 0.7335730710431243
    Epoch 78, Loss: 1.145341029429744, fit: 0.45
      exponential decayed learning_rate :              0.09150614614820807
      batch rate adapted learning_rate :              9.150614614820807e-05
      fit adapted learning_rate :              7.662169958184514e-07
    epoch : 79 ; learning_rate : 7.662169958184514e-07 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.7222091618521324
    Epoch 79, Loss: 1.1439392850188632, fit: 0.23333333333333334
      exponential decayed learning_rate :              0.0914033879937705
      batch rate adapted learning_rate :              9.14033879937705e-05
      fit adapted learning_rate :              1.0909753653122526e-05
    epoch : 80 ; learning_rate : 1.0909753653122526e-05 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 0.6864130204663127
    Epoch 80, Loss: 1.1400842015262032, fit: 0.4
    Epoch 80, Loss: 1.1400842015262032
      exponential decayed learning_rate :              0.09130074523309333
      batch rate adapted learning_rate :              9.130074523309333e-05
      fit adapted learning_rate :              1.5335019250542723e-06
    epoch : 81 ; learning_rate : 1.5335019250542723e-06 ; fit : 0.4
    batch_size :          60
    best_batch_loss : 0.7626152142008407
    Epoch 81, Loss: 1.1360089123710027, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.09119821773659344
      batch rate adapted learning_rate :              9.119821773659343e-05
      fit adapted learning_rate :              5.970025341139397e-07
    epoch : 82 ; learning_rate : 5.970025341139397e-07 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.7586919881780202
    Epoch 82, Loss: 1.1353210531978601, fit: 0.38333333333333336
      exponential decayed learning_rate :              0.09109580537483329
      batch rate adapted learning_rate :              9.109580537483329e-05
      fit adapted learning_rate :              1.9050315352883532e-06
    epoch : 83 ; learning_rate : 1.9050315352883532e-06 ; fit : 0.38333333333333336
    batch_size :          60
    best_batch_loss : 0.6375547342354435
    Epoch 83, Loss: 1.1345115058752118, fit: 0.3333333333333333
      exponential decayed learning_rate :              0.09099350801852062
      batch rate adapted learning_rate :              9.099350801852062e-05
      fit adapted learning_rate :              3.550424943261896e-06
    epoch : 84 ; learning_rate : 3.550424943261896e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 0.775935094091359
    Epoch 84, Loss: 1.132759215913538, fit: 0.5
      exponential decayed learning_rate :              0.09089132553850841
      batch rate adapted learning_rate :              9.089132553850842e-05
      fit adapted learning_rate :              3.550442403847985e-07
    epoch : 85 ; learning_rate : 3.550442403847985e-07 ; fit : 0.5
    batch_size :          60
    best_batch_loss : 0.7120032958268373
    Epoch 85, Loss: 1.131506505191921, fit: 0.48333333333333334
      exponential decayed learning_rate :              0.09078925780579465
      batch rate adapted learning_rate :              9.078925780579466e-05
      fit adapted learning_rate :              4.6101813913705597e-07
    epoch : 86 ; learning_rate : 4.6101813913705597e-07 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.7234286352614124
    Epoch 86, Loss: 1.1312493875058915, fit: 0.35
      exponential decayed learning_rate :              0.0906873046915222
      batch rate adapted learning_rate :              9.06873046915222e-05
      fit adapted learning_rate :              2.88970392349852e-06
    epoch : 87 ; learning_rate : 2.88970392349852e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.7832147705890479
    Epoch 87, Loss: 1.130181979039146, fit: 0.4166666666666667
      exponential decayed learning_rate :              0.09058546606697859
      batch rate adapted learning_rate :              9.058546606697859e-05
      fit adapted learning_rate :              1.214487012415486e-06
    epoch : 88 ; learning_rate : 1.214487012415486e-06 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.7079496675812057
    Epoch 88, Loss: 1.1288801074924144, fit: 0.4
      Weights saved to src/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.1462139001397218_fit_0.42024-01-02173035
  self.fit : 0.4
  self.loss : 1.1462139001397218
  current_accuracy : 0.418
  LR: 0.1, Epochs: 1, Hidden Size: 784, Samples: 1, Accuracy mean: 0.1424
  LR: 0.1, Epochs: 10, Hidden Size: 784, Samples: 1, Accuracy mean: 0.2474
  LR: 0.1, Epochs: 100, Hidden Size: 784, Samples: 1, Accuracy mean: 0.418
  LR: 0.0001
  LR: 0.0001, Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0, 0.0]
  LR: 0.0001, Hidden Size: 784, Sample: 1/1
  LR: 0.0001, Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.001
      exponential decayed learning_rate :              0.0001
      batch rate adapted learning_rate :              1.0000000000000001e-07
      fit adapted learning_rate :              1.0000000000000001e-07
    epoch : 0 ; learning_rate : 1.0000000000000001e-07 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5156943155428202
    Epoch 0, Loss: 1.772134345993451, fit: 0.06666666666666667
    Epoch 0, Loss: 1.772134345993451
      Weights saved to src/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7634358547677238_fit_0.066666666666666672024-01-02173042
  self.fit : 0.06666666666666667
  self.loss : 1.7634358547677238
  current_accuracy : 0.0799
  LR: 0.0001, Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.001
      exponential decayed learning_rate :              0.0001
      batch rate adapted learning_rate :              1.0000000000000001e-07
      fit adapted learning_rate :              5.7582990144185347e-08
    epoch : 0 ; learning_rate : 5.7582990144185347e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5109106008901538
    Epoch 0, Loss: 1.7720452323666573, fit: 0.06666666666666667
    Epoch 0, Loss: 1.7720452323666573
      exponential decayed learning_rate :              9.889503892939224e-05
      batch rate adapted learning_rate :              9.889503892939224e-08
      fit adapted learning_rate :              5.6946720519800195e-08
    epoch : 1 ; learning_rate : 5.6946720519800195e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5212474491845545
    Epoch 1, Loss: 1.7719855771142596, fit: 0.1
      exponential decayed learning_rate :              9.780228724846005e-05
      batch rate adapted learning_rate :              9.780228724846006e-08
      fit adapted learning_rate :              4.210067772346319e-08
    epoch : 2 ; learning_rate : 4.210067772346319e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4576307184791044
    Epoch 2, Loss: 1.7719299050954238, fit: 0.1
      exponential decayed learning_rate :              9.672161004820059e-05
      batch rate adapted learning_rate :              9.67216100482006e-08
      fit adapted learning_rate :              4.1635481624156887e-08
    epoch : 3 ; learning_rate : 4.1635481624156887e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5161607845604175
    Epoch 3, Loss: 1.7718841097525386, fit: 0.1
      exponential decayed learning_rate :              9.565287391030293e-05
      batch rate adapted learning_rate :              9.565287391030293e-08
      fit adapted learning_rate :              4.11754257606499e-08
    epoch : 4 ; learning_rate : 4.11754257606499e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4841278641561564
    Epoch 4, Loss: 1.7718391148823938, fit: 0.03333333333333333
      exponential decayed learning_rate :              9.459594689067654e-05
      batch rate adapted learning_rate :              9.459594689067654e-08
      fit adapted learning_rate :              7.212510762492029e-08
    epoch : 5 ; learning_rate : 7.212510762492029e-08 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.5170879353729236
    Epoch 5, Loss: 1.771775988727615, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.355069850316179e-05
      batch rate adapted learning_rate :              9.35506985031618e-08
      fit adapted learning_rate :              4.663785280735795e-08
    epoch : 6 ; learning_rate : 4.663785280735795e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5117547884860703
    Epoch 6, Loss: 1.7717111184059222, fit: 0.03333333333333333
      exponential decayed learning_rate :              9.25169997034202e-05
      batch rate adapted learning_rate :              9.251699970342021e-08
      fit adapted learning_rate :              7.054000493758552e-08
    epoch : 7 ; learning_rate : 7.054000493758552e-08 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.5790181029795844
    Epoch 7, Loss: 1.7716468173837328, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.14947228730031e-05
      batch rate adapted learning_rate :              9.149472287300311e-08
      fit adapted learning_rate :              3.391509202309601e-08
    epoch : 8 ; learning_rate : 3.391509202309601e-08 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5078567985525266
    Epoch 8, Loss: 1.7715900136305083, fit: 0.06666666666666667
      Weights saved to src/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.821069827475121_fit_0.066666666666666672024-01-02173145
  self.fit : 0.06666666666666667
  self.loss : 1.821069827475121
  current_accuracy : 0.08
  LR: 0.0001, Epochs: 100
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          89
    learning_rate :          0.0001
    batch_rate :          0.001
      exponential decayed learning_rate :              0.0001
      batch rate adapted learning_rate :              1.0000000000000001e-07
      fit adapted learning_rate :              5.7582990144185347e-08
    epoch : 0 ; learning_rate : 5.7582990144185347e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.551472578384048
    Epoch 0, Loss: 1.771539486988067, fit: 0.08333333333333333
    Epoch 0, Loss: 1.771539486988067
      exponential decayed learning_rate :              9.988770354914616e-05
      batch rate adapted learning_rate :              9.988770354914616e-08
      fit adapted learning_rate :              4.9797041496516846e-08
    epoch : 1 ; learning_rate : 4.9797041496516846e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5319975208858252
    Epoch 1, Loss: 1.7714825120677018, fit: 0.05
      exponential decayed learning_rate :              9.977553320322105e-05
      batch rate adapted learning_rate :              9.977553320322105e-08
      fit adapted learning_rate :              6.619312726977706e-08
    epoch : 2 ; learning_rate : 6.619312726977706e-08 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5031011608131808
    Epoch 2, Loss: 1.7714154354135954, fit: 0.18333333333333332
      exponential decayed learning_rate :              9.966348882061334e-05
      batch rate adapted learning_rate :              9.966348882061334e-08
      fit adapted learning_rate :              1.971944661339049e-08
    epoch : 3 ; learning_rate : 1.971944661339049e-08 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.425053332922083
    Epoch 3, Loss: 1.7713705253848437, fit: 0.016666666666666666
      exponential decayed learning_rate :              9.955157025987067e-05
      batch rate adapted learning_rate :              9.955157025987067e-08
      fit adapted learning_rate :              8.702703847459248e-08
    epoch : 4 ; learning_rate : 8.702703847459248e-08 ; fit : 0.016666666666666666
    batch_size :          60
    best_batch_loss : 1.494950844560127
    Epoch 4, Loss: 1.7713145987996044, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.943977737969955e-05
      batch rate adapted learning_rate :              9.943977737969955e-08
      fit adapted learning_rate :              5.7260397207952235e-08
    epoch : 5 ; learning_rate : 5.7260397207952235e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4781075287166323
    Epoch 5, Loss: 1.7712330748804168, fit: 0.05
      exponential decayed learning_rate :              9.932811003896519e-05
      batch rate adapted learning_rate :              9.932811003896519e-08
      fit adapted learning_rate :              6.589629760117772e-08
    epoch : 6 ; learning_rate : 6.589629760117772e-08 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5459463154333577
    Epoch 6, Loss: 1.7711672305489472, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.921656809669122e-05
      batch rate adapted learning_rate :              9.921656809669122e-08
      fit adapted learning_rate :              5.713186662851665e-08
    epoch : 7 ; learning_rate : 5.713186662851665e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4991024112659448
    Epoch 7, Loss: 1.7711024573926921, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.910515141205966e-05
      batch rate adapted learning_rate :              9.910515141205966e-08
      fit adapted learning_rate :              3.673611137954086e-08
    epoch : 8 ; learning_rate : 3.673611137954086e-08 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5037044423390629
    Epoch 8, Loss: 1.7710465556429007, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.899385984441058e-05
      batch rate adapted learning_rate :              9.899385984441058e-08
      fit adapted learning_rate :              5.70036245575556e-08
    epoch : 9 ; learning_rate : 5.70036245575556e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5544116223039646
    Epoch 9, Loss: 1.7709974280278182, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.888269325324206e-05
      batch rate adapted learning_rate :              9.888269325324207e-08
      fit adapted learning_rate :              5.693961151031941e-08
    epoch : 10 ; learning_rate : 5.693961151031941e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5506053876360995
    Epoch 10, Loss: 1.770933790407297, fit: 0.03333333333333333
    Epoch 10, Loss: 1.770933790407297
      exponential decayed learning_rate :              9.877165149820999e-05
      batch rate adapted learning_rate :              9.877165149821e-08
      fit adapted learning_rate :              7.530889249232363e-08
    epoch : 11 ; learning_rate : 7.530889249232363e-08 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4632635257817697
    Epoch 11, Loss: 1.770865669689217, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.866073443912775e-05
      batch rate adapted learning_rate :              9.866073443912775e-08
      fit adapted learning_rate :              5.681180098826381e-08
    epoch : 12 ; learning_rate : 5.681180098826381e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5072497537079153
    Epoch 12, Loss: 1.7707929260962905, fit: 0.05
      exponential decayed learning_rate :              9.854994193596629e-05
      batch rate adapted learning_rate :              9.854994193596629e-08
      fit adapted learning_rate :              6.53800449826708e-08
    epoch : 13 ; learning_rate : 6.53800449826708e-08 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4679285685470211
    Epoch 13, Loss: 1.7707258865539355, fit: 0.13333333333333333
      exponential decayed learning_rate :              9.843927384885367e-05
      batch rate adapted learning_rate :              9.843927384885367e-08
      fit adapted learning_rate :              3.133178569859643e-08
    epoch : 14 ; learning_rate : 3.133178569859643e-08 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4276320517979504
    Epoch 14, Loss: 1.7706720522575838, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.83287300380751e-05
      batch rate adapted learning_rate :              9.83287300380751e-08
      fit adapted learning_rate :              5.66206229267274e-08
    epoch : 15 ; learning_rate : 5.66206229267274e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.49730847691638
    Epoch 15, Loss: 1.7706254432598645, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.821831036407267e-05
      batch rate adapted learning_rate :              9.821831036407268e-08
      fit adapted learning_rate :              4.896479850005828e-08
    epoch : 16 ; learning_rate : 4.896479850005828e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5292041320248988
    Epoch 16, Loss: 1.7705665608101464, fit: 0.15
      exponential decayed learning_rate :              9.81080146874452e-05
      batch rate adapted learning_rate :              9.810801468744521e-08
      fit adapted learning_rate :              2.6733504432721995e-08
    epoch : 17 ; learning_rate : 2.6733504432721995e-08 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.5037989362807849
    Epoch 17, Loss: 1.770524040763028, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.799784286894803e-05
      batch rate adapted learning_rate :              9.799784286894804e-08
      fit adapted learning_rate :              4.885488878531593e-08
    epoch : 18 ; learning_rate : 4.885488878531593e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4768828719312816
    Epoch 18, Loss: 1.7704837525020363, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.788779476949289e-05
      batch rate adapted learning_rate :              9.78877947694929e-08
      fit adapted learning_rate :              5.636671921447747e-08
    epoch : 19 ; learning_rate : 5.636671921447747e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4474679843169382
    Epoch 19, Loss: 1.7704247974151406, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.777787025014764e-05
      batch rate adapted learning_rate :              9.777787025014764e-08
      fit adapted learning_rate :              5.630342138933685e-08
    epoch : 20 ; learning_rate : 5.630342138933685e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5263311722532271
    Epoch 20, Loss: 1.7703636125978215, fit: 0.2
    Epoch 20, Loss: 1.7703636125978215
      exponential decayed learning_rate :              9.766806917213624e-05
      batch rate adapted learning_rate :              9.766806917213624e-08
      fit adapted learning_rate :              1.6385982928038718e-08
    epoch : 21 ; learning_rate : 1.6385982928038718e-08 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.5283192509129928
    Epoch 21, Loss: 1.7703253189381147, fit: 0.05
      exponential decayed learning_rate :              9.755839139683846e-05
      batch rate adapted learning_rate :              9.755839139683846e-08
      fit adapted learning_rate :              6.472223009635772e-08
    epoch : 22 ; learning_rate : 6.472223009635772e-08 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4915375016780978
    Epoch 22, Loss: 1.7702824181090642, fit: 0.03333333333333333
      exponential decayed learning_rate :              9.74488367857897e-05
      batch rate adapted learning_rate :              9.74488367857897e-08
      fit adapted learning_rate :              7.430030643089962e-08
    epoch : 23 ; learning_rate : 7.430030643089962e-08 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.5359772947712287
    Epoch 23, Loss: 1.7702040794251679, fit: 0.1
      exponential decayed learning_rate :              9.73394052006809e-05
      batch rate adapted learning_rate :              9.73394052006809e-08
      fit adapted learning_rate :              4.19014221797966e-08
    epoch : 24 ; learning_rate : 4.19014221797966e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.549587709204456
    Epoch 24, Loss: 1.7701398150921435, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.723009650335829e-05
      batch rate adapted learning_rate :              9.723009650335828e-08
      fit adapted learning_rate :              5.59879968867107e-08
    epoch : 25 ; learning_rate : 5.59879968867107e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.498306158144366
    Epoch 25, Loss: 1.770085085775162, fit: 0.1
      exponential decayed learning_rate :              9.712091055582324e-05
      batch rate adapted learning_rate :              9.712091055582324e-08
      fit adapted learning_rate :              4.180736739962479e-08
    epoch : 26 ; learning_rate : 4.180736739962479e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5600371365071062
    Epoch 26, Loss: 1.770031764989491, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.70118472202321e-05
      batch rate adapted learning_rate :              9.701184722023211e-08
      fit adapted learning_rate :              5.58623224235184e-08
    epoch : 27 ; learning_rate : 5.58623224235184e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4736535094228216
    Epoch 27, Loss: 1.7699787538580545, fit: 0.05
      exponential decayed learning_rate :              9.690290635889605e-05
      batch rate adapted learning_rate :              9.690290635889605e-08
      fit adapted learning_rate :              6.428736792978243e-08
    epoch : 28 ; learning_rate : 6.428736792978243e-08 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5074240732179591
    Epoch 28, Loss: 1.7699141065312678, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.679408783428076e-05
      batch rate adapted learning_rate :              9.679408783428077e-08
      fit adapted learning_rate :              4.825478049087032e-08
    epoch : 29 ; learning_rate : 4.825478049087032e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5343377116793735
    Epoch 29, Loss: 1.7698502706056585, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.668539150900652e-05
      batch rate adapted learning_rate :              9.668539150900651e-08
      fit adapted learning_rate :              5.5674339463498234e-08
    epoch : 30 ; learning_rate : 5.5674339463498234e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5350067398620397
    Epoch 30, Loss: 1.7697922890963678, fit: 0.11666666666666667
    Epoch 30, Loss: 1.7697922890963678
      exponential decayed learning_rate :              9.657681724584776e-05
      batch rate adapted learning_rate :              9.657681724584776e-08
      fit adapted learning_rate :              3.579891321969468e-08
    epoch : 31 ; learning_rate : 3.579891321969468e-08 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.534897156838903
    Epoch 31, Loss: 1.7697435410411149, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.646836490773307e-05
      batch rate adapted learning_rate :              9.646836490773307e-08
      fit adapted learning_rate :              4.8092397759930044e-08
    epoch : 32 ; learning_rate : 4.8092397759930044e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4809241366291248
    Epoch 32, Loss: 1.7696952414873872, fit: 0.13333333333333333
      exponential decayed learning_rate :              9.636003435774494e-05
      batch rate adapted learning_rate :              9.636003435774494e-08
      fit adapted learning_rate :              3.0669994082259394e-08
    epoch : 33 ; learning_rate : 3.0669994082259394e-08 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.5358108395439738
    Epoch 33, Loss: 1.7696523658244798, fit: 0.1
      exponential decayed learning_rate :              9.625182545911963e-05
      batch rate adapted learning_rate :              9.625182545911963e-08
      fit adapted learning_rate :              4.143325476279421e-08
    epoch : 34 ; learning_rate : 4.143325476279421e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5181653159823394
    Epoch 34, Loss: 1.7696142853329717, fit: 0.1
      exponential decayed learning_rate :              9.614373807524702e-05
      batch rate adapted learning_rate :              9.614373807524701e-08
      fit adapted learning_rate :              4.138672668822236e-08
    epoch : 35 ; learning_rate : 4.138672668822236e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.457866634075908
    Epoch 35, Loss: 1.7695680760296313, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.603577206967029e-05
      batch rate adapted learning_rate :              9.60357720696703e-08
      fit adapted learning_rate :              5.5300269165770544e-08
    epoch : 36 ; learning_rate : 5.5300269165770544e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5425413651682838
    Epoch 36, Loss: 1.7695130317132404, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.592792730608597e-05
      batch rate adapted learning_rate :              9.592792730608597e-08
      fit adapted learning_rate :              3.555838391561199e-08
    epoch : 37 ; learning_rate : 3.555838391561199e-08 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.524657200462063
    Epoch 37, Loss: 1.7694636723571424, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.582020364834356e-05
      batch rate adapted learning_rate :              9.582020364834357e-08
      fit adapted learning_rate :              3.551845311249377e-08
    epoch : 38 ; learning_rate : 3.551845311249377e-08 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.48774431761393
    Epoch 38, Loss: 1.769422625782301, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.571260096044555e-05
      batch rate adapted learning_rate :              9.571260096044555e-08
      fit adapted learning_rate :              4.771562657281259e-08
    epoch : 39 ; learning_rate : 4.771562657281259e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5428173939611944
    Epoch 39, Loss: 1.7693798003016108, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.560511910654706e-05
      batch rate adapted learning_rate :              9.560511910654705e-08
      fit adapted learning_rate :              5.505228631245965e-08
    epoch : 40 ; learning_rate : 5.505228631245965e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4915657018375592
    Epoch 40, Loss: 1.7693193087682355, fit: 0.1
    Epoch 40, Loss: 1.7693193087682355
      exponential decayed learning_rate :              9.549775795095581e-05
      batch rate adapted learning_rate :              9.549775795095581e-08
      fit adapted learning_rate :              4.1108653426403274e-08
    epoch : 41 ; learning_rate : 4.1108653426403274e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4840982491438155
    Epoch 41, Loss: 1.7692674813798366, fit: 0.1
      exponential decayed learning_rate :              9.539051735813189e-05
      batch rate adapted learning_rate :              9.539051735813189e-08
      fit adapted learning_rate :              4.1062489867611614e-08
    epoch : 42 ; learning_rate : 4.1062489867611614e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4234469481704877
    Epoch 42, Loss: 1.7692224401319976, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.528339719268758e-05
      batch rate adapted learning_rate :              9.528339719268759e-08
      fit adapted learning_rate :              4.750165551257104e-08
    epoch : 43 ; learning_rate : 4.750165551257104e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5403412914969679
    Epoch 43, Loss: 1.7691684074090173, fit: 0.13333333333333333
      exponential decayed learning_rate :              9.517639731938723e-05
      batch rate adapted learning_rate :              9.517639731938723e-08
      fit adapted learning_rate :              3.0293259669450874e-08
    epoch : 44 ; learning_rate : 3.0293259669450874e-08 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4968336216912836
    Epoch 44, Loss: 1.769128996989172, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.5069517603147e-05
      batch rate adapted learning_rate :              9.5069517603147e-08
      fit adapted learning_rate :              4.7395030068025e-08
    epoch : 45 ; learning_rate : 4.7395030068025e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4172283931853367
    Epoch 45, Loss: 1.7690838459619118, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.49627579090348e-05
      batch rate adapted learning_rate :              9.49627579090348e-08
      fit adapted learning_rate :              4.73418071313775e-08
    epoch : 46 ; learning_rate : 4.73418071313775e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4757013353558113
    Epoch 46, Loss: 1.7690315233594502, fit: 0.1
      exponential decayed learning_rate :              9.485611810227e-05
      batch rate adapted learning_rate :              9.485611810227001e-08
      fit adapted learning_rate :              4.083244851091467e-08
    epoch : 47 ; learning_rate : 4.083244851091467e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.509538846709942
    Epoch 47, Loss: 1.7689836607065932, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.474959804822344e-05
      batch rate adapted learning_rate :              9.474959804822343e-08
      fit adapted learning_rate :              4.7235540493372425e-08
    epoch : 48 ; learning_rate : 4.7235540493372425e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4544066588236955
    Epoch 48, Loss: 1.7689338943737405, fit: 0.03333333333333333
      exponential decayed learning_rate :              9.464319761241698e-05
      batch rate adapted learning_rate :              9.464319761241699e-08
      fit adapted learning_rate :              7.216113415145661e-08
    epoch : 49 ; learning_rate : 7.216113415145661e-08 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.5291501013171456
    Epoch 49, Loss: 1.7688671605820099, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.453691666052364e-05
      batch rate adapted learning_rate :              9.453691666052365e-08
      fit adapted learning_rate :              5.4437183403246044e-08
    epoch : 50 ; learning_rate : 5.4437183403246044e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4370037480491153
    Epoch 50, Loss: 1.7687954866406164, fit: 0.06666666666666667
    Epoch 50, Loss: 1.7687954866406164
      exponential decayed learning_rate :              9.443075505836723e-05
      batch rate adapted learning_rate :              9.443075505836723e-08
      fit adapted learning_rate :              5.43760523783394e-08
    epoch : 51 ; learning_rate : 5.43760523783394e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5193823665604638
    Epoch 51, Loss: 1.768730404412647, fit: 0.13333333333333333
      exponential decayed learning_rate :              9.43247126719222e-05
      batch rate adapted learning_rate :              9.432471267192219e-08
      fit adapted learning_rate :              3.0022180863057686e-08
    epoch : 52 ; learning_rate : 3.0022180863057686e-08 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.5455685070442589
    Epoch 52, Loss: 1.7686876402057574, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.421878936731354e-05
      batch rate adapted learning_rate :              9.421878936731354e-08
      fit adapted learning_rate :              3.492484387468428e-08
    epoch : 53 ; learning_rate : 3.492484387468428e-08 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4817277531564872
    Epoch 53, Loss: 1.7686520302530935, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.411298501081657e-05
      batch rate adapted learning_rate :              9.411298501081657e-08
      fit adapted learning_rate :              5.419307088317714e-08
    epoch : 54 ; learning_rate : 5.419307088317714e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4072656332596158
    Epoch 54, Loss: 1.7685998063772703, fit: 0.1
      exponential decayed learning_rate :              9.400729946885681e-05
      batch rate adapted learning_rate :              9.400729946885681e-08
      fit adapted learning_rate :              4.046705992199328e-08
    epoch : 55 ; learning_rate : 4.046705992199328e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5038482854014883
    Epoch 55, Loss: 1.7685480221678125, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.390173260800975e-05
      batch rate adapted learning_rate :              9.390173260800975e-08
      fit adapted learning_rate :              3.4807317870662626e-08
    epoch : 56 ; learning_rate : 3.4807317870662626e-08 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4960607426361685
    Epoch 56, Loss: 1.7685041515555413, fit: 0.03333333333333333
      exponential decayed learning_rate :              9.379628429500068e-05
      batch rate adapted learning_rate :              9.379628429500068e-08
      fit adapted learning_rate :              7.151540126146056e-08
    epoch : 57 ; learning_rate : 7.151540126146056e-08 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4873312813339035
    Epoch 57, Loss: 1.7684434873801298, fit: 0.05
      exponential decayed learning_rate :              9.36909543967046e-05
      batch rate adapted learning_rate :              9.36909543967046e-08
      fit adapted learning_rate :              6.215649337374563e-08
    epoch : 58 ; learning_rate : 6.215649337374563e-08 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4965349697244146
    Epoch 58, Loss: 1.7683697120913229, fit: 0.03333333333333333
      exponential decayed learning_rate :              9.3585742780146e-05
      batch rate adapted learning_rate :              9.3585742780146e-08
      fit adapted learning_rate :              7.135487293104533e-08
    epoch : 59 ; learning_rate : 7.135487293104533e-08 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.513554458299318
    Epoch 59, Loss: 1.7682925696185545, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.348064931249869e-05
      batch rate adapted learning_rate :              9.348064931249868e-08
      fit adapted learning_rate :              4.6602931167052816e-08
    epoch : 60 ; learning_rate : 4.6602931167052816e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4756225513719872
    Epoch 60, Loss: 1.7682251278051133, fit: 0.1
    Epoch 60, Loss: 1.7682251278051133
      exponential decayed learning_rate :              9.337567386108562e-05
      batch rate adapted learning_rate :              9.337567386108561e-08
      fit adapted learning_rate :              4.019516580885146e-08
    epoch : 61 ; learning_rate : 4.019516580885146e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5042195929662932
    Epoch 61, Loss: 1.7681779828760589, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.327081629337876e-05
      batch rate adapted learning_rate :              9.327081629337876e-08
      fit adapted learning_rate :              4.649832306025704e-08
    epoch : 62 ; learning_rate : 4.649832306025704e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5471408080918365
    Epoch 62, Loss: 1.768129232610044, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.316607647699888e-05
      batch rate adapted learning_rate :              9.316607647699889e-08
      fit adapted learning_rate :              5.364781263547445e-08
    epoch : 63 ; learning_rate : 5.364781263547445e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5320987334935834
    Epoch 63, Loss: 1.7680689009563486, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.306145427971543e-05
      batch rate adapted learning_rate :              9.306145427971543e-08
      fit adapted learning_rate :              3.449584507819709e-08
    epoch : 64 ; learning_rate : 3.449584507819709e-08 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4638418638310848
    Epoch 64, Loss: 1.7680207857326218, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.295694956944634e-05
      batch rate adapted learning_rate :              9.295694956944634e-08
      fit adapted learning_rate :              4.634185100493195e-08
    epoch : 65 ; learning_rate : 4.634185100493195e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4954319354490886
    Epoch 65, Loss: 1.7679719950754509, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.285256221425784e-05
      batch rate adapted learning_rate :              9.285256221425785e-08
      fit adapted learning_rate :              4.6289810750993425e-08
    epoch : 66 ; learning_rate : 4.6289810750993425e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.501997308691188
    Epoch 66, Loss: 1.7679216217810738, fit: 0.1
      exponential decayed learning_rate :              9.274829208236437e-05
      batch rate adapted learning_rate :              9.274829208236438e-08
      fit adapted learning_rate :              3.9925098524960494e-08
    epoch : 67 ; learning_rate : 3.9925098524960494e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4836838719958036
    Epoch 67, Loss: 1.7678709798704477, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.264413904212832e-05
      batch rate adapted learning_rate :              9.264413904212832e-08
      fit adapted learning_rate :              5.334726545379411e-08
    epoch : 68 ; learning_rate : 5.334726545379411e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5047312118721468
    Epoch 68, Loss: 1.7678179711876847, fit: 0.05
      exponential decayed learning_rate :              9.25401029620599e-05
      batch rate adapted learning_rate :              9.254010296205991e-08
      fit adapted learning_rate :              6.139299501862401e-08
    epoch : 69 ; learning_rate : 6.139299501862401e-08 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5379463783845608
    Epoch 69, Loss: 1.7677540794871114, fit: 0.05
      exponential decayed learning_rate :              9.243618371081702e-05
      batch rate adapted learning_rate :              9.243618371081702e-08
      fit adapted learning_rate :              6.132405286414521e-08
    epoch : 70 ; learning_rate : 6.132405286414521e-08 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5321668364302299
    Epoch 70, Loss: 1.7676800901122531, fit: 0.08333333333333333
    Epoch 70, Loss: 1.7676800901122531
      exponential decayed learning_rate :              9.233238115720502e-05
      batch rate adapted learning_rate :              9.233238115720503e-08
      fit adapted learning_rate :              4.6030484760272095e-08
    epoch : 71 ; learning_rate : 4.6030484760272095e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5188057054838353
    Epoch 71, Loss: 1.7676177343476431, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.222869517017664e-05
      batch rate adapted learning_rate :              9.222869517017665e-08
      fit adapted learning_rate :              5.3108040449953564e-08
    epoch : 72 ; learning_rate : 5.3108040449953564e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4650605346715144
    Epoch 72, Loss: 1.7675606093424274, fit: 0.05
      exponential decayed learning_rate :              9.21251256188317e-05
      batch rate adapted learning_rate :              9.212512561883171e-08
      fit adapted learning_rate :              6.111769057060437e-08
    epoch : 73 ; learning_rate : 6.111769057060437e-08 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5513567581868015
    Epoch 73, Loss: 1.7674952437381524, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.202167237241712e-05
      batch rate adapted learning_rate :              9.202167237241712e-08
      fit adapted learning_rate :              4.58755870331279e-08
    epoch : 74 ; learning_rate : 4.58755870331279e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5416338918967376
    Epoch 74, Loss: 1.7674344405368088, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.191833530032654e-05
      batch rate adapted learning_rate :              9.191833530032654e-08
      fit adapted learning_rate :              5.292932595668627e-08
    epoch : 75 ; learning_rate : 5.292932595668627e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.496670723377753
    Epoch 75, Loss: 1.7673754006430589, fit: 0.05
      exponential decayed learning_rate :              9.181511427210033e-05
      batch rate adapted learning_rate :              9.181511427210033e-08
      fit adapted learning_rate :              6.091202270925134e-08
    epoch : 76 ; learning_rate : 6.091202270925134e-08 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.473084566050394
    Epoch 76, Loss: 1.7673130987383372, fit: 0.23333333333333334
      exponential decayed learning_rate :              9.171200915742537e-05
      batch rate adapted learning_rate :              9.171200915742536e-08
      fit adapted learning_rate :              1.0946590152748163e-08
    epoch : 77 ; learning_rate : 1.0946590152748163e-08 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.5372451771582643
    Epoch 77, Loss: 1.7672685874412666, fit: 0.16666666666666666
      exponential decayed learning_rate :              9.160901982613482e-05
      batch rate adapted learning_rate :              9.160901982613482e-08
      fit adapted learning_rate :              2.1305330128781775e-08
    epoch : 78 ; learning_rate : 2.1305330128781775e-08 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.525859264331655
    Epoch 78, Loss: 1.7672495183839672, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.150614614820807e-05
      batch rate adapted learning_rate :              9.150614614820807e-08
      fit adapted learning_rate :              3.3919326381293025e-08
    epoch : 79 ; learning_rate : 3.3919326381293025e-08 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5132209432750756
    Epoch 79, Loss: 1.7672170612283535, fit: 0.05
      exponential decayed learning_rate :              9.14033879937705e-05
      batch rate adapted learning_rate :              9.14033879937705e-08
      fit adapted learning_rate :              6.063887508410872e-08
    epoch : 80 ; learning_rate : 6.063887508410872e-08 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.515150880823669
    Epoch 80, Loss: 1.767160745609576, fit: 0.16666666666666666
    Epoch 80, Loss: 1.767160745609576
      exponential decayed learning_rate :              9.130074523309332e-05
      batch rate adapted learning_rate :              9.130074523309332e-08
      fit adapted learning_rate :              2.1233635311093185e-08
    epoch : 81 ; learning_rate : 2.1233635311093185e-08 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.406867928201169
    Epoch 81, Loss: 1.767113491375404, fit: 0.18333333333333332
      exponential decayed learning_rate :              9.119821773659343e-05
      batch rate adapted learning_rate :              9.119821773659343e-08
      fit adapted learning_rate :              1.8044505637666965e-08
    epoch : 82 ; learning_rate : 1.8044505637666965e-08 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.5118505103738002
    Epoch 82, Loss: 1.767092987736301, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.109580537483329e-05
      batch rate adapted learning_rate :              9.10958053748333e-08
      fit adapted learning_rate :              5.245568863075652e-08
    epoch : 83 ; learning_rate : 5.245568863075652e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4403744271999932
    Epoch 83, Loss: 1.767050781666045, fit: 0.1
      exponential decayed learning_rate :              9.099350801852062e-05
      batch rate adapted learning_rate :              9.099350801852062e-08
      fit adapted learning_rate :              3.9169721524845205e-08
    epoch : 84 ; learning_rate : 3.9169721524845205e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.544919230974262
    Epoch 84, Loss: 1.7669973307231572, fit: 0.15
      exponential decayed learning_rate :              9.08913255385084e-05
      batch rate adapted learning_rate :              9.08913255385084e-08
      fit adapted learning_rate :              2.47670250174845e-08
    epoch : 85 ; learning_rate : 2.47670250174845e-08 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.538018046546775
    Epoch 85, Loss: 1.766960344720115, fit: 0.016666666666666666
      exponential decayed learning_rate :              9.078925780579464e-05
      batch rate adapted learning_rate :              9.078925780579464e-08
      fit adapted learning_rate :              7.936710803776779e-08
    epoch : 86 ; learning_rate : 7.936710803776779e-08 ; fit : 0.016666666666666666
    batch_size :          60
    best_batch_loss : 1.5055042998334187
    Epoch 86, Loss: 1.7669011018600587, fit: 0.13333333333333333
      exponential decayed learning_rate :              9.06873046915222e-05
      batch rate adapted learning_rate :              9.06873046915222e-08
      fit adapted learning_rate :              2.8864446933455113e-08
    epoch : 87 ; learning_rate : 2.8864446933455113e-08 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.5320051828132628
    Epoch 87, Loss: 1.7668338271088522, fit: 0.03333333333333333
      exponential decayed learning_rate :              9.058546606697859e-05
      batch rate adapted learning_rate :              9.05854660669786e-08
      fit adapted learning_rate :              6.906729837891545e-08
    epoch : 88 ; learning_rate : 6.906729837891545e-08 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.470713132022211
    Epoch 88, Loss: 1.7667778870173438, fit: 0.11666666666666667
      Weights saved to src/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7178772999280274_fit_0.116666666666666672024-01-02174204
  self.fit : 0.11666666666666667
  self.loss : 1.7178772999280274
  current_accuracy : 0.0822
  LR: 0.0001, Epochs: 1, Hidden Size: 784, Samples: 1, Accuracy mean: 0.0799
  LR: 0.0001, Epochs: 10, Hidden Size: 784, Samples: 1, Accuracy mean: 0.08
  LR: 0.0001, Epochs: 100, Hidden Size: 784, Samples: 1, Accuracy mean: 0.0822
  LR: 1e-07
  LR: 1e-07, Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0, 0.0]
  LR: 1e-07, Hidden Size: 784, Sample: 1/1
  LR: 1e-07, Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.001
      exponential decayed learning_rate :              1e-07
      batch rate adapted learning_rate :              1e-10
      fit adapted learning_rate :              1e-10
    epoch : 0 ; learning_rate : 1e-10 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5109193431622752
    Epoch 0, Loss: 1.7383924416256076, fit: 0.1
    Epoch 0, Loss: 1.7383924416256076
      Weights saved to src/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7731907420479134_fit_0.12024-01-02174211
  self.fit : 0.1
  self.loss : 1.7731907420479134
  current_accuracy : 0.1041
  LR: 1e-07, Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.001
      exponential decayed learning_rate :              1e-07
      batch rate adapted learning_rate :              1e-10
      fit adapted learning_rate :              4.304672100000001e-11
    epoch : 0 ; learning_rate : 4.304672100000001e-11 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4532084020926905
    Epoch 0, Loss: 1.738392309026006, fit: 0.1
    Epoch 0, Loss: 1.738392309026006
      exponential decayed learning_rate :              9.889503892939223e-08
      batch rate adapted learning_rate :              9.889503892939223e-11
      fit adapted learning_rate :              4.257107149077687e-11
    epoch : 1 ; learning_rate : 4.257107149077687e-11 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4707000137876567
    Epoch 1, Loss: 1.7383922261193023, fit: 0.03333333333333333
      exponential decayed learning_rate :              9.780228724846004e-08
      batch rate adapted learning_rate :              9.780228724846004e-11
      fit adapted learning_rate :              7.45697963350457e-11
    epoch : 2 ; learning_rate : 7.45697963350457e-11 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.505888172663506
    Epoch 2, Loss: 1.738392109091157, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.672161004820058e-08
      batch rate adapted learning_rate :              9.672161004820058e-11
      fit adapted learning_rate :              5.5695195181352716e-11
    epoch : 3 ; learning_rate : 5.5695195181352716e-11 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4508272436432679
    Epoch 3, Loss: 1.7383919782287929, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.565287391030293e-08
      batch rate adapted learning_rate :              9.565287391030293e-11
      fit adapted learning_rate :              3.54564276394867e-11
    epoch : 4 ; learning_rate : 3.54564276394867e-11 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4940781310660616
    Epoch 4, Loss: 1.7383918921333923, fit: 0.1
      exponential decayed learning_rate :              9.459594689067654e-08
      batch rate adapted learning_rate :              9.459594689067655e-11
      fit adapted learning_rate :              4.072045333533772e-11
    epoch : 5 ; learning_rate : 4.072045333533772e-11 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4993094059285357
    Epoch 5, Loss: 1.7383918114440446, fit: 0.15
      exponential decayed learning_rate :              9.355069850316177e-08
      batch rate adapted learning_rate :              9.355069850316177e-11
      fit adapted learning_rate :              2.5491678952897587e-11
    epoch : 6 ; learning_rate : 2.5491678952897587e-11 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.432680424291546
    Epoch 6, Loss: 1.7383917510524187, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.25169997034202e-08
      batch rate adapted learning_rate :              9.25169997034202e-11
      fit adapted learning_rate :              5.327405482091644e-11
    epoch : 7 ; learning_rate : 5.327405482091644e-11 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4466658822544027
    Epoch 7, Loss: 1.7383916721226684, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.14947228730031e-08
      batch rate adapted learning_rate :              9.14947228730031e-11
      fit adapted learning_rate :              5.268539725441106e-11
    epoch : 8 ; learning_rate : 5.268539725441106e-11 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4311800424318037
    Epoch 8, Loss: 1.7383915744994398, fit: 0.06666666666666667
      Weights saved to src/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.826582025795543_fit_0.066666666666666672024-01-02174316
  self.fit : 0.06666666666666667
  self.loss : 1.826582025795543
  current_accuracy : 0.1041
  LR: 1e-07, Epochs: 100
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          89
    learning_rate :          1e-07
    batch_rate :          0.001
      exponential decayed learning_rate :              1e-07
      batch rate adapted learning_rate :              1e-10
      fit adapted learning_rate :              5.7582990144185346e-11
    epoch : 0 ; learning_rate : 5.7582990144185346e-11 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4667076918247508
    Epoch 0, Loss: 1.7383914582843807, fit: 0.1
    Epoch 0, Loss: 1.7383914582843807
      exponential decayed learning_rate :              9.988770354914615e-08
      batch rate adapted learning_rate :              9.988770354914615e-11
      fit adapted learning_rate :              4.299838106010805e-11
    epoch : 1 ; learning_rate : 4.299838106010805e-11 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5016240875717122
    Epoch 1, Loss: 1.7383913646062346, fit: 0.03333333333333333
      exponential decayed learning_rate :              9.977553320322104e-08
      batch rate adapted learning_rate :              9.977553320322104e-11
      fit adapted learning_rate :              7.607430663950995e-11
    epoch : 2 ; learning_rate : 7.607430663950995e-11 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4753294238495551
    Epoch 2, Loss: 1.7383912385148408, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.966348882061334e-08
      batch rate adapted learning_rate :              9.966348882061333e-11
      fit adapted learning_rate :              4.9685263672578925e-11
    epoch : 3 ; learning_rate : 4.9685263672578925e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4178067705613286
    Epoch 3, Loss: 1.738391124049494, fit: 0.05
      exponential decayed learning_rate :              9.955157025987065e-08
      batch rate adapted learning_rate :              9.955157025987065e-11
      fit adapted learning_rate :              6.604454567730677e-11
    epoch : 4 ; learning_rate : 6.604454567730677e-11 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.482976030134608
    Epoch 4, Loss: 1.7383910078900788, fit: 0.18333333333333332
      exponential decayed learning_rate :              9.943977737969955e-08
      batch rate adapted learning_rate :              9.943977737969954e-11
      fit adapted learning_rate :              1.9675183003234872e-11
    epoch : 5 ; learning_rate : 1.9675183003234872e-11 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.4254533824521682
    Epoch 5, Loss: 1.7383909227104766, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.932811003896517e-08
      batch rate adapted learning_rate :              9.932811003896517e-11
      fit adapted learning_rate :              5.7196095814142884e-11
    epoch : 6 ; learning_rate : 5.7196095814142884e-11 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4792365315544904
    Epoch 6, Loss: 1.7383908429843904, fit: 0.13333333333333333
      exponential decayed learning_rate :              9.921656809669121e-08
      batch rate adapted learning_rate :              9.921656809669121e-11
      fit adapted learning_rate :              3.1579187125341933e-11
    epoch : 7 ; learning_rate : 3.1579187125341933e-11 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4052806540346465
    Epoch 7, Loss: 1.7383907630048379, fit: 0.18333333333333332
      exponential decayed learning_rate :              9.910515141205964e-08
      batch rate adapted learning_rate :              9.910515141205964e-11
      fit adapted learning_rate :              1.9608973812864203e-11
    epoch : 8 ; learning_rate : 1.9608973812864203e-11 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.4690491521763926
    Epoch 8, Loss: 1.7383907095183435, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.899385984441057e-08
      batch rate adapted learning_rate :              9.899385984441057e-11
      fit adapted learning_rate :              3.6694858030279914e-11
    epoch : 9 ; learning_rate : 3.6694858030279914e-11 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4785498965688666
    Epoch 9, Loss: 1.73839065749075, fit: 0.2
      exponential decayed learning_rate :              9.888269325324206e-08
      batch rate adapted learning_rate :              9.888269325324206e-11
      fit adapted learning_rate :              1.6589763033713856e-11
    epoch : 10 ; learning_rate : 1.6589763033713856e-11 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.495253155440205
    Epoch 10, Loss: 1.7383906021869033, fit: 0.11666666666666667
    Epoch 10, Loss: 1.7383906021869033
      exponential decayed learning_rate :              9.877165149820997e-08
      batch rate adapted learning_rate :              9.877165149820997e-11
      fit adapted learning_rate :              3.6612490257876754e-11
    epoch : 11 ; learning_rate : 3.6612490257876754e-11 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4591716070675569
    Epoch 11, Loss: 1.7383905536983026, fit: 0.05
      exponential decayed learning_rate :              9.866073443912774e-08
      batch rate adapted learning_rate :              9.866073443912775e-11
      fit adapted learning_rate :              6.545354699290177e-11
    epoch : 12 ; learning_rate : 6.545354699290177e-11 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4431096578469178
    Epoch 12, Loss: 1.7383904546945295, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.854994193596627e-08
      batch rate adapted learning_rate :              9.854994193596627e-11
      fit adapted learning_rate :              4.913012686942073e-11
    epoch : 13 ; learning_rate : 4.913012686942073e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4355067002787303
    Epoch 13, Loss: 1.7383903376223762, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.843927384885365e-08
      batch rate adapted learning_rate :              9.843927384885366e-11
      fit adapted learning_rate :              4.9074955480646383e-11
    epoch : 14 ; learning_rate : 4.9074955480646383e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.491679248194338
    Epoch 14, Loss: 1.7383902449257371, fit: 0.15
      exponential decayed learning_rate :              9.832873003807509e-08
      batch rate adapted learning_rate :              9.832873003807509e-11
      fit adapted learning_rate :              2.6793647274499312e-11
    epoch : 15 ; learning_rate : 2.6793647274499312e-11 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4489454275558975
    Epoch 15, Loss: 1.7383901681617269, fit: 0.1
      exponential decayed learning_rate :              9.821831036407267e-08
      batch rate adapted learning_rate :              9.821831036407267e-11
      fit adapted learning_rate :              4.227976203333646e-11
    epoch : 16 ; learning_rate : 4.227976203333646e-11 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4930236257321754
    Epoch 16, Loss: 1.7383900962355208, fit: 0.16666666666666666
      exponential decayed learning_rate :              9.81080146874452e-08
      batch rate adapted learning_rate :              9.81080146874452e-11
      fit adapted learning_rate :              2.28167886214964e-11
    epoch : 17 ; learning_rate : 2.28167886214964e-11 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4592474006804514
    Epoch 17, Loss: 1.7383900353933077, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.799784286894802e-08
      batch rate adapted learning_rate :              9.799784286894802e-11
      fit adapted learning_rate :              3.632565632860073e-11
    epoch : 18 ; learning_rate : 3.632565632860073e-11 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.435585945833098
    Epoch 18, Loss: 1.738389981846317, fit: 0.13333333333333333
      exponential decayed learning_rate :              9.788779476949287e-08
      batch rate adapted learning_rate :              9.788779476949287e-11
      fit adapted learning_rate :              3.115625794776883e-11
    epoch : 19 ; learning_rate : 3.115625794776883e-11 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4587914110496636
    Epoch 19, Loss: 1.7383899112433039, fit: 0.13333333333333333
      exponential decayed learning_rate :              9.777787025014763e-08
      batch rate adapted learning_rate :              9.777787025014763e-11
      fit adapted learning_rate :              3.1121270575874613e-11
    epoch : 20 ; learning_rate : 3.1121270575874613e-11 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.434073001936293
    Epoch 20, Loss: 1.7383898500048987, fit: 0.08333333333333333
    Epoch 20, Loss: 1.7383898500048987
      exponential decayed learning_rate :              9.766806917213623e-08
      batch rate adapted learning_rate :              9.766806917213623e-11
      fit adapted learning_rate :              4.869048662287641e-11
    epoch : 21 ; learning_rate : 4.869048662287641e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.453467705484836
    Epoch 21, Loss: 1.7383897723620056, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.755839139683845e-08
      batch rate adapted learning_rate :              9.755839139683846e-11
      fit adapted learning_rate :              3.616276128232611e-11
    epoch : 22 ; learning_rate : 3.616276128232611e-11 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4817356130998922
    Epoch 22, Loss: 1.7383896874162945, fit: 0.05
      exponential decayed learning_rate :              9.744883678578969e-08
      batch rate adapted learning_rate :              9.74488367857897e-11
      fit adapted learning_rate :              6.464954932904603e-11
    epoch : 23 ; learning_rate : 6.464954932904603e-11 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5039517481822378
    Epoch 23, Loss: 1.7383895895365258, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.73394052006809e-08
      batch rate adapted learning_rate :              9.73394052006809e-11
      fit adapted learning_rate :              4.8526637692092664e-11
    epoch : 24 ; learning_rate : 4.8526637692092664e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4449951217173322
    Epoch 24, Loss: 1.7383894823363455, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.723009650335828e-08
      batch rate adapted learning_rate :              9.723009650335829e-11
      fit adapted learning_rate :              3.604106954783617e-11
    epoch : 25 ; learning_rate : 3.604106954783617e-11 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4274003332871128
    Epoch 25, Loss: 1.738389393155996, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.712091055582324e-08
      batch rate adapted learning_rate :              9.712091055582324e-11
      fit adapted learning_rate :              5.592512435330276e-11
    epoch : 26 ; learning_rate : 5.592512435330276e-11 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4360593403027584
    Epoch 26, Loss: 1.7383893084057238, fit: 0.16666666666666666
      exponential decayed learning_rate :              9.70118472202321e-08
      batch rate adapted learning_rate :              9.70118472202321e-11
      fit adapted learning_rate :              2.256185510283492e-11
    epoch : 27 ; learning_rate : 2.256185510283492e-11 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4610141454651653
    Epoch 27, Loss: 1.73838922818419, fit: 0.15
      exponential decayed learning_rate :              9.690290635889603e-08
      batch rate adapted learning_rate :              9.690290635889603e-11
      fit adapted learning_rate :              2.6405123831546683e-11
    epoch : 28 ; learning_rate : 2.6405123831546683e-11 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4842900907875862
    Epoch 28, Loss: 1.7383891793867494, fit: 0.2
      exponential decayed learning_rate :              9.679408783428076e-08
      batch rate adapted learning_rate :              9.679408783428077e-11
      fit adapted learning_rate :              1.6239353191187016e-11
    epoch : 29 ; learning_rate : 1.6239353191187016e-11 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.4541340146545418
    Epoch 29, Loss: 1.7383891387224728, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.668539150900651e-08
      batch rate adapted learning_rate :              9.668539150900652e-11
      fit adapted learning_rate :              4.820059208501176e-11
    epoch : 30 ; learning_rate : 4.820059208501176e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4188491412114952
    Epoch 30, Loss: 1.7383890781855595, fit: 0.11666666666666667
    Epoch 30, Loss: 1.7383890781855595
      exponential decayed learning_rate :              9.657681724584774e-08
      batch rate adapted learning_rate :              9.657681724584774e-11
      fit adapted learning_rate :              3.5798913219694674e-11
    epoch : 31 ; learning_rate : 3.5798913219694674e-11 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4478063167631845
    Epoch 31, Loss: 1.738388993579129, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.646836490773306e-08
      batch rate adapted learning_rate :              9.646836490773307e-11
      fit adapted learning_rate :              3.5758712310704716e-11
    epoch : 32 ; learning_rate : 3.5758712310704716e-11 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4229367121615144
    Epoch 32, Loss: 1.7383889235651615, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.636003435774494e-08
      batch rate adapted learning_rate :              9.636003435774494e-11
      fit adapted learning_rate :              5.5486989087153876e-11
    epoch : 33 ; learning_rate : 5.5486989087153876e-11 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4179108372972027
    Epoch 33, Loss: 1.7383888332413857, fit: 0.1
      exponential decayed learning_rate :              9.625182545911963e-08
      batch rate adapted learning_rate :              9.625182545911964e-11
      fit adapted learning_rate :              4.143325476279421e-11
    epoch : 34 ; learning_rate : 4.143325476279421e-11 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4989663894300316
    Epoch 34, Loss: 1.7383887365354456, fit: 0.13333333333333333
      exponential decayed learning_rate :              9.6143738075247e-08
      batch rate adapted learning_rate :              9.6143738075247e-11
      fit adapted learning_rate :              3.060115012897065e-11
    epoch : 35 ; learning_rate : 3.060115012897065e-11 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4240085695804807
    Epoch 35, Loss: 1.738388668512726, fit: 0.13333333333333333
      exponential decayed learning_rate :              9.603577206967028e-08
      batch rate adapted learning_rate :              9.603577206967028e-11
      fit adapted learning_rate :              3.056678612345536e-11
    epoch : 36 ; learning_rate : 3.056678612345536e-11 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4310015144676427
    Epoch 36, Loss: 1.7383886041451466, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.592792730608595e-08
      batch rate adapted learning_rate :              9.592792730608596e-11
      fit adapted learning_rate :              5.523816892618476e-11
    epoch : 37 ; learning_rate : 5.523816892618476e-11 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4683915113974426
    Epoch 37, Loss: 1.7383885172551423, fit: 0.05
      exponential decayed learning_rate :              9.582020364834356e-08
      batch rate adapted learning_rate :              9.582020364834356e-11
      fit adapted learning_rate :              6.356908083058987e-11
    epoch : 38 ; learning_rate : 6.356908083058987e-11 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.484439444129317
    Epoch 38, Loss: 1.7383884039534778, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.571260096044553e-08
      batch rate adapted learning_rate :              9.571260096044554e-11
      fit adapted learning_rate :              4.771562657281258e-11
    epoch : 39 ; learning_rate : 4.771562657281258e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4538922136110919
    Epoch 39, Loss: 1.7383882967057707, fit: 0.15
      exponential decayed learning_rate :              9.560511910654705e-08
      batch rate adapted learning_rate :              9.560511910654705e-11
      fit adapted learning_rate :              2.605148910176511e-11
    epoch : 40 ; learning_rate : 2.605148910176511e-11 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.492965134613091
    Epoch 40, Loss: 1.7383882229815795, fit: 0.1
    Epoch 40, Loss: 1.7383882229815795
      exponential decayed learning_rate :              9.549775795095581e-08
      batch rate adapted learning_rate :              9.549775795095581e-11
      fit adapted learning_rate :              4.1108653426403274e-11
    epoch : 41 ; learning_rate : 4.1108653426403274e-11 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4512582536930938
    Epoch 41, Loss: 1.7383881619518695, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.539051735813189e-08
      batch rate adapted learning_rate :              9.539051735813189e-11
      fit adapted learning_rate :              3.535917780550345e-11
    epoch : 42 ; learning_rate : 3.535917780550345e-11 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4391302131585093
    Epoch 42, Loss: 1.7383880807227812, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.528339719268757e-08
      batch rate adapted learning_rate :              9.528339719268757e-11
      fit adapted learning_rate :              3.531947070377676e-11
    epoch : 43 ; learning_rate : 3.531947070377676e-11 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.42409865419205
    Epoch 43, Loss: 1.7383880130102614, fit: 0.21666666666666667
      exponential decayed learning_rate :              9.517639731938721e-08
      batch rate adapted learning_rate :              9.517639731938721e-11
      fit adapted learning_rate :              1.3492801211738814e-11
    epoch : 44 ; learning_rate : 1.3492801211738814e-11 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.4765221941395257
    Epoch 44, Loss: 1.7383879645873526, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.506951760314699e-08
      batch rate adapted learning_rate :              9.506951760314699e-11
      fit adapted learning_rate :              4.7395030068024994e-11
    epoch : 45 ; learning_rate : 4.7395030068024994e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.426796097810425
    Epoch 45, Loss: 1.7383879039888763, fit: 0.13333333333333333
      exponential decayed learning_rate :              9.496275790903479e-08
      batch rate adapted learning_rate :              9.496275790903478e-11
      fit adapted learning_rate :              3.022526135983092e-11
    epoch : 46 ; learning_rate : 3.022526135983092e-11 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4310277338851838
    Epoch 46, Loss: 1.7383878275878146, fit: 0.16666666666666666
      exponential decayed learning_rate :              9.485611810227e-08
      batch rate adapted learning_rate :              9.485611810226999e-11
      fit adapted learning_rate :              2.206050140847624e-11
    epoch : 47 ; learning_rate : 2.206050140847624e-11 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4159854292023268
    Epoch 47, Loss: 1.7383877773349754, fit: 0.13333333333333333
      exponential decayed learning_rate :              9.474959804822342e-08
      batch rate adapted learning_rate :              9.474959804822342e-11
      fit adapted learning_rate :              3.015741568383844e-11
    epoch : 48 ; learning_rate : 3.015741568383844e-11 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4725653886970604
    Epoch 48, Loss: 1.738387726908811, fit: 0.13333333333333333
      exponential decayed learning_rate :              9.464319761241698e-08
      batch rate adapted learning_rate :              9.464319761241698e-11
      fit adapted learning_rate :              3.012354997635625e-11
    epoch : 49 ; learning_rate : 3.012354997635625e-11 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4694168672175125
    Epoch 49, Loss: 1.7383876690769042, fit: 0.05
      exponential decayed learning_rate :              9.453691666052363e-08
      batch rate adapted learning_rate :              9.453691666052363e-11
      fit adapted learning_rate :              6.271772202366272e-11
    epoch : 50 ; learning_rate : 6.271772202366272e-11 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4416124983143486
    Epoch 50, Loss: 1.738387581341698, fit: 0.08333333333333333
    Epoch 50, Loss: 1.738387581341698
      exponential decayed learning_rate :              9.443075505836722e-08
      batch rate adapted learning_rate :              9.443075505836721e-11
      fit adapted learning_rate :              4.70765876189685e-11
    epoch : 51 ; learning_rate : 4.70765876189685e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4378487965838447
    Epoch 51, Loss: 1.7383874692273051, fit: 0.16666666666666666
      exponential decayed learning_rate :              9.432471267192218e-08
      batch rate adapted learning_rate :              9.432471267192218e-11
      fit adapted learning_rate :              2.1936913489434263e-11
    epoch : 52 ; learning_rate : 2.1936913489434263e-11 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4206914727460676
    Epoch 52, Loss: 1.738387399309384, fit: 0.1
      exponential decayed learning_rate :              9.421878936731353e-08
      batch rate adapted learning_rate :              9.421878936731354e-11
      fit adapted learning_rate :              4.055809938852513e-11
    epoch : 53 ; learning_rate : 4.055809938852513e-11 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4408781324430968
    Epoch 53, Loss: 1.738387341169116, fit: 0.05
      exponential decayed learning_rate :              9.411298501081656e-08
      batch rate adapted learning_rate :              9.411298501081656e-11
      fit adapted learning_rate :              6.243647710577697e-11
    epoch : 54 ; learning_rate : 6.243647710577697e-11 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4514433453029822
    Epoch 54, Loss: 1.7383872370236266, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.40072994688568e-08
      batch rate adapted learning_rate :              9.400729946885681e-11
      fit adapted learning_rate :              5.413221398796661e-11
    epoch : 55 ; learning_rate : 5.413221398796661e-11 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.389873060169035
    Epoch 55, Loss: 1.7383871227359218, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.390173260800974e-08
      batch rate adapted learning_rate :              9.390173260800974e-11
      fit adapted learning_rate :              5.407142543288952e-11
    epoch : 56 ; learning_rate : 5.407142543288952e-11 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4836226766145741
    Epoch 56, Loss: 1.7383870161773773, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.379628429500066e-08
      batch rate adapted learning_rate :              9.379628429500067e-11
      fit adapted learning_rate :              3.476823048805645e-11
    epoch : 57 ; learning_rate : 3.476823048805645e-11 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4802860535173288
    Epoch 57, Loss: 1.7383869323671557, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.369095439670459e-08
      batch rate adapted learning_rate :              9.36909543967046e-11
      fit adapted learning_rate :              4.6707774612572397e-11
    epoch : 58 ; learning_rate : 4.6707774612572397e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5005299277304076
    Epoch 58, Loss: 1.7383868509937928, fit: 0.03333333333333333
      exponential decayed learning_rate :              9.358574278014599e-08
      batch rate adapted learning_rate :              9.358574278014599e-11
      fit adapted learning_rate :              7.135487293104533e-11
    epoch : 59 ; learning_rate : 7.135487293104533e-11 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4762586527288302
    Epoch 59, Loss: 1.7383867279163734, fit: 0.15
      exponential decayed learning_rate :              9.348064931249868e-08
      batch rate adapted learning_rate :              9.348064931249868e-11
      fit adapted learning_rate :              2.5472591212155238e-11
    epoch : 60 ; learning_rate : 2.5472591212155238e-11 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.383297389313515
    Epoch 60, Loss: 1.7383866410198274, fit: 0.03333333333333333
    Epoch 60, Loss: 1.7383866410198274
      exponential decayed learning_rate :              9.337567386108561e-08
      batch rate adapted learning_rate :              9.337567386108562e-11
      fit adapted learning_rate :              7.119470493343132e-11
    epoch : 61 ; learning_rate : 7.119470493343132e-11 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4996057199254074
    Epoch 61, Loss: 1.7383865421544633, fit: 0.1
      exponential decayed learning_rate :              9.327081629337875e-08
      batch rate adapted learning_rate :              9.327081629337876e-11
      fit adapted learning_rate :              4.0150028064233303e-11
    epoch : 62 ; learning_rate : 4.0150028064233303e-11 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4529711395299634
    Epoch 62, Loss: 1.7383864353952043, fit: 0.13333333333333333
      exponential decayed learning_rate :              9.316607647699887e-08
      batch rate adapted learning_rate :              9.316607647699888e-11
      fit adapted learning_rate :              2.965340385422162e-11
    epoch : 63 ; learning_rate : 2.965340385422162e-11 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4407406622787855
    Epoch 63, Loss: 1.7383863657617347, fit: 0.15
      exponential decayed learning_rate :              9.306145427971542e-08
      batch rate adapted learning_rate :              9.306145427971543e-11
      fit adapted learning_rate :              2.5358364537578362e-11
    epoch : 64 ; learning_rate : 2.5358364537578362e-11 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4698350666897362
    Epoch 64, Loss: 1.7383863125847967, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.295694956944633e-08
      batch rate adapted learning_rate :              9.295694956944633e-11
      fit adapted learning_rate :              4.634185100493194e-11
    epoch : 65 ; learning_rate : 4.634185100493194e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5118931285682953
    Epoch 65, Loss: 1.7383862422884164, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.285256221425783e-08
      batch rate adapted learning_rate :              9.285256221425783e-11
      fit adapted learning_rate :              4.628981075099342e-11
    epoch : 66 ; learning_rate : 4.628981075099342e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4495292599635572
    Epoch 66, Loss: 1.738386149173909, fit: 0.16666666666666666
      exponential decayed learning_rate :              9.274829208236436e-08
      batch rate adapted learning_rate :              9.274829208236436e-11
      fit adapted learning_rate :              2.1570288443711892e-11
    epoch : 67 ; learning_rate : 2.1570288443711892e-11 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4818244840734054
    Epoch 67, Loss: 1.7383860839039937, fit: 0.13333333333333333
      exponential decayed learning_rate :              9.264413904212832e-08
      batch rate adapted learning_rate :              9.264413904212832e-11
      fit adapted learning_rate :              2.948727877814123e-11
    epoch : 68 ; learning_rate : 2.948727877814123e-11 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4478446901218374
    Epoch 68, Loss: 1.7383860328122804, fit: 0.05
      exponential decayed learning_rate :              9.25401029620599e-08
      batch rate adapted learning_rate :              9.25401029620599e-11
      fit adapted learning_rate :              6.139299501862401e-11
    epoch : 69 ; learning_rate : 6.139299501862401e-11 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4612940899582192
    Epoch 69, Loss: 1.73838594368183, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.243618371081701e-08
      batch rate adapted learning_rate :              9.243618371081701e-11
      fit adapted learning_rate :              4.608223347293638e-11
    epoch : 70 ; learning_rate : 4.608223347293638e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4308106037560475
    Epoch 70, Loss: 1.738385842536317, fit: 0.11666666666666667
    Epoch 70, Loss: 1.738385842536317
      exponential decayed learning_rate :              9.233238115720502e-08
      batch rate adapted learning_rate :              9.233238115720502e-11
      fit adapted learning_rate :              3.4225593622538517e-11
    epoch : 71 ; learning_rate : 3.4225593622538517e-11 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4221491634363574
    Epoch 71, Loss: 1.7383857606683066, fit: 0.05
      exponential decayed learning_rate :              9.222869517017663e-08
      batch rate adapted learning_rate :              9.222869517017663e-11
      fit adapted learning_rate :              6.118640072702604e-11
    epoch : 72 ; learning_rate : 6.118640072702604e-11 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4594371546850542
    Epoch 72, Loss: 1.7383856684943673, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.21251256188317e-08
      batch rate adapted learning_rate :              9.21251256188317e-11
      fit adapted learning_rate :              3.4148768528855417e-11
    epoch : 73 ; learning_rate : 3.4148768528855417e-11 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.459798514270311
    Epoch 73, Loss: 1.738385572030264, fit: 0.05
      exponential decayed learning_rate :              9.202167237241711e-08
      batch rate adapted learning_rate :              9.202167237241711e-11
      fit adapted learning_rate :              6.104905757324974e-11
    epoch : 74 ; learning_rate : 6.104905757324974e-11 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.479220883785451
    Epoch 74, Loss: 1.7383854842952546, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.191833530032653e-08
      batch rate adapted learning_rate :              9.191833530032653e-11
      fit adapted learning_rate :              4.582407037708132e-11
    epoch : 75 ; learning_rate : 4.582407037708132e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4161695835453798
    Epoch 75, Loss: 1.7383853758911845, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.181511427210033e-08
      batch rate adapted learning_rate :              9.181511427210034e-11
      fit adapted learning_rate :              4.57726115724111e-11
    epoch : 76 ; learning_rate : 4.57726115724111e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4698540758704859
    Epoch 76, Loss: 1.7383852833948676, fit: 0.03333333333333333
      exponential decayed learning_rate :              9.171200915742535e-08
      batch rate adapted learning_rate :              9.171200915742535e-11
      fit adapted learning_rate :              6.992623625430332e-11
    epoch : 77 ; learning_rate : 6.992623625430332e-11 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.412748089954753
    Epoch 77, Loss: 1.7383851745584973, fit: 0.1
      exponential decayed learning_rate :              9.16090198261348e-08
      batch rate adapted learning_rate :              9.16090198261348e-11
      fit adapted learning_rate :              3.9434679175390943e-11
    epoch : 78 ; learning_rate : 3.9434679175390943e-11 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4930452733821407
    Epoch 78, Loss: 1.7383850628676334, fit: 0.06666666666666667
      exponential decayed learning_rate :              9.150614614820807e-08
      batch rate adapted learning_rate :              9.150614614820807e-11
      fit adapted learning_rate :              5.269197511784649e-11
    epoch : 79 ; learning_rate : 5.269197511784649e-11 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4135996066151337
    Epoch 79, Loss: 1.7383849749353657, fit: 0.05
      exponential decayed learning_rate :              9.140338799377049e-08
      batch rate adapted learning_rate :              9.14033879937705e-11
      fit adapted learning_rate :              6.063887508410872e-11
    epoch : 80 ; learning_rate : 6.063887508410872e-11 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.450603788342019
    Epoch 80, Loss: 1.7383848589312516, fit: 0.05
    Epoch 80, Loss: 1.7383848589312516
      exponential decayed learning_rate :              9.130074523309331e-08
      batch rate adapted learning_rate :              9.130074523309331e-11
      fit adapted learning_rate :              6.057077977955156e-11
    epoch : 81 ; learning_rate : 6.057077977955156e-11 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4597674094034903
    Epoch 81, Loss: 1.738384738700395, fit: 0.15
      exponential decayed learning_rate :              9.119821773659343e-08
      batch rate adapted learning_rate :              9.119821773659343e-11
      fit adapted learning_rate :              2.4850650233671082e-11
    epoch : 82 ; learning_rate : 2.4850650233671082e-11 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.425826367653955
    Epoch 82, Loss: 1.7383846559855258, fit: 0.16666666666666666
      exponential decayed learning_rate :              9.109580537483327e-08
      batch rate adapted learning_rate :              9.109580537483328e-11
      fit adapted learning_rate :              2.118597285007065e-11
    epoch : 83 ; learning_rate : 2.118597285007065e-11 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.460631782440067
    Epoch 83, Loss: 1.7383846132446847, fit: 0.1
      exponential decayed learning_rate :              9.09935080185206e-08
      batch rate adapted learning_rate :              9.099350801852061e-11
      fit adapted learning_rate :              3.9169721524845206e-11
    epoch : 84 ; learning_rate : 3.9169721524845206e-11 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.440054300527238
    Epoch 84, Loss: 1.7383845557306636, fit: 0.11666666666666667
      exponential decayed learning_rate :              9.08913255385084e-08
      batch rate adapted learning_rate :              9.08913255385084e-11
      fit adapted learning_rate :              3.369142583248648e-11
    epoch : 85 ; learning_rate : 3.369142583248648e-11 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4308675500297283
    Epoch 85, Loss: 1.7383844836098676, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.078925780579464e-08
      batch rate adapted learning_rate :              9.078925780579464e-11
      fit adapted learning_rate :              4.526119109514524e-11
    epoch : 86 ; learning_rate : 4.526119109514524e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4513455049309119
    Epoch 86, Loss: 1.73838440437682, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.068730469152219e-08
      batch rate adapted learning_rate :              9.068730469152218e-11
      fit adapted learning_rate :              4.521036438393122e-11
    epoch : 87 ; learning_rate : 4.521036438393122e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4883378013468327
    Epoch 87, Loss: 1.7383843193996646, fit: 0.08333333333333333
      exponential decayed learning_rate :              9.058546606697858e-08
      batch rate adapted learning_rate :              9.058546606697859e-11
      fit adapted learning_rate :              4.5159594749309966e-11
    epoch : 88 ; learning_rate : 4.5159594749309966e-11 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.424148983601573
    Epoch 88, Loss: 1.7383842292155187, fit: 0.03333333333333333
      Weights saved to src/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.8419713507516633_fit_0.033333333333333332024-01-02175349
  self.fit : 0.03333333333333333
  self.loss : 1.8419713507516633
  current_accuracy : 0.1041
  LR: 1e-07, Epochs: 1, Hidden Size: 784, Samples: 1, Accuracy mean: 0.1041
  LR: 1e-07, Epochs: 10, Hidden Size: 784, Samples: 1, Accuracy mean: 0.1041
  LR: 1e-07, Epochs: 100, Hidden Size: 784, Samples: 1, Accuracy mean: 0.1041
  Results saved to test_combinations_results20240102175349
  normalized_accuracies :      [5.14346440e-02 5.14346440e-02 5.14346440e-02 0.00000000e+00
   2.12539851e-04 4.88841658e-03 1.32837407e-01 3.56004251e-01
   7.18597237e-01 8.43145590e-01 8.74814028e-01 1.00000000e+00]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.9
  LR: 0.9, Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0, 0.0]
  LR: 0.9, Hidden Size: 784, Sample: 1/1
  LR: 0.9, Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
      exponential decayed learning_rate :              0.9
      batch rate adapted learning_rate :              0.0009000000000000001
      fit adapted learning_rate :              0.0009000000000000001
    epoch : 0 ; learning_rate : 0.0009000000000000001 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 0.7672203728321075
    Epoch 0, Loss: 1.3505071135524103, fit: 0.4166666666666667
    Epoch 0, Loss: 1.3505071135524103
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.9
  LR: 0.9, Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0, 0.0]
  LR: 0.9, Hidden Size: 784, Sample: 1/1
  LR: 0.9, Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
      exponential decayed learning_rate :              0.9
      batch rate adapted learning_rate :              0.0009000000000000001
      fit adapted learning_rate :              0.0009000000000000001
    epoch : 0 ; learning_rate : 0.0009000000000000001 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 0.7025108008205069
    Epoch 0, Loss: 1.346000930746073, fit: 0.5
    Epoch 0, Loss: 1.346000930746073
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.9641597587073093_fit_0.52024-01-02191039
  self.fit : 0.5
  self.loss : 0.9641597587073093
  current_accuracy : 0.4778
  LR: 0.9, Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.001
      exponential decayed learning_rate :              0.9
      batch rate adapted learning_rate :              0.0009000000000000001
      fit adapted learning_rate :              3.5156250000000003e-06
    epoch : 0 ; learning_rate : 3.5156250000000003e-06 ; fit : 0.5
    batch_size :          60
    best_batch_loss : 0.5998333033357272
    Epoch 0, Loss: 1.0058298547763185, fit: 0.4666666666666667
    Epoch 0, Loss: 1.0058298547763185
      exponential decayed learning_rate :              0.8900553503645301
      batch rate adapted learning_rate :              0.0008900553503645301
      fit adapted learning_rate :              5.8264877632151775e-06
    epoch : 1 ; learning_rate : 5.8264877632151775e-06 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.6623746497784212
    Epoch 1, Loss: 1.00344794696642, fit: 0.48333333333333334
      exponential decayed learning_rate :              0.8802205852361404
      batch rate adapted learning_rate :              0.0008802205852361405
      fit adapted learning_rate :              4.469665971977972e-06
    epoch : 2 ; learning_rate : 4.469665971977972e-06 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.5762605747392978
    Epoch 2, Loss: 1.000829579636582, fit: 0.5
      exponential decayed learning_rate :              0.8704944904338053
      batch rate adapted learning_rate :              0.0008704944904338053
      fit adapted learning_rate :              3.400369103257052e-06
    epoch : 3 ; learning_rate : 3.400369103257052e-06 ; fit : 0.5
    batch_size :          60
    best_batch_loss : 0.5837676581244816
    Epoch 3, Loss: 0.9988768120497349, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.8608758651927264
      batch rate adapted learning_rate :              0.0008608758651927265
      fit adapted learning_rate :              2.56397928792866e-06
    epoch : 4 ; learning_rate : 2.56397928792866e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.5905675724768819
    Epoch 4, Loss: 0.9974180842262093, fit: 0.5666666666666667
      exponential decayed learning_rate :              0.8513635220160889
      batch rate adapted learning_rate :              0.0008513635220160889
      fit adapted learning_rate :              1.058502331423996e-06
    epoch : 5 ; learning_rate : 1.058502331423996e-06 ; fit : 0.5666666666666667
    batch_size :          60
    best_batch_loss : 0.6183612188572832
    Epoch 5, Loss: 0.9965354373274768, fit: 0.45
      exponential decayed learning_rate :              0.841956286528456
      batch rate adapted learning_rate :              0.0008419562865284561
      fit adapted learning_rate :              7.0500315402795055e-06
    epoch : 6 ; learning_rate : 7.0500315402795055e-06 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.6401497763875472
    Epoch 6, Loss: 0.9946446428217868, fit: 0.45
      exponential decayed learning_rate :              0.8326529973307818
      batch rate adapted learning_rate :              0.0008326529973307818
      fit adapted learning_rate :              6.972131436293847e-06
    epoch : 7 ; learning_rate : 6.972131436293847e-06 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.5585742372942948
    Epoch 7, Loss: 0.9911934185868458, fit: 0.5
      exponential decayed learning_rate :              0.8234525058570279
      batch rate adapted learning_rate :              0.0008234525058570279
      fit adapted learning_rate :              3.2166113510040154e-06
    epoch : 8 ; learning_rate : 3.2166113510040154e-06 ; fit : 0.5
    batch_size :          60
    best_batch_loss : 0.6563588153112453
    Epoch 8, Loss: 0.988795022946926, fit: 0.4666666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.0301864498429143_fit_0.46666666666666672024-01-02191156
  self.fit : 0.4666666666666667
  self.loss : 1.0301864498429143
  current_accuracy : 0.4889
  LR: 0.9, Epochs: 100
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          89
    learning_rate :          0.9
    batch_rate :          0.001
      exponential decayed learning_rate :              0.9
      batch rate adapted learning_rate :              0.0009000000000000001
      fit adapted learning_rate :              5.891587511659807e-06
    epoch : 0 ; learning_rate : 5.891587511659807e-06 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.6718306062118169
    Epoch 0, Loss: 0.9866537408565826, fit: 0.38333333333333336
    Epoch 0, Loss: 0.9866537408565826
      exponential decayed learning_rate :              0.8989893319423155
      batch rate adapted learning_rate :              0.0008989893319423154
      fit adapted learning_rate :              1.880002070557526e-05
    epoch : 1 ; learning_rate : 1.880002070557526e-05 ; fit : 0.38333333333333336
    batch_size :          60
    best_batch_loss : 0.5897961761122358
    Epoch 1, Loss: 0.9811059922676909, fit: 0.4
      exponential decayed learning_rate :              0.8979797988289894
      batch rate adapted learning_rate :              0.0008979797988289894
      fit adapted learning_rate :              1.5082612377899514e-05
    epoch : 2 ; learning_rate : 1.5082612377899514e-05 ; fit : 0.4
    batch_size :          60
    best_batch_loss : 0.5919189332569225
    Epoch 2, Loss: 0.9734694811429471, fit: 0.38333333333333336
      exponential decayed learning_rate :              0.8969713993855201
      batch rate adapted learning_rate :              0.0008969713993855201
      fit adapted learning_rate :              1.875782090130368e-05
    epoch : 3 ; learning_rate : 1.875782090130368e-05 ; fit : 0.38333333333333336
    batch_size :          60
    best_batch_loss : 0.5619723037308921
    Epoch 3, Loss: 0.9663407140719228, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.895964132338836
      batch rate adapted learning_rate :              0.000895964132338836
      fit adapted learning_rate :              2.668484006726639e-06
    epoch : 4 ; learning_rate : 2.668484006726639e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.5980240879283077
    Epoch 4, Loss: 0.9617502333957607, fit: 0.5
      exponential decayed learning_rate :              0.8949579964172959
      batch rate adapted learning_rate :              0.0008949579964172959
      fit adapted learning_rate :              3.4959296735050623e-06
    epoch : 5 ; learning_rate : 3.4959296735050623e-06 ; fit : 0.5
    batch_size :          60
    best_batch_loss : 0.5771117338166029
    Epoch 5, Loss: 0.9604763492921253, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8939529903506866
      batch rate adapted learning_rate :              0.0008939529903506867
      fit adapted learning_rate :              2.0108002434659613e-06
    epoch : 6 ; learning_rate : 2.0108002434659613e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.5506382882358661
    Epoch 6, Loss: 0.9593322571341304, fit: 0.4
      exponential decayed learning_rate :              0.8929491128702209
      batch rate adapted learning_rate :              0.000892949112870221
      fit adapted learning_rate :              1.4998116171626285e-05
    epoch : 7 ; learning_rate : 1.4998116171626285e-05 ; fit : 0.4
    batch_size :          60
    best_batch_loss : 0.5935040671082739
    Epoch 7, Loss: 0.9558132561186813, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8919463627085368
      batch rate adapted learning_rate :              0.0008919463627085368
      fit adapted learning_rate :              2.0062866645698296e-06
    epoch : 8 ; learning_rate : 2.0062866645698296e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.5138959509399231
    Epoch 8, Loss: 0.9523724025629535, fit: 0.4166666666666667
      exponential decayed learning_rate :              0.8909447385996951
      batch rate adapted learning_rate :              0.0008909447385996951
      fit adapted learning_rate :              1.1944971536705265e-05
    epoch : 9 ; learning_rate : 1.1944971536705265e-05 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.5995417774328278
    Epoch 9, Loss: 0.9495457458300516, fit: 0.43333333333333335
      exponential decayed learning_rate :              0.8899442392791785
      batch rate adapted learning_rate :              0.0008899442392791785
      fit adapted learning_rate :              9.462025833297993e-06
    epoch : 10 ; learning_rate : 9.462025833297993e-06 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.549688181130364
    Epoch 10, Loss: 0.945263192066032, fit: 0.4166666666666667
    Epoch 10, Loss: 0.945263192066032
      exponential decayed learning_rate :              0.8889448634838898
      batch rate adapted learning_rate :              0.0008889448634838898
      fit adapted learning_rate :              1.1918159041720665e-05
    epoch : 11 ; learning_rate : 1.1918159041720665e-05 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.6057083607191003
    Epoch 11, Loss: 0.9409608360959587, fit: 0.5833333333333334
      exponential decayed learning_rate :              0.8879466099521498
      batch rate adapted learning_rate :              0.0008879466099521498
      fit adapted learning_rate :              8.066718833365369e-07
    epoch : 12 ; learning_rate : 8.066718833365369e-07 ; fit : 0.5833333333333334
    batch_size :          60
    best_batch_loss : 0.5888019125834184
    Epoch 12, Loss: 0.9385436263951226, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.8869494774236966
      batch rate adapted learning_rate :              0.0008869494774236965
      fit adapted learning_rate :              2.6416353175894802e-06
    epoch : 13 ; learning_rate : 2.6416353175894802e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.5877180036719724
    Epoch 13, Loss: 0.9378784750072802, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8859534646396829
      batch rate adapted learning_rate :              0.0008859534646396829
      fit adapted learning_rate :              1.9928066258810045e-06
    epoch : 14 ; learning_rate : 1.9928066258810045e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.5323371888132461
    Epoch 14, Loss: 0.9369588923479835, fit: 0.5666666666666667
      exponential decayed learning_rate :              0.8849585703426759
      batch rate adapted learning_rate :              0.0008849585703426759
      fit adapted learning_rate :              1.1002711364742578e-06
    epoch : 15 ; learning_rate : 1.1002711364742578e-06 ; fit : 0.5666666666666667
    batch_size :          60
    best_batch_loss : 0.5121335362250949
    Epoch 15, Loss: 0.9363603508946152, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.883964793276654
      batch rate adapted learning_rate :              0.000883964793276654
      fit adapted learning_rate :              1.9883334366818906e-06
    epoch : 16 ; learning_rate : 1.9883334366818906e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.5816430981704357
    Epoch 16, Loss: 0.9357485204202828, fit: 0.55
      exponential decayed learning_rate :              0.8829721321870068
      batch rate adapted learning_rate :              0.0008829721321870068
      fit adapted learning_rate :              1.484728711915202e-06
    epoch : 17 ; learning_rate : 1.484728711915202e-06 ; fit : 0.55
    batch_size :          60
    best_batch_loss : 0.5528096978896577
    Epoch 17, Loss: 0.9350849520621554, fit: 0.5833333333333334
      exponential decayed learning_rate :              0.8819805858205323
      batch rate adapted learning_rate :              0.0008819805858205323
      fit adapted learning_rate :              8.012519359339081e-07
    epoch : 18 ; learning_rate : 8.012519359339081e-07 ; fit : 0.5833333333333334
    batch_size :          60
    best_batch_loss : 0.5910747855334513
    Epoch 18, Loss: 0.9346327351039379, fit: 0.48333333333333334
      exponential decayed learning_rate :              0.8809901529254359
      batch rate adapted learning_rate :              0.000880990152925436
      fit adapted learning_rate :              4.473573754381238e-06
    epoch : 19 ; learning_rate : 4.473573754381238e-06 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.522395035640183
    Epoch 19, Loss: 0.9336103433304368, fit: 0.43333333333333335
      exponential decayed learning_rate :              0.8800008322513287
      batch rate adapted learning_rate :              0.0008800008322513288
      fit adapted learning_rate :              9.356305980282576e-06
    epoch : 20 ; learning_rate : 9.356305980282576e-06 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.5818204958787259
    Epoch 20, Loss: 0.9308974613947069, fit: 0.5
    Epoch 20, Loss: 0.9308974613947069
      exponential decayed learning_rate :              0.8790126225492262
      batch rate adapted learning_rate :              0.0008790126225492262
      fit adapted learning_rate :              3.433643056832915e-06
    epoch : 21 ; learning_rate : 3.433643056832915e-06 ; fit : 0.5
    batch_size :          60
    best_batch_loss : 0.6072161785046312
    Epoch 21, Loss: 0.9284539162411507, fit: 0.43333333333333335
      exponential decayed learning_rate :              0.8780255225715461
      batch rate adapted learning_rate :              0.0008780255225715461
      fit adapted learning_rate :              9.335304179951798e-06
    epoch : 22 ; learning_rate : 9.335304179951798e-06 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.44553441599166266
    Epoch 22, Loss: 0.9259682278300496, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.8770395310721073
      batch rate adapted learning_rate :              0.0008770395310721073
      fit adapted learning_rate :              2.612120147961313e-06
    epoch : 23 ; learning_rate : 2.612120147961313e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.5414644967347482
    Epoch 23, Loss: 0.9236994499296276, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8760546468061281
      batch rate adapted learning_rate :              0.0008760546468061281
      fit adapted learning_rate :              1.970540863000197e-06
    epoch : 24 ; learning_rate : 1.970540863000197e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.5319916921131036
    Epoch 24, Loss: 0.922835701977349, fit: 0.45
      exponential decayed learning_rate :              0.8750708685302246
      batch rate adapted learning_rate :              0.0008750708685302247
      fit adapted learning_rate :              7.327312975540516e-06
    epoch : 25 ; learning_rate : 7.327312975540516e-06 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.5479650609840548
    Epoch 25, Loss: 0.9210192718236376, fit: 0.6333333333333333
      exponential decayed learning_rate :              0.8740881950024092
      batch rate adapted learning_rate :              0.0008740881950024093
      fit adapted learning_rate :              2.8557928269475134e-07
    epoch : 26 ; learning_rate : 2.8557928269475134e-07 ; fit : 0.6333333333333333
    batch_size :          60
    best_batch_loss : 0.5496654771061651
    Epoch 26, Loss: 0.9196301373850619, fit: 0.55
      exponential decayed learning_rate :              0.873106624982089
      batch rate adapted learning_rate :              0.000873106624982089
      fit adapted learning_rate :              1.4681397378459212e-06
    epoch : 27 ; learning_rate : 1.4681397378459212e-06 ; fit : 0.55
    batch_size :          60
    best_batch_loss : 0.42538383210546005
    Epoch 27, Loss: 0.9193067269365476, fit: 0.43333333333333335
      exponential decayed learning_rate :              0.8721261572300644
      batch rate adapted learning_rate :              0.0008721261572300643
      fit adapted learning_rate :              9.272581208334638e-06
    epoch : 28 ; learning_rate : 9.272581208334638e-06 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.5687996307524151
    Epoch 28, Loss: 0.9172169531558728, fit: 0.5666666666666667
      exponential decayed learning_rate :              0.8711467905085268
      batch rate adapted learning_rate :              0.0008711467905085269
      fit adapted learning_rate :              1.0830989171137888e-06
    epoch : 29 ; learning_rate : 1.0830989171137888e-06 ; fit : 0.5666666666666667
    batch_size :          60
    best_batch_loss : 0.5694191987234081
    Epoch 29, Loss: 0.9153664621532488, fit: 0.6166666666666667
      exponential decayed learning_rate :              0.8701685235810587
      batch rate adapted learning_rate :              0.0008701685235810587
      fit adapted learning_rate :              4.0571031975252524e-07
    epoch : 30 ; learning_rate : 4.0571031975252524e-07 ; fit : 0.6166666666666667
    batch_size :          60
    best_batch_loss : 0.4957391509137416
    Epoch 30, Loss: 0.9150846458369467, fit: 0.45
    Epoch 30, Loss: 0.9150846458369467
      exponential decayed learning_rate :              0.8691913552126297
      batch rate adapted learning_rate :              0.0008691913552126297
      fit adapted learning_rate :              7.278081495244256e-06
    epoch : 31 ; learning_rate : 7.278081495244256e-06 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.48742818918190844
    Epoch 31, Loss: 0.9136603114088105, fit: 0.4166666666666667
      exponential decayed learning_rate :              0.8682152841695976
      batch rate adapted learning_rate :              0.0008682152841695976
      fit adapted learning_rate :              1.1640235816912948e-05
    epoch : 32 ; learning_rate : 1.1640235816912948e-05 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.5067287842355938
    Epoch 32, Loss: 0.9101972871631759, fit: 0.5666666666666667
      exponential decayed learning_rate :              0.8672403092197045
      batch rate adapted learning_rate :              0.0008672403092197046
      fit adapted learning_rate :              1.078241979454432e-06
    epoch : 33 ; learning_rate : 1.078241979454432e-06 ; fit : 0.5666666666666667
    batch_size :          60
    best_batch_loss : 0.5513648036976607
    Epoch 33, Loss: 0.9079258388526464, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8662664291320767
      batch rate adapted learning_rate :              0.0008662664291320767
      fit adapted learning_rate :              1.9485238769902737e-06
    epoch : 34 ; learning_rate : 1.9485238769902737e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.5309356895362424
    Epoch 34, Loss: 0.9073717097484685, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8652936426772231
      batch rate adapted learning_rate :              0.0008652936426772231
      fit adapted learning_rate :              1.946335753832374e-06
    epoch : 35 ; learning_rate : 1.946335753832374e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.473328571816881
    Epoch 35, Loss: 0.9066928383028453, fit: 0.45
      exponential decayed learning_rate :              0.8643219486270326
      batch rate adapted learning_rate :              0.0008643219486270326
      fit adapted learning_rate :              7.237308036383997e-06
    epoch : 36 ; learning_rate : 7.237308036383997e-06 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.5138827559223221
    Epoch 36, Loss: 0.9050856563446709, fit: 0.43333333333333335
      exponential decayed learning_rate :              0.8633513457547737
      batch rate adapted learning_rate :              0.0008633513457547737
      fit adapted learning_rate :              9.179286045337944e-06
    epoch : 37 ; learning_rate : 9.179286045337944e-06 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.5427383039427693
    Epoch 37, Loss: 0.9021932713893664, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.8623818328350921
      batch rate adapted learning_rate :              0.0008623818328350921
      fit adapted learning_rate :              5.645331151792804e-06
    epoch : 38 ; learning_rate : 5.645331151792804e-06 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.5486210482034136
    Epoch 38, Loss: 0.899566823689792, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.8614134086440098
      batch rate adapted learning_rate :              0.0008614134086440099
      fit adapted learning_rate :              2.565580274007116e-06
    epoch : 39 ; learning_rate : 2.565580274007116e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.46686317234390096
    Epoch 39, Loss: 0.8981590471285265, fit: 0.48333333333333334
      exponential decayed learning_rate :              0.8604460719589235
      batch rate adapted learning_rate :              0.0008604460719589236
      fit adapted learning_rate :              4.369253108895599e-06
    epoch : 40 ; learning_rate : 4.369253108895599e-06 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.5483988975238513
    Epoch 40, Loss: 0.8969775055063518, fit: 0.6166666666666667
    Epoch 40, Loss: 0.8969775055063518
      exponential decayed learning_rate :              0.8594798215586023
      batch rate adapted learning_rate :              0.0008594798215586023
      fit adapted learning_rate :              4.007267831182376e-07
    epoch : 41 ; learning_rate : 4.007267831182376e-07 ; fit : 0.6166666666666667
    batch_size :          60
    best_batch_loss : 0.5538894070174422
    Epoch 41, Loss: 0.8961451678199284, fit: 0.48333333333333334
      exponential decayed learning_rate :              0.858514656223187
      batch rate adapted learning_rate :              0.000858514656223187
      fit adapted learning_rate :              4.359445586398896e-06
    epoch : 42 ; learning_rate : 4.359445586398896e-06 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.4622453311596438
    Epoch 42, Loss: 0.8953318648944745, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.8575505747341883
      batch rate adapted learning_rate :              0.0008575505747341883
      fit adapted learning_rate :              5.613704729689593e-06
    epoch : 43 ; learning_rate : 5.613704729689593e-06 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.5689271844284256
    Epoch 43, Loss: 0.8936398255617193, fit: 0.45
      exponential decayed learning_rate :              0.856587575874485
      batch rate adapted learning_rate :              0.0008565875758744849
      fit adapted learning_rate :              7.17254508761552e-06
    epoch : 44 ; learning_rate : 7.17254508761552e-06 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.5269538590010296
    Epoch 44, Loss: 0.89151658256326, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8556256584283229
      batch rate adapted learning_rate :              0.000855625658428323
      fit adapted learning_rate :              1.9245892131402427e-06
    epoch : 45 ; learning_rate : 1.9245892131402427e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.4275945056546219
    Epoch 45, Loss: 0.8899486902414451, fit: 0.55
      exponential decayed learning_rate :              0.8546648211813132
      batch rate adapted learning_rate :              0.0008546648211813131
      fit adapted learning_rate :              1.4371296135119862e-06
    epoch : 46 ; learning_rate : 1.4371296135119862e-06 ; fit : 0.55
    batch_size :          60
    best_batch_loss : 0.5640183927872122
    Epoch 46, Loss: 0.889399910338571, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.85370506292043
      batch rate adapted learning_rate :              0.00085370506292043
      fit adapted learning_rate :              1.9202691493823535e-06
    epoch : 47 ; learning_rate : 1.9202691493823535e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.5045639670481797
    Epoch 47, Loss: 0.8888409632994491, fit: 0.48333333333333334
      exponential decayed learning_rate :              0.8527463824340109
      batch rate adapted learning_rate :              0.0008527463824340109
      fit adapted learning_rate :              4.330154909147106e-06
    epoch : 48 ; learning_rate : 4.330154909147106e-06 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.5450269080537546
    Epoch 48, Loss: 0.8878094989930073, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.8517887785117528
      batch rate adapted learning_rate :              0.0008517887785117528
      fit adapted learning_rate :              2.5369148725121436e-06
    epoch : 49 ; learning_rate : 2.5369148725121436e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.4282353649629741
    Epoch 49, Loss: 0.8866753325320057, fit: 0.5666666666666667
      exponential decayed learning_rate :              0.8508322499447127
      batch rate adapted learning_rate :              0.0008508322499447128
      fit adapted learning_rate :              1.0578417995693537e-06
    epoch : 50 ; learning_rate : 1.0578417995693537e-06 ; fit : 0.5666666666666667
    batch_size :          60
    best_batch_loss : 0.5781737263664368
    Epoch 50, Loss: 0.8860863404223256, fit: 0.5
    Epoch 50, Loss: 0.8860863404223256
      exponential decayed learning_rate :              0.849876795525305
      batch rate adapted learning_rate :              0.0008498767955253051
      fit adapted learning_rate :              3.319831232520723e-06
    epoch : 51 ; learning_rate : 3.319831232520723e-06 ; fit : 0.5
    batch_size :          60
    best_batch_loss : 0.4940816864699025
    Epoch 51, Loss: 0.8853871819441286, fit: 0.4
      exponential decayed learning_rate :              0.8489224140472996
      batch rate adapted learning_rate :              0.0008489224140472996
      fit adapted learning_rate :              1.4258636693924686e-05
    epoch : 52 ; learning_rate : 1.4258636693924686e-05 ; fit : 0.4
    batch_size :          60
    best_batch_loss : 0.5136314553384788
    Epoch 52, Loss: 0.8824868034953006, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.8479691043058217
      batch rate adapted learning_rate :              0.0008479691043058218
      fit adapted learning_rate :              2.5255385917420353e-06
    epoch : 53 ; learning_rate : 2.5255385917420353e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.5114431020096271
    Epoch 53, Loss: 0.8798192556677416, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.8470168650973491
      batch rate adapted learning_rate :              0.0008470168650973491
      fit adapted learning_rate :              2.522702501538565e-06
    epoch : 54 ; learning_rate : 2.522702501538565e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.5044200360857999
    Epoch 54, Loss: 0.8789917095337844, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.8460656952197113
      batch rate adapted learning_rate :              0.0008460656952197113
      fit adapted learning_rate :              2.5198695961637357e-06
    epoch : 55 ; learning_rate : 2.5198695961637357e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.504375407260814
    Epoch 55, Loss: 0.878193088869291, fit: 0.5833333333333334
      exponential decayed learning_rate :              0.8451155934720876
      batch rate adapted learning_rate :              0.0008451155934720877
      fit adapted learning_rate :              7.677612367481662e-07
    epoch : 56 ; learning_rate : 7.677612367481662e-07 ; fit : 0.5833333333333334
    batch_size :          60
    best_batch_loss : 0.5055801104361595
    Epoch 56, Loss: 0.877684200381184, fit: 0.5833333333333334
      exponential decayed learning_rate :              0.844166558655006
      batch rate adapted learning_rate :              0.000844166558655006
      fit adapted learning_rate :              7.668990681282663e-07
    epoch : 57 ; learning_rate : 7.668990681282663e-07 ; fit : 0.5833333333333334
    batch_size :          60
    best_batch_loss : 0.5432718825655015
    Epoch 57, Loss: 0.8774423116053157, fit: 0.6666666666666666
      exponential decayed learning_rate :              0.8432185895703413
      batch rate adapted learning_rate :              0.0008432185895703414
      fit adapted learning_rate :              1.2851982770467034e-07
    epoch : 58 ; learning_rate : 1.2851982770467034e-07 ; fit : 0.6666666666666666
    batch_size :          60
    best_batch_loss : 0.5265541097202596
    Epoch 58, Loss: 0.8772988000396212, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.842271685021314
      batch rate adapted learning_rate :              0.000842271685021314
      fit adapted learning_rate :              2.50856975148206e-06
    epoch : 59 ; learning_rate : 2.50856975148206e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.49499493067555406
    Epoch 59, Loss: 0.8768926055767476, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8413258438124882
      batch rate adapted learning_rate :              0.0008413258438124882
      fit adapted learning_rate :              1.892424131730583e-06
    epoch : 60 ; learning_rate : 1.892424131730583e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.5136847890964348
    Epoch 60, Loss: 0.8761852934084622, fit: 0.5666666666666667
    Epoch 60, Loss: 0.8761852934084622
      exponential decayed learning_rate :              0.8403810647497706
      batch rate adapted learning_rate :              0.0008403810647497706
      fit adapted learning_rate :              1.044847815673035e-06
    epoch : 61 ; learning_rate : 1.044847815673035e-06 ; fit : 0.5666666666666667
    batch_size :          60
    best_batch_loss : 0.5366974164235453
    Epoch 61, Loss: 0.8757223638386967, fit: 0.5666666666666667
      exponential decayed learning_rate :              0.8394373466404088
      batch rate adapted learning_rate :              0.0008394373466404088
      fit adapted learning_rate :              1.0436744886592102e-06
    epoch : 62 ; learning_rate : 1.0436744886592102e-06 ; fit : 0.5666666666666667
    batch_size :          60
    best_batch_loss : 0.504882976598292
    Epoch 62, Loss: 0.8753982509782798, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.83849468829299
      batch rate adapted learning_rate :              0.00083849468829299
      fit adapted learning_rate :              2.4973205786645263e-06
    epoch : 63 ; learning_rate : 2.4973205786645263e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.47607831045131815
    Epoch 63, Loss: 0.8748377007751122, fit: 0.6
      exponential decayed learning_rate :              0.8375530885174388
      batch rate adapted learning_rate :              0.0008375530885174388
      fit adapted learning_rate :              5.48898792090789e-07
    epoch : 64 ; learning_rate : 5.48898792090789e-07 ; fit : 0.6
    batch_size :          60
    best_batch_loss : 0.43412472295031834
    Epoch 64, Loss: 0.874360283327813, fit: 0.5833333333333334
      exponential decayed learning_rate :              0.836612546125017
      batch rate adapted learning_rate :              0.000836612546125017
      fit adapted learning_rate :              7.600364826461928e-07
    epoch : 65 ; learning_rate : 7.600364826461928e-07 ; fit : 0.5833333333333334
    batch_size :          60
    best_batch_loss : 0.49348856008102887
    Epoch 65, Loss: 0.8741574702994167, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8356730599283205
      batch rate adapted learning_rate :              0.0008356730599283205
      fit adapted learning_rate :              1.8797091239692847e-06
    epoch : 66 ; learning_rate : 1.8797091239692847e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.5425576451329992
    Epoch 66, Loss: 0.8737392921571087, fit: 0.5
      exponential decayed learning_rate :              0.8347346287412793
      batch rate adapted learning_rate :              0.0008347346287412794
      fit adapted learning_rate :              3.2606821435206225e-06
    epoch : 67 ; learning_rate : 3.2606821435206225e-06 ; fit : 0.5
    batch_size :          60
    best_batch_loss : 0.49105285303047047
    Epoch 67, Loss: 0.8729504794035977, fit: 0.4166666666666667
      exponential decayed learning_rate :              0.8337972513791548
      batch rate adapted learning_rate :              0.0008337972513791549
      fit adapted learning_rate :              1.1178790337502642e-05
    epoch : 68 ; learning_rate : 1.1178790337502642e-05 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.48809640458414344
    Epoch 68, Loss: 0.8707031860259824, fit: 0.55
      exponential decayed learning_rate :              0.8328609266585392
      batch rate adapted learning_rate :              0.0008328609266585392
      fit adapted learning_rate :              1.4004660914715457e-06
    epoch : 69 ; learning_rate : 1.4004660914715457e-06 ; fit : 0.55
    batch_size :          60
    best_batch_loss : 0.4684865786204071
    Epoch 69, Loss: 0.8687546367416056, fit: 0.5666666666666667
      exponential decayed learning_rate :              0.8319256533973531
      batch rate adapted learning_rate :              0.0008319256533973531
      fit adapted learning_rate :              1.0343351822347496e-06
    epoch : 70 ; learning_rate : 1.0343351822347496e-06 ; fit : 0.5666666666666667
    batch_size :          60
    best_batch_loss : 0.5218732489396999
    Epoch 70, Loss: 0.8683857922782543, fit: 0.5333333333333333
    Epoch 70, Loss: 0.8683857922782543
      exponential decayed learning_rate :              0.8309914304148452
      batch rate adapted learning_rate :              0.0008309914304148453
      fit adapted learning_rate :              1.8691785682609575e-06
    epoch : 71 ; learning_rate : 1.8691785682609575e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.4994110956920769
    Epoch 71, Loss: 0.867940521373808, fit: 0.6333333333333333
      exponential decayed learning_rate :              0.8300582565315897
      batch rate adapted learning_rate :              0.0008300582565315898
      fit adapted learning_rate :              2.7119396286377477e-07
    epoch : 72 ; learning_rate : 2.7119396286377477e-07 ; fit : 0.6333333333333333
    batch_size :          60
    best_batch_loss : 0.45818876040771583
    Epoch 72, Loss: 0.8676094592470381, fit: 0.6
      exponential decayed learning_rate :              0.8291261305694854
      batch rate adapted learning_rate :              0.0008291261305694854
      fit adapted learning_rate :              5.433761009300182e-07
    epoch : 73 ; learning_rate : 5.433761009300182e-07 ; fit : 0.6
    batch_size :          60
    best_batch_loss : 0.512528072427114
    Epoch 73, Loss: 0.8674824458316716, fit: 0.45
      exponential decayed learning_rate :              0.8281950513517541
      batch rate adapted learning_rate :              0.0008281950513517541
      fit adapted learning_rate :              6.934803299121081e-06
    epoch : 74 ; learning_rate : 6.934803299121081e-06 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.5144984278311059
    Epoch 74, Loss: 0.866325657372044, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8272650177029388
      batch rate adapted learning_rate :              0.0008272650177029388
      fit adapted learning_rate :              1.8607966156647516e-06
    epoch : 75 ; learning_rate : 1.8607966156647516e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.5069883047469675
    Epoch 75, Loss: 0.8650086155006234, fit: 0.6
      exponential decayed learning_rate :              0.826336028448903
      batch rate adapted learning_rate :              0.000826336028448903
      fit adapted learning_rate :              5.415475796042733e-07
    epoch : 76 ; learning_rate : 5.415475796042733e-07 ; fit : 0.6
    batch_size :          60
    best_batch_loss : 0.48799403021320525
    Epoch 76, Loss: 0.8646344995611501, fit: 0.5833333333333334
      exponential decayed learning_rate :              0.8254080824168282
      batch rate adapted learning_rate :              0.0008254080824168282
      fit adapted learning_rate :              7.498575757840475e-07
    epoch : 77 ; learning_rate : 7.498575757840475e-07 ; fit : 0.5833333333333334
    batch_size :          60
    best_batch_loss : 0.46316336446351714
    Epoch 77, Loss: 0.8644407778934186, fit: 0.55
      exponential decayed learning_rate :              0.8244811784352133
      batch rate adapted learning_rate :              0.0008244811784352133
      fit adapted learning_rate :              1.3863754397598365e-06
    epoch : 78 ; learning_rate : 1.3863754397598365e-06 ; fit : 0.55
    batch_size :          60
    best_batch_loss : 0.42820933900536823
    Epoch 78, Loss: 0.8641196020322718, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8235553153338726
      batch rate adapted learning_rate :              0.0008235553153338726
      fit adapted learning_rate :              1.8524522502367907e-06
    epoch : 79 ; learning_rate : 1.8524522502367907e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.45527667389058596
    Epoch 79, Loss: 0.8636204101098535, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.8226304919439344
      batch rate adapted learning_rate :              0.0008226304919439345
      fit adapted learning_rate :              5.385110592274942e-06
    epoch : 80 ; learning_rate : 5.385110592274942e-06 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.5221841093766686
    Epoch 80, Loss: 0.8625141666979803, fit: 0.48333333333333334
    Epoch 80, Loss: 0.8625141666979803
      exponential decayed learning_rate :              0.8217067070978399
      batch rate adapted learning_rate :              0.0008217067070978399
      fit adapted learning_rate :              4.17253875819773e-06
    epoch : 81 ; learning_rate : 4.17253875819773e-06 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.5467386309118767
    Epoch 81, Loss: 0.8610823954011974, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.8207839596293409
      batch rate adapted learning_rate :              0.0008207839596293409
      fit adapted learning_rate :              2.444572042779443e-06
    epoch : 82 ; learning_rate : 2.444572042779443e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.5288360508891637
    Epoch 82, Loss: 0.8600978817457784, fit: 0.6833333333333333
      exponential decayed learning_rate :              0.8198622483734995
      batch rate adapted learning_rate :              0.0008198622483734996
      fit adapted learning_rate :              8.290098558353412e-08
    epoch : 83 ; learning_rate : 8.290098558353412e-08 ; fit : 0.6833333333333333
    batch_size :          60
    best_batch_loss : 0.4492409231566877
    Epoch 83, Loss: 0.8597158901291442, fit: 0.5333333333333333
      exponential decayed learning_rate :              0.8189415721666855
      batch rate adapted learning_rate :              0.0008189415721666856
      fit adapted learning_rate :              1.8420743937006994e-06
    epoch : 84 ; learning_rate : 1.8420743937006994e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.47614965363581874
    Epoch 84, Loss: 0.8594349223980725, fit: 0.6166666666666667
      exponential decayed learning_rate :              0.8180219298465756
      batch rate adapted learning_rate :              0.0008180219298465757
      fit adapted learning_rate :              3.8139731526581293e-07
    epoch : 85 ; learning_rate : 3.8139731526581293e-07 ; fit : 0.6166666666666667
    batch_size :          60
    best_batch_loss : 0.43950666392786863
    Epoch 85, Loss: 0.8590972347594712, fit: 0.55
      exponential decayed learning_rate :              0.8171033202521518
      batch rate adapted learning_rate :              0.0008171033202521519
      fit adapted learning_rate :              1.3739694787135938e-06
    epoch : 86 ; learning_rate : 1.3739694787135938e-06 ; fit : 0.55
    batch_size :          60
    best_batch_loss : 0.4511511531410715
    Epoch 86, Loss: 0.8588288917888287, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.8161857422236997
      batch rate adapted learning_rate :              0.0008161857422236997
      fit adapted learning_rate :              2.430876997226252e-06
    epoch : 87 ; learning_rate : 2.430876997226252e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.4846380391578632
    Epoch 87, Loss: 0.8582702440660959, fit: 0.6166666666666667
      exponential decayed learning_rate :              0.8152691946028073
      batch rate adapted learning_rate :              0.0008152691946028074
      fit adapted learning_rate :              3.8011387066206286e-07
    epoch : 88 ; learning_rate : 3.8011387066206286e-07 ; fit : 0.6166666666666667
    batch_size :          60
    best_batch_loss : 0.48285725361764115
    Epoch 88, Loss: 0.8578471457370971, fit: 0.5666666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.8390022258763781_fit_0.56666666666666672024-01-02192255
  self.fit : 0.5666666666666667
  self.loss : 0.8390022258763781
  current_accuracy : 0.5604
  LR: 0.9, Epochs: 1, Hidden Size: 784, Samples: 1, Accuracy mean: 0.4778
  LR: 0.9, Epochs: 10, Hidden Size: 784, Samples: 1, Accuracy mean: 0.4889
  LR: 0.9, Epochs: 100, Hidden Size: 784, Samples: 1, Accuracy mean: 0.5604
  LR: 0.1
  LR: 0.1, Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0, 0.0]
  LR: 0.1, Hidden Size: 784, Sample: 1/1
  LR: 0.1, Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.001
      exponential decayed learning_rate :              0.1
      batch rate adapted learning_rate :              0.0001
      fit adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.3943510473149023
    Epoch 0, Loss: 1.7210191360170484, fit: 0.23333333333333334
    Epoch 0, Loss: 1.7210191360170484
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.4738799599234045_fit_0.233333333333333342024-01-02192325
  self.fit : 0.23333333333333334
  self.loss : 1.4738799599234045
  current_accuracy : 0.1348
  LR: 0.1, Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.001
      exponential decayed learning_rate :              0.1
      batch rate adapted learning_rate :              0.0001
      fit adapted learning_rate :              1.1935830708885835e-05
    epoch : 0 ; learning_rate : 1.1935830708885835e-05 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.351112743405167
    Epoch 0, Loss: 1.6556859559504133, fit: 0.08333333333333333
    Epoch 0, Loss: 1.6556859559504133
      exponential decayed learning_rate :              0.09889503892939223
      batch rate adapted learning_rate :              9.889503892939224e-05
      fit adapted learning_rate :              4.9302168158702156e-05
    epoch : 1 ; learning_rate : 4.9302168158702156e-05 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.128642975247458
    Epoch 1, Loss: 1.6210262077702173, fit: 0.25
      exponential decayed learning_rate :              0.09780228724846006
      batch rate adapted learning_rate :              9.780228724846007e-05
      fit adapted learning_rate :              9.79127207393107e-06
    epoch : 2 ; learning_rate : 9.79127207393107e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.2192933638443042
    Epoch 2, Loss: 1.5843696148654702, fit: 0.1
      exponential decayed learning_rate :              0.0967216100482006
      batch rate adapted learning_rate :              9.672161004820059e-05
      fit adapted learning_rate :              4.163548162415689e-05
    epoch : 3 ; learning_rate : 4.163548162415689e-05 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.276885089473204
    Epoch 3, Loss: 1.5509831051866483, fit: 0.25
      exponential decayed learning_rate :              0.09565287391030293
      batch rate adapted learning_rate :              9.565287391030293e-05
      fit adapted learning_rate :              9.576088039024314e-06
    epoch : 4 ; learning_rate : 9.576088039024314e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.131928271205911
    Epoch 4, Loss: 1.517860243783201, fit: 0.11666666666666667
      exponential decayed learning_rate :              0.09459594689067655
      batch rate adapted learning_rate :              9.459594689067654e-05
      fit adapted learning_rate :              3.5064647917042157e-05
    epoch : 5 ; learning_rate : 3.5064647917042157e-05 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.1410396292159861
    Epoch 5, Loss: 1.4908029703601096, fit: 0.23333333333333334
      exponential decayed learning_rate :              0.09355069850316178
      batch rate adapted learning_rate :              9.355069850316178e-05
      fit adapted learning_rate :              1.1166053000317583e-05
    epoch : 6 ; learning_rate : 1.1166053000317583e-05 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.1464987804402957
    Epoch 6, Loss: 1.4623599190485934, fit: 0.25
      exponential decayed learning_rate :              0.0925169997034202
      batch rate adapted learning_rate :              9.25169997034202e-05
      fit adapted learning_rate :              9.262146530977477e-06
    epoch : 7 ; learning_rate : 9.262146530977477e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.0660560422217396
    Epoch 7, Loss: 1.4502250729110377, fit: 0.3333333333333333
      exponential decayed learning_rate :              0.0914947228730031
      batch rate adapted learning_rate :              9.14947228730031e-05
      fit adapted learning_rate :              3.5699815661467484e-06
    epoch : 8 ; learning_rate : 3.5699815661467484e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 1.1046469875727245
    Epoch 8, Loss: 1.4428007525505377, fit: 0.25
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.452920461149639_fit_0.252024-01-02192436
  self.fit : 0.25
  self.loss : 1.452920461149639
  current_accuracy : 0.2539
  LR: 0.1, Epochs: 100
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          89
    learning_rate :          0.1
    batch_rate :          0.001
      exponential decayed learning_rate :              0.1
      batch rate adapted learning_rate :              0.0001
      fit adapted learning_rate :              1.001129150390625e-05
    epoch : 0 ; learning_rate : 1.001129150390625e-05 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.059690048968948
    Epoch 0, Loss: 1.434802752982311, fit: 0.3
    Epoch 0, Loss: 1.434802752982311
      exponential decayed learning_rate :              0.09988770354914617
      batch rate adapted learning_rate :              9.988770354914618e-05
      fit adapted learning_rate :              5.758327333078211e-06
    epoch : 1 ; learning_rate : 5.758327333078211e-06 ; fit : 0.3
    batch_size :          60
    best_batch_loss : 1.1256451997110806
    Epoch 1, Loss: 1.4256413034275983, fit: 0.21666666666666667
      exponential decayed learning_rate :              0.09977553320322105
      batch rate adapted learning_rate :              9.977553320322105e-05
      fit adapted learning_rate :              1.4144803472531514e-05
    epoch : 2 ; learning_rate : 1.4144803472531514e-05 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.0830241950063255
    Epoch 2, Loss: 1.4143308422891259, fit: 0.2
      exponential decayed learning_rate :              0.09966348882061334
      batch rate adapted learning_rate :              9.966348882061334e-05
      fit adapted learning_rate :              1.672075879257016e-05
    epoch : 3 ; learning_rate : 1.672075879257016e-05 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.0142533728685132
    Epoch 3, Loss: 1.397431365084045, fit: 0.25
      exponential decayed learning_rate :              0.09955157025987066
      batch rate adapted learning_rate :              9.955157025987067e-05
      fit adapted learning_rate :              9.966397895431693e-06
    epoch : 4 ; learning_rate : 9.966397895431693e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.0411934652261756
    Epoch 4, Loss: 1.3829066161670072, fit: 0.25
      exponential decayed learning_rate :              0.09943977737969956
      batch rate adapted learning_rate :              9.943977737969956e-05
      fit adapted learning_rate :              9.955205984317152e-06
    epoch : 5 ; learning_rate : 9.955205984317152e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.0076679279749972
    Epoch 5, Loss: 1.3722213325347066, fit: 0.26666666666666666
      exponential decayed learning_rate :              0.09932811003896519
      batch rate adapted learning_rate :              9.932811003896519e-05
      fit adapted learning_rate :              8.307753094144412e-06
    epoch : 6 ; learning_rate : 8.307753094144412e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 1.0060949207550902
    Epoch 6, Loss: 1.3625267001761812, fit: 0.25
      exponential decayed learning_rate :              0.09921656809669122
      batch rate adapted learning_rate :              9.921656809669122e-05
      fit adapted learning_rate :              9.932859852331408e-06
    epoch : 7 ; learning_rate : 9.932859852331408e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.0196024544914644
    Epoch 7, Loss: 1.3526987751823152, fit: 0.3
      exponential decayed learning_rate :              0.09910515141205965
      batch rate adapted learning_rate :              9.910515141205966e-05
      fit adapted learning_rate :              5.713214759653926e-06
    epoch : 8 ; learning_rate : 5.713214759653926e-06 ; fit : 0.3
    batch_size :          60
    best_batch_loss : 0.9762689553395582
    Epoch 8, Loss: 1.3445999002846079, fit: 0.4
      exponential decayed learning_rate :              0.09899385984441057
      batch rate adapted learning_rate :              9.899385984441058e-05
      fit adapted learning_rate :              1.6627167089642945e-06
    epoch : 9 ; learning_rate : 1.6627167089642945e-06 ; fit : 0.4
    batch_size :          60
    best_batch_loss : 0.9432161693821043
    Epoch 9, Loss: 1.340760283873229, fit: 0.23333333333333334
      exponential decayed learning_rate :              0.09888269325324206
      batch rate adapted learning_rate :              9.888269325324206e-05
      fit adapted learning_rate :              1.1802470867093846e-05
    epoch : 10 ; learning_rate : 1.1802470867093846e-05 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 0.8308579055482805
    Epoch 10, Loss: 1.3338106658324957, fit: 0.18333333333333332
    Epoch 10, Loss: 1.3338106658324957
      exponential decayed learning_rate :              0.09877165149820999
      batch rate adapted learning_rate :              9.877165149820999e-05
      fit adapted learning_rate :              1.954298742382091e-05
    epoch : 11 ; learning_rate : 1.954298742382091e-05 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 0.9319258364605908
    Epoch 11, Loss: 1.3182932538613803, fit: 0.36666666666666664
      exponential decayed learning_rate :              0.09866073443912776
      batch rate adapted learning_rate :              9.866073443912777e-05
      fit adapted learning_rate :              2.5538954473682145e-06
    epoch : 12 ; learning_rate : 2.5538954473682145e-06 ; fit : 0.36666666666666664
    batch_size :          60
    best_batch_loss : 0.9681883695772674
    Epoch 12, Loss: 1.3077827736198342, fit: 0.3
      exponential decayed learning_rate :              0.09854994193596628
      batch rate adapted learning_rate :              9.854994193596629e-05
      fit adapted learning_rate :              5.681208038224001e-06
    epoch : 13 ; learning_rate : 5.681208038224001e-06 ; fit : 0.3
    batch_size :          60
    best_batch_loss : 0.8758087729749012
    Epoch 13, Loss: 1.3039664545891818, fit: 0.31666666666666665
      exponential decayed learning_rate :              0.09843927384885366
      batch rate adapted learning_rate :              9.843927384885367e-05
      fit adapted learning_rate :              4.679821109658772e-06
    epoch : 14 ; learning_rate : 4.679821109658772e-06 ; fit : 0.31666666666666665
    batch_size :          60
    best_batch_loss : 0.8995167762336209
    Epoch 14, Loss: 1.2992180020481348, fit: 0.2833333333333333
      exponential decayed learning_rate :              0.0983287300380751
      batch rate adapted learning_rate :              9.83287300380751e-05
      fit adapted learning_rate :              6.842551450612421e-06
    epoch : 15 ; learning_rate : 6.842551450612421e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 0.9383586186328249
    Epoch 15, Loss: 1.2940158711227547, fit: 0.31666666666666665
      exponential decayed learning_rate :              0.09821831036407268
      batch rate adapted learning_rate :              9.821831036407269e-05
      fit adapted learning_rate :              4.669316465118936e-06
    epoch : 16 ; learning_rate : 4.669316465118936e-06 ; fit : 0.31666666666666665
    batch_size :          60
    best_batch_loss : 0.9457662006854759
    Epoch 16, Loss: 1.2887008877684303, fit: 0.35
      exponential decayed learning_rate :              0.0981080146874452
      batch rate adapted learning_rate :              9.81080146874452e-05
      fit adapted learning_rate :              3.126160998315168e-06
    epoch : 17 ; learning_rate : 3.126160998315168e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.8663787244505775
    Epoch 17, Loss: 1.2851743495620707, fit: 0.31666666666666665
      exponential decayed learning_rate :              0.09799784286894803
      batch rate adapted learning_rate :              9.799784286894803e-05
      fit adapted learning_rate :              4.658835400018211e-06
    epoch : 18 ; learning_rate : 4.658835400018211e-06 ; fit : 0.31666666666666665
    batch_size :          60
    best_batch_loss : 0.8160551097639369
    Epoch 18, Loss: 1.2816791377460266, fit: 0.26666666666666666
      exponential decayed learning_rate :              0.09788779476949289
      batch rate adapted learning_rate :              9.788779476949289e-05
      fit adapted learning_rate :              8.187285850462761e-06
    epoch : 19 ; learning_rate : 8.187285850462761e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 0.8656833060640853
    Epoch 19, Loss: 1.2759546791688101, fit: 0.21666666666666667
      exponential decayed learning_rate :              0.09777787025014764
      batch rate adapted learning_rate :              9.777787025014764e-05
      fit adapted learning_rate :              1.3861602281132939e-05
    epoch : 20 ; learning_rate : 1.3861602281132939e-05 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 0.8757757048351474
    Epoch 20, Loss: 1.2662338210757975, fit: 0.3
    Epoch 20, Loss: 1.2662338210757975
      exponential decayed learning_rate :              0.09766806917213625
      batch rate adapted learning_rate :              9.766806917213625e-05
      fit adapted learning_rate :              5.630369828316e-06
    epoch : 21 ; learning_rate : 5.630369828316e-06 ; fit : 0.3
    batch_size :          60
    best_batch_loss : 0.9237960496563686
    Epoch 21, Loss: 1.2578802624885241, fit: 0.36666666666666664
      exponential decayed learning_rate :              0.09755839139683846
      batch rate adapted learning_rate :              9.755839139683846e-05
      fit adapted learning_rate :              2.5253606012296257e-06
    epoch : 22 ; learning_rate : 2.5253606012296257e-06 ; fit : 0.36666666666666664
    batch_size :          60
    best_batch_loss : 0.7922518027259013
    Epoch 22, Loss: 1.254452514305651, fit: 0.4
      exponential decayed learning_rate :              0.0974488367857897
      batch rate adapted learning_rate :              9.74488367857897e-05
      fit adapted learning_rate :              1.636766254468009e-06
    epoch : 23 ; learning_rate : 1.636766254468009e-06 ; fit : 0.4
    batch_size :          60
    best_batch_loss : 0.864622303742247
    Epoch 23, Loss: 1.2526617644059206, fit: 0.36666666666666664
      exponential decayed learning_rate :              0.0973394052006809
      batch rate adapted learning_rate :              9.733940520068091e-05
      fit adapted learning_rate :              2.519692005180928e-06
    epoch : 24 ; learning_rate : 2.519692005180928e-06 ; fit : 0.36666666666666664
    batch_size :          60
    best_batch_loss : 0.8754136311342239
    Epoch 24, Loss: 1.2508989682095883, fit: 0.25
      exponential decayed learning_rate :              0.09723009650335829
      batch rate adapted learning_rate :              9.723009650335829e-05
      fit adapted learning_rate :              9.733988390480556e-06
    epoch : 25 ; learning_rate : 9.733988390480556e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 0.8204990805832821
    Epoch 25, Loss: 1.2457860005398438, fit: 0.45
      exponential decayed learning_rate :              0.09712091055582324
      batch rate adapted learning_rate :              9.712091055582324e-05
      fit adapted learning_rate :              8.132316292362255e-07
    epoch : 26 ; learning_rate : 8.132316292362255e-07 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.829454303501291
    Epoch 26, Loss: 1.2414792417666891, fit: 0.31666666666666665
      exponential decayed learning_rate :              0.09701184722023211
      batch rate adapted learning_rate :              9.701184722023212e-05
      fit adapted learning_rate :              4.611960986275812e-06
    epoch : 27 ; learning_rate : 4.611960986275812e-06 ; fit : 0.31666666666666665
    batch_size :          60
    best_batch_loss : 0.7987636045301727
    Epoch 27, Loss: 1.239305736968721, fit: 0.38333333333333336
      exponential decayed learning_rate :              0.09690290635889603
      batch rate adapted learning_rate :              9.690290635889603e-05
      fit adapted learning_rate :              2.0264719293627416e-06
    epoch : 28 ; learning_rate : 2.0264719293627416e-06 ; fit : 0.38333333333333336
    batch_size :          60
    best_batch_loss : 0.8662689970079015
    Epoch 28, Loss: 1.2365570237307721, fit: 0.48333333333333334
      exponential decayed learning_rate :              0.09679408783428077
      batch rate adapted learning_rate :              9.679408783428078e-05
      fit adapted learning_rate :              4.915100236669271e-07
    epoch : 29 ; learning_rate : 4.915100236669271e-07 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.8625591915376941
    Epoch 29, Loss: 1.2355521141598471, fit: 0.31666666666666665
      exponential decayed learning_rate :              0.09668539150900651
      batch rate adapted learning_rate :              9.668539150900652e-05
      fit adapted learning_rate :              4.596441221968043e-06
    epoch : 30 ; learning_rate : 4.596441221968043e-06 ; fit : 0.31666666666666665
    batch_size :          60
    best_batch_loss : 0.805515476901889
    Epoch 30, Loss: 1.233525226543388, fit: 0.38333333333333336
    Epoch 30, Loss: 1.233525226543388
      exponential decayed learning_rate :              0.09657681724584775
      batch rate adapted learning_rate :              9.657681724584776e-05
      fit adapted learning_rate :              2.0196526247733034e-06
    epoch : 31 ; learning_rate : 2.0196526247733034e-06 ; fit : 0.38333333333333336
    batch_size :          60
    best_batch_loss : 0.7789105245531787
    Epoch 31, Loss: 1.230896467306338, fit: 0.35
      exponential decayed learning_rate :              0.09646836490773307
      batch rate adapted learning_rate :              9.646836490773307e-05
      fit adapted learning_rate :              3.073914408588915e-06
    epoch : 32 ; learning_rate : 3.073914408588915e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.7959086911160647
    Epoch 32, Loss: 1.2288943135744146, fit: 0.31666666666666665
      exponential decayed learning_rate :              0.09636003435774494
      batch rate adapted learning_rate :              9.636003435774494e-05
      fit adapted learning_rate :              4.580973683402183e-06
    epoch : 33 ; learning_rate : 4.580973683402183e-06 ; fit : 0.31666666666666665
    batch_size :          60
    best_batch_loss : 0.817565844815133
    Epoch 33, Loss: 1.2259063557697434, fit: 0.38333333333333336
      exponential decayed learning_rate :              0.09625182545911964
      batch rate adapted learning_rate :              9.625182545911965e-05
      fit adapted learning_rate :              2.012856267906165e-06
    epoch : 34 ; learning_rate : 2.012856267906165e-06 ; fit : 0.38333333333333336
    batch_size :          60
    best_batch_loss : 0.8102775844069037
    Epoch 34, Loss: 1.223368645095848, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.09614373807524701
      batch rate adapted learning_rate :              9.614373807524702e-05
      fit adapted learning_rate :              2.863485469345624e-07
    epoch : 35 ; learning_rate : 2.863485469345624e-07 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.8843266082688149
    Epoch 35, Loss: 1.2224837917074551, fit: 0.36666666666666664
      exponential decayed learning_rate :              0.09603577206967029
      batch rate adapted learning_rate :              9.603577206967029e-05
      fit adapted learning_rate :              2.4859466379154883e-06
    epoch : 36 ; learning_rate : 2.4859466379154883e-06 ; fit : 0.36666666666666664
    batch_size :          60
    best_batch_loss : 0.8281511988070211
    Epoch 36, Loss: 1.2214316577179825, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.09592792730608596
      batch rate adapted learning_rate :              9.592792730608595e-05
      fit adapted learning_rate :              6.279641983732731e-07
    epoch : 37 ; learning_rate : 6.279641983732731e-07 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.8564422997960441
    Epoch 37, Loss: 1.220237165144818, fit: 0.36666666666666664
      exponential decayed learning_rate :              0.09582020364834357
      batch rate adapted learning_rate :              9.582020364834356e-05
      fit adapted learning_rate :              2.4803665131277253e-06
    epoch : 38 ; learning_rate : 2.4803665131277253e-06 ; fit : 0.36666666666666664
    batch_size :          60
    best_batch_loss : 0.7799606156290897
    Epoch 38, Loss: 1.219090425176514, fit: 0.3333333333333333
      exponential decayed learning_rate :              0.09571260096044554
      batch rate adapted learning_rate :              9.571260096044555e-05
      fit adapted learning_rate :              3.734556598974864e-06
    epoch : 39 ; learning_rate : 3.734556598974864e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 0.842107545127861
    Epoch 39, Loss: 1.2166942692489726, fit: 0.25
      exponential decayed learning_rate :              0.09560511910654707
      batch rate adapted learning_rate :              9.560511910654707e-05
      fit adapted learning_rate :              9.571307166413197e-06
    epoch : 40 ; learning_rate : 9.571307166413197e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 0.8661645092816674
    Epoch 40, Loss: 1.2117752129954897, fit: 0.43333333333333335
    Epoch 40, Loss: 1.2117752129954897
      exponential decayed learning_rate :              0.09549775795095582
      batch rate adapted learning_rate :              9.549775795095582e-05
      fit adapted learning_rate :              1.0153470440865674e-06
    epoch : 41 ; learning_rate : 1.0153470440865674e-06 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.8163069128399653
    Epoch 41, Loss: 1.2078285066173404, fit: 0.3
      exponential decayed learning_rate :              0.0953905173581319
      batch rate adapted learning_rate :              9.53905173581319e-05
      fit adapted learning_rate :              5.499073498566759e-06
    epoch : 42 ; learning_rate : 5.499073498566759e-06 ; fit : 0.3
    batch_size :          60
    best_batch_loss : 0.753942618287553
    Epoch 42, Loss: 1.205433145003385, fit: 0.5166666666666667
      exponential decayed learning_rate :              0.09528339719268758
      batch rate adapted learning_rate :              9.528339719268758e-05
      fit adapted learning_rate :              2.8378616100573073e-07
    epoch : 43 ; learning_rate : 2.8378616100573073e-07 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.8441690514987994
    Epoch 43, Loss: 1.2032915329583813, fit: 0.4166666666666667
      exponential decayed learning_rate :              0.09517639731938722
      batch rate adapted learning_rate :              9.517639731938723e-05
      fit adapted learning_rate :              1.2760380163791908e-06
    epoch : 44 ; learning_rate : 1.2760380163791908e-06 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.892987492892856
    Epoch 44, Loss: 1.2027211369650253, fit: 0.3333333333333333
      exponential decayed learning_rate :              0.095069517603147
      batch rate adapted learning_rate :              9.5069517603147e-05
      fit adapted learning_rate :              3.7094644880971883e-06
    epoch : 45 ; learning_rate : 3.7094644880971883e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 0.7952728996874752
    Epoch 45, Loss: 1.2009335781166801, fit: 0.35
      exponential decayed learning_rate :              0.0949627579090348
      batch rate adapted learning_rate :              9.49627579090348e-05
      fit adapted learning_rate :              3.0259390225502123e-06
    epoch : 46 ; learning_rate : 3.0259390225502123e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.8299301100456103
    Epoch 46, Loss: 1.1984008269465387, fit: 0.36666666666666664
      exponential decayed learning_rate :              0.09485611810227002
      batch rate adapted learning_rate :              9.485611810227002e-05
      fit adapted learning_rate :              2.455410549633354e-06
    epoch : 47 ; learning_rate : 2.455410549633354e-06 ; fit : 0.36666666666666664
    batch_size :          60
    best_batch_loss : 0.8259411154409716
    Epoch 47, Loss: 1.196426226281249, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.09474959804822343
      batch rate adapted learning_rate :              9.474959804822344e-05
      fit adapted learning_rate :              6.202506095507773e-07
    epoch : 48 ; learning_rate : 6.202506095507773e-07 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.8444938857055031
    Epoch 48, Loss: 1.195292229472996, fit: 0.38333333333333336
      exponential decayed learning_rate :              0.09464319761241699
      batch rate adapted learning_rate :              9.4643197612417e-05
      fit adapted learning_rate :              1.9792160057239264e-06
    epoch : 49 ; learning_rate : 1.9792160057239264e-06 ; fit : 0.38333333333333336
    batch_size :          60
    best_batch_loss : 0.8087936170748568
    Epoch 49, Loss: 1.1943308623688966, fit: 0.4
      exponential decayed learning_rate :              0.09453691666052365
      batch rate adapted learning_rate :              9.453691666052365e-05
      fit adapted learning_rate :              1.5878571781368203e-06
    epoch : 50 ; learning_rate : 1.5878571781368203e-06 ; fit : 0.4
    batch_size :          60
    best_batch_loss : 0.834422878912463
    Epoch 50, Loss: 1.1930411810594796, fit: 0.2833333333333333
    Epoch 50, Loss: 1.1930411810594796
      exponential decayed learning_rate :              0.09443075505836723
      batch rate adapted learning_rate :              9.443075505836723e-05
      fit adapted learning_rate :              6.571297114860063e-06
    epoch : 51 ; learning_rate : 6.571297114860063e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 0.8073774310780591
    Epoch 51, Loss: 1.1900751785991945, fit: 0.31666666666666665
      exponential decayed learning_rate :              0.09432471267192219
      batch rate adapted learning_rate :              9.43247126719222e-05
      fit adapted learning_rate :              4.484214117653208e-06
    epoch : 52 ; learning_rate : 4.484214117653208e-06 ; fit : 0.31666666666666665
    batch_size :          60
    best_batch_loss : 0.7759340090658582
    Epoch 52, Loss: 1.1860760238515682, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.09421878936731354
      batch rate adapted learning_rate :              9.421878936731354e-05
      fit adapted learning_rate :              6.167758253335224e-07
    epoch : 53 ; learning_rate : 6.167758253335224e-07 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.7678527297610297
    Epoch 53, Loss: 1.1842279261648458, fit: 0.35
      exponential decayed learning_rate :              0.09411298501081657
      batch rate adapted learning_rate :              9.411298501081657e-05
      fit adapted learning_rate :              2.998861449934985e-06
    epoch : 54 ; learning_rate : 2.998861449934985e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.8233473165559013
    Epoch 54, Loss: 1.182956111804716, fit: 0.38333333333333336
      exponential decayed learning_rate :              0.09400729946885682
      batch rate adapted learning_rate :              9.400729946885681e-05
      fit adapted learning_rate :              1.965917852074273e-06
    epoch : 55 ; learning_rate : 1.965917852074273e-06 ; fit : 0.38333333333333336
    batch_size :          60
    best_batch_loss : 0.7134602954484875
    Epoch 55, Loss: 1.1811570244851655, fit: 0.48333333333333334
      exponential decayed learning_rate :              0.09390173260800974
      batch rate adapted learning_rate :              9.390173260800975e-05
      fit adapted learning_rate :              4.7682295323188593e-07
    epoch : 56 ; learning_rate : 4.7682295323188593e-07 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.7095905881382107
    Epoch 56, Loss: 1.180275247680625, fit: 0.3333333333333333
      exponential decayed learning_rate :              0.09379628429500067
      batch rate adapted learning_rate :              9.379628429500068e-05
      fit adapted learning_rate :              3.659784907715317e-06
    epoch : 57 ; learning_rate : 3.659784907715317e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 0.772612972419827
    Epoch 57, Loss: 1.1788311165179335, fit: 0.3333333333333333
      exponential decayed learning_rate :              0.0936909543967046
      batch rate adapted learning_rate :              9.36909543967046e-05
      fit adapted learning_rate :              3.655675099155068e-06
    epoch : 58 ; learning_rate : 3.655675099155068e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 0.8173593133211677
    Epoch 58, Loss: 1.1762488369301127, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.093585742780146
      batch rate adapted learning_rate :              9.3585742780146e-05
      fit adapted learning_rate :              6.126317704810168e-07
    epoch : 59 ; learning_rate : 6.126317704810168e-07 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.8075303486671708
    Epoch 59, Loss: 1.1747029685644979, fit: 0.45
      exponential decayed learning_rate :              0.09348064931249869
      batch rate adapted learning_rate :              9.34806493124987e-05
      fit adapted learning_rate :              7.827502883508069e-07
    epoch : 60 ; learning_rate : 7.827502883508069e-07 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.7660171512504826
    Epoch 60, Loss: 1.1742159502800409, fit: 0.45
    Epoch 60, Loss: 1.1742159502800409
      exponential decayed learning_rate :              0.09337567386108563
      batch rate adapted learning_rate :              9.337567386108563e-05
      fit adapted learning_rate :              7.818712875579406e-07
    epoch : 61 ; learning_rate : 7.818712875579406e-07 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.7465873672710395
    Epoch 61, Loss: 1.1736740878159233, fit: 0.3333333333333333
      exponential decayed learning_rate :              0.09327081629337876
      batch rate adapted learning_rate :              9.327081629337876e-05
      fit adapted learning_rate :              3.639281964807954e-06
    epoch : 62 ; learning_rate : 3.639281964807954e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 0.7576448411045618
    Epoch 62, Loss: 1.1720843662563392, fit: 0.45
      exponential decayed learning_rate :              0.09316607647699889
      batch rate adapted learning_rate :              9.316607647699888e-05
      fit adapted learning_rate :              7.801162461238249e-07
    epoch : 63 ; learning_rate : 7.801162461238249e-07 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.8302446235191628
    Epoch 63, Loss: 1.170580701722229, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.09306145427971543
      batch rate adapted learning_rate :              9.306145427971543e-05
      fit adapted learning_rate :              6.091996687236351e-07
    epoch : 64 ; learning_rate : 6.091996687236351e-07 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.7920166288299965
    Epoch 64, Loss: 1.1700962731972775, fit: 0.31666666666666665
      exponential decayed learning_rate :              0.09295694956944633
      batch rate adapted learning_rate :              9.295694956944633e-05
      fit adapted learning_rate :              4.4191904092316375e-06
    epoch : 65 ; learning_rate : 4.4191904092316375e-06 ; fit : 0.31666666666666665
    batch_size :          60
    best_batch_loss : 0.7991451574828051
    Epoch 65, Loss: 1.1684016631012597, fit: 0.45
      exponential decayed learning_rate :              0.09285256221425785
      batch rate adapted learning_rate :              9.285256221425785e-05
      fit adapted learning_rate :              7.774910677434067e-07
    epoch : 66 ; learning_rate : 7.774910677434067e-07 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.7698316514642687
    Epoch 66, Loss: 1.1665667681149237, fit: 0.38333333333333336
      exponential decayed learning_rate :              0.09274829208236438
      batch rate adapted learning_rate :              9.274829208236437e-05
      fit adapted learning_rate :              1.939588991326403e-06
    epoch : 67 ; learning_rate : 1.939588991326403e-06 ; fit : 0.38333333333333336
    batch_size :          60
    best_batch_loss : 0.8003057387571731
    Epoch 67, Loss: 1.1656504287384792, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.09264413904212832
      batch rate adapted learning_rate :              9.264413904212832e-05
      fit adapted learning_rate :              6.064678362323088e-07
    epoch : 68 ; learning_rate : 6.064678362323088e-07 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.6596161232320339
    Epoch 68, Loss: 1.1647686651424052, fit: 0.35
      exponential decayed learning_rate :              0.0925401029620599
      batch rate adapted learning_rate :              9.25401029620599e-05
      fit adapted learning_rate :              2.948742379322476e-06
    epoch : 69 ; learning_rate : 2.948742379322476e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.7697843946157872
    Epoch 69, Loss: 1.163527191074241, fit: 0.4666666666666667
      exponential decayed learning_rate :              0.09243618371081702
      batch rate adapted learning_rate :              9.243618371081702e-05
      fit adapted learning_rate :              6.051065173068236e-07
    epoch : 70 ; learning_rate : 6.051065173068236e-07 ; fit : 0.4666666666666667
    batch_size :          60
    best_batch_loss : 0.7649237354707453
    Epoch 70, Loss: 1.1623520507701546, fit: 0.45
    Epoch 70, Loss: 1.1623520507701546
      exponential decayed learning_rate :              0.09233238115720503
      batch rate adapted learning_rate :              9.233238115720504e-05
      fit adapted learning_rate :              7.731353869110927e-07
    epoch : 71 ; learning_rate : 7.731353869110927e-07 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.764495985562031
    Epoch 71, Loss: 1.1618909162762021, fit: 0.4
      exponential decayed learning_rate :              0.09222869517017664
      batch rate adapted learning_rate :              9.222869517017664e-05
      fit adapted learning_rate :              1.5490879206695136e-06
    epoch : 72 ; learning_rate : 1.5490879206695136e-06 ; fit : 0.4
    batch_size :          60
    best_batch_loss : 0.8123806332678559
    Epoch 72, Loss: 1.16111416069929, fit: 0.45
      exponential decayed learning_rate :              0.09212512561883171
      batch rate adapted learning_rate :              9.212512561883172e-05
      fit adapted learning_rate :              7.713999546733287e-07
    epoch : 73 ; learning_rate : 7.713999546733287e-07 ; fit : 0.45
    batch_size :          60
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0009000000000000001
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.453225606335326
    Epoch 0, Loss: 1.717762301202198, fit: 0.08333333333333333
    Epoch 0, Loss: 1.717762301202198
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.772917766832314_fit_0.08333333333333333_2024-01-02_194138
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5201602747781309
    Epoch 0, Loss: 1.8167572133189085, fit: 0.1
    Epoch 0, Loss: 1.8167572133189085
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.781453381862332_fit_0.1_2024-01-02_194250
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4690549794672914
    Epoch 0, Loss: 1.7397120183973007, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7397120183973007
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.775541822798391_fit_0.08333333333333333_2024-01-02_194444
  self.fit : 0.08333333333333333
  self.loss : 1.775541822798391
  current_accuracy : 0.1032
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              5.0377339618129914e-06
    epoch : 0 ; learning_rate : 5.0377339618129914e-06 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4549073336844007
    Epoch 0, Loss: 1.7375837954206574, fit: 0.11666666666666667
    Epoch 0, Loss: 1.7375837954206574
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.4187396519520105e-06
    epoch : 1 ; learning_rate : 4.4187396519520105e-06 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4361087120336564
    Epoch 1, Loss: 1.7337426615215867, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              5.173659659592089e-06
    epoch : 2 ; learning_rate : 5.173659659592089e-06 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4610099319925631
    Epoch 2, Loss: 1.729491302100637, fit: 0.15
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.97695938322368e-06
    epoch : 3 ; learning_rate : 3.97695938322368e-06 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4254079526659411
    Epoch 3, Loss: 1.7241321121028832, fit: 0.05
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              5.1498867535928275e-06
    epoch : 4 ; learning_rate : 5.1498867535928275e-06 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4425523856698619
    Epoch 4, Loss: 1.7190536696259673, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.589078420424242e-06
    epoch : 5 ; learning_rate : 3.589078420424242e-06 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.42772785184779
    Epoch 5, Loss: 1.7140785771455807, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.729429588750871e-06
    epoch : 6 ; learning_rate : 4.729429588750871e-06 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4249276113886153
    Epoch 6, Loss: 1.7093433300827956, fit: 0.15
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.05018395277246e-06
    epoch : 7 ; learning_rate : 4.05018395277246e-06 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.439707865022403
    Epoch 7, Loss: 1.7047318750245262, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.2444614421320375e-06
    epoch : 8 ; learning_rate : 4.2444614421320375e-06 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4152092234222229
    Epoch 8, Loss: 1.7002270951626437, fit: 0.08333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.729691703767017_fit_0.08333333333333333_2024-01-02_194550
  self.fit : 0.08333333333333333
  self.loss : 1.729691703767017
  current_accuracy : 0.1257
  Epochs: 100
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          89
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.657454454487768e-06
    epoch : 0 ; learning_rate : 4.657454454487768e-06 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4387752905800515
    Epoch 0, Loss: 1.6954533740087008, fit: 0.11666666666666667
    Epoch 0, Loss: 1.6954533740087008
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.845701101259953e-06
    epoch : 1 ; learning_rate : 3.845701101259953e-06 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4028796246989372
    Epoch 1, Loss: 1.6904285757617117, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.7669820689590067e-06
    epoch : 2 ; learning_rate : 3.7669820689590067e-06 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.3328103806239417
    Epoch 2, Loss: 1.6859507318744913, fit: 0.1
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              5.120603365680919e-06
    epoch : 3 ; learning_rate : 5.120603365680919e-06 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.364067856714602
    Epoch 3, Loss: 1.6806373567747162, fit: 0.1
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.962526511058894e-06
    epoch : 4 ; learning_rate : 4.962526511058894e-06 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.3096137306681106
    Epoch 4, Loss: 1.6750249054510813, fit: 0.1
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.8424544798232054e-06
    epoch : 5 ; learning_rate : 4.8424544798232054e-06 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.3281304732037316
    Epoch 5, Loss: 1.66927652409198, fit: 0.25
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.8583265224412365e-06
    epoch : 6 ; learning_rate : 2.8583265224412365e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.2846762488504122
    Epoch 6, Loss: 1.664838665334039, fit: 0.1
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.776300358198462e-06
    epoch : 7 ; learning_rate : 4.776300358198462e-06 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.3348440053642459
    Epoch 7, Loss: 1.6604902549775618, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.4533866176412085e-06
    epoch : 8 ; learning_rate : 3.4533866176412085e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.3525826529621532
    Epoch 8, Loss: 1.655917953598668, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.363669948972068e-06
    epoch : 9 ; learning_rate : 3.363669948972068e-06 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.2433861197216662
    Epoch 9, Loss: 1.6520845826482409, fit: 0.26666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.5374394848581135e-06
    epoch : 10 ; learning_rate : 2.5374394848581135e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 1.3167090957649588
    Epoch 10, Loss: 1.6489154887152342, fit: 0.11666666666666667
    Epoch 10, Loss: 1.6489154887152342
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.255365028342026e-06
    epoch : 11 ; learning_rate : 4.255365028342026e-06 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.3227049182358412
    Epoch 11, Loss: 1.6451018161416067, fit: 0.15
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.63523501373532e-06
    epoch : 12 ; learning_rate : 3.63523501373532e-06 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.3548192453647399
    Epoch 12, Loss: 1.640821414858363, fit: 0.15
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.0799533816370595e-06
    epoch : 13 ; learning_rate : 4.0799533816370595e-06 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.3315522294779243
    Epoch 13, Loss: 1.636357429806146, fit: 0.1
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.86370953824228e-06
    epoch : 14 ; learning_rate : 4.86370953824228e-06 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.3654233090085925
    Epoch 14, Loss: 1.6311484143088624, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.9750297901860632e-06
    epoch : 15 ; learning_rate : 2.9750297901860632e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.2713323745767535
    Epoch 15, Loss: 1.6266263203705629, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.316569156768185e-06
    epoch : 16 ; learning_rate : 4.316569156768185e-06 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.2527536157848607
    Epoch 16, Loss: 1.6223443467540737, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.3778434980500572e-06
    epoch : 17 ; learning_rate : 3.3778434980500572e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.319665877959373
    Epoch 17, Loss: 1.6179140221942636, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.081187190049067e-06
    epoch : 18 ; learning_rate : 4.081187190049067e-06 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.330854233341941
    Epoch 18, Loss: 1.6137188099107356, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.459911509953758e-06
    epoch : 19 ; learning_rate : 3.459911509953758e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.3367843320625103
    Epoch 19, Loss: 1.6094812001295373, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.00026087901575e-06
    epoch : 20 ; learning_rate : 4.00026087901575e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.2932918460683254
    Epoch 20, Loss: 1.60524092252151, fit: 0.1
    Epoch 20, Loss: 1.60524092252151
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.191409807235003e-06
    epoch : 21 ; learning_rate : 4.191409807235003e-06 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.2531099545113669
    Epoch 21, Loss: 1.6008018233868073, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.573795781468967e-06
    epoch : 22 ; learning_rate : 4.573795781468967e-06 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.2949851951289915
    Epoch 22, Loss: 1.5959700789857247, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.1678978673978084e-06
    epoch : 23 ; learning_rate : 3.1678978673978084e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.2433233039525198
    Epoch 23, Loss: 1.591750867305837, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.298732460278284e-06
    epoch : 24 ; learning_rate : 4.298732460278284e-06 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.2497104976054638
    Epoch 24, Loss: 1.5876082290520839, fit: 0.21666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.1027398086474766e-06
    epoch : 25 ; learning_rate : 3.1027398086474766e-06 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.2525063744718694
    Epoch 25, Loss: 1.5836143626212023, fit: 0.1
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.390715157981141e-06
    epoch : 26 ; learning_rate : 4.390715157981141e-06 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.1637988983788068
    Epoch 26, Loss: 1.579602976178521, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.339976557360924e-06
    epoch : 27 ; learning_rate : 4.339976557360924e-06 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.216820945519729
    Epoch 27, Loss: 1.5749054139003118, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.5032361347929793e-06
    epoch : 28 ; learning_rate : 3.5032361347929793e-06 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.2797996700243919
    Epoch 28, Loss: 1.5709000456713835, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.7243909744086103e-06
    epoch : 29 ; learning_rate : 3.7243909744086103e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.2713679437626817
    Epoch 29, Loss: 1.567243107636096, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.240426403323076e-06
    epoch : 30 ; learning_rate : 4.240426403323076e-06 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.236894770322528
    Epoch 30, Loss: 1.5632998172520605, fit: 0.2
    Epoch 30, Loss: 1.5632998172520605
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.3517307811102797e-06
    epoch : 31 ; learning_rate : 3.3517307811102797e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.180339365699964
    Epoch 31, Loss: 1.5595936363148795, fit: 0.15
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.711586447642729e-06
    epoch : 32 ; learning_rate : 3.711586447642729e-06 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.2746015924261844
    Epoch 32, Loss: 1.5561618680529166, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.4926471034601563e-06
    epoch : 33 ; learning_rate : 3.4926471034601563e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.236883243290378
    Epoch 33, Loss: 1.552656970156981, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.1193667277134942e-06
    epoch : 34 ; learning_rate : 3.1193667277134942e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.2397548368483222
    Epoch 34, Loss: 1.5492973974233017, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.7086253587180203e-06
    epoch : 35 ; learning_rate : 3.7086253587180203e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.1843251674800077
    Epoch 35, Loss: 1.5459753054833942, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.877821904411709e-06
    epoch : 36 ; learning_rate : 4.877821904411709e-06 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.1673876560881027
    Epoch 36, Loss: 1.5416443744107398, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.385561297857273e-06
    epoch : 37 ; learning_rate : 3.385561297857273e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.1957979818118905
    Epoch 37, Loss: 1.5374090635005313, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.8591952332585988e-06
    epoch : 38 ; learning_rate : 2.8591952332585988e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.2524558708497044
    Epoch 38, Loss: 1.534292895784233, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.2548885158132625e-06
    epoch : 39 ; learning_rate : 3.2548885158132625e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.154137732969072
    Epoch 39, Loss: 1.531139900291207, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.821309877276798e-06
    epoch : 40 ; learning_rate : 3.821309877276798e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.163331177509986
    Epoch 40, Loss: 1.5276350118308595, fit: 0.16666666666666666
    Epoch 40, Loss: 1.5276350118308595
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.556381335730166e-06
    epoch : 41 ; learning_rate : 3.556381335730166e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.203108533577716
    Epoch 41, Loss: 1.5238889242407152, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.939969645630402e-06
    epoch : 42 ; learning_rate : 2.939969645630402e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.2156782717798085
    Epoch 42, Loss: 1.5206092140333916, fit: 0.25
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.977551617806872e-06
    epoch : 43 ; learning_rate : 2.977551617806872e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.1599996447109717
    Epoch 43, Loss: 1.5176408794660698, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.2637020089206904e-06
    epoch : 44 ; learning_rate : 3.2637020089206904e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.1865264623045162
    Epoch 44, Loss: 1.5144321124470708, fit: 0.2833333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.1568640591700857e-06
    epoch : 45 ; learning_rate : 2.1568640591700857e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 1.1749372386924477
    Epoch 45, Loss: 1.511732303580244, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.896272371466177e-06
    epoch : 46 ; learning_rate : 2.896272371466177e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.07109095840115
    Epoch 46, Loss: 1.5091226175429393, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.838659471708006e-06
    epoch : 47 ; learning_rate : 3.838659471708006e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.1384958170953448
    Epoch 47, Loss: 1.50585957540373, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.5041771282309036e-06
    epoch : 48 ; learning_rate : 3.5041771282309036e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.18137471241713
    Epoch 48, Loss: 1.5021708228800752, fit: 0.2833333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.253094995388767e-06
    epoch : 49 ; learning_rate : 2.253094995388767e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 1.106227363695937
    Epoch 49, Loss: 1.4993993308541174, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.1960661001435107e-06
    epoch : 50 ; learning_rate : 3.1960661001435107e-06 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.0689119748715286
    Epoch 50, Loss: 1.4968032391483816, fit: 0.2
    Epoch 50, Loss: 1.4968032391483816
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.378097956523416e-06
    epoch : 51 ; learning_rate : 3.378097956523416e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.1427528294723603
    Epoch 51, Loss: 1.4936267480181784, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.7991584354374852e-06
    epoch : 52 ; learning_rate : 2.7991584354374852e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.1684617047821155
    Epoch 52, Loss: 1.490686889143549, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.5551636075485602e-06
    epoch : 53 ; learning_rate : 3.5551636075485602e-06 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.1480952995677436
    Epoch 53, Loss: 1.4876063156243644, fit: 0.25
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.6895146784874153e-06
    epoch : 54 ; learning_rate : 2.6895146784874153e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.1447457547339568
    Epoch 54, Loss: 1.484615941942808, fit: 0.21666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.058868905337588e-06
    epoch : 55 ; learning_rate : 3.058868905337588e-06 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.1284572731086058
    Epoch 55, Loss: 1.4818560930943836, fit: 0.15
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.994103596586872e-06
    epoch : 56 ; learning_rate : 3.994103596586872e-06 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.0920350633048868
    Epoch 56, Loss: 1.4784790907257797, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.9298773391575902e-06
    epoch : 57 ; learning_rate : 2.9298773391575902e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.1403402980341473
    Epoch 57, Loss: 1.4750969707699082, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.345560583265559e-06
    epoch : 58 ; learning_rate : 3.345560583265559e-06 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.152149373612984
    Epoch 58, Loss: 1.4719740087814601, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.811910179350099e-06
    epoch : 59 ; learning_rate : 2.811910179350099e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.1470062995886485
    Epoch 59, Loss: 1.4690075509688945, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.459144580932909e-06
    epoch : 60 ; learning_rate : 3.459144580932909e-06 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.144580208858837
    Epoch 60, Loss: 1.465963693087602, fit: 0.13333333333333333
    Epoch 60, Loss: 1.465963693087602
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.634662267138576e-06
    epoch : 61 ; learning_rate : 3.634662267138576e-06 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.125446943891402
    Epoch 61, Loss: 1.462480123565483, fit: 0.26666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.349179105151695e-06
    epoch : 62 ; learning_rate : 2.349179105151695e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 1.1232297957134383
    Epoch 62, Loss: 1.459527627841648, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.742941515657928e-06
    epoch : 63 ; learning_rate : 2.742941515657928e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.131520916970228
    Epoch 63, Loss: 1.4570348479665027, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.853898009047385e-06
    epoch : 64 ; learning_rate : 2.853898009047385e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.0639837417822569
    Epoch 64, Loss: 1.4543087202762024, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.277163345515585e-06
    epoch : 65 ; learning_rate : 3.277163345515585e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.1310769518186206
    Epoch 65, Loss: 1.4513406049949535, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.091669701885744e-06
    epoch : 66 ; learning_rate : 4.091669701885744e-06 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.09990360347228
    Epoch 66, Loss: 1.4477659330852046, fit: 0.25
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.6030252729895836e-06
    epoch : 67 ; learning_rate : 2.6030252729895836e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.1096468193074174
    Epoch 67, Loss: 1.4445235602819766, fit: 0.2833333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.3650988786213986e-06
    epoch : 68 ; learning_rate : 2.3650988786213986e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 1.1124063182467456
    Epoch 68, Loss: 1.4421255262428918, fit: 0.26666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.3974369648904798e-06
    epoch : 69 ; learning_rate : 2.3974369648904798e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 1.1147914276600963
    Epoch 69, Loss: 1.4398372581175234, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.393018763634363e-06
    epoch : 70 ; learning_rate : 3.393018763634363e-06 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.1033235237989063
    Epoch 70, Loss: 1.4370608009218606, fit: 0.16666666666666666
    Epoch 70, Loss: 1.4370608009218606
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.3430369738549275e-06
    epoch : 71 ; learning_rate : 3.3430369738549275e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.0691164986091897
    Epoch 71, Loss: 1.4339032347179943, fit: 0.31666666666666665
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              1.9318086263022e-06
    epoch : 72 ; learning_rate : 1.9318086263022e-06 ; fit : 0.31666666666666665
    batch_size :          60
    best_batch_loss : 1.096034404439531
    Epoch 72, Loss: 1.431298353455544, fit: 0.35
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              1.7773975893687236e-06
    epoch : 73 ; learning_rate : 1.7773975893687236e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 1.0755012717906658
    Epoch 73, Loss: 1.4295924217014497, fit: 0.3
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.2877139908304005e-06
    epoch : 74 ; learning_rate : 2.2877139908304005e-06 ; fit : 0.3
    batch_size :          60
    best_batch_loss : 1.0786205397860376
    Epoch 74, Loss: 1.4276547639539803, fit: 0.3
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.23365420502654e-06
    epoch : 75 ; learning_rate : 2.23365420502654e-06 ; fit : 0.3
    batch_size :          60
    best_batch_loss : 1.0926855120666012
    Epoch 75, Loss: 1.4255616877263144, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.5131308604144543e-06
    epoch : 76 ; learning_rate : 2.5131308604144543e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.1132446576216093
    Epoch 76, Loss: 1.4233297624988548, fit: 0.25
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.4599624354349056e-06
    epoch : 77 ; learning_rate : 2.4599624354349056e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.0671602067910753
    Epoch 77, Loss: 1.4210556360024906, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.3008670368764427e-06
    epoch : 78 ; learning_rate : 3.3008670368764427e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.1125639373670775
    Epoch 78, Loss: 1.418406614834919, fit: 0.36666666666666664
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              1.4344030807570301e-06
    epoch : 79 ; learning_rate : 1.4344030807570301e-06 ; fit : 0.36666666666666664
    batch_size :          60
    best_batch_loss : 1.0224038191555485
    Epoch 79, Loss: 1.4162068849876088, fit: 0.2833333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.3222777788256465e-06
    epoch : 80 ; learning_rate : 2.3222777788256465e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 1.043305969486725
    Epoch 80, Loss: 1.414484236885987, fit: 0.23333333333333334
    Epoch 80, Loss: 1.414484236885987
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.754615735949267e-06
    epoch : 81 ; learning_rate : 2.754615735949267e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.0674515902326152
    Epoch 81, Loss: 1.4121522481724595, fit: 0.2833333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.2744357767234114e-06
    epoch : 82 ; learning_rate : 2.2744357767234114e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 1.032067517966202
    Epoch 82, Loss: 1.4098541398292677, fit: 0.2833333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.1006213387466015e-06
    epoch : 83 ; learning_rate : 2.1006213387466015e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 1.0755324013696048
    Epoch 83, Loss: 1.407821731511738, fit: 0.25
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.736463798108166e-06
    epoch : 84 ; learning_rate : 2.736463798108166e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 0.995024422515252
    Epoch 84, Loss: 1.4056096020002014, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.752049903415519e-06
    epoch : 85 ; learning_rate : 2.752049903415519e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 0.9862139987107933
    Epoch 85, Loss: 1.4030889502737625, fit: 0.25
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.8151345788004663e-06
    epoch : 86 ; learning_rate : 2.8151345788004663e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.097094180213697
    Epoch 86, Loss: 1.4005103811254358, fit: 0.25
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.7207049000489204e-06
    epoch : 87 ; learning_rate : 2.7207049000489204e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.01712472135207
    Epoch 87, Loss: 1.3979329026962874, fit: 0.4
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              1.430712873544999e-06
    epoch : 88 ; learning_rate : 1.430712873544999e-06 ; fit : 0.4
    batch_size :          60
    best_batch_loss : 1.0535437864128716
    Epoch 88, Loss: 1.3960037002369525, fit: 0.26666666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.4197921845899077_fit_0.26666666666666666_2024-01-02_195617
  self.fit : 0.26666666666666666
  self.loss : 1.4197921845899077
  current_accuracy : 0.2753
   Accuracy mean: 0.1032
   Accuracy mean: 0.1257
   Accuracy mean: 0.2753
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4927902608011299
    Epoch 0, Loss: 1.759182364745043, fit: 0.05
    Epoch 0, Loss: 1.759182364745043
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.864382308394407_fit_0.05_2024-01-02_195624
  self.fit : 0.05
  self.loss : 1.864382308394407
  current_accuracy : 0.1018
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.48044634834237e-07
    epoch : 0 ; learning_rate : 6.48044634834237e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4824413536915289
    Epoch 0, Loss: 1.7587877780869035, fit: 0.06666666666666667
    Epoch 0, Loss: 1.7587877780869035
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.079475537879607e-07
    epoch : 1 ; learning_rate : 6.079475537879607e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.3707999074022765
    Epoch 1, Loss: 1.7581873729268693, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              3.8400319026416695e-07
    epoch : 2 ; learning_rate : 3.8400319026416695e-07 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.4529020334009992
    Epoch 2, Loss: 1.7577991411036786, fit: 0.15
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.651121275732065e-07
    epoch : 3 ; learning_rate : 4.651121275732065e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4987230538672696
    Epoch 3, Loss: 1.7574958225793678, fit: 0.2
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              3.7407286339920067e-07
    epoch : 4 ; learning_rate : 3.7407286339920067e-07 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.482524947869303
    Epoch 4, Loss: 1.757314492733975, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.7679333543326407e-07
    epoch : 5 ; learning_rate : 4.7679333543326407e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.5039691359223062
    Epoch 5, Loss: 1.7570400256549, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.880845232408449e-07
    epoch : 6 ; learning_rate : 5.880845232408449e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4948704376902309
    Epoch 6, Loss: 1.7568057093567087, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.838095784216914e-07
    epoch : 7 ; learning_rate : 5.838095784216914e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.433314407669991
    Epoch 7, Loss: 1.7562904140861149, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              3.92868376252656e-07
    epoch : 8 ; learning_rate : 3.92868376252656e-07 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.5239325010049691
    Epoch 8, Loss: 1.7558838866092, fit: 0.11666666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7351628883479624_fit_0.11666666666666667_2024-01-02_195727
  self.fit : 0.11666666666666667
  self.loss : 1.7351628883479624
  current_accuracy : 0.102
  Epochs: 100
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          89
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.224211504838314e-07
    epoch : 0 ; learning_rate : 5.224211504838314e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4807688389356684
    Epoch 0, Loss: 1.7554682017958578, fit: 0.13333333333333333
    Epoch 0, Loss: 1.7554682017958578
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.003957815969575e-07
    epoch : 1 ; learning_rate : 5.003957815969575e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4929641821296902
    Epoch 1, Loss: 1.754909634025032, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.599720588425714e-07
    epoch : 2 ; learning_rate : 5.599720588425714e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5063373581031205
    Epoch 2, Loss: 1.7543147514382735, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.210453152146258e-07
    epoch : 3 ; learning_rate : 4.210453152146258e-07 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4186795246253565
    Epoch 3, Loss: 1.7538762373245536, fit: 0.15
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.593284435445305e-07
    epoch : 4 ; learning_rate : 4.593284435445305e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4734473642227457
    Epoch 4, Loss: 1.7533083541755032, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.403020211531371e-07
    epoch : 5 ; learning_rate : 5.403020211531371e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4803642126563825
    Epoch 5, Loss: 1.7526302168284242, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.167403271456672e-07
    epoch : 6 ; learning_rate : 6.167403271456672e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4737198246523067
    Epoch 6, Loss: 1.7517397870938392, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.057111082771405e-07
    epoch : 7 ; learning_rate : 5.057111082771405e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4736966809093053
    Epoch 7, Loss: 1.7507791494125997, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.175173021688555e-07
    epoch : 8 ; learning_rate : 5.175173021688555e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4784357682576992
    Epoch 8, Loss: 1.7499457417862507, fit: 0.15
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.434752970350822e-07
    epoch : 9 ; learning_rate : 4.434752970350822e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.519208501484321
    Epoch 9, Loss: 1.7491992169116017, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.358837931508789e-07
    epoch : 10 ; learning_rate : 5.358837931508789e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4473829611839268
    Epoch 10, Loss: 1.7484274197997687, fit: 0.18333333333333332
    Epoch 10, Loss: 1.7484274197997687
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.137588558320589e-07
    epoch : 11 ; learning_rate : 4.137588558320589e-07 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.4645002008203936
    Epoch 11, Loss: 1.7476209225511108, fit: 0.15
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.5164893028247954e-07
    epoch : 12 ; learning_rate : 4.5164893028247954e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4992115183841201
    Epoch 12, Loss: 1.746952860542928, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.084403638438687e-07
    epoch : 13 ; learning_rate : 5.084403638438687e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5208265608023102
    Epoch 13, Loss: 1.7462773392110784, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.339633257177487e-07
    epoch : 14 ; learning_rate : 5.339633257177487e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4329261568337337
    Epoch 14, Loss: 1.74566209953986, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.287398929322082e-07
    epoch : 15 ; learning_rate : 6.287398929322082e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.485237637817249
    Epoch 15, Loss: 1.745036013774799, fit: 0.03333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.887341669461535e-07
    epoch : 16 ; learning_rate : 6.887341669461535e-07 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4397730095547479
    Epoch 16, Loss: 1.7443889248929552, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.404886548752738e-07
    epoch : 17 ; learning_rate : 4.404886548752738e-07 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4430252813927134
    Epoch 17, Loss: 1.7437184335422609, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.273617226544177e-07
    epoch : 18 ; learning_rate : 5.273617226544177e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4966470506866643
    Epoch 18, Loss: 1.7431546807166953, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.621756361518664e-07
    epoch : 19 ; learning_rate : 5.621756361518664e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.454618569743576
    Epoch 19, Loss: 1.7425519536401628, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.149761256523394e-07
    epoch : 20 ; learning_rate : 5.149761256523394e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4676186025381868
    Epoch 20, Loss: 1.7419456653994643, fit: 0.05
    Epoch 20, Loss: 1.7419456653994643
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.964206687050711e-07
    epoch : 21 ; learning_rate : 5.964206687050711e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4873526367550454
    Epoch 21, Loss: 1.7413221733035997, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.617471652663393e-07
    epoch : 22 ; learning_rate : 4.617471652663393e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4347355517218878
    Epoch 22, Loss: 1.7407963238246034, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.174748122098138e-07
    epoch : 23 ; learning_rate : 5.174748122098138e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.459503566814005
    Epoch 23, Loss: 1.7401464391906045, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.317471686819854e-07
    epoch : 24 ; learning_rate : 4.317471686819854e-07 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4966302584180855
    Epoch 24, Loss: 1.7395937765105567, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.916851618615137e-07
    epoch : 25 ; learning_rate : 5.916851618615137e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4229745149656992
    Epoch 25, Loss: 1.7390062515638551, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.985230930312649e-07
    epoch : 26 ; learning_rate : 5.985230930312649e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4908698690950573
    Epoch 26, Loss: 1.7383182773344352, fit: 0.03333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.317222775197684e-07
    epoch : 27 ; learning_rate : 6.317222775197684e-07 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.3855019036850769
    Epoch 27, Loss: 1.7375884748749961, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.6380524143119003e-07
    epoch : 28 ; learning_rate : 4.6380524143119003e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4718523196452424
    Epoch 28, Loss: 1.7369010012776198, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.125303163411941e-07
    epoch : 29 ; learning_rate : 5.125303163411941e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.479760755354553
    Epoch 29, Loss: 1.7362455889164232, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.689132365317487e-07
    epoch : 30 ; learning_rate : 5.689132365317487e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.3994874726871798
    Epoch 30, Loss: 1.7354499330651645, fit: 0.25
    Epoch 30, Loss: 1.7354499330651645
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              3.1696319173766944e-07
    epoch : 31 ; learning_rate : 3.1696319173766944e-07 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.443226603831231
    Epoch 31, Loss: 1.734676565253756, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.277963550025015e-07
    epoch : 32 ; learning_rate : 5.277963550025015e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4934516162380964
    Epoch 32, Loss: 1.7339588324233486, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.778731080604134e-07
    epoch : 33 ; learning_rate : 4.778731080604134e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.462843971661281
    Epoch 33, Loss: 1.733168891990374, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.026733255383374e-07
    epoch : 34 ; learning_rate : 5.026733255383374e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4762568062422772
    Epoch 34, Loss: 1.732344838717002, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.269617240120999e-07
    epoch : 35 ; learning_rate : 4.269617240120999e-07 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.483006959565684
    Epoch 35, Loss: 1.731546571534546, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.485749360517425e-07
    epoch : 36 ; learning_rate : 5.485749360517425e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4682672308628544
    Epoch 36, Loss: 1.7307343815287517, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.829257092764135e-07
    epoch : 37 ; learning_rate : 5.829257092764135e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4259610810236993
    Epoch 37, Loss: 1.729836623087557, fit: 0.03333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.527347374428639e-07
    epoch : 38 ; learning_rate : 6.527347374428639e-07 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4192710907005817
    Epoch 38, Loss: 1.7289364414572246, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.6859120593104376e-07
    epoch : 39 ; learning_rate : 4.6859120593104376e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4583950687730047
    Epoch 39, Loss: 1.7280439608360392, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.497593391034294e-07
    epoch : 40 ; learning_rate : 5.497593391034294e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5153241633497316
    Epoch 40, Loss: 1.7273439868925111, fit: 0.13333333333333333
    Epoch 40, Loss: 1.7273439868925111
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.6364691630530787e-07
    epoch : 41 ; learning_rate : 4.6364691630530787e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4661248395866424
    Epoch 41, Loss: 1.7266037862706307, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.1338282088579876e-07
    epoch : 42 ; learning_rate : 4.1338282088579876e-07 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4548137953512323
    Epoch 42, Loss: 1.7259537399408271, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.3680456783603654e-07
    epoch : 43 ; learning_rate : 4.3680456783603654e-07 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.470817451769044
    Epoch 43, Loss: 1.725306526154511, fit: 0.05
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.673755957158816e-07
    epoch : 44 ; learning_rate : 6.673755957158816e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4295456385427783
    Epoch 44, Loss: 1.7243936614038438, fit: 0.016666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              7.157760608405135e-07
    epoch : 45 ; learning_rate : 7.157760608405135e-07 ; fit : 0.016666666666666666
    batch_size :          60
    best_batch_loss : 1.4714774691193113
    Epoch 45, Loss: 1.7234482245785485, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.352202302119925e-07
    epoch : 46 ; learning_rate : 5.352202302119925e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.421045034201606
    Epoch 46, Loss: 1.7225723399060642, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.765335227379746e-07
    epoch : 47 ; learning_rate : 4.765335227379746e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.432642123161178
    Epoch 47, Loss: 1.721867942948118, fit: 0.05
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.280503533802451e-07
    epoch : 48 ; learning_rate : 6.280503533802451e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4328768021893112
    Epoch 48, Loss: 1.7210080860278507, fit: 0.2
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              3.8219944785810523e-07
    epoch : 49 ; learning_rate : 3.8219944785810523e-07 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.4116794140090865
    Epoch 49, Loss: 1.720280863904483, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              3.9247410528421866e-07
    epoch : 50 ; learning_rate : 3.9247410528421866e-07 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4463202895002263
    Epoch 50, Loss: 1.719769106965194, fit: 0.13333333333333333
    Epoch 50, Loss: 1.719769106965194
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.395536557726517e-07
    epoch : 51 ; learning_rate : 4.395536557726517e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.424357542086652
    Epoch 51, Loss: 1.7191010852254849, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.572226159116566e-07
    epoch : 52 ; learning_rate : 5.572226159116566e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4639844836009919
    Epoch 52, Loss: 1.718392412502293, fit: 0.05
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.38996820873803e-07
    epoch : 53 ; learning_rate : 6.38996820873803e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4164377159228105
    Epoch 53, Loss: 1.7175829752180427, fit: 0.21666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              3.3197223009150356e-07
    epoch : 54 ; learning_rate : 3.3197223009150356e-07 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.423306805269612
    Epoch 54, Loss: 1.716927068070175, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.733495263931043e-07
    epoch : 55 ; learning_rate : 5.733495263931043e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4169439600136813
    Epoch 55, Loss: 1.7163082306436788, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.422574778038386e-07
    epoch : 56 ; learning_rate : 5.422574778038386e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4128420094738203
    Epoch 56, Loss: 1.7156038657957051, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.220054613838932e-07
    epoch : 57 ; learning_rate : 5.220054613838932e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4341383284598304
    Epoch 57, Loss: 1.7149447392108252, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.463697850754401e-07
    epoch : 58 ; learning_rate : 5.463697850754401e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4323342826168508
    Epoch 58, Loss: 1.7142675844818467, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.888938490142994e-07
    epoch : 59 ; learning_rate : 4.888938490142994e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.373414856833172
    Epoch 59, Loss: 1.7136950078366062, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.659828277021375e-07
    epoch : 60 ; learning_rate : 5.659828277021375e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.3974481381056962
    Epoch 60, Loss: 1.7130785339312458, fit: 0.11666666666666667
    Epoch 60, Loss: 1.7130785339312458
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.000540306106418e-07
    epoch : 61 ; learning_rate : 5.000540306106418e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4464342163728725
    Epoch 61, Loss: 1.7124771867246587, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.293932396388343e-07
    epoch : 62 ; learning_rate : 5.293932396388343e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.3968084519675041
    Epoch 62, Loss: 1.7118497026983965, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.121853159860914e-07
    epoch : 63 ; learning_rate : 5.121853159860914e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4660920002607554
    Epoch 63, Loss: 1.7112685551970412, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.167687100737243e-07
    epoch : 64 ; learning_rate : 6.167687100737243e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4392154088207758
    Epoch 64, Loss: 1.7105782477918987, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.3119063792601397e-07
    epoch : 65 ; learning_rate : 4.3119063792601397e-07 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4496508353126634
    Epoch 65, Loss: 1.7098878137999851, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.082558348358369e-07
    epoch : 66 ; learning_rate : 4.082558348358369e-07 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.4288112512761628
    Epoch 66, Loss: 1.7093472801347225, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.047646374123284e-07
    epoch : 67 ; learning_rate : 4.047646374123284e-07 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.353521147395214
    Epoch 67, Loss: 1.7088498858979997, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.945776134025161e-07
    epoch : 68 ; learning_rate : 5.945776134025161e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4253273641675213
    Epoch 68, Loss: 1.7081345287606557, fit: 0.05
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.53865517439172e-07
    epoch : 69 ; learning_rate : 6.53865517439172e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.419241489729374
    Epoch 69, Loss: 1.707423259954209, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              3.8276998671319346e-07
    epoch : 70 ; learning_rate : 3.8276998671319346e-07 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.463060781662177
    Epoch 70, Loss: 1.7067535997134538, fit: 0.18333333333333332
    Epoch 70, Loss: 1.7067535997134538
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              3.3735420358431105e-07
    epoch : 71 ; learning_rate : 3.3735420358431105e-07 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.4324658443858749
    Epoch 71, Loss: 1.7062867626089437, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.733047109912882e-07
    epoch : 72 ; learning_rate : 4.733047109912882e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4380352780966434
    Epoch 72, Loss: 1.7057884020346492, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.903336191045194e-07
    epoch : 73 ; learning_rate : 4.903336191045194e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.416188526114348
    Epoch 73, Loss: 1.7051955849660305, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.701543561909617e-07
    epoch : 74 ; learning_rate : 4.701543561909617e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.452145059327642
    Epoch 74, Loss: 1.7046287906758069, fit: 0.15
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.3486919494714246e-07
    epoch : 75 ; learning_rate : 4.3486919494714246e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4084467496394202
    Epoch 75, Loss: 1.7040862983739382, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.170946034291483e-07
    epoch : 76 ; learning_rate : 6.170946034291483e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4254664483521002
    Epoch 76, Loss: 1.7035049404164422, fit: 0.15
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.269489814467095e-07
    epoch : 77 ; learning_rate : 4.269489814467095e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.442881555580398
    Epoch 77, Loss: 1.7028757324451107, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.669460422496154e-07
    epoch : 78 ; learning_rate : 5.669460422496154e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4587768178773275
    Epoch 78, Loss: 1.7023604639288288, fit: 0.15
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.22896771959623e-07
    epoch : 79 ; learning_rate : 4.22896771959623e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4072976953422984
    Epoch 79, Loss: 1.7018095149917456, fit: 0.15
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.450144777786492e-07
    epoch : 80 ; learning_rate : 4.450144777786492e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4143514185205581
    Epoch 80, Loss: 1.7013380568681047, fit: 0.11666666666666667
    Epoch 80, Loss: 1.7013380568681047
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.034175855334915e-07
    epoch : 81 ; learning_rate : 5.034175855334915e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4399521936289144
    Epoch 81, Loss: 1.7008132611203253, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.5112498570227646e-07
    epoch : 82 ; learning_rate : 4.5112498570227646e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4392229802437793
    Epoch 82, Loss: 1.7002986824884818, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.459612379537711e-07
    epoch : 83 ; learning_rate : 5.459612379537711e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4317492372904532
    Epoch 83, Loss: 1.6997820489532882, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.3425587103772555e-07
    epoch : 84 ; learning_rate : 4.3425587103772555e-07 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4485269233814781
    Epoch 84, Loss: 1.699283923958184, fit: 0.2
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              3.9410819530650543e-07
    epoch : 85 ; learning_rate : 3.9410819530650543e-07 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.4514496515837214
    Epoch 85, Loss: 1.6988587609306762, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.5531608044674476e-07
    epoch : 86 ; learning_rate : 4.5531608044674476e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4416070489140034
    Epoch 86, Loss: 1.6984253651543884, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.211283617584861e-07
    epoch : 87 ; learning_rate : 4.211283617584861e-07 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4055186120480614
    Epoch 87, Loss: 1.697992338062906, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.26148898344243e-07
    epoch : 88 ; learning_rate : 4.26148898344243e-07 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.3726895357374762
    Epoch 88, Loss: 1.697579294352005, fit: 0.18333333333333332
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.5439370283472793_fit_0.18333333333333332_2024-01-02_200742
  self.fit : 0.18333333333333332
  self.loss : 1.5439370283472793
  current_accuracy : 0.1121
   Accuracy mean: 0.1018
   Accuracy mean: 0.102
   Accuracy mean: 0.1121
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4615234654232552
    Epoch 0, Loss: 1.7064794159461312, fit: 0.15
    Epoch 0, Loss: 1.7064794159461312
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6562723878389651_fit_0.15_2024-01-02_200749
  self.fit : 0.15
  self.loss : 1.6562723878389651
  current_accuracy : 0.1118
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.543549721551909e-10
    epoch : 0 ; learning_rate : 4.543549721551909e-10 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.441675718322224
    Epoch 0, Loss: 1.7064790435888897, fit: 0.21666666666666667
    Epoch 0, Loss: 1.7064790435888897
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              3.431153097403999e-10
    epoch : 1 ; learning_rate : 3.431153097403999e-10 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.4046949350808082
    Epoch 1, Loss: 1.7064785029412757, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.938516625367669e-10
    epoch : 2 ; learning_rate : 4.938516625367669e-10 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4154902748178206
    Epoch 2, Loss: 1.706477874249659, fit: 0.15
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.333050726072495e-10
    epoch : 3 ; learning_rate : 4.333050726072495e-10 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.3824571279114508
    Epoch 3, Loss: 1.7064771516140878, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.991905267979424e-10
    epoch : 4 ; learning_rate : 5.991905267979424e-10 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.453750024546956
    Epoch 4, Loss: 1.7064764414798679, fit: 0.1
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.135883727058112e-10
    epoch : 5 ; learning_rate : 5.135883727058112e-10 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4375391772714206
    Epoch 5, Loss: 1.706475567874087, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.9579005081301e-10
    epoch : 6 ; learning_rate : 4.9579005081301e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4244126287728904
    Epoch 6, Loss: 1.7064748422703238, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.80578423207331e-10
    epoch : 7 ; learning_rate : 4.80578423207331e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4184527659086057
    Epoch 7, Loss: 1.7064741299239188, fit: 0.16666666666666666
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.3565140848218894e-10
    epoch : 8 ; learning_rate : 4.3565140848218894e-10 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4625602954446184
    Epoch 8, Loss: 1.7064734467894043, fit: 0.1
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7020281840688198_fit_0.1_2024-01-02_200852
  self.fit : 0.1
  self.loss : 1.7020281840688198
  current_accuracy : 0.1118
  Epochs: 100
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          89
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.930605343225812e-10
    epoch : 0 ; learning_rate : 4.930605343225812e-10 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.3952422886944413
    Epoch 0, Loss: 1.7064727526277204, fit: 0.1
    Epoch 0, Loss: 1.7064727526277204
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.262949704286636e-10
    epoch : 1 ; learning_rate : 5.262949704286636e-10 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4331571773095844
    Epoch 1, Loss: 1.7064719872819383, fit: 0.1
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.508081373791189e-10
    epoch : 2 ; learning_rate : 5.508081373791189e-10 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.3839547933830838
    Epoch 2, Loss: 1.7064712204289454, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.725862115352835e-10
    epoch : 3 ; learning_rate : 4.725862115352835e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4211994475190843
    Epoch 3, Loss: 1.7064704510835889, fit: 0.18333333333333332
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              3.7417716128847743e-10
    epoch : 4 ; learning_rate : 3.7417716128847743e-10 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.3752231700226114
    Epoch 4, Loss: 1.7064698315650788, fit: 0.05
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              6.034596444746407e-10
    epoch : 5 ; learning_rate : 6.034596444746407e-10 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.2865173952553326
    Epoch 5, Loss: 1.7064690727490546, fit: 0.16666666666666666
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.1620552296781516e-10
    epoch : 6 ; learning_rate : 4.1620552296781516e-10 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4224115720068375
    Epoch 6, Loss: 1.7064683509968634, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.815059109022475e-10
    epoch : 7 ; learning_rate : 4.815059109022475e-10 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4053207735242323
    Epoch 7, Loss: 1.706467669539952, fit: 0.18333333333333332
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.095576506552336e-10
    epoch : 8 ; learning_rate : 4.095576506552336e-10 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.452005360099567
    Epoch 8, Loss: 1.706467002171897, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.852283265041255e-10
    epoch : 9 ; learning_rate : 5.852283265041255e-10 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.434203991864051
    Epoch 9, Loss: 1.7064662475388241, fit: 0.1
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.268019760650862e-10
    epoch : 10 ; learning_rate : 5.268019760650862e-10 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.2905691220795272
    Epoch 10, Loss: 1.7064654268750048, fit: 0.1
    Epoch 10, Loss: 1.7064654268750048
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.389162785025702e-10
    epoch : 11 ; learning_rate : 5.389162785025702e-10 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4070382270323436
    Epoch 11, Loss: 1.7064646360159634, fit: 0.2
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              3.5484095244785514e-10
    epoch : 12 ; learning_rate : 3.5484095244785514e-10 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.438127667691605
    Epoch 12, Loss: 1.7064640066161703, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.473207283308015e-10
    epoch : 13 ; learning_rate : 5.473207283308015e-10 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.3725164602363156
    Epoch 13, Loss: 1.7064633156447084, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.997166854433823e-10
    epoch : 14 ; learning_rate : 4.997166854433823e-10 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4279829846362608
    Epoch 14, Loss: 1.706462570255354, fit: 0.1
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.243368935738271e-10
    epoch : 15 ; learning_rate : 5.243368935738271e-10 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.379420075203145
    Epoch 15, Loss: 1.706461804326312, fit: 0.16666666666666666
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.081063688051704e-10
    epoch : 16 ; learning_rate : 4.081063688051704e-10 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4540672094407785
    Epoch 16, Loss: 1.706461102626901, fit: 0.05
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              6.265120774887296e-10
    epoch : 17 ; learning_rate : 6.265120774887296e-10 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4028300303198726
    Epoch 17, Loss: 1.7064603107311465, fit: 0.1
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.47742336953699e-10
    epoch : 18 ; learning_rate : 5.47742336953699e-10 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4234442327813497
    Epoch 18, Loss: 1.706459491685981, fit: 0.2
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              3.6548276634402144e-10
    epoch : 19 ; learning_rate : 3.6548276634402144e-10 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.4621497818319837
    Epoch 19, Loss: 1.7064587986244648, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.602828117037832e-10
    epoch : 20 ; learning_rate : 5.602828117037832e-10 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4078739530988442
    Epoch 20, Loss: 1.7064581489058734, fit: 0.15
    Epoch 20, Loss: 1.7064581489058734
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.5234821282149346e-10
    epoch : 21 ; learning_rate : 4.5234821282149346e-10 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4020508555679634
    Epoch 21, Loss: 1.706457360551519, fit: 0.15
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.5382797932397193e-10
    epoch : 22 ; learning_rate : 4.5382797932397193e-10 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4251635398084592
    Epoch 22, Loss: 1.7064566923339117, fit: 0.18333333333333332
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              3.761427870803941e-10
    epoch : 23 ; learning_rate : 3.761427870803941e-10 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.4370713444530663
    Epoch 23, Loss: 1.706456078636258, fit: 0.2
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              3.719604881453923e-10
    epoch : 24 ; learning_rate : 3.719604881453923e-10 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.3865541886471948
    Epoch 24, Loss: 1.706455527147956, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.791122059940189e-10
    epoch : 25 ; learning_rate : 4.791122059940189e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.427601328096602
    Epoch 25, Loss: 1.7064549186403768, fit: 0.1
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.184197125865773e-10
    epoch : 26 ; learning_rate : 5.184197125865773e-10 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4592648930168424
    Epoch 26, Loss: 1.7064541731703198, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.071090504609815e-10
    epoch : 27 ; learning_rate : 5.071090504609815e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.3835184264725053
    Epoch 27, Loss: 1.706453395517635, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.193683845324557e-10
    epoch : 28 ; learning_rate : 5.193683845324557e-10 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.3062860289731744
    Epoch 28, Loss: 1.7064526322269529, fit: 0.15
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.531063370041373e-10
    epoch : 29 ; learning_rate : 4.531063370041373e-10 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.419268088202924
    Epoch 29, Loss: 1.706451917159886, fit: 0.1
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.255375691651378e-10
    epoch : 30 ; learning_rate : 5.255375691651378e-10 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4346523258326593
    Epoch 30, Loss: 1.7064512012928479, fit: 0.18333333333333332
    Epoch 30, Loss: 1.7064512012928479
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              3.957879092862358e-10
    epoch : 31 ; learning_rate : 3.957879092862358e-10 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.479430160230365
    Epoch 31, Loss: 1.7064505066445275, fit: 0.1
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.426312187987468e-10
    epoch : 32 ; learning_rate : 5.426312187987468e-10 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4405568376740585
    Epoch 32, Loss: 1.7064498203192138, fit: 0.18333333333333332
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              3.775392107665154e-10
    epoch : 33 ; learning_rate : 3.775392107665154e-10 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.4279584453673204
    Epoch 33, Loss: 1.7064491340146046, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.6172897479734044e-10
    epoch : 34 ; learning_rate : 4.6172897479734044e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4109303679329648
    Epoch 34, Loss: 1.7064485012280963, fit: 0.2
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              3.559307224269946e-10
    epoch : 35 ; learning_rate : 3.559307224269946e-10 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.3520344326708895
    Epoch 35, Loss: 1.7064479132360302, fit: 0.1
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.15868598624823e-10
    epoch : 36 ; learning_rate : 5.15868598624823e-10 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4084889183200378
    Epoch 36, Loss: 1.7064472503698112, fit: 0.1
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.397690007056104e-10
    epoch : 37 ; learning_rate : 5.397690007056104e-10 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4026269207393207
    Epoch 37, Loss: 1.7064464864972209, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.619658867229183e-10
    epoch : 38 ; learning_rate : 5.619658867229183e-10 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4404573966025758
    Epoch 38, Loss: 1.7064457020214905, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.71619906485685e-10
    epoch : 39 ; learning_rate : 5.71619906485685e-10 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4585155420337585
    Epoch 39, Loss: 1.7064448247858377, fit: 0.15
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.5684597056574684e-10
    epoch : 40 ; learning_rate : 4.5684597056574684e-10 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.401342234869089
    Epoch 40, Loss: 1.7064440270796204, fit: 0.11666666666666667
    Epoch 40, Loss: 1.7064440270796204
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.066356134126357e-10
    epoch : 41 ; learning_rate : 5.066356134126357e-10 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4296747063187012
    Epoch 41, Loss: 1.706443345880965, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.676222358164435e-10
    epoch : 42 ; learning_rate : 5.676222358164435e-10 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4507699295792904
    Epoch 42, Loss: 1.706442563840896, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.933547559215425e-10
    epoch : 43 ; learning_rate : 4.933547559215425e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4162539865253136
    Epoch 43, Loss: 1.7064417554825577, fit: 0.18333333333333332
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              3.8417170328592603e-10
    epoch : 44 ; learning_rate : 3.8417170328592603e-10 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.3729275947492876
    Epoch 44, Loss: 1.7064411272295892, fit: 0.16666666666666666
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.2256804300893966e-10
    epoch : 45 ; learning_rate : 4.2256804300893966e-10 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4388162349626328
    Epoch 45, Loss: 1.706440512837551, fit: 0.1
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.502984687767006e-10
    epoch : 46 ; learning_rate : 5.502984687767006e-10 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.429996945387326
    Epoch 46, Loss: 1.7064398201439663, fit: 0.03333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              6.185898092340369e-10
    epoch : 47 ; learning_rate : 6.185898092340369e-10 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4089708399447016
    Epoch 47, Loss: 1.706438899777901, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.326407759914771e-10
    epoch : 48 ; learning_rate : 5.326407759914771e-10 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.3576899444901307
    Epoch 48, Loss: 1.706438088119741, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.109253068543584e-10
    epoch : 49 ; learning_rate : 5.109253068543584e-10 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.377842568674858
    Epoch 49, Loss: 1.7064373259572962, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.782855864173155e-10
    epoch : 50 ; learning_rate : 4.782855864173155e-10 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.3532046521180694
    Epoch 50, Loss: 1.7064365823855785, fit: 0.13333333333333333
    Epoch 50, Loss: 1.7064365823855785
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.962486192729003e-10
    epoch : 51 ; learning_rate : 4.962486192729003e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4043066945722893
    Epoch 51, Loss: 1.706435872053515, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.335914861094383e-10
    epoch : 52 ; learning_rate : 5.335914861094383e-10 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4319602386905268
    Epoch 52, Loss: 1.706435107399542, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.936293250838279e-10
    epoch : 53 ; learning_rate : 5.936293250838279e-10 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4018267478082937
    Epoch 53, Loss: 1.7064342643184798, fit: 0.15
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.5705338921850415e-10
    epoch : 54 ; learning_rate : 4.5705338921850415e-10 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.3404785404474466
    Epoch 54, Loss: 1.7064334799950933, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.706626839137806e-10
    epoch : 55 ; learning_rate : 5.706626839137806e-10 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4526099174689264
    Epoch 55, Loss: 1.7064327472329972, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.605685519429501e-10
    epoch : 56 ; learning_rate : 5.605685519429501e-10 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4225380314751321
    Epoch 56, Loss: 1.706431898375996, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.487389841087634e-10
    epoch : 57 ; learning_rate : 4.487389841087634e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.35402541726178
    Epoch 57, Loss: 1.7064311353054338, fit: 0.15
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              3.928136421014149e-10
    epoch : 58 ; learning_rate : 3.928136421014149e-10 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4302309685420687
    Epoch 58, Loss: 1.7064305464612426, fit: 0.16666666666666666
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.105103204050633e-10
    epoch : 59 ; learning_rate : 4.105103204050633e-10 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4479665295646744
    Epoch 59, Loss: 1.7064299274220354, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              6.059036517332385e-10
    epoch : 60 ; learning_rate : 6.059036517332385e-10 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.3770899351811954
    Epoch 60, Loss: 1.7064291965988712, fit: 0.05
    Epoch 60, Loss: 1.7064291965988712
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              6.051569644139604e-10
    epoch : 61 ; learning_rate : 6.051569644139604e-10 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.3694478517688027
    Epoch 61, Loss: 1.7064282840036307, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.034576892194405e-10
    epoch : 62 ; learning_rate : 5.034576892194405e-10 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.3770709045923737
    Epoch 62, Loss: 1.7064274638686248, fit: 0.16666666666666666
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.2353778285752826e-10
    epoch : 63 ; learning_rate : 4.2353778285752826e-10 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4697521102997533
    Epoch 63, Loss: 1.706426770947888, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.713955592418606e-10
    epoch : 64 ; learning_rate : 4.713955592418606e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.3729983657429121
    Epoch 64, Loss: 1.7064261232037699, fit: 0.15
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.456553557335832e-10
    epoch : 65 ; learning_rate : 4.456553557335832e-10 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.3411205933482477
    Epoch 65, Loss: 1.7064254534973053, fit: 0.16666666666666666
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.412710778970206e-10
    epoch : 66 ; learning_rate : 4.412710778970206e-10 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4459255338558354
    Epoch 66, Loss: 1.706424787456039, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              6.08146509176887e-10
    epoch : 67 ; learning_rate : 6.08146509176887e-10 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.449639613164708
    Epoch 67, Loss: 1.7064240503159698, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.778907180883269e-10
    epoch : 68 ; learning_rate : 5.778907180883269e-10 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.3774289214128264
    Epoch 68, Loss: 1.7064231420126106, fit: 0.1
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.486359430638133e-10
    epoch : 69 ; learning_rate : 5.486359430638133e-10 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4321467776051642
    Epoch 69, Loss: 1.7064222996415273, fit: 0.2
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              3.6707410898336237e-10
    epoch : 70 ; learning_rate : 3.6707410898336237e-10 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.4290092423372356
    Epoch 70, Loss: 1.706421595292347, fit: 0.11666666666666667
    Epoch 70, Loss: 1.706421595292347
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.826423291533236e-10
    epoch : 71 ; learning_rate : 4.826423291533236e-10 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.3964279909800745
    Epoch 71, Loss: 1.7064209958812486, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.798356063290006e-10
    epoch : 72 ; learning_rate : 4.798356063290006e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4342908385149065
    Epoch 72, Loss: 1.7064202855884525, fit: 0.15
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.408752790315647e-10
    epoch : 73 ; learning_rate : 4.408752790315647e-10 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.36521883640601
    Epoch 73, Loss: 1.706419619243735, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.145680761631992e-10
    epoch : 74 ; learning_rate : 5.145680761631992e-10 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.376410770188511
    Epoch 74, Loss: 1.7064188658139379, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.593357521323301e-10
    epoch : 75 ; learning_rate : 4.593357521323301e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.396845445032619
    Epoch 75, Loss: 1.70641817315619, fit: 0.16666666666666666
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.1277073318609086e-10
    epoch : 76 ; learning_rate : 4.1277073318609086e-10 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4265008630991074
    Epoch 76, Loss: 1.7064175222387186, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.840940979700817e-10
    epoch : 77 ; learning_rate : 4.840940979700817e-10 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.432487420693703
    Epoch 77, Loss: 1.70641687462059, fit: 0.21666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              3.2733264399460833e-10
    epoch : 78 ; learning_rate : 3.2733264399460833e-10 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.437836852577756
    Epoch 78, Loss: 1.7064162702100578, fit: 0.15
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.619638996839034e-10
    epoch : 79 ; learning_rate : 4.619638996839034e-10 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.3646533818736917
    Epoch 79, Loss: 1.7064156506867247, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              6.0544434186749e-10
    epoch : 80 ; learning_rate : 6.0544434186749e-10 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.3539830923886795
    Epoch 80, Loss: 1.7064148767224665, fit: 0.13333333333333333
    Epoch 80, Loss: 1.7064148767224665
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.790211902997437e-10
    epoch : 81 ; learning_rate : 4.790211902997437e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.3908750345052907
    Epoch 81, Loss: 1.7064140978951687, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.521887861418076e-10
    epoch : 82 ; learning_rate : 4.521887861418076e-10 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4082611684894184
    Epoch 82, Loss: 1.7064133874415965, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.859155645301682e-10
    epoch : 83 ; learning_rate : 5.859155645301682e-10 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4573569521821832
    Epoch 83, Loss: 1.7064126296939137, fit: 0.05
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              6.372298048901306e-10
    epoch : 84 ; learning_rate : 6.372298048901306e-10 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4118657818366622
    Epoch 84, Loss: 1.7064117223334274, fit: 0.1
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.124695766005504e-10
    epoch : 85 ; learning_rate : 5.124695766005504e-10 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.459741287214366
    Epoch 85, Loss: 1.7064108886533969, fit: 0.2
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              3.7766364060414015e-10
    epoch : 86 ; learning_rate : 3.7766364060414015e-10 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.429052866800828
    Epoch 86, Loss: 1.706410224958407, fit: 0.18333333333333332
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              3.7384521597670353e-10
    epoch : 87 ; learning_rate : 3.7384521597670353e-10 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.4074724751717695
    Epoch 87, Loss: 1.7064096594703613, fit: 0.15
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.442775655413725e-10
    epoch : 88 ; learning_rate : 4.442775655413725e-10 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4095686205645284
    Epoch 88, Loss: 1.70640906645051, fit: 0.13333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.671004230695524_fit_0.13333333333333333_2024-01-02_201911
  self.fit : 0.13333333333333333
  self.loss : 1.671004230695524
  current_accuracy : 0.1116
   Accuracy mean: 0.1118
   Accuracy mean: 0.1118
   Accuracy mean: 0.1116
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.456567470158398
    Epoch 0, Loss: 1.7495650176253692, fit: 0.13333333333333333
    Epoch 0, Loss: 1.7495650176253692
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.696657731937358_fit_0.13333333333333333_2024-01-02_201919
  self.fit : 0.13333333333333333
  self.loss : 1.696657731937358
  current_accuracy : 0.0959
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.884079469415826e-13
    epoch : 0 ; learning_rate : 4.884079469415826e-13 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.445565009249082
    Epoch 0, Loss: 1.7495650172806558, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7495650172806558
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.432939447282199e-13
    epoch : 1 ; learning_rate : 5.432939447282199e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.46914748777532
    Epoch 1, Loss: 1.7495650165106271, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.605768223328384e-13
    epoch : 2 ; learning_rate : 5.605768223328384e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.407662877124732
    Epoch 2, Loss: 1.7495650157503477, fit: 0.03333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.936175264810438e-13
    epoch : 3 ; learning_rate : 5.936175264810438e-13 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4953831549330383
    Epoch 3, Loss: 1.749565014898988, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.650736993423831e-13
    epoch : 4 ; learning_rate : 5.650736993423831e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4990676457200214
    Epoch 4, Loss: 1.749565014039656, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.904441323849623e-13
    epoch : 5 ; learning_rate : 5.904441323849623e-13 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.475648762601716
    Epoch 5, Loss: 1.7495650132013763, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.800863424354389e-13
    epoch : 6 ; learning_rate : 4.800863424354389e-13 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4310976563008457
    Epoch 6, Loss: 1.7495650124453062, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.283457327617963e-13
    epoch : 7 ; learning_rate : 5.283457327617963e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4059305962466138
    Epoch 7, Loss: 1.7495650116943613, fit: 0.16666666666666666
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.3176606600842183e-13
    epoch : 8 ; learning_rate : 4.3176606600842183e-13 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4836099864478205
    Epoch 8, Loss: 1.7495650109733496, fit: 0.11666666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.721120964269082_fit_0.11666666666666667_2024-01-02_202021
  self.fit : 0.11666666666666667
  self.loss : 1.721120964269082
  current_accuracy : 0.0959
  Epochs: 100
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          89
    learning_rate :          1e-07
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.098403267343721e-13
    epoch : 0 ; learning_rate : 5.098403267343721e-13 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.47035636722237
    Epoch 0, Loss: 1.7495650103346905, fit: 0.1
    Epoch 0, Loss: 1.7495650103346905
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.373574876019753e-13
    epoch : 1 ; learning_rate : 5.373574876019753e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.471220979839157
    Epoch 1, Loss: 1.749565009620554, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.286145922381704e-13
    epoch : 2 ; learning_rate : 5.286145922381704e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4663557242471092
    Epoch 2, Loss: 1.749565008772662, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.176086223213624e-13
    epoch : 3 ; learning_rate : 5.176086223213624e-13 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4126474930753077
    Epoch 3, Loss: 1.7495650079958873, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.825004329870296e-13
    epoch : 4 ; learning_rate : 5.825004329870296e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4842178504121082
    Epoch 4, Loss: 1.749565007166716, fit: 0.05
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.999938751713499e-13
    epoch : 5 ; learning_rate : 5.999938751713499e-13 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.489284625912835
    Epoch 5, Loss: 1.7495650063603176, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.893211802336463e-13
    epoch : 6 ; learning_rate : 4.893211802336463e-13 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.514463594016559
    Epoch 6, Loss: 1.7495650056026424, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.633319984590093e-13
    epoch : 7 ; learning_rate : 5.633319984590093e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4957117170406513
    Epoch 7, Loss: 1.7495650048270939, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.624644654208854e-13
    epoch : 8 ; learning_rate : 4.624644654208854e-13 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.519229053971401
    Epoch 8, Loss: 1.749565004045198, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.499697241649486e-13
    epoch : 9 ; learning_rate : 5.499697241649486e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4810306630545955
    Epoch 9, Loss: 1.7495650033512027, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.161946982163151e-13
    epoch : 10 ; learning_rate : 5.161946982163151e-13 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.476139901272724
    Epoch 10, Loss: 1.7495650025909586, fit: 0.06666666666666667
    Epoch 10, Loss: 1.7495650025909586
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              6.203486168126616e-13
    epoch : 11 ; learning_rate : 6.203486168126616e-13 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5051695084650367
    Epoch 11, Loss: 1.749565001761882, fit: 0.05
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              6.547039178551196e-13
    epoch : 12 ; learning_rate : 6.547039178551196e-13 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4672308645993215
    Epoch 12, Loss: 1.7495650007704957, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              6.232308564791591e-13
    epoch : 13 ; learning_rate : 6.232308564791591e-13 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4348085889335584
    Epoch 13, Loss: 1.7495649998224774, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.144013317805239e-13
    epoch : 14 ; learning_rate : 5.144013317805239e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4748525143750821
    Epoch 14, Loss: 1.7495649990495237, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              6.142282439242195e-13
    epoch : 15 ; learning_rate : 6.142282439242195e-13 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4907737571510231
    Epoch 15, Loss: 1.7495649982090462, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.494396935248538e-13
    epoch : 16 ; learning_rate : 5.494396935248538e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5045171895450637
    Epoch 16, Loss: 1.74956499729737, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.915980635658015e-13
    epoch : 17 ; learning_rate : 4.915980635658015e-13 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4895780940066847
    Epoch 17, Loss: 1.7495649966964308, fit: 0.03333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              6.660468341177229e-13
    epoch : 18 ; learning_rate : 6.660468341177229e-13 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4300252760105925
    Epoch 18, Loss: 1.7495649957715673, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.122073339997082e-13
    epoch : 19 ; learning_rate : 5.122073339997082e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4669371842480883
    Epoch 19, Loss: 1.7495649948952616, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.782205979882764e-13
    epoch : 20 ; learning_rate : 4.782205979882764e-13 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4110557953606604
    Epoch 20, Loss: 1.749564994173074, fit: 0.13333333333333333
    Epoch 20, Loss: 1.749564994173074
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.877326740456348e-13
    epoch : 21 ; learning_rate : 4.877326740456348e-13 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4284817364944318
    Epoch 21, Loss: 1.749564993497691, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.974172294404826e-13
    epoch : 22 ; learning_rate : 4.974172294404826e-13 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4728089519021783
    Epoch 22, Loss: 1.7495649927994132, fit: 0.03333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              6.512764149794371e-13
    epoch : 23 ; learning_rate : 6.512764149794371e-13 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4702800148806072
    Epoch 23, Loss: 1.7495649918567793, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.890279896713996e-13
    epoch : 24 ; learning_rate : 4.890279896713996e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4512255710119115
    Epoch 24, Loss: 1.7495649911550524, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.910590334885249e-13
    epoch : 25 ; learning_rate : 4.910590334885249e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4918391575686734
    Epoch 25, Loss: 1.7495649904103159, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.836358774559117e-13
    epoch : 26 ; learning_rate : 5.836358774559117e-13 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.407565555137342
    Epoch 26, Loss: 1.7495649895683094, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              6.181512415657861e-13
    epoch : 27 ; learning_rate : 6.181512415657861e-13 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.482383708608351
    Epoch 27, Loss: 1.7495649887365354, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.336816323014052e-13
    epoch : 28 ; learning_rate : 5.336816323014052e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4560392196529564
    Epoch 28, Loss: 1.7495649879589021, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.284130527715013e-13
    epoch : 29 ; learning_rate : 4.284130527715013e-13 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.5128689298744769
    Epoch 29, Loss: 1.749564987232101, fit: 0.15
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.549125451104732e-13
    epoch : 30 ; learning_rate : 4.549125451104732e-13 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.5073663616083997
    Epoch 30, Loss: 1.7495649866149081, fit: 0.11666666666666667
    Epoch 30, Loss: 1.7495649866149081
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.916623763971937e-13
    epoch : 31 ; learning_rate : 4.916623763971937e-13 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4639837378815088
    Epoch 31, Loss: 1.7495649859253122, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.658563655566652e-13
    epoch : 32 ; learning_rate : 4.658563655566652e-13 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4898518928421283
    Epoch 32, Loss: 1.7495649852492596, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.909068591227831e-13
    epoch : 33 ; learning_rate : 4.909068591227831e-13 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4481843228639035
    Epoch 33, Loss: 1.7495649845190786, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.778001605267107e-13
    epoch : 34 ; learning_rate : 5.778001605267107e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4313003530017394
    Epoch 34, Loss: 1.749564983709831, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.717868532821256e-13
    epoch : 35 ; learning_rate : 5.717868532821256e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4097013618630305
    Epoch 35, Loss: 1.7495649828316782, fit: 0.15
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.60626921240062e-13
    epoch : 36 ; learning_rate : 4.60626921240062e-13 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4657812466998148
    Epoch 36, Loss: 1.749564982146436, fit: 0.03333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              6.620914369480143e-13
    epoch : 37 ; learning_rate : 6.620914369480143e-13 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4828062945700187
    Epoch 37, Loss: 1.7495649813360803, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.693846948453089e-13
    epoch : 38 ; learning_rate : 5.693846948453089e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4903414295986357
    Epoch 38, Loss: 1.7495649804002462, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.723030007070476e-13
    epoch : 39 ; learning_rate : 4.723030007070476e-13 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4450993443767266
    Epoch 39, Loss: 1.7495649796616508, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.323968875190722e-13
    epoch : 40 ; learning_rate : 5.323968875190722e-13 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4927119464765435
    Epoch 40, Loss: 1.7495649789377183, fit: 0.06666666666666667
    Epoch 40, Loss: 1.7495649789377183
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.834143446588183e-13
    epoch : 41 ; learning_rate : 5.834143446588183e-13 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4836671731235578
    Epoch 41, Loss: 1.7495649781512497, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.380235915226914e-13
    epoch : 42 ; learning_rate : 5.380235915226914e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4465350087001583
    Epoch 42, Loss: 1.7495649773376898, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              6.130716318840963e-13
    epoch : 43 ; learning_rate : 6.130716318840963e-13 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4072267987547369
    Epoch 43, Loss: 1.7495649765288366, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.409495734852874e-13
    epoch : 44 ; learning_rate : 5.409495734852874e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.512124722112971
    Epoch 44, Loss: 1.749564975681334, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.542216386037956e-13
    epoch : 45 ; learning_rate : 5.542216386037956e-13 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4734489536496154
    Epoch 45, Loss: 1.7495649748580646, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.659775024953165e-13
    epoch : 46 ; learning_rate : 5.659775024953165e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5001162115886824
    Epoch 46, Loss: 1.7495649740461272, fit: 0.15
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.321419831179803e-13
    epoch : 47 ; learning_rate : 4.321419831179803e-13 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.5089066699359681
    Epoch 47, Loss: 1.7495649733276255, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.339452313415253e-13
    epoch : 48 ; learning_rate : 5.339452313415253e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4193011331485779
    Epoch 48, Loss: 1.7495649726293345, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.521724791309219e-13
    epoch : 49 ; learning_rate : 5.521724791309219e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5256533436140312
    Epoch 49, Loss: 1.7495649717730202, fit: 0.016666666666666666
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              6.866509300525512e-13
    epoch : 50 ; learning_rate : 6.866509300525512e-13 ; fit : 0.016666666666666666
    batch_size :          60
    best_batch_loss : 1.4422946381425557
    Epoch 50, Loss: 1.7495649709782515, fit: 0.1
    Epoch 50, Loss: 1.7495649709782515
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.553144295194914e-13
    epoch : 51 ; learning_rate : 5.553144295194914e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5448816478128677
    Epoch 51, Loss: 1.7495649700194364, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.21328419625148e-13
    epoch : 52 ; learning_rate : 5.21328419625148e-13 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5150592821953035
    Epoch 52, Loss: 1.7495649693332174, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.323881240437113e-13
    epoch : 53 ; learning_rate : 5.323881240437113e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4799368034962177
    Epoch 53, Loss: 1.7495649685534942, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.010847847138697e-13
    epoch : 54 ; learning_rate : 5.010847847138697e-13 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4383524201853475
    Epoch 54, Loss: 1.749564967678878, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.922732343416348e-13
    epoch : 55 ; learning_rate : 5.922732343416348e-13 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4685523412533608
    Epoch 55, Loss: 1.74956496696764, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.711909856437386e-13
    epoch : 56 ; learning_rate : 5.711909856437386e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4894040956320191
    Epoch 56, Loss: 1.7495649660762325, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.359400855319375e-13
    epoch : 57 ; learning_rate : 5.359400855319375e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.500175729832422
    Epoch 57, Loss: 1.7495649652808989, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.905968087814093e-13
    epoch : 58 ; learning_rate : 4.905968087814093e-13 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.476561995833669
    Epoch 58, Loss: 1.7495649645460354, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              6.045911283932136e-13
    epoch : 59 ; learning_rate : 6.045911283932136e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.450992660740009
    Epoch 59, Loss: 1.749564963762362, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.337590195158973e-13
    epoch : 60 ; learning_rate : 5.337590195158973e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4392492102720453
    Epoch 60, Loss: 1.7495649629113361, fit: 0.05
    Epoch 60, Loss: 1.7495649629113361
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              6.400417794363161e-13
    epoch : 61 ; learning_rate : 6.400417794363161e-13 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4503261909996448
    Epoch 61, Loss: 1.7495649620473823, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.540877599085429e-13
    epoch : 62 ; learning_rate : 5.540877599085429e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5399299585506787
    Epoch 62, Loss: 1.7495649612655675, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.003221550147516e-13
    epoch : 63 ; learning_rate : 5.003221550147516e-13 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.3407887005828523
    Epoch 63, Loss: 1.7495649604862726, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.570597391751184e-13
    epoch : 64 ; learning_rate : 5.570597391751184e-13 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4799748072743701
    Epoch 64, Loss: 1.7495649596900986, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.224170763460426e-13
    epoch : 65 ; learning_rate : 5.224170763460426e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4943271970836964
    Epoch 65, Loss: 1.7495649589036626, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.143328695555452e-13
    epoch : 66 ; learning_rate : 5.143328695555452e-13 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4857793738469967
    Epoch 66, Loss: 1.7495649581471189, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.101406609790095e-13
    epoch : 67 ; learning_rate : 5.101406609790095e-13 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4883112738243034
    Epoch 67, Loss: 1.7495649573627836, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.571126294748538e-13
    epoch : 68 ; learning_rate : 5.571126294748538e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.46791492117436
    Epoch 68, Loss: 1.7495649565818756, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.593600760558082e-13
    epoch : 69 ; learning_rate : 5.593600760558082e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4762493333395763
    Epoch 69, Loss: 1.7495649558657465, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.062273093225175e-13
    epoch : 70 ; learning_rate : 5.062273093225175e-13 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5150100445270174
    Epoch 70, Loss: 1.7495649550075072, fit: 0.08333333333333333
    Epoch 70, Loss: 1.7495649550075072
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.307948273323788e-13
    epoch : 71 ; learning_rate : 5.307948273323788e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5122632814060377
    Epoch 71, Loss: 1.749564954305533, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.565778872535248e-13
    epoch : 72 ; learning_rate : 5.565778872535248e-13 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4592370058126685
    Epoch 72, Loss: 1.7495649534556983, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.223626470332926e-13
    epoch : 73 ; learning_rate : 5.223626470332926e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4195747955578866
    Epoch 73, Loss: 1.7495649527144264, fit: 0.05
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              6.052944357524348e-13
    epoch : 74 ; learning_rate : 6.052944357524348e-13 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4715163722780693
    Epoch 74, Loss: 1.7495649518212641, fit: 0.18333333333333332
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.0275080288363536e-13
    epoch : 75 ; learning_rate : 4.0275080288363536e-13 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.4653349441757513
    Epoch 75, Loss: 1.7495649511010765, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.83915387288027e-13
    epoch : 76 ; learning_rate : 4.83915387288027e-13 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4779812808047843
    Epoch 76, Loss: 1.7495649505634885, fit: 0.05
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              6.252049481613907e-13
    epoch : 77 ; learning_rate : 6.252049481613907e-13 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4968866656563213
    Epoch 77, Loss: 1.7495649497656742, fit: 0.16666666666666666
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.2903514941842037e-13
    epoch : 78 ; learning_rate : 4.2903514941842037e-13 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.3813517065976593
    Epoch 78, Loss: 1.7495649488900964, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.253941180421404e-13
    epoch : 79 ; learning_rate : 5.253941180421404e-13 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4867364679476027
    Epoch 79, Loss: 1.7495649482551037, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.466927746830602e-13
    epoch : 80 ; learning_rate : 5.466927746830602e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4321668244445855
    Epoch 80, Loss: 1.749564947476852, fit: 0.13333333333333333
    Epoch 80, Loss: 1.749564947476852
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.905372131581096e-13
    epoch : 81 ; learning_rate : 4.905372131581096e-13 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4494458644716146
    Epoch 81, Loss: 1.749564946708036, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.697709959226923e-13
    epoch : 82 ; learning_rate : 4.697709959226923e-13 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.5230870098223837
    Epoch 82, Loss: 1.7495649460289924, fit: 0.03333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              6.629046695142419e-13
    epoch : 83 ; learning_rate : 6.629046695142419e-13 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4758331359673988
    Epoch 83, Loss: 1.7495649450932338, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.086541003102899e-13
    epoch : 84 ; learning_rate : 5.086541003102899e-13 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4730289774810743
    Epoch 84, Loss: 1.7495649443542765, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.229786841044451e-13
    epoch : 85 ; learning_rate : 5.229786841044451e-13 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.494099337886129
    Epoch 85, Loss: 1.7495649436153085, fit: 0.15
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.837508787130163e-13
    epoch : 86 ; learning_rate : 4.837508787130163e-13 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.462225027517007
    Epoch 86, Loss: 1.7495649428210822, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              5.636642090150653e-13
    epoch : 87 ; learning_rate : 5.636642090150653e-13 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.491514181531187
    Epoch 87, Loss: 1.7495649420397394, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              4.922620061216947e-13
    epoch : 88 ; learning_rate : 4.922620061216947e-13 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.5183005691520306
    Epoch 88, Loss: 1.7495649412306973, fit: 0.08333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7658563560190297_fit_0.08333333333333333_2024-01-02_203038
  self.fit : 0.08333333333333333
  self.loss : 1.7658563560190297
  current_accuracy : 0.0959
   Accuracy mean: 0.0959
   Accuracy mean: 0.0959
   Accuracy mean: 0.0959
  Results saved to test_combinations_results20240102203039
  normalized_accuracies :      [0.         0.         0.         0.08862876 0.08862876 0.08751394
   0.0328874  0.03400223 0.090301   0.04069119 0.16610925 1.        ]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5673650113550048
    Epoch 0, Loss: 1.797164556066859, fit: 0.11666666666666667
    Epoch 0, Loss: 1.797164556066859
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7131752552830628_fit_0.11666666666666667_2024-01-03_005700
  self.fit : 0.11666666666666667
  self.loss : 1.7131752552830628
  current_accuracy : 0.0794
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.525305341270288e-06
    epoch : 0 ; learning_rate : 4.525305341270288e-06 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.544604411601823
    Epoch 0, Loss: 1.7949283453134146, fit: 0.06666666666666667
    Epoch 0, Loss: 1.7949283453134146
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              5.375629264089335e-06
    epoch : 1 ; learning_rate : 5.375629264089335e-06 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.549708484080238
    Epoch 1, Loss: 1.788669331116755, fit: 0.03333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              5.84762681723226e-06
    epoch : 2 ; learning_rate : 5.84762681723226e-06 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4943181475295462
    Epoch 2, Loss: 1.7808038185240171, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.364999154176171e-06
    epoch : 3 ; learning_rate : 4.364999154176171e-06 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.529568546041648
    Epoch 3, Loss: 1.7732669783103676, fit: 0.03333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              5.919134523661396e-06
    epoch : 4 ; learning_rate : 5.919134523661396e-06 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.513760199021541
    Epoch 4, Loss: 1.7653169653141512, fit: 0.1
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.679993494909197e-06
    epoch : 5 ; learning_rate : 4.679993494909197e-06 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4587579601342429
    Epoch 5, Loss: 1.7566430399002773, fit: 0.1
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.953716777416674e-06
    epoch : 6 ; learning_rate : 4.953716777416674e-06 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4810637036608745
    Epoch 6, Loss: 1.7496416300659607, fit: 0.1
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.519942348189014e-06
    epoch : 7 ; learning_rate : 4.519942348189014e-06 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4996658084104306
    Epoch 7, Loss: 1.7410296020722171, fit: 0.03333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              5.670858071941901e-06
    epoch : 8 ; learning_rate : 5.670858071941901e-06 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4211662627574613
    Epoch 8, Loss: 1.7338211697169765, fit: 0.08333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7634199857573327_fit_0.08333333333333333_2024-01-03_005802
  self.fit : 0.08333333333333333
  self.loss : 1.7634199857573327
  current_accuracy : 0.1
  Epochs: 100
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          89
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.935257136112098e-06
    epoch : 0 ; learning_rate : 4.935257136112098e-06 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.465956009764658
    Epoch 0, Loss: 1.7265827195084111, fit: 0.1
    Epoch 0, Loss: 1.7265827195084111
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.663394419326845e-06
    epoch : 1 ; learning_rate : 4.663394419326845e-06 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.459097549084776
    Epoch 1, Loss: 1.7201087078540995, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.032192673433945e-06
    epoch : 2 ; learning_rate : 4.032192673433945e-06 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.435038005054473
    Epoch 2, Loss: 1.7137769075235028, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.684080200433007e-06
    epoch : 3 ; learning_rate : 3.684080200433007e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4677597271400662
    Epoch 3, Loss: 1.7079252939136633, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.381655733973153e-06
    epoch : 4 ; learning_rate : 4.381655733973153e-06 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.3922509106477576
    Epoch 4, Loss: 1.702185856797586, fit: 0.05
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              5.50747804685916e-06
    epoch : 5 ; learning_rate : 5.50747804685916e-06 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.363700936629543
    Epoch 5, Loss: 1.6955386553055916, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.825782620499916e-06
    epoch : 6 ; learning_rate : 3.825782620499916e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.3024352092231795
    Epoch 6, Loss: 1.6894481382577016, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.4283191313582233e-06
    epoch : 7 ; learning_rate : 3.4283191313582233e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.3882232780140185
    Epoch 7, Loss: 1.6847435107789375, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              5.12584624576568e-06
    epoch : 8 ; learning_rate : 5.12584624576568e-06 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.3858550960332823
    Epoch 8, Loss: 1.6794748985623897, fit: 0.15
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.9992639272246985e-06
    epoch : 9 ; learning_rate : 3.9992639272246985e-06 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.3157371028528757
    Epoch 9, Loss: 1.6737628154916833, fit: 0.15
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.901591199869857e-06
    epoch : 10 ; learning_rate : 3.901591199869857e-06 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.3662750818605
    Epoch 10, Loss: 1.6685590695780197, fit: 0.25
    Epoch 10, Loss: 1.6685590695780197
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.939296712523065e-06
    epoch : 11 ; learning_rate : 2.939296712523065e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.2739706160018334
    Epoch 11, Loss: 1.664132281238891, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.314877324604885e-06
    epoch : 12 ; learning_rate : 4.314877324604885e-06 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.335188101304252
    Epoch 12, Loss: 1.6596045951438332, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.9807352158447874e-06
    epoch : 13 ; learning_rate : 4.9807352158447874e-06 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.2463580561397676
    Epoch 13, Loss: 1.65350500913332, fit: 0.15
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.932163252276673e-06
    epoch : 14 ; learning_rate : 3.932163252276673e-06 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.264006571606206
    Epoch 14, Loss: 1.6481164591917292, fit: 0.15
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.94404422180063e-06
    epoch : 15 ; learning_rate : 3.94404422180063e-06 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.3291221882260078
    Epoch 15, Loss: 1.6431108607666023, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.611569735186743e-06
    epoch : 16 ; learning_rate : 3.611569735186743e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.2849137857639037
    Epoch 16, Loss: 1.6384766504399213, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.3142924465456386e-06
    epoch : 17 ; learning_rate : 4.3142924465456386e-06 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.3031813555536003
    Epoch 17, Loss: 1.633507572795946, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.625108400591349e-06
    epoch : 18 ; learning_rate : 4.625108400591349e-06 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.3455002393125293
    Epoch 18, Loss: 1.6279699626908146, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.2102412664026568e-06
    epoch : 19 ; learning_rate : 3.2102412664026568e-06 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.3135324276544493
    Epoch 19, Loss: 1.623149832727065, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.5751358621315922e-06
    epoch : 20 ; learning_rate : 3.5751358621315922e-06 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.2975661899831863
    Epoch 20, Loss: 1.619101456560335, fit: 0.13333333333333333
    Epoch 20, Loss: 1.619101456560335
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.145228867873381e-06
    epoch : 21 ; learning_rate : 4.145228867873381e-06 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.313587725484236
    Epoch 21, Loss: 1.6144521752941816, fit: 0.2833333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.464468680634305e-06
    epoch : 22 ; learning_rate : 2.464468680634305e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 1.2603155145104392
    Epoch 22, Loss: 1.610534415204441, fit: 0.1
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.843533110721106e-06
    epoch : 23 ; learning_rate : 4.843533110721106e-06 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.3179523400826707
    Epoch 23, Loss: 1.6063168057247081, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.8792681192060166e-06
    epoch : 24 ; learning_rate : 3.8792681192060166e-06 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.2421705842355253
    Epoch 24, Loss: 1.601100121651639, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.362044471638882e-06
    epoch : 25 ; learning_rate : 4.362044471638882e-06 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.2962227693230772
    Epoch 25, Loss: 1.5962698543958416, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.4778284803612743e-06
    epoch : 26 ; learning_rate : 3.4778284803612743e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.28771035347683
    Epoch 26, Loss: 1.5914132024720409, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.6570615964892074e-06
    epoch : 27 ; learning_rate : 3.6570615964892074e-06 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.2863841128078521
    Epoch 27, Loss: 1.5869784664764799, fit: 0.15
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.6140180321644678e-06
    epoch : 28 ; learning_rate : 3.6140180321644678e-06 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.2955703128614309
    Epoch 28, Loss: 1.582392310639568, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.784462645963819e-06
    epoch : 29 ; learning_rate : 3.784462645963819e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.283437655276564
    Epoch 29, Loss: 1.5777730348753158, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.996407595775485e-06
    epoch : 30 ; learning_rate : 3.996407595775485e-06 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.2578916178656172
    Epoch 30, Loss: 1.5728857907065068, fit: 0.26666666666666666
    Epoch 30, Loss: 1.5728857907065068
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.7868251614966182e-06
    epoch : 31 ; learning_rate : 2.7868251614966182e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 1.237138778778715
    Epoch 31, Loss: 1.5685864564822543, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.88485379458025e-06
    epoch : 32 ; learning_rate : 2.88485379458025e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.106967575962311
    Epoch 32, Loss: 1.56504152446192, fit: 0.1
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.457854779917346e-06
    epoch : 33 ; learning_rate : 4.457854779917346e-06 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.1730380718641036
    Epoch 33, Loss: 1.5605945786244202, fit: 0.21666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.010129886744271e-06
    epoch : 34 ; learning_rate : 3.010129886744271e-06 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.2363886976654024
    Epoch 34, Loss: 1.5561180121993945, fit: 0.26666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.3669824851713237e-06
    epoch : 35 ; learning_rate : 2.3669824851713237e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 1.1652930138551725
    Epoch 35, Loss: 1.5529574101214036, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.304280163674426e-06
    epoch : 36 ; learning_rate : 3.304280163674426e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.1491921638960045
    Epoch 36, Loss: 1.5497024765676137, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.2492896831617825e-06
    epoch : 37 ; learning_rate : 4.2492896831617825e-06 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.230014001058677
    Epoch 37, Loss: 1.5452265906393554, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.3019894071177987e-06
    epoch : 38 ; learning_rate : 3.3019894071177987e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.2127066712720713
    Epoch 38, Loss: 1.540726515554021, fit: 0.21666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.956452329033741e-06
    epoch : 39 ; learning_rate : 2.956452329033741e-06 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.1790600126874873
    Epoch 39, Loss: 1.5370468877387253, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.2209170128809577e-06
    epoch : 40 ; learning_rate : 3.2209170128809577e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.2034683503458041
    Epoch 40, Loss: 1.5333003447860867, fit: 0.13333333333333333
    Epoch 40, Loss: 1.5333003447860867
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.191939729828329e-06
    epoch : 41 ; learning_rate : 4.191939729828329e-06 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.1478084921617913
    Epoch 41, Loss: 1.5288106022208026, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.84424859684289e-06
    epoch : 42 ; learning_rate : 2.84424859684289e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.1501474089357164
    Epoch 42, Loss: 1.524379638201874, fit: 0.26666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.4886911898517243e-06
    epoch : 43 ; learning_rate : 2.4886911898517243e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 1.1704520100566531
    Epoch 43, Loss: 1.5209809665382839, fit: 0.15
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.007023912790444e-06
    epoch : 44 ; learning_rate : 4.007023912790444e-06 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.1659442785109246
    Epoch 44, Loss: 1.5167264899748039, fit: 0.2833333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.3093082913467814e-06
    epoch : 45 ; learning_rate : 2.3093082913467814e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 1.152455000738279
    Epoch 45, Loss: 1.5127362742400068, fit: 0.3
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.205126061939697e-06
    epoch : 46 ; learning_rate : 2.205126061939697e-06 ; fit : 0.3
    batch_size :          60
    best_batch_loss : 1.166856687598751
    Epoch 46, Loss: 1.5098766965114054, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.1727764647881305e-06
    epoch : 47 ; learning_rate : 4.1727764647881305e-06 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.1851174017156556
    Epoch 47, Loss: 1.5057927026195241, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.691473425612597e-06
    epoch : 48 ; learning_rate : 2.691473425612597e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.1859889695560417
    Epoch 48, Loss: 1.5014528570952743, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.6621012211074115e-06
    epoch : 49 ; learning_rate : 3.6621012211074115e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.0986306047706802
    Epoch 49, Loss: 1.4974272936981987, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.2735384387887765e-06
    epoch : 50 ; learning_rate : 3.2735384387887765e-06 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.125216600297463
    Epoch 50, Loss: 1.4932130174023246, fit: 0.16666666666666666
    Epoch 50, Loss: 1.4932130174023246
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.5210478351712356e-06
    epoch : 51 ; learning_rate : 3.5210478351712356e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.1386911440757612
    Epoch 51, Loss: 1.4889813008950046, fit: 0.3
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.1591893954509622e-06
    epoch : 52 ; learning_rate : 2.1591893954509622e-06 ; fit : 0.3
    batch_size :          60
    best_batch_loss : 1.105900391623232
    Epoch 52, Loss: 1.4856194598057582, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.8194141086459994e-06
    epoch : 53 ; learning_rate : 2.8194141086459994e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.1345667699221904
    Epoch 53, Loss: 1.482554313560567, fit: 0.3333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              1.8604197831508672e-06
    epoch : 54 ; learning_rate : 1.8604197831508672e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 1.1166917148084574
    Epoch 54, Loss: 1.4797703231675448, fit: 0.21666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.0622371743363136e-06
    epoch : 55 ; learning_rate : 3.0622371743363136e-06 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.1045547890298262
    Epoch 55, Loss: 1.476856896747155, fit: 0.21666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.800314784914361e-06
    epoch : 56 ; learning_rate : 2.800314784914361e-06 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.1876503705974057
    Epoch 56, Loss: 1.473352075805467, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.6918591256715822e-06
    epoch : 57 ; learning_rate : 2.6918591256715822e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.139542169733976
    Epoch 57, Loss: 1.4700994314851314, fit: 0.21666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.0673192691701966e-06
    epoch : 58 ; learning_rate : 3.0673192691701966e-06 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.129189120337348
    Epoch 58, Loss: 1.4666935479197616, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.5256620480164222e-06
    epoch : 59 ; learning_rate : 3.5256620480164222e-06 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.0899266310055207
    Epoch 59, Loss: 1.4628145503412175, fit: 0.21666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.1833890615948262e-06
    epoch : 60 ; learning_rate : 3.1833890615948262e-06 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.1551977251233145
    Epoch 60, Loss: 1.4589519662889137, fit: 0.26666666666666666
    Epoch 60, Loss: 1.4589519662889137
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.3562558177757487e-06
    epoch : 61 ; learning_rate : 2.3562558177757487e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 1.1089463959193162
    Epoch 61, Loss: 1.455799053291512, fit: 0.21666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.01268238267585e-06
    epoch : 62 ; learning_rate : 3.01268238267585e-06 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.1148396699944263
    Epoch 62, Loss: 1.4528600276767811, fit: 0.35
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              1.9014548380142697e-06
    epoch : 63 ; learning_rate : 1.9014548380142697e-06 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 1.0528170302883058
    Epoch 63, Loss: 1.4500623322481785, fit: 0.26666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.602117587647268e-06
    epoch : 64 ; learning_rate : 2.602117587647268e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 1.108433786393094
    Epoch 64, Loss: 1.4475711840562326, fit: 0.3333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              1.708663727363511e-06
    epoch : 65 ; learning_rate : 1.708663727363511e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 1.0960114618800432
    Epoch 65, Loss: 1.4451644808700572, fit: 0.36666666666666664
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              1.5803075255471731e-06
    epoch : 66 ; learning_rate : 1.5803075255471731e-06 ; fit : 0.36666666666666664
    batch_size :          60
    best_batch_loss : 1.04925108003581
    Epoch 66, Loss: 1.4433436224738068, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.875284366078391e-06
    epoch : 67 ; learning_rate : 2.875284366078391e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 1.0307422021474684
    Epoch 67, Loss: 1.4408779557285476, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.401963611867277e-06
    epoch : 68 ; learning_rate : 3.401963611867277e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.0675744418669202
    Epoch 68, Loss: 1.4374016468159143, fit: 0.2833333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.370651675572477e-06
    epoch : 69 ; learning_rate : 2.370651675572477e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 1.1280698398024316
    Epoch 69, Loss: 1.4341640004790308, fit: 0.36666666666666664
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              1.7150180841882883e-06
    epoch : 70 ; learning_rate : 1.7150180841882883e-06 ; fit : 0.36666666666666664
    batch_size :          60
    best_batch_loss : 1.0659026434530299
    Epoch 70, Loss: 1.4318863018314263, fit: 0.26666666666666666
    Epoch 70, Loss: 1.4318863018314263
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.5353484598590327e-06
    epoch : 71 ; learning_rate : 2.5353484598590327e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 1.0634505317873313
    Epoch 71, Loss: 1.4295203961169363, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.678054787579691e-06
    epoch : 72 ; learning_rate : 2.678054787579691e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 0.9989083560929547
    Epoch 72, Loss: 1.4265397673362503, fit: 0.3333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              1.7963332610841348e-06
    epoch : 73 ; learning_rate : 1.7963332610841348e-06 ; fit : 0.3333333333333333
    batch_size :          60
    best_batch_loss : 1.0913201140060944
    Epoch 73, Loss: 1.4240567418015286, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.2415226078333256e-06
    epoch : 74 ; learning_rate : 3.2415226078333256e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.1093146078405278
    Epoch 74, Loss: 1.4212497568920384, fit: 0.25
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.6950761682066474e-06
    epoch : 75 ; learning_rate : 2.6950761682066474e-06 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.0740484314880987
    Epoch 75, Loss: 1.417873450477271, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.3181301887035615e-06
    epoch : 76 ; learning_rate : 3.3181301887035615e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.0191118714297556
    Epoch 76, Loss: 1.4145097173922347, fit: 0.26666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.3174419441501626e-06
    epoch : 77 ; learning_rate : 2.3174419441501626e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 1.0556780029967263
    Epoch 77, Loss: 1.411368183815878, fit: 0.21666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.8909009616927117e-06
    epoch : 78 ; learning_rate : 2.8909009616927117e-06 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.058236182696253
    Epoch 78, Loss: 1.4084458687925276, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.897890499277987e-06
    epoch : 79 ; learning_rate : 2.897890499277987e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 0.9924001358829215
    Epoch 79, Loss: 1.4052488528553797, fit: 0.21666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.8893672990305778e-06
    epoch : 80 ; learning_rate : 2.8893672990305778e-06 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 0.9777744692548348
    Epoch 80, Loss: 1.402035477171781, fit: 0.26666666666666666
    Epoch 80, Loss: 1.402035477171781
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.439650399360453e-06
    epoch : 81 ; learning_rate : 2.439650399360453e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 1.0482555955473671
    Epoch 81, Loss: 1.3991582265811835, fit: 0.26666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.246199795296174e-06
    epoch : 82 ; learning_rate : 2.246199795296174e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 1.0294777659738206
    Epoch 82, Loss: 1.3966151365854536, fit: 0.2
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.283968769146611e-06
    epoch : 83 ; learning_rate : 3.283968769146611e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.0204307803525692
    Epoch 83, Loss: 1.3936481564216683, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              3.7109071866388783e-06
    epoch : 84 ; learning_rate : 3.7109071866388783e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.021027743817726
    Epoch 84, Loss: 1.3898071602042077, fit: 0.23333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.5357458955332135e-06
    epoch : 85 ; learning_rate : 2.5357458955332135e-06 ; fit : 0.23333333333333334
    batch_size :          60
    best_batch_loss : 0.9594338291338288
    Epoch 85, Loss: 1.386514127008156, fit: 0.2833333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.2382307965319303e-06
    epoch : 86 ; learning_rate : 2.2382307965319303e-06 ; fit : 0.2833333333333333
    batch_size :          60
    best_batch_loss : 1.0543112704846185
    Epoch 86, Loss: 1.383974649005492, fit: 0.26666666666666666
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              2.4416846520467063e-06
    epoch : 87 ; learning_rate : 2.4416846520467063e-06 ; fit : 0.26666666666666666
    batch_size :          60
    best_batch_loss : 0.9705880726075958
    Epoch 87, Loss: 1.3814409517191477, fit: 0.1
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              4.576048880381622e-06
    epoch : 88 ; learning_rate : 4.576048880381622e-06 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.007027519490765
    Epoch 88, Loss: 1.3777010215245684, fit: 0.4
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.1831605787243218_fit_0.4_2024-01-03_010823
  self.fit : 0.4
  self.loss : 1.1831605787243218
  current_accuracy : 0.2842
   Accuracy mean: 0.0794
   Accuracy mean: 0.1
   Accuracy mean: 0.2842
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5686098860632265
    Epoch 0, Loss: 1.8344683637335062, fit: 0.05
    Epoch 0, Loss: 1.8344683637335062
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.866914186587684_fit_0.05_2024-01-03_010830
  self.fit : 0.05
  self.loss : 1.866914186587684
  current_accuracy : 0.0518
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.506884047642719e-07
    epoch : 0 ; learning_rate : 6.506884047642719e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.6203283648214573
    Epoch 0, Loss: 1.8339471249886983, fit: 0.03333333333333333
    Epoch 0, Loss: 1.8339471249886983
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.910543215798555e-07
    epoch : 1 ; learning_rate : 6.910543215798555e-07 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.577280844422448
    Epoch 1, Loss: 1.8328562718590666, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.905424852697999e-07
    epoch : 2 ; learning_rate : 5.905424852697999e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.6007568834100696
    Epoch 2, Loss: 1.8318037521302535, fit: 0.016666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              7.108707561619244e-07
    epoch : 3 ; learning_rate : 7.108707561619244e-07 ; fit : 0.016666666666666666
    batch_size :          60
    best_batch_loss : 1.588129041817807
    Epoch 3, Loss: 1.8307365013871888, fit: 0.05
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.458773722354358e-07
    epoch : 4 ; learning_rate : 6.458773722354358e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5808460595429314
    Epoch 4, Loss: 1.8295876488719716, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.282856284507168e-07
    epoch : 5 ; learning_rate : 5.282856284507168e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5912700577176337
    Epoch 5, Loss: 1.8286498615126314, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.790207633892874e-07
    epoch : 6 ; learning_rate : 5.790207633892874e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.6002767840573215
    Epoch 6, Loss: 1.8277476998775501, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.781139169550771e-07
    epoch : 7 ; learning_rate : 5.781139169550771e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5860069818743239
    Epoch 7, Loss: 1.8268219196960505, fit: 0.016666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.645947768983531e-07
    epoch : 8 ; learning_rate : 6.645947768983531e-07 ; fit : 0.016666666666666666
    batch_size :          60
    best_batch_loss : 1.6209066033293449
    Epoch 8, Loss: 1.8257700368506513, fit: 0.03333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.8943348364923254_fit_0.03333333333333333_2024-01-03_010933
  self.fit : 0.03333333333333333
  self.loss : 1.8943348364923254
  current_accuracy : 0.0547
  Epochs: 100
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          89
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.797829033635829e-07
    epoch : 0 ; learning_rate : 6.797829033635829e-07 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.6304027286862393
    Epoch 0, Loss: 1.82471088632857, fit: 0.05
    Epoch 0, Loss: 1.82471088632857
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.11041393565598e-07
    epoch : 1 ; learning_rate : 6.11041393565598e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5967559376424008
    Epoch 1, Loss: 1.823643890118414, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.868924565650847e-07
    epoch : 2 ; learning_rate : 5.868924565650847e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.6241648793373136
    Epoch 2, Loss: 1.8226591814086661, fit: 0.016666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.818783177271458e-07
    epoch : 3 ; learning_rate : 6.818783177271458e-07 ; fit : 0.016666666666666666
    batch_size :          60
    best_batch_loss : 1.6317849608792092
    Epoch 3, Loss: 1.8216509781138843, fit: 0.05
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.630537067116055e-07
    epoch : 4 ; learning_rate : 6.630537067116055e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5531392238391362
    Epoch 4, Loss: 1.8206378746964802, fit: 0.016666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              7.035253513567773e-07
    epoch : 5 ; learning_rate : 7.035253513567773e-07 ; fit : 0.016666666666666666
    batch_size :          60
    best_batch_loss : 1.5841528984623983
    Epoch 5, Loss: 1.8194942423079163, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.25768189088316e-07
    epoch : 6 ; learning_rate : 5.25768189088316e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5602082282760723
    Epoch 6, Loss: 1.8185296427890527, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.765376748200184e-07
    epoch : 7 ; learning_rate : 5.765376748200184e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.534085110517513
    Epoch 7, Loss: 1.8176264047858763, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.711079799658849e-07
    epoch : 8 ; learning_rate : 5.711079799658849e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5823600822598396
    Epoch 8, Loss: 1.8167552495296178, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.894463665200962e-07
    epoch : 9 ; learning_rate : 5.894463665200962e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5453782887986403
    Epoch 9, Loss: 1.8157853226100351, fit: 0.03333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.434830774683374e-07
    epoch : 10 ; learning_rate : 6.434830774683374e-07 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.570172539893025
    Epoch 10, Loss: 1.8147665730114504, fit: 0.1
    Epoch 10, Loss: 1.8147665730114504
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.382088342387615e-07
    epoch : 11 ; learning_rate : 5.382088342387615e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5491334984654626
    Epoch 11, Loss: 1.8138146337998258, fit: 0.05
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.219185948909818e-07
    epoch : 12 ; learning_rate : 6.219185948909818e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.6018888674107474
    Epoch 12, Loss: 1.8127510301259557, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.99692674707849e-07
    epoch : 13 ; learning_rate : 5.99692674707849e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.596184601995985
    Epoch 13, Loss: 1.811685867149003, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.966873240716911e-07
    epoch : 14 ; learning_rate : 5.966873240716911e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5952937495988018
    Epoch 14, Loss: 1.8106645553254337, fit: 0.0
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              7.226347091881335e-07
    epoch : 15 ; learning_rate : 7.226347091881335e-07 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5182826331095114
    Epoch 15, Loss: 1.8094627115902766, fit: 0.05
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.115489287229629e-07
    epoch : 16 ; learning_rate : 6.115489287229629e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5816438928764094
    Epoch 16, Loss: 1.8083115717662475, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.117914892760886e-07
    epoch : 17 ; learning_rate : 6.117914892760886e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5877209512952741
    Epoch 17, Loss: 1.8072772944349202, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.190644590227245e-07
    epoch : 18 ; learning_rate : 5.190644590227245e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.6034482196214912
    Epoch 18, Loss: 1.8062526253265176, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.654373827392027e-07
    epoch : 19 ; learning_rate : 5.654373827392027e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5433371282240615
    Epoch 19, Loss: 1.8053878963132588, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.815321001152569e-07
    epoch : 20 ; learning_rate : 5.815321001152569e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5875773009286935
    Epoch 20, Loss: 1.8043609661919717, fit: 0.0
    Epoch 20, Loss: 1.8043609661919717
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              7.314082170978491e-07
    epoch : 21 ; learning_rate : 7.314082170978491e-07 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5778492514963338
    Epoch 21, Loss: 1.8032101813849966, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.154718675219751e-07
    epoch : 22 ; learning_rate : 6.154718675219751e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.6042102587122438
    Epoch 22, Loss: 1.802069437133725, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.974133920213422e-07
    epoch : 23 ; learning_rate : 4.974133920213422e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.542252038580276
    Epoch 23, Loss: 1.8010519483239054, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.987188683759119e-07
    epoch : 24 ; learning_rate : 4.987188683759119e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5735783870732432
    Epoch 24, Loss: 1.8001857889975486, fit: 0.05
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.285858401989157e-07
    epoch : 25 ; learning_rate : 6.285858401989157e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5439602543627824
    Epoch 25, Loss: 1.799190833904049, fit: 0.03333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.660860523971007e-07
    epoch : 26 ; learning_rate : 6.660860523971007e-07 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.580380140359427
    Epoch 26, Loss: 1.7980667076265249, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.821798761506302e-07
    epoch : 27 ; learning_rate : 5.821798761506302e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5553572359697059
    Epoch 27, Loss: 1.7969886011412148, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.071568907161787e-07
    epoch : 28 ; learning_rate : 6.071568907161787e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.597320299419647
    Epoch 28, Loss: 1.795985343504588, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.376826591980823e-07
    epoch : 29 ; learning_rate : 5.376826591980823e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5406002528899843
    Epoch 29, Loss: 1.795001401722993, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.987115825889426e-07
    epoch : 30 ; learning_rate : 5.987115825889426e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5056030469761505
    Epoch 30, Loss: 1.7940869008352387, fit: 0.06666666666666667
    Epoch 30, Loss: 1.7940869008352387
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.951825040783887e-07
    epoch : 31 ; learning_rate : 5.951825040783887e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5721640058477553
    Epoch 31, Loss: 1.7930447638217486, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.517895176772207e-07
    epoch : 32 ; learning_rate : 5.517895176772207e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.518575677156673
    Epoch 32, Loss: 1.792081821079791, fit: 0.15
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.2875268042412534e-07
    epoch : 33 ; learning_rate : 4.2875268042412534e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.5192420930944293
    Epoch 33, Loss: 1.791231277509976, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.647727856178108e-07
    epoch : 34 ; learning_rate : 4.647727856178108e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.5767376850377237
    Epoch 34, Loss: 1.7904312102714928, fit: 0.05
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.826034173380324e-07
    epoch : 35 ; learning_rate : 5.826034173380324e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.538437686509378
    Epoch 35, Loss: 1.7895391554460172, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.735936898785952e-07
    epoch : 36 ; learning_rate : 4.735936898785952e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.5585729381641484
    Epoch 36, Loss: 1.7885616133723843, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.909192090493218e-07
    epoch : 37 ; learning_rate : 5.909192090493218e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5627682841818473
    Epoch 37, Loss: 1.7875669301777612, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.735256431128033e-07
    epoch : 38 ; learning_rate : 5.735256431128033e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.498658845757855
    Epoch 38, Loss: 1.7865435665782485, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.193085481330941e-07
    epoch : 39 ; learning_rate : 5.193085481330941e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4449012127109622
    Epoch 39, Loss: 1.7855772217517447, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.794157161959646e-07
    epoch : 40 ; learning_rate : 4.794157161959646e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.495191890147477
    Epoch 40, Loss: 1.784665714210297, fit: 0.05
    Epoch 40, Loss: 1.784665714210297
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.091794601848017e-07
    epoch : 41 ; learning_rate : 6.091794601848017e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5517941043076897
    Epoch 41, Loss: 1.783690260538581, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.253262284437452e-07
    epoch : 42 ; learning_rate : 5.253262284437452e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.525039179993352
    Epoch 42, Loss: 1.7826519854659564, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.996866936950387e-07
    epoch : 43 ; learning_rate : 5.996866936950387e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.501303073757729
    Epoch 43, Loss: 1.7816403173688906, fit: 0.05
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.28709436517946e-07
    epoch : 44 ; learning_rate : 6.28709436517946e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5375077974590012
    Epoch 44, Loss: 1.78054156727456, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.686030431011261e-07
    epoch : 45 ; learning_rate : 5.686030431011261e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.535309127678542
    Epoch 45, Loss: 1.7795068162003806, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.255427153492532e-07
    epoch : 46 ; learning_rate : 5.255427153492532e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5213188009903447
    Epoch 46, Loss: 1.7785084417417085, fit: 0.03333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.495935072292853e-07
    epoch : 47 ; learning_rate : 6.495935072292853e-07 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4332441518519299
    Epoch 47, Loss: 1.7775345446158235, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.825743176670285e-07
    epoch : 48 ; learning_rate : 5.825743176670285e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5341948962906329
    Epoch 48, Loss: 1.7764682424712803, fit: 0.016666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.930262205430973e-07
    epoch : 49 ; learning_rate : 6.930262205430973e-07 ; fit : 0.016666666666666666
    batch_size :          60
    best_batch_loss : 1.5022330371697048
    Epoch 49, Loss: 1.7754386419390942, fit: 0.03333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.33407806040021e-07
    epoch : 50 ; learning_rate : 6.33407806040021e-07 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4643370691186148
    Epoch 50, Loss: 1.774308649789523, fit: 0.05
    Epoch 50, Loss: 1.774308649789523
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.5640105432231e-07
    epoch : 51 ; learning_rate : 6.5640105432231e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.534782161987948
    Epoch 51, Loss: 1.7732628489493696, fit: 0.05
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.276855797781714e-07
    epoch : 52 ; learning_rate : 6.276855797781714e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5490279959732534
    Epoch 52, Loss: 1.772259487637531, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.205376792615382e-07
    epoch : 53 ; learning_rate : 5.205376792615382e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5070845683674192
    Epoch 53, Loss: 1.771324573798262, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.417379551646781e-07
    epoch : 54 ; learning_rate : 5.417379551646781e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4830880566908424
    Epoch 54, Loss: 1.7704842696249916, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.46354396017152e-07
    epoch : 55 ; learning_rate : 5.46354396017152e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5073951511151444
    Epoch 55, Loss: 1.769637208864908, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.234415473230778e-07
    epoch : 56 ; learning_rate : 5.234415473230778e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5319184984523742
    Epoch 56, Loss: 1.768810660605932, fit: 0.05
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.459530752913365e-07
    epoch : 57 ; learning_rate : 6.459530752913365e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5136392010823045
    Epoch 57, Loss: 1.767881815726914, fit: 0.03333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.648322162813527e-07
    epoch : 58 ; learning_rate : 6.648322162813527e-07 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.518542444850756
    Epoch 58, Loss: 1.7668321224792098, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.051022579710345e-07
    epoch : 59 ; learning_rate : 6.051022579710345e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4874338801238616
    Epoch 59, Loss: 1.7658174536055136, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.6156464909707473e-07
    epoch : 60 ; learning_rate : 4.6156464909707473e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4715510275229018
    Epoch 60, Loss: 1.765030805811553, fit: 0.06666666666666667
    Epoch 60, Loss: 1.765030805811553
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.911102375522953e-07
    epoch : 61 ; learning_rate : 5.911102375522953e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5191338189322978
    Epoch 61, Loss: 1.7641893271855582, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.726356623337686e-07
    epoch : 62 ; learning_rate : 4.726356623337686e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4906986112350822
    Epoch 62, Loss: 1.7633917316756562, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.790524338819875e-07
    epoch : 63 ; learning_rate : 5.790524338819875e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.527458686542154
    Epoch 63, Loss: 1.7625621090483559, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.106812145840785e-07
    epoch : 64 ; learning_rate : 5.106812145840785e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5025268596753534
    Epoch 64, Loss: 1.7617616023180538, fit: 0.03333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.619257076099923e-07
    epoch : 65 ; learning_rate : 6.619257076099923e-07 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4871767700453224
    Epoch 65, Loss: 1.7608605647494766, fit: 0.15
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.449447928751824e-07
    epoch : 66 ; learning_rate : 4.449447928751824e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4987707247403461
    Epoch 66, Loss: 1.7599795901670903, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.771681725343355e-07
    epoch : 67 ; learning_rate : 5.771681725343355e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4363509624858868
    Epoch 67, Loss: 1.7591780473562688, fit: 0.05
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.168684498567593e-07
    epoch : 68 ; learning_rate : 6.168684498567593e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4776522267038845
    Epoch 68, Loss: 1.758206878219115, fit: 0.05
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.377289612611501e-07
    epoch : 69 ; learning_rate : 6.377289612611501e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.434394469782766
    Epoch 69, Loss: 1.75716384501364, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.522897306937768e-07
    epoch : 70 ; learning_rate : 5.522897306937768e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.496267462182839
    Epoch 70, Loss: 1.7561866988862596, fit: 0.13333333333333333
    Epoch 70, Loss: 1.7561866988862596
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.5900457611831054e-07
    epoch : 71 ; learning_rate : 4.5900457611831054e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4644882437888769
    Epoch 71, Loss: 1.75532854935491, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.950327758840019e-07
    epoch : 72 ; learning_rate : 4.950327758840019e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4669776825489655
    Epoch 72, Loss: 1.75450947361874, fit: 0.15
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.569870435380344e-07
    epoch : 73 ; learning_rate : 4.569870435380344e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.531004669635649
    Epoch 73, Loss: 1.7537043990774532, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.117451923133858e-07
    epoch : 74 ; learning_rate : 5.117451923133858e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5122449157945517
    Epoch 74, Loss: 1.7528976465914796, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.921049324468114e-07
    epoch : 75 ; learning_rate : 4.921049324468114e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4397800428879715
    Epoch 75, Loss: 1.7520944516243087, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.876086639562966e-07
    epoch : 76 ; learning_rate : 4.876086639562966e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.472823561954697
    Epoch 76, Loss: 1.7512848137298662, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.56133814107736e-07
    epoch : 77 ; learning_rate : 5.56133814107736e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4504898273231808
    Epoch 77, Loss: 1.7504611925863136, fit: 0.05
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.113377205059734e-07
    epoch : 78 ; learning_rate : 6.113377205059734e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5120058324377534
    Epoch 78, Loss: 1.7495150628557914, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.7411128486078035e-07
    epoch : 79 ; learning_rate : 4.7411128486078035e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4278837145591758
    Epoch 79, Loss: 1.7487047871558443, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.236339323689625e-07
    epoch : 80 ; learning_rate : 5.236339323689625e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5139088666272689
    Epoch 80, Loss: 1.7479211840261253, fit: 0.06666666666666667
    Epoch 80, Loss: 1.7479211840261253
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              6.097502586725144e-07
    epoch : 81 ; learning_rate : 6.097502586725144e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4312070143635742
    Epoch 81, Loss: 1.7470626582502513, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.96433077856698e-07
    epoch : 82 ; learning_rate : 5.96433077856698e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.492362721340939
    Epoch 82, Loss: 1.746170268570461, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.984250138592766e-07
    epoch : 83 ; learning_rate : 4.984250138592766e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4752263005550634
    Epoch 83, Loss: 1.7453400336114113, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.318443293381118e-07
    epoch : 84 ; learning_rate : 5.318443293381118e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4447550771436615
    Epoch 84, Loss: 1.7445923807278758, fit: 0.1
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.111975032716178e-07
    epoch : 85 ; learning_rate : 5.111975032716178e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4757907636262497
    Epoch 85, Loss: 1.7438246559275947, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              5.848895297505231e-07
    epoch : 86 ; learning_rate : 5.848895297505231e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4566303457976204
    Epoch 86, Loss: 1.7430530573376986, fit: 0.15
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              4.356073626662706e-07
    epoch : 87 ; learning_rate : 4.356073626662706e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4027671046576222
    Epoch 87, Loss: 1.7423233410843657, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              3.9504733105967616e-07
    epoch : 88 ; learning_rate : 3.9504733105967616e-07 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.463659318013819
    Epoch 88, Loss: 1.7417152822974546, fit: 0.1
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7207608051014838_fit_0.1_2024-01-03_012021
  self.fit : 0.1
  self.loss : 1.7207608051014838
  current_accuracy : 0.083
   Accuracy mean: 0.0518
   Accuracy mean: 0.0547
   Accuracy mean: 0.083
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.3796938171615012
    Epoch 0, Loss: 1.6767958427999659, fit: 0.11666666666666667
    Epoch 0, Loss: 1.6767958427999659
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7192855317438727_fit_0.11666666666666667_2024-01-03_012029
  self.fit : 0.11666666666666667
  self.loss : 1.7192855317438727
  current_accuracy : 0.1348
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.082109584967336e-10
    epoch : 0 ; learning_rate : 5.082109584967336e-10 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.3859005523208117
    Epoch 0, Loss: 1.6767956246971734, fit: 0.06666666666666667
    Epoch 0, Loss: 1.6767956246971734
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.83337703284725e-10
    epoch : 1 ; learning_rate : 5.83337703284725e-10 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.3821163651524675
    Epoch 1, Loss: 1.6767951282807356, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.986053805953672e-10
    epoch : 2 ; learning_rate : 4.986053805953672e-10 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4135699652824176
    Epoch 2, Loss: 1.6767946757013878, fit: 0.15
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.5125831466612854e-10
    epoch : 3 ; learning_rate : 4.5125831466612854e-10 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.3785803291585674
    Epoch 3, Loss: 1.6767942337869965, fit: 0.15
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.3359026614134747e-10
    epoch : 4 ; learning_rate : 4.3359026614134747e-10 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.3944039290089398
    Epoch 4, Loss: 1.6767938336222257, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.728945701824347e-10
    epoch : 5 ; learning_rate : 4.728945701824347e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.318333849274008
    Epoch 5, Loss: 1.6767934271052014, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.424528215514123e-10
    epoch : 6 ; learning_rate : 4.424528215514123e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.3425441751202285
    Epoch 6, Loss: 1.6767930133079618, fit: 0.15
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.48437332027806e-10
    epoch : 7 ; learning_rate : 4.48437332027806e-10 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.3832453029474727
    Epoch 7, Loss: 1.6767926397029684, fit: 0.16666666666666666
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              3.7567150865465475e-10
    epoch : 8 ; learning_rate : 3.7567150865465475e-10 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.3770776037137769
    Epoch 8, Loss: 1.6767922578209085, fit: 0.2
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.52936513898466_fit_0.2_2024-01-03_012137
  self.fit : 0.2
  self.loss : 1.52936513898466
  current_accuracy : 0.1348
  Epochs: 100
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          89
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              3.577120411284349e-10
    epoch : 0 ; learning_rate : 3.577120411284349e-10 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.3234086255721436
    Epoch 0, Loss: 1.676791937508442, fit: 0.11666666666666667
    Epoch 0, Loss: 1.676791937508442
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.159532676195755e-10
    epoch : 1 ; learning_rate : 5.159532676195755e-10 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.3053625040089551
    Epoch 1, Loss: 1.6767915372105002, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.283387732961193e-10
    epoch : 2 ; learning_rate : 5.283387732961193e-10 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.3700337567215328
    Epoch 2, Loss: 1.676791107906277, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.520057815163817e-10
    epoch : 3 ; learning_rate : 4.520057815163817e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.354216898477136
    Epoch 3, Loss: 1.6767906558436632, fit: 0.1
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.632077008400651e-10
    epoch : 4 ; learning_rate : 5.632077008400651e-10 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.3722102324466368
    Epoch 4, Loss: 1.676790185156613, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.672006571797141e-10
    epoch : 5 ; learning_rate : 5.672006571797141e-10 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.3814012269943143
    Epoch 5, Loss: 1.6767896553981827, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.972766973390315e-10
    epoch : 6 ; learning_rate : 4.972766973390315e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.3738600025138827
    Epoch 6, Loss: 1.6767892400560327, fit: 0.16666666666666666
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.1553218605432505e-10
    epoch : 7 ; learning_rate : 4.1553218605432505e-10 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4216749201935166
    Epoch 7, Loss: 1.676788801512241, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              5.798273726942441e-10
    epoch : 8 ; learning_rate : 5.798273726942441e-10 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.3965917259068321
    Epoch 8, Loss: 1.6767884076404698, fit: 0.18333333333333332
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.0524222185824305e-10
    epoch : 9 ; learning_rate : 4.0524222185824305e-10 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.318883698585051
    Epoch 9, Loss: 1.6767879522615476, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.570985806476246e-10
    epoch : 10 ; learning_rate : 4.570985806476246e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.379823807336264
    Epoch 10, Loss: 1.6767875777417982, fit: 0.06666666666666667
    Epoch 10, Loss: 1.6767875777417982
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              6.005258944868218e-10
    epoch : 11 ; learning_rate : 6.005258944868218e-10 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.3988884492877542
    Epoch 11, Loss: 1.6767870452040485, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              6.152681520375829e-10
    epoch : 12 ; learning_rate : 6.152681520375829e-10 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.3276654123668787
    Epoch 12, Loss: 1.6767865548452432, fit: 0.16666666666666666
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.3905036576736483e-10
    epoch : 13 ; learning_rate : 4.3905036576736483e-10 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.3199675303768754
    Epoch 13, Loss: 1.6767860561151147, fit: 0.16666666666666666
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.3259507999879773e-10
    epoch : 14 ; learning_rate : 4.3259507999879773e-10 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.3889786154703174
    Epoch 14, Loss: 1.6767856737092908, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.849545573973341e-10
    epoch : 15 ; learning_rate : 4.849545573973341e-10 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.3609541471961306
    Epoch 15, Loss: 1.6767852529146408, fit: 0.18333333333333332
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.07261438715862e-10
    epoch : 16 ; learning_rate : 4.07261438715862e-10 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.3812501024223023
    Epoch 16, Loss: 1.6767848789109316, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.830908924456648e-10
    epoch : 17 ; learning_rate : 4.830908924456648e-10 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4251892100948846
    Epoch 17, Loss: 1.6767844952687114, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              4.486516438919946e-10
    epoch : 18 ; learning_rate : 4.486516438919946e-10 ; fit : 0.13333333333333333
    batch_size :          60
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0009000000000000001
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5688062522454511
    Epoch 0, Loss: 1.789520694087262, fit: 0.06666666666666667
    Epoch 0, Loss: 1.789520694087262
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.8079991988048598_fit_0.06666666666666667_2024-01-03_131347
  self.fit : 0.06666666666666667
  self.loss : 1.8079991988048598
  current_accuracy : 0.0766
   Accuracy mean: 0.0766
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0001
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.427111202329475
    Epoch 0, Loss: 1.7118436969616841, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7118436969616841
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7708671631557498_fit_0.08333333333333333_2024-01-03_131359
  self.fit : 0.08333333333333333
  self.loss : 1.7708671631557498
  current_accuracy : 0.1171
   Accuracy mean: 0.1171
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.52253280731545
    Epoch 0, Loss: 1.7940877557018835, fit: 0.06666666666666667
    Epoch 0, Loss: 1.7940877557018835
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7646360214598042_fit_0.06666666666666667_2024-01-03_131407
  self.fit : 0.06666666666666667
  self.loss : 1.7646360214598042
  current_accuracy : 0.0735
   Accuracy mean: 0.0735
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-10
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.407483539687409
    Epoch 0, Loss: 1.7298268984635214, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7298268984635214
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7258114261243227_fit_0.08333333333333333_2024-01-03_131415
  self.fit : 0.08333333333333333
  self.loss : 1.7258114261243227
  current_accuracy : 0.0985
   Accuracy mean: 0.0985
  Results saved to test_combinations_results20240103131415
  normalized_accuracies :      [0.5733945  0.         1.         0.07110092]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0009000000000000001
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.3396219620759127
    Epoch 0, Loss: 1.6826321426946342, fit: 0.1
    Epoch 0, Loss: 1.6826321426946342
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.6818621047410123_fit_0.1_2024-01-03_132201
  self.fit : 0.1
  self.loss : 1.6818621047410123
  current_accuracy : 0.1259
   Accuracy mean: 0.1259
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0001
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.331137834790317
    Epoch 0, Loss: 1.6316563454888573, fit: 0.23333333333333334
    Epoch 0, Loss: 1.6316563454888573
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.5277643069821751_fit_0.23333333333333334_2024-01-03_132208
  self.fit : 0.23333333333333334
  self.loss : 1.5277643069821751
  current_accuracy : 0.1585
   Accuracy mean: 0.1585
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.3472346794091525
    Epoch 0, Loss: 1.6556319864714868, fit: 0.06666666666666667
    Epoch 0, Loss: 1.6556319864714868
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.8457614332998953_fit_0.06666666666666667_2024-01-03_132215
  self.fit : 0.06666666666666667
  self.loss : 1.8457614332998953
  current_accuracy : 0.14
   Accuracy mean: 0.14
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-10
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.486330484835851
    Epoch 0, Loss: 1.7381816241456771, fit: 0.11666666666666667
    Epoch 0, Loss: 1.7381816241456771
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7242766376407137_fit_0.11666666666666667_2024-01-03_132222
  self.fit : 0.11666666666666667
  self.loss : 1.7242766376407137
  current_accuracy : 0.0891
   Accuracy mean: 0.0891
  Results saved to test_combinations_results20240103132223
  normalized_accuracies :      [0.         0.73342939 1.         0.53025937]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0009000000000000001
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4678054170267714
    Epoch 0, Loss: 1.7255197093045311, fit: 0.06666666666666667
    Epoch 0, Loss: 1.7255197093045311
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.8222672026348465_fit_0.06666666666666667_2024-01-03_133537
  self.fit : 0.06666666666666667
  self.loss : 1.8222672026348465
  current_accuracy : 0.0946
   Accuracy mean: 0.0946
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0001
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4023520710836386
    Epoch 0, Loss: 1.7021732668607727, fit: 0.06666666666666667
    Epoch 0, Loss: 1.7021732668607727
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.818686473331579_fit_0.06666666666666667_2024-01-03_133544
  self.fit : 0.06666666666666667
  self.loss : 1.818686473331579
  current_accuracy : 0.1254
   Accuracy mean: 0.1254
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4160401347064364
    Epoch 0, Loss: 1.7621921625754595, fit: 0.18333333333333332
    Epoch 0, Loss: 1.7621921625754595
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.5657642029576564_fit_0.18333333333333332_2024-01-03_133552
  self.fit : 0.18333333333333332
  self.loss : 1.5657642029576564
  current_accuracy : 0.0911
   Accuracy mean: 0.0911
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-10
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4474933456142707
    Epoch 0, Loss: 1.7235705795337315, fit: 0.23333333333333334
    Epoch 0, Loss: 1.7235705795337315
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.4763006166909893_fit_0.23333333333333334_2024-01-03_133559
  self.fit : 0.23333333333333334
  self.loss : 1.4763006166909893
  current_accuracy : 0.1172
   Accuracy mean: 0.1172
  Results saved to test_combinations_results20240103133559
  normalized_accuracies :      [0.76093294 0.         1.         0.10204082]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              3e-05
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.460493800923413
    Epoch 0, Loss: 1.7215510232278672, fit: 0.1
    Epoch 0, Loss: 1.7215510232278672
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7588002864812067_fit_0.1_2024-01-03_134028
  self.fit : 0.1
  self.loss : 1.7588002864812067
  current_accuracy : 0.1101
   Accuracy mean: 0.1101
  Results saved to test_combinations_results20240103134029
  normalized_accuracies :      [0.]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              3e-05
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5215677079866075
    Epoch 0, Loss: 1.7952841388849952, fit: 0.05
    Epoch 0, Loss: 1.7952841388849952
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.8413402306512836_fit_0.05_2024-01-03_134536
  self.fit : 0.05
  self.loss : 1.8413402306512836
  current_accuracy : 0.0706
   Accuracy mean: 0.0706
  Results saved to test_combinations_results20240103134536
  normalized_accuracies :      [0.]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              3e-05
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.525825649590846
    Epoch 0, Loss: 1.7997758537021575, fit: 0.06666666666666667
    Epoch 0, Loss: 1.7997758537021575
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.8140974586226344_fit_0.06666666666666667_2024-01-03_134740
  self.fit : 0.06666666666666667
  self.loss : 1.8140974586226344
  current_accuracy : 0.0668
   Accuracy mean: 0.0668
  Results saved to test_combinations_results20240103134740
  normalized_accuracies :      [0.]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              3e-05
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4127416036984457
    Epoch 0, Loss: 1.7149774445546346, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7149774445546346
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7970841481851507_fit_0.08333333333333333_2024-01-03_135336
  self.fit : 0.08333333333333333
  self.loss : 1.7970841481851507
  current_accuracy : 0.1071
   Accuracy mean: 0.1071
  Results saved to test_combinations_results20240103135336
  normalized_accuracies :      [0.]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              3e-05
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4321452527554308
    Epoch 0, Loss: 1.7465816897903212, fit: 0.11666666666666667
    Epoch 0, Loss: 1.7465816897903212
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7395072319444258_fit_0.11666666666666667_2024-01-03_135555
  self.fit : 0.11666666666666667
  self.loss : 1.7395072319444258
  current_accuracy : 0.0965
   Accuracy mean: 0.0965
  Results saved to test_combinations_results20240103135555
  normalized_accuracies :      [0.]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              3e-05
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5285279914829615
    Epoch 0, Loss: 1.8000571086442854, fit: 0.0
    Epoch 0, Loss: 1.8000571086442854
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.909573689279945_fit_0.0_2024-01-03_135633
  self.fit : 0.0
  self.loss : 1.909573689279945
  current_accuracy : 0.0669
   Accuracy mean: 0.0669
  Results saved to test_combinations_results20240103135634
  normalized_accuracies :      [0.]
batch_rate :  0.01
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.0003
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.7241102065847393
    Epoch 0, Loss: 1.7740298767559213, fit: 0.07333333333333333
    Epoch 0, Loss: 1.7740298767559213
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7789384809499098_fit_0.07333333333333333_2024-01-03_135637
  self.fit : 0.07333333333333333
  self.loss : 1.7789384809499098
  current_accuracy : 0.0826
   Accuracy mean: 0.0826
  Results saved to test_combinations_results20240103135637
  normalized_accuracies :      [0.]
batch_rate :  0.1
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.003
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.559615734928825
    Epoch 0, Loss: 1.5752616447405945, fit: 0.191
    Epoch 0, Loss: 1.5752616447405945
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.5665068510567035_fit_0.191_2024-01-03_135640
  self.fit : 0.191
  self.loss : 1.5665068510567035
  current_accuracy : 0.1903
   Accuracy mean: 0.1903
  Results saved to test_combinations_results20240103135641
  normalized_accuracies :      [0.]
batch_rate :  1.0
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.03
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7427913809159146
    Epoch 0, Loss: 1.7427913809159146, fit: 0.09756666666666666
    Epoch 0, Loss: 1.7427913809159146
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7427913809159146_fit_0.09756666666666666_2024-01-03_135644
  self.fit : 0.09756666666666666
  self.loss : 1.7427913809159146
  current_accuracy : 0.0985
   Accuracy mean: 0.0985
  Results saved to test_combinations_results20240103135645
  normalized_accuracies :      [0.]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              3e-05
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4708863689336842
    Epoch 0, Loss: 1.7274564025210124, fit: 0.1
    Epoch 0, Loss: 1.7274564025210124
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.746192601162946_fit_0.1_2024-01-03_140511
  self.fit : 0.1
  self.loss : 1.746192601162946
  current_accuracy : 0.1185
   Accuracy mean: 0.1185
  Results saved to test_combinations_results20240103140511
  normalized_accuracies :      [0.]
batch_rate :  0.01
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.0003
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6015458595570182
    Epoch 0, Loss: 1.6932836614113407, fit: 0.125
    Epoch 0, Loss: 1.6932836614113407
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6872504693582298_fit_0.125_2024-01-03_140514
  self.fit : 0.125
  self.loss : 1.6872504693582298
  current_accuracy : 0.1246
   Accuracy mean: 0.1246
  Results saved to test_combinations_results20240103140515
  normalized_accuracies :      [0.]
batch_rate :  0.1
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.003
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.710863445173513
    Epoch 0, Loss: 1.7268579866466902, fit: 0.114
    Epoch 0, Loss: 1.7268579866466902
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7169817085351087_fit_0.114_2024-01-03_140518
  self.fit : 0.114
  self.loss : 1.7169817085351087
  current_accuracy : 0.1039
   Accuracy mean: 0.1039
  Results saved to test_combinations_results20240103140518
  normalized_accuracies :      [0.]
batch_rate :  1.0
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.03
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7455956040506295
    Epoch 0, Loss: 1.7455956040506295, fit: 0.10033333333333333
    Epoch 0, Loss: 1.7455956040506295
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7455956040506295_fit_0.10033333333333333_2024-01-03_140521
  self.fit : 0.10033333333333333
  self.loss : 1.7455956040506295
  current_accuracy : 0.1083
   Accuracy mean: 0.1083
  Results saved to test_combinations_results20240103140522
  normalized_accuracies :      [0.]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              3e-05
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.403507031700451
    Epoch 0, Loss: 1.7207853991850819, fit: 0.05
    Epoch 0, Loss: 1.7207853991850819
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.8369939912986462_fit_0.05_2024-01-03_141208
  self.fit : 0.05
  self.loss : 1.8369939912986462
  current_accuracy : 0.1051
   Accuracy mean: 0.1051
  Results saved to test_combinations_results20240103141209
  normalized_accuracies :      [0.]
batch_rate :  0.01
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.0003
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.639253140621812
    Epoch 0, Loss: 1.7069478739172894, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7069478739172894
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7668137599910037_fit_0.08333333333333333_2024-01-03_141212
  self.fit : 0.08333333333333333
  self.loss : 1.7668137599910037
  current_accuracy : 0.1226
   Accuracy mean: 0.1226
  Results saved to test_combinations_results20240103141212
  normalized_accuracies :      [0.]
batch_rate :  0.1
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.003
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7557442726234904
    Epoch 0, Loss: 1.7618801186590611, fit: 0.09266666666666666
    Epoch 0, Loss: 1.7618801186590611
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7557442726234904_fit_0.09266666666666666_2024-01-03_141215
  self.fit : 0.09266666666666666
  self.loss : 1.7557442726234904
  current_accuracy : 0.0858
   Accuracy mean: 0.0858
  Results saved to test_combinations_results20240103141215
  normalized_accuracies :      [0.]
batch_rate :  1.0
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.03
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.8193949316528046
    Epoch 0, Loss: 1.8193949316528046, fit: 0.06475
    Epoch 0, Loss: 1.8193949316528046
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.8193949316528046_fit_0.06475_2024-01-03_141219
  self.fit : 0.06475
  self.loss : 1.8193949316528046
  current_accuracy : 0.0531
   Accuracy mean: 0.0531
  Results saved to test_combinations_results20240103141219
  normalized_accuracies :      [0.]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              3e-05
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4700862988509509
    Epoch 0, Loss: 1.7400428295977266, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7400428295977266
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7691204195096866_fit_0.08333333333333333_2024-01-03_141704
  self.fit : 0.08333333333333333
  self.loss : 1.7691204195096866
  current_accuracy : 0.1046
   Accuracy mean: 0.1046
  Results saved to test_combinations_results20240103141704
  normalized_accuracies :      [0.]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              3e-05
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4255699379775275
    Epoch 0, Loss: 1.7120967883689613, fit: 0.1
    Epoch 0, Loss: 1.7120967883689613
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.77805422518779_fit_0.1_2024-01-03_141857
  self.fit : 0.1
  self.loss : 1.77805422518779
  current_accuracy : 0.115
   Accuracy mean: 0.115
  Results saved to test_combinations_results20240103141858
  normalized_accuracies :      [0.]
batch_rate :  0.01
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.0003
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.700723181505793
    Epoch 0, Loss: 1.772636371372602, fit: 0.06666666666666667
    Epoch 0, Loss: 1.772636371372602
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.8058021492385532_fit_0.06666666666666667_2024-01-03_141901
  self.fit : 0.06666666666666667
  self.loss : 1.8058021492385532
  current_accuracy : 0.0817
   Accuracy mean: 0.0817
  Results saved to test_combinations_results20240103141901
  normalized_accuracies :      [0.]
batch_rate :  0.1
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.003
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.8199415460102595
    Epoch 0, Loss: 1.828969416817581, fit: 0.06416666666666666
    Epoch 0, Loss: 1.828969416817581
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.8248342201533287_fit_0.06416666666666666_2024-01-03_141904
  self.fit : 0.06416666666666666
  self.loss : 1.8248342201533287
  current_accuracy : 0.0596
   Accuracy mean: 0.0596
  Results saved to test_combinations_results20240103141905
  normalized_accuracies :      [0.]
batch_rate :  1.0
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.03
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7342297678445107
    Epoch 0, Loss: 1.7342297678445107, fit: 0.10765
    Epoch 0, Loss: 1.7342297678445107
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7342297678445107_fit_0.10765_2024-01-03_141908
  self.fit : 0.10765
  self.loss : 1.7342297678445107
  current_accuracy : 0.1032
   Accuracy mean: 0.1032
  Results saved to test_combinations_results20240103141908
  normalized_accuracies :      [0.]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.001
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              3e-05
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4984719508051734
    Epoch 0, Loss: 1.7711916371515515, fit: 0.05
    Epoch 0, Loss: 1.7711916371515515
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.8749283541066326_fit_0.05_2024-01-03_141942
  self.fit : 0.05
  self.loss : 1.8749283541066326
  current_accuracy : 0.0883
   Accuracy mean: 0.0883
  Results saved to test_combinations_results20240103141942
  normalized_accuracies :      [0.]
batch_rate :  0.01
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.0003
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.7509016218444662
    Epoch 0, Loss: 1.7983458139810964, fit: 0.085
    Epoch 0, Loss: 1.7983458139810964
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7894082217014193_fit_0.085_2024-01-03_141946
  self.fit : 0.085
  self.loss : 1.7894082217014193
  current_accuracy : 0.0731
   Accuracy mean: 0.0731
  Results saved to test_combinations_results20240103141946
  normalized_accuracies :      [0.]
batch_rate :  0.1
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.003
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.771174491692438
    Epoch 0, Loss: 1.7882478668011266, fit: 0.07833333333333334
    Epoch 0, Loss: 1.7882478668011266
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.797904280817284_fit_0.07833333333333334_2024-01-03_141949
  self.fit : 0.07833333333333334
  self.loss : 1.797904280817284
  current_accuracy : 0.0792
   Accuracy mean: 0.0792
  Results saved to test_combinations_results20240103141949
  normalized_accuracies :      [0.]
batch_rate :  1.0
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.03
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7675480394816963
    Epoch 0, Loss: 1.7675480394816963, fit: 0.09203333333333333
    Epoch 0, Loss: 1.7675480394816963
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7675480394816963_fit_0.09203333333333333_2024-01-03_141953
  self.fit : 0.09203333333333333
  self.loss : 1.7675480394816963
  current_accuracy : 0.0917
   Accuracy mean: 0.0917
  Results saved to test_combinations_results20240103141953
  normalized_accuracies :      [0.]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.0002
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.0002
      batch rate adapted learning_rate :              2.0000000000000003e-14
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          12
    best_batch_loss : 0.7987154091223054
    Epoch 0, Loss: 1.7255291073269077, fit: 0.16666666666666666
    Epoch 0, Loss: 1.7255291073269077
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.6416344685245707_fit_0.16666666666666666_2024-01-03_143521
  self.fit : 0.16666666666666666
  self.loss : 1.6416344685245707
  current_accuracy : 0.1241
   Accuracy mean: 0.1241
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.0002
      batch rate adapted learning_rate :              2e-11
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          12
    best_batch_loss : 0.8875555053699392
    Epoch 0, Loss: 1.765574185745647, fit: 0.0
    Epoch 0, Loss: 1.765574185745647
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.907010256496088_fit_0.0_2024-01-03_143544
  self.fit : 0.0
  self.loss : 1.907010256496088
  current_accuracy : 0.0913
   Accuracy mean: 0.0913
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.0002
      batch rate adapted learning_rate :              2e-08
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          12
    best_batch_loss : 0.9694664381320566
    Epoch 0, Loss: 1.750977233691493, fit: 0.08333333333333333
    Epoch 0, Loss: 1.750977233691493
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6712209530370579_fit_0.08333333333333333_2024-01-03_143607
  self.fit : 0.08333333333333333
  self.loss : 1.6712209530370579
  current_accuracy : 0.093
   Accuracy mean: 0.093
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.0002
      batch rate adapted learning_rate :              2.0000000000000002e-07
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          12
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.0002
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.0002
      batch rate adapted learning_rate :              2.0000000000000003e-14
      loss :              0.0
      loss_factor :              0.0
      loss adapted learning_rate :              0.0
    epoch : 0 ; learning_rate : 0.0 ; fit : 0.0
    batch_size :          12
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.0002
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.0002
      batch rate adapted learning_rate :              2.0000000000000003e-14
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  0.0002
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.0002
      batch rate adapted learning_rate :              2.0000000000000003e-14
    epoch : 0 ; learning_rate : 2.0000000000000003e-14 ; fit : 0.0
    batch_size :          12
    best_batch_loss : 1.0000014540110136
    Epoch 0, Loss: 1.7510028271746743, fit: 0.0
    Epoch 0, Loss: 1.7510028271746743
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.944043719297751_fit_0.0_2024-01-03_144147
  self.fit : 0.0
  self.loss : 1.944043719297751
  current_accuracy : 0.0981
   Accuracy mean: 0.0981
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.0002
      batch rate adapted learning_rate :              2e-11
    epoch : 0 ; learning_rate : 2e-11 ; fit : 0.0
    batch_size :          12
    best_batch_loss : 0.9733808327312601
    Epoch 0, Loss: 1.7485659020926734, fit: 0.0
    Epoch 0, Loss: 1.7485659020926734
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.9642388871172844_fit_0.0_2024-01-03_144210
  self.fit : 0.0
  self.loss : 1.9642388871172844
  current_accuracy : 0.0979
   Accuracy mean: 0.0979
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.0002
      batch rate adapted learning_rate :              2e-08
    epoch : 0 ; learning_rate : 2e-08 ; fit : 0.0
    batch_size :          12
    best_batch_loss : 0.7475758568999868
    Epoch 0, Loss: 1.6950427994381798, fit: 0.08333333333333333
    Epoch 0, Loss: 1.6950427994381798
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7584007086451834_fit_0.08333333333333333_2024-01-03_144232
  self.fit : 0.08333333333333333
  self.loss : 1.7584007086451834
  current_accuracy : 0.1254
   Accuracy mean: 0.1254
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.0002
      batch rate adapted learning_rate :              2.0000000000000002e-07
    epoch : 0 ; learning_rate : 2.0000000000000002e-07 ; fit : 0.0
    batch_size :          12
    best_batch_loss : 0.990917279748122
    Epoch 0, Loss: 1.7800410881267543, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7800410881267543
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.80580220674828_fit_0.08333333333333333_2024-01-03_144255
  self.fit : 0.08333333333333333
  self.loss : 1.80580220674828
  current_accuracy : 0.0899
   Accuracy mean: 0.0899
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.0002
      batch rate adapted learning_rate :              4.000000000000001e-06
    epoch : 0 ; learning_rate : 4.000000000000001e-06 ; fit : 0.0
    batch_size :          12
    best_batch_loss : 0.9395633890320827
    Epoch 0, Loss: 1.7112912119120303, fit: 0.25
    Epoch 0, Loss: 1.7112912119120303
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.4967521156627612_fit_0.25_2024-01-03_144318
  self.fit : 0.25
  self.loss : 1.4967521156627612
  current_accuracy : 0.1246
   Accuracy mean: 0.1246
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.0002
      batch rate adapted learning_rate :              5e-06
    epoch : 0 ; learning_rate : 5e-06 ; fit : 0.0
    batch_size :          12
    best_batch_loss : 0.9189496455602302
    Epoch 0, Loss: 1.7348763234773172, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7348763234773172
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7886513476750443_fit_0.08333333333333333_2024-01-03_144340
  self.fit : 0.08333333333333333
  self.loss : 1.7886513476750443
  current_accuracy : 0.1093
   Accuracy mean: 0.1093
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.0002
      batch rate adapted learning_rate :              6e-06
    epoch : 0 ; learning_rate : 6e-06 ; fit : 0.0
    batch_size :          12
    best_batch_loss : 0.9257287590000732
    Epoch 0, Loss: 1.7672463699704872, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7672463699704872
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6691580622322537_fit_0.08333333333333333_2024-01-03_144403
  self.fit : 0.08333333333333333
  self.loss : 1.6691580622322537
  current_accuracy : 0.0854
   Accuracy mean: 0.0854
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.0002
      batch rate adapted learning_rate :              2e-05
    epoch : 0 ; learning_rate : 2e-05 ; fit : 0.0
    batch_size :          12
    best_batch_loss : 0.8400940419311121
    Epoch 0, Loss: 1.6402827059586886, fit: 0.3333333333333333
    Epoch 0, Loss: 1.6402827059586886
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.2974039008367477_fit_0.3333333333333333_2024-01-03_144426
  self.fit : 0.3333333333333333
  self.loss : 1.2974039008367477
  current_accuracy : 0.166
   Accuracy mean: 0.166
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.0002
      batch rate adapted learning_rate :              0.00018
    epoch : 0 ; learning_rate : 0.00018 ; fit : 0.0
    batch_size :          12
    best_batch_loss : 0.17327068464707085
    Epoch 0, Loss: 1.326355579207604, fit: 0.5
    Epoch 0, Loss: 1.326355579207604
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.8843025017254921_fit_0.5_2024-01-03_144449
  self.fit : 0.5
  self.loss : 0.8843025017254921
  current_accuracy : 0.4851
   Accuracy mean: 0.4851
  Results saved to test_combinations_results20240103144449
  normalized_accuracies :      [0.03177383 0.03127346 0.10007506 0.01125844 0.09807356 0.05979485
   0.         0.20165124 1.        ]
batch_rate :  0.001
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-13
    epoch : 0 ; learning_rate : 1e-13 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5422237009779645
    Epoch 0, Loss: 1.8040612240471303, fit: 0.15
    Epoch 0, Loss: 1.8040612240471303
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.669950181778762_fit_0.15_2024-01-03_144457
  self.fit : 0.15
  self.loss : 1.669950181778762
  current_accuracy : 0.0719
   Accuracy mean: 0.0719
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-10
    epoch : 0 ; learning_rate : 1e-10 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4487357319894085
    Epoch 0, Loss: 1.73102447589233, fit: 0.11666666666666667
    Epoch 0, Loss: 1.73102447589233
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.6915599811701447_fit_0.11666666666666667_2024-01-03_144504
  self.fit : 0.11666666666666667
  self.loss : 1.6915599811701447
  current_accuracy : 0.1015
   Accuracy mean: 0.1015
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-07
    epoch : 0 ; learning_rate : 1.0000000000000001e-07 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5130354905300565
    Epoch 0, Loss: 1.7892554138051355, fit: 0.06666666666666667
    Epoch 0, Loss: 1.7892554138051355
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.8261935484554137_fit_0.06666666666666667_2024-01-03_144511
  self.fit : 0.06666666666666667
  self.loss : 1.8261935484554137
  current_accuracy : 0.078
   Accuracy mean: 0.078
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-06
    epoch : 0 ; learning_rate : 1e-06 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4737240983754891
    Epoch 0, Loss: 1.7455368591455682, fit: 0.05
    Epoch 0, Loss: 1.7455368591455682
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.8457403545683944_fit_0.05_2024-01-03_144518
  self.fit : 0.05
  self.loss : 1.8457403545683944
  current_accuracy : 0.1017
   Accuracy mean: 0.1017
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.001
      batch rate adapted learning_rate :              2e-05
    epoch : 0 ; learning_rate : 2e-05 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5724817780087503
    Epoch 0, Loss: 1.8062281374925422, fit: 0.05
    Epoch 0, Loss: 1.8062281374925422
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.8278442682210922_fit_0.05_2024-01-03_144525
  self.fit : 0.05
  self.loss : 1.8278442682210922
  current_accuracy : 0.0631
   Accuracy mean: 0.0631
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.001
      batch rate adapted learning_rate :              2.5e-05
    epoch : 0 ; learning_rate : 2.5e-05 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5233214780518913
    Epoch 0, Loss: 1.7559330275745375, fit: 0.1
    Epoch 0, Loss: 1.7559330275745375
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7722758089138186_fit_0.1_2024-01-03_144532
  self.fit : 0.1
  self.loss : 1.7722758089138186
  current_accuracy : 0.0952
   Accuracy mean: 0.0952
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              3e-05
    epoch : 0 ; learning_rate : 3e-05 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.3680731216189719
    Epoch 0, Loss: 1.6827915528405066, fit: 0.06666666666666667
    Epoch 0, Loss: 1.6827915528405066
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7676821586856448_fit_0.06666666666666667_2024-01-03_144538
  self.fit : 0.06666666666666667
  self.loss : 1.7676821586856448
  current_accuracy : 0.1474
   Accuracy mean: 0.1474
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.3348061495711605
    Epoch 0, Loss: 1.7035996747472315, fit: 0.23333333333333334
    Epoch 0, Loss: 1.7035996747472315
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.4451496462821025_fit_0.23333333333333334_2024-01-03_144546
  self.fit : 0.23333333333333334
  self.loss : 1.4451496462821025
  current_accuracy : 0.1525
   Accuracy mean: 0.1525
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0009000000000000001
    epoch : 0 ; learning_rate : 0.0009000000000000001 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 0.7940077471879353
    Epoch 0, Loss: 1.3961719175570604, fit: 0.5
    Epoch 0, Loss: 1.3961719175570604
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.9749309583138627_fit_0.5_2024-01-03_144553
  self.fit : 0.5
  self.loss : 0.9749309583138627
  current_accuracy : 0.4558
   Accuracy mean: 0.4558
  Results saved to test_combinations_results20240103144553
  normalized_accuracies :      [0.02240896 0.09778457 0.03794245 0.09829386 0.         0.08174179
   0.21466769 0.2276547  1.        ]
batch_rate :  0.01
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-12
    epoch : 0 ; learning_rate : 1e-12 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6941339868827796
    Epoch 0, Loss: 1.7582799509969027, fit: 0.12
    Epoch 0, Loss: 1.7582799509969027
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7019397149229225_fit_0.12_2024-01-03_144556
  self.fit : 0.12
  self.loss : 1.7019397149229225
  current_accuracy : 0.0955
   Accuracy mean: 0.0955
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-09
    epoch : 0 ; learning_rate : 1e-09 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.620611061179427
    Epoch 0, Loss: 1.6810568162967527, fit: 0.125
    Epoch 0, Loss: 1.6810568162967527
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.6948759904735506_fit_0.125_2024-01-03_144559
  self.fit : 0.125
  self.loss : 1.6948759904735506
  current_accuracy : 0.1282
   Accuracy mean: 0.1282
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.01
      batch rate adapted learning_rate :              1.0000000000000002e-06
    epoch : 0 ; learning_rate : 1.0000000000000002e-06 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6751284621677296
    Epoch 0, Loss: 1.731082520418867, fit: 0.10666666666666667
    Epoch 0, Loss: 1.731082520418867
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7222501877276006_fit_0.10666666666666667_2024-01-03_144602
  self.fit : 0.10666666666666667
  self.loss : 1.7222501877276006
  current_accuracy : 0.1022
   Accuracy mean: 0.1022
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-05
    epoch : 0 ; learning_rate : 1e-05 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.7140694904480873
    Epoch 0, Loss: 1.7652164226326845, fit: 0.10833333333333334
    Epoch 0, Loss: 1.7652164226326845
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7329767252408284_fit_0.10833333333333334_2024-01-03_144604
  self.fit : 0.10833333333333334
  self.loss : 1.7329767252408284
  current_accuracy : 0.0939
   Accuracy mean: 0.0939
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.0002
    epoch : 0 ; learning_rate : 0.0002 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6818792949151955
    Epoch 0, Loss: 1.742799594644776, fit: 0.09166666666666666
    Epoch 0, Loss: 1.742799594644776
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7495403237375282_fit_0.09166666666666666_2024-01-03_144607
  self.fit : 0.09166666666666666
  self.loss : 1.7495403237375282
  current_accuracy : 0.1064
   Accuracy mean: 0.1064
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.00025
    epoch : 0 ; learning_rate : 0.00025 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.7422307981769771
    Epoch 0, Loss: 1.8019877582777715, fit: 0.07833333333333334
    Epoch 0, Loss: 1.8019877582777715
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.785411536357423_fit_0.07833333333333334_2024-01-03_144609
  self.fit : 0.07833333333333334
  self.loss : 1.785411536357423
  current_accuracy : 0.077
   Accuracy mean: 0.077
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.0003
    epoch : 0 ; learning_rate : 0.0003 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6562596808390515
    Epoch 0, Loss: 1.709075213168083, fit: 0.12666666666666668
    Epoch 0, Loss: 1.709075213168083
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.683381456158229_fit_0.12666666666666668_2024-01-03_144612
  self.fit : 0.12666666666666668
  self.loss : 1.683381456158229
  current_accuracy : 0.1205
   Accuracy mean: 0.1205
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.001
    epoch : 0 ; learning_rate : 0.001 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.543040881536338
    Epoch 0, Loss: 1.6275977074066499, fit: 0.16333333333333333
    Epoch 0, Loss: 1.6275977074066499
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.589102391486501_fit_0.16333333333333333_2024-01-03_144614
  self.fit : 0.16333333333333333
  self.loss : 1.589102391486501
  current_accuracy : 0.1685
   Accuracy mean: 0.1685
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.009000000000000001
    epoch : 0 ; learning_rate : 0.009000000000000001 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.0560061105009435
    Epoch 0, Loss: 1.4144237549181022, fit: 0.435
    Epoch 0, Loss: 1.4144237549181022
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.0729933820793864_fit_0.435_2024-01-03_144617
  self.fit : 0.435
  self.loss : 1.0729933820793864
  current_accuracy : 0.4407
   Accuracy mean: 0.4407
  Results saved to test_combinations_results20240103144617
  normalized_accuracies :      [0.0508661  0.14077536 0.06928787 0.04646687 0.08083585 0.
   0.11960407 0.25158097 1.        ]
batch_rate :  0.1
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.1
      batch rate adapted learning_rate :              1.0000000000000001e-11
    epoch : 0 ; learning_rate : 1.0000000000000001e-11 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.6083211531779207
    Epoch 0, Loss: 1.6232168403687828, fit: 0.16966666666666666
    Epoch 0, Loss: 1.6232168403687828
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.6083211531779207_fit_0.16966666666666666_2024-01-03_144620
  self.fit : 0.16966666666666666
  self.loss : 1.6083211531779207
  current_accuracy : 0.1577
   Accuracy mean: 0.1577
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.1
      batch rate adapted learning_rate :              1e-08
    epoch : 0 ; learning_rate : 1e-08 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7338339331229757
    Epoch 0, Loss: 1.7492479404808698, fit: 0.093
    Epoch 0, Loss: 1.7492479404808698
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.757325608671003_fit_0.093_2024-01-03_144623
  self.fit : 0.093
  self.loss : 1.757325608671003
  current_accuracy : 0.0995
   Accuracy mean: 0.0995
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.1
      batch rate adapted learning_rate :              1e-05
    epoch : 0 ; learning_rate : 1e-05 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7162545524302808
    Epoch 0, Loss: 1.7262864678743473, fit: 0.11416666666666667
    Epoch 0, Loss: 1.7262864678743473
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7203170860046244_fit_0.11416666666666667_2024-01-03_144625
  self.fit : 0.11416666666666667
  self.loss : 1.7203170860046244
  current_accuracy : 0.1082
   Accuracy mean: 0.1082
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.8243006340039747
    Epoch 0, Loss: 1.835741929422719, fit: 0.05383333333333333
    Epoch 0, Loss: 1.835741929422719
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.8320533770799972_fit_0.05383333333333333_2024-01-03_144627
  self.fit : 0.05383333333333333
  self.loss : 1.8320533770799972
  current_accuracy : 0.0537
   Accuracy mean: 0.0537
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.002
    epoch : 0 ; learning_rate : 0.002 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.6981240828294382
    Epoch 0, Loss: 1.7117562015094923, fit: 0.11216666666666666
    Epoch 0, Loss: 1.7117562015094923
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7053250808359195_fit_0.11216666666666666_2024-01-03_144629
  self.fit : 0.11216666666666666
  self.loss : 1.7053250808359195
  current_accuracy : 0.1127
   Accuracy mean: 0.1127
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.0025000000000000005
    epoch : 0 ; learning_rate : 0.0025000000000000005 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7604798141418474
    Epoch 0, Loss: 1.7756332945525144, fit: 0.08416666666666667
    Epoch 0, Loss: 1.7756332945525144
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7606320510283835_fit_0.08416666666666667_2024-01-03_144632
  self.fit : 0.08416666666666667
  self.loss : 1.7606320510283835
  current_accuracy : 0.0868
   Accuracy mean: 0.0868
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.003
    epoch : 0 ; learning_rate : 0.003 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.733356983707332
    Epoch 0, Loss: 1.7572322414119732, fit: 0.09883333333333333
    Epoch 0, Loss: 1.7572322414119732
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.74452794603826_fit_0.09883333333333333_2024-01-03_144634
  self.fit : 0.09883333333333333
  self.loss : 1.74452794603826
  current_accuracy : 0.1046
   Accuracy mean: 0.1046
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.010000000000000002
    epoch : 0 ; learning_rate : 0.010000000000000002 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.6467671509983275
    Epoch 0, Loss: 1.6708883783178328, fit: 0.14283333333333334
    Epoch 0, Loss: 1.6708883783178328
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6467671509983275_fit_0.14283333333333334_2024-01-03_144636
  self.fit : 0.14283333333333334
  self.loss : 1.6467671509983275
  current_accuracy : 0.1466
   Accuracy mean: 0.1466
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.09000000000000001
    epoch : 0 ; learning_rate : 0.09000000000000001 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.0302358176446516
    Epoch 0, Loss: 1.3637169094215926, fit: 0.456
    Epoch 0, Loss: 1.3637169094215926
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.0302358176446516_fit_0.456_2024-01-03_144638
  self.fit : 0.456
  self.loss : 1.0302358176446516
  current_accuracy : 0.4957
   Accuracy mean: 0.4957
  Results saved to test_combinations_results20240103144639
  normalized_accuracies :      [0.23529412 0.10361991 0.12330317 0.         0.13348416 0.07488688
   0.11515837 0.210181   1.        ]
batch_rate :  1.0
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-10
    epoch : 0 ; learning_rate : 1e-10 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.755824928329719
    Epoch 0, Loss: 1.755824928329719, fit: 0.09333333333333334
    Epoch 0, Loss: 1.755824928329719
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.755824928329719_fit_0.09333333333333334_2024-01-03_144642
  self.fit : 0.09333333333333334
  self.loss : 1.755824928329719
  current_accuracy : 0.0947
   Accuracy mean: 0.0947
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-07
    epoch : 0 ; learning_rate : 1e-07 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7271006147683234
    Epoch 0, Loss: 1.7271006147683234, fit: 0.10725
    Epoch 0, Loss: 1.7271006147683234
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7271006147683234_fit_0.10725_2024-01-03_144645
  self.fit : 0.10725
  self.loss : 1.7271006147683234
  current_accuracy : 0.0977
   Accuracy mean: 0.0977
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7610878152283387
    Epoch 0, Loss: 1.7610878152283387, fit: 0.08611666666666666
    Epoch 0, Loss: 1.7610878152283387
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7610878152283387_fit_0.08611666666666666_2024-01-03_144647
  self.fit : 0.08611666666666666
  self.loss : 1.7610878152283387
  current_accuracy : 0.0834
   Accuracy mean: 0.0834
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.001
    epoch : 0 ; learning_rate : 0.001 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7110367692537376
    Epoch 0, Loss: 1.7110367692537376, fit: 0.11845
    Epoch 0, Loss: 1.7110367692537376
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7110367692537376_fit_0.11845_2024-01-03_144650
  self.fit : 0.11845
  self.loss : 1.7110367692537376
  current_accuracy : 0.1203
   Accuracy mean: 0.1203
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.02
    epoch : 0 ; learning_rate : 0.02 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7948207965145764
    Epoch 0, Loss: 1.7948207965145764, fit: 0.07483333333333334
    Epoch 0, Loss: 1.7948207965145764
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7948207965145764_fit_0.07483333333333334_2024-01-03_144652
  self.fit : 0.07483333333333334
  self.loss : 1.7948207965145764
  current_accuracy : 0.0879
   Accuracy mean: 0.0879
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.025
    epoch : 0 ; learning_rate : 0.025 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.8024510157070268
    Epoch 0, Loss: 1.8024510157070268, fit: 0.06825
    Epoch 0, Loss: 1.8024510157070268
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.8024510157070268_fit_0.06825_2024-01-03_144655
  self.fit : 0.06825
  self.loss : 1.8024510157070268
  current_accuracy : 0.0743
   Accuracy mean: 0.0743
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.03
    epoch : 0 ; learning_rate : 0.03 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6996124565247868
    Epoch 0, Loss: 1.6996124565247868, fit: 0.11816666666666667
    Epoch 0, Loss: 1.6996124565247868
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6996124565247868_fit_0.11816666666666667_2024-01-03_144657
  self.fit : 0.11816666666666667
  self.loss : 1.6996124565247868
  current_accuracy : 0.1302
   Accuracy mean: 0.1302
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.1
    epoch : 0 ; learning_rate : 0.1 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7797134649639583
    Epoch 0, Loss: 1.7797134649639583, fit: 0.07665
    Epoch 0, Loss: 1.7797134649639583
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7797134649639583_fit_0.07665_2024-01-03_144700
  self.fit : 0.07665
  self.loss : 1.7797134649639583
  current_accuracy : 0.1414
   Accuracy mean: 0.1414
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.9
    epoch : 0 ; learning_rate : 0.9 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7514359996951951
    Epoch 0, Loss: 1.7514359996951951, fit: 0.09463333333333333
    Epoch 0, Loss: 1.7514359996951951
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7514359996951951_fit_0.09463333333333333_2024-01-03_144703
  self.fit : 0.09463333333333333
  self.loss : 1.7514359996951951
  current_accuracy : 0.2688
   Accuracy mean: 0.2688
  Results saved to test_combinations_results20240103144703
  normalized_accuracies :      [0.10488432 0.12030848 0.04678663 0.23650386 0.06992288 0.
   0.2874036  0.34498715 1.        ]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  1.0
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-10
    epoch : 0 ; learning_rate : 1e-10 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.663949169433236
    Epoch 0, Loss: 1.663949169433236, fit: 0.13666666666666666
    Epoch 0, Loss: 1.663949169433236
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.663949169433236_fit_0.13666666666666666_2024-01-03_145510
  self.fit : 0.13666666666666666
  self.loss : 1.663949169433236
  current_accuracy : 0.139
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-10
      loss :              1.663949169433236
      loss_factor :              0.1663949169433236
      loss adapted learning_rate :              2.768726838457556e-12
    epoch : 0 ; learning_rate : 2.768726838457556e-12 ; fit : 0.13666666666666666
    batch_size :          60000
    best_batch_loss : 1.6639491692573578
    Epoch 0, Loss: 1.6639491692573578, fit: 0.13666666666666666
    Epoch 0, Loss: 1.6639491692573578
      batch rate adapted learning_rate :              1e-10
      loss :              1.6639491692573578
      loss_factor :              0.16639491692573577
      loss adapted learning_rate :              2.768726837872251e-12
    epoch : 1 ; learning_rate : 2.768726837872251e-12 ; fit : 0.13666666666666666
    batch_size :          60000
    best_batch_loss : 1.663949169252489
    Epoch 1, Loss: 1.663949169252489, fit: 0.13666666666666666
      batch rate adapted learning_rate :              1e-10
      loss :              1.663949169252489
      loss_factor :              0.1663949169252489
      loss adapted learning_rate :              2.7687268378560486e-12
    epoch : 2 ; learning_rate : 2.7687268378560486e-12 ; fit : 0.13666666666666666
    batch_size :          60000
    best_batch_loss : 1.6639491692476185
    Epoch 2, Loss: 1.6639491692476185, fit: 0.13666666666666666
      batch rate adapted learning_rate :              1e-10
      loss :              1.6639491692476185
      loss_factor :              0.16639491692476185
      loss adapted learning_rate :              2.7687268378398402e-12
    epoch : 3 ; learning_rate : 2.7687268378398402e-12 ; fit : 0.13666666666666666
    batch_size :          60000
    best_batch_loss : 1.6639491692427497
    Epoch 3, Loss: 1.6639491692427497, fit: 0.13666666666666666
      batch rate adapted learning_rate :              1e-10
      loss :              1.6639491692427497
      loss_factor :              0.16639491692427497
      loss adapted learning_rate :              2.7687268378236367e-12
    epoch : 4 ; learning_rate : 2.7687268378236367e-12 ; fit : 0.13666666666666666
    batch_size :          60000
    best_batch_loss : 1.6639491692378807
    Epoch 4, Loss: 1.6639491692378807, fit: 0.13666666666666666
      batch rate adapted learning_rate :              1e-10
      loss :              1.6639491692378807
      loss_factor :              0.16639491692378808
      loss adapted learning_rate :              2.768726837807434e-12
    epoch : 5 ; learning_rate : 2.768726837807434e-12 ; fit : 0.13666666666666666
    batch_size :          60000
    best_batch_loss : 1.66394916923301
    Epoch 5, Loss: 1.66394916923301, fit: 0.13666666666666666
      batch rate adapted learning_rate :              1e-10
      loss :              1.66394916923301
      loss_factor :              0.166394916923301
      loss adapted learning_rate :              2.768726837791224e-12
    epoch : 6 ; learning_rate : 2.768726837791224e-12 ; fit : 0.13666666666666666
    batch_size :          60000
    best_batch_loss : 1.6639491692281405
    Epoch 6, Loss: 1.6639491692281405, fit: 0.13666666666666666
      batch rate adapted learning_rate :              1e-10
      loss :              1.6639491692281405
      loss_factor :              0.16639491692281405
      loss adapted learning_rate :              2.7687268377750192e-12
    epoch : 7 ; learning_rate : 2.7687268377750192e-12 ; fit : 0.13666666666666666
    batch_size :          60000
    best_batch_loss : 1.6639491692232709
    Epoch 7, Loss: 1.6639491692232709, fit: 0.13666666666666666
      batch rate adapted learning_rate :              1e-10
      loss :              1.6639491692232709
      loss_factor :              0.16639491692232708
      loss adapted learning_rate :              2.7687268377588133e-12
    epoch : 8 ; learning_rate : 2.7687268377588133e-12 ; fit : 0.13666666666666666
    batch_size :          60000
    best_batch_loss : 1.6639491692184014
    Epoch 8, Loss: 1.6639491692184014, fit: 0.13666666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.6639491692184014_fit_0.13666666666666666_2024-01-03_145531
  self.fit : 0.13666666666666666
  self.loss : 1.6639491692184014
  current_accuracy : 0.139
   Accuracy mean: 0.139
   Accuracy mean: 0.139
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-07
    epoch : 0 ; learning_rate : 1e-07 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.77403036597476
    Epoch 0, Loss: 1.77403036597476, fit: 0.07965
    Epoch 0, Loss: 1.77403036597476
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.77403036597476_fit_0.07965_2024-01-03_145533
  self.fit : 0.07965
  self.loss : 1.77403036597476
  current_accuracy : 0.0808
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-07
      loss :              1.77403036597476
      loss_factor :              0.177403036597476
      loss adapted learning_rate :              3.1471837394005406e-09
    epoch : 0 ; learning_rate : 3.1471837394005406e-09 ; fit : 0.07965
    batch_size :          60000
    best_batch_loss : 1.774030289378644
    Epoch 0, Loss: 1.774030289378644, fit: 0.07965
    Epoch 0, Loss: 1.774030289378644
      batch rate adapted learning_rate :              1e-07
      loss :              1.774030289378644
      loss_factor :              0.1774030289378644
      loss adapted learning_rate :              3.1471834676328744e-09
    epoch : 1 ; learning_rate : 3.1471834676328744e-09 ; fit : 0.07965
    batch_size :          60000
    best_batch_loss : 1.7740302869680258
    Epoch 1, Loss: 1.7740302869680258, fit: 0.07965
      batch rate adapted learning_rate :              1e-07
      loss :              1.7740302869680258
      loss_factor :              0.17740302869680258
      loss adapted learning_rate :              3.147183459079856e-09
    epoch : 2 ; learning_rate : 3.147183459079856e-09 ; fit : 0.07965
    batch_size :          60000
    best_batch_loss : 1.7740302845574074
    Epoch 2, Loss: 1.7740302845574074, fit: 0.07965
      batch rate adapted learning_rate :              1e-07
      loss :              1.7740302845574074
      loss_factor :              0.17740302845574074
      loss adapted learning_rate :              3.1471834505268356e-09
    epoch : 3 ; learning_rate : 3.1471834505268356e-09 ; fit : 0.07965
    batch_size :          60000
    best_batch_loss : 1.7740302821467897
    Epoch 3, Loss: 1.7740302821467897, fit: 0.07965
      batch rate adapted learning_rate :              1e-07
      loss :              1.7740302821467897
      loss_factor :              0.17740302821467896
      loss adapted learning_rate :              3.147183441973818e-09
    epoch : 4 ; learning_rate : 3.147183441973818e-09 ; fit : 0.07965
    batch_size :          60000
    best_batch_loss : 1.7740302797361727
    Epoch 4, Loss: 1.7740302797361727, fit: 0.07965
      batch rate adapted learning_rate :              1e-07
      loss :              1.7740302797361727
      loss_factor :              0.17740302797361726
      loss adapted learning_rate :              3.1471834334208026e-09
    epoch : 5 ; learning_rate : 3.1471834334208026e-09 ; fit : 0.07965
    batch_size :          60000
    best_batch_loss : 1.7740302773255543
    Epoch 5, Loss: 1.7740302773255543, fit: 0.07965
      batch rate adapted learning_rate :              1e-07
      loss :              1.7740302773255543
      loss_factor :              0.17740302773255542
      loss adapted learning_rate :              3.1471834248677824e-09
    epoch : 6 ; learning_rate : 3.1471834248677824e-09 ; fit : 0.07965
    batch_size :          60000
    best_batch_loss : 1.774030274914936
    Epoch 6, Loss: 1.774030274914936, fit: 0.07965
      batch rate adapted learning_rate :              1e-07
      loss :              1.774030274914936
      loss_factor :              0.1774030274914936
      loss adapted learning_rate :              3.147183416314763e-09
    epoch : 7 ; learning_rate : 3.147183416314763e-09 ; fit : 0.07965
    batch_size :          60000
    best_batch_loss : 1.7740302725043184
    Epoch 7, Loss: 1.7740302725043184, fit: 0.07965
      batch rate adapted learning_rate :              1e-07
      loss :              1.7740302725043184
      loss_factor :              0.17740302725043183
      loss adapted learning_rate :              3.1471834077617456e-09
    epoch : 8 ; learning_rate : 3.1471834077617456e-09 ; fit : 0.07965
    batch_size :          60000
    best_batch_loss : 1.7740302700937003
    Epoch 8, Loss: 1.7740302700937003, fit: 0.07965
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7740302700937003_fit_0.07965_2024-01-03_145554
  self.fit : 0.07965
  self.loss : 1.7740302700937003
  current_accuracy : 0.0808
   Accuracy mean: 0.0808
   Accuracy mean: 0.0808
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7605772227848115
    Epoch 0, Loss: 1.7605772227848115, fit: 0.09308333333333334
    Epoch 0, Loss: 1.7605772227848115
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7605772227848115_fit_0.09308333333333334_2024-01-03_145557
  self.fit : 0.09308333333333334
  self.loss : 1.7605772227848115
  current_accuracy : 0.0983
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.0001
      loss :              1.7605772227848115
      loss_factor :              0.17605772227848115
      loss adapted learning_rate :              3.09963215738868e-06
    epoch : 0 ; learning_rate : 3.09963215738868e-06 ; fit : 0.09308333333333334
    batch_size :          60000
    best_batch_loss : 1.7604751706597344
    Epoch 0, Loss: 1.7604751706597344, fit: 0.09315
    Epoch 0, Loss: 1.7604751706597344
      batch rate adapted learning_rate :              0.0001
      loss :              1.7604751706597344
      loss_factor :              0.17604751706597344
      loss adapted learning_rate :              3.099272826509421e-06
    epoch : 1 ; learning_rate : 3.099272826509421e-06 ; fit : 0.09315
    batch_size :          60000
    best_batch_loss : 1.7604720094958184
    Epoch 1, Loss: 1.7604720094958184, fit: 0.09315
      batch rate adapted learning_rate :              0.0001
      loss :              1.7604720094958184
      loss_factor :              0.17604720094958184
      loss adapted learning_rate :              3.0992616962182453e-06
    epoch : 2 ; learning_rate : 3.0992616962182453e-06 ; fit : 0.09315
    batch_size :          60000
    best_batch_loss : 1.760468848712957
    Epoch 2, Loss: 1.760468848712957, fit: 0.09315
      batch rate adapted learning_rate :              0.0001
      loss :              1.760468848712957
      loss_factor :              0.1760468848712957
      loss adapted learning_rate :              3.099250567288724e-06
    epoch : 3 ; learning_rate : 3.099250567288724e-06 ; fit : 0.09315
    batch_size :          60000
    best_batch_loss : 1.7604656879533023
    Epoch 3, Loss: 1.7604656879533023, fit: 0.09315
      batch rate adapted learning_rate :              0.0001
      loss :              1.7604656879533023
      loss_factor :              0.17604656879533023
      loss adapted learning_rate :              3.0992394384608944e-06
    epoch : 4 ; learning_rate : 3.0992394384608944e-06 ; fit : 0.09315
    batch_size :          60000
    best_batch_loss : 1.760462527214125
    Epoch 4, Loss: 1.760462527214125, fit: 0.09315
      batch rate adapted learning_rate :              0.0001
      loss :              1.760462527214125
      loss_factor :              0.1760462527214125
      loss adapted learning_rate :              3.099228309725144e-06
    epoch : 5 ; learning_rate : 3.099228309725144e-06 ; fit : 0.09315
    batch_size :          60000
    best_batch_loss : 1.7604593664927022
    Epoch 5, Loss: 1.7604593664927022, fit: 0.09318333333333334
      batch rate adapted learning_rate :              0.0001
      loss :              1.7604593664927022
      loss_factor :              0.1760459366492702
      loss adapted learning_rate :              3.099217181071886e-06
    epoch : 6 ; learning_rate : 3.099217181071886e-06 ; fit : 0.09318333333333334
    batch_size :          60000
    best_batch_loss : 1.7604562057863093
    Epoch 6, Loss: 1.7604562057863093, fit: 0.09318333333333334
      batch rate adapted learning_rate :              0.0001
      loss :              1.7604562057863093
      loss_factor :              0.17604562057863093
      loss adapted learning_rate :              3.0992060524915284e-06
    epoch : 7 ; learning_rate : 3.0992060524915284e-06 ; fit : 0.09318333333333334
    batch_size :          60000
    best_batch_loss : 1.7604530450922302
    Epoch 7, Loss: 1.7604530450922302, fit: 0.0932
      batch rate adapted learning_rate :              0.0001
      loss :              1.7604530450922302
      loss_factor :              0.176045304509223
      loss adapted learning_rate :              3.0991949239745057e-06
    epoch : 8 ; learning_rate : 3.0991949239745057e-06 ; fit : 0.0932
    batch_size :          60000
    best_batch_loss : 1.7604498844077459
    Epoch 8, Loss: 1.7604498844077459, fit: 0.09318333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7604498844077459_fit_0.09318333333333334_2024-01-03_145618
  self.fit : 0.09318333333333334
  self.loss : 1.7604498844077459
  current_accuracy : 0.0982
   Accuracy mean: 0.0983
   Accuracy mean: 0.0982
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.001
    epoch : 0 ; learning_rate : 0.001 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7677683077101822
    Epoch 0, Loss: 1.7677683077101822, fit: 0.0869
    Epoch 0, Loss: 1.7677683077101822
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7677683077101822_fit_0.0869_2024-01-03_145621
  self.fit : 0.0869
  self.loss : 1.7677683077101822
  current_accuracy : 0.086
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.001
      loss :              1.7677683077101822
      loss_factor :              0.17677683077101822
      loss adapted learning_rate :              3.1250047897445213e-05
    epoch : 0 ; learning_rate : 3.1250047897445213e-05 ; fit : 0.0869
    batch_size :          60000
    best_batch_loss : 1.7660612226259762
    Epoch 0, Loss: 1.7660612226259762, fit: 0.08785
    Epoch 0, Loss: 1.7660612226259762
      batch rate adapted learning_rate :              0.001
      loss :              1.7660612226259762
      loss_factor :              0.17660612226259761
      loss adapted learning_rate :              3.118972242063158e-05
    epoch : 1 ; learning_rate : 3.118972242063158e-05 ; fit : 0.08785
    batch_size :          60000
    best_batch_loss : 1.7660076669803746
    Epoch 1, Loss: 1.7660076669803746, fit: 0.08786666666666666
      batch rate adapted learning_rate :              0.001
      loss :              1.7660076669803746
      loss_factor :              0.17660076669803745
      loss adapted learning_rate :              3.118783079833465e-05
    epoch : 2 ; learning_rate : 3.118783079833465e-05 ; fit : 0.08786666666666666
    batch_size :          60000
    best_batch_loss : 1.7659541838522457
    Epoch 2, Loss: 1.7659541838522457, fit: 0.08785
      batch rate adapted learning_rate :              0.001
      loss :              1.7659541838522457
      loss_factor :              0.17659541838522458
      loss adapted learning_rate :              3.118594179465252e-05
    epoch : 3 ; learning_rate : 3.118594179465252e-05 ; fit : 0.08785
    batch_size :          60000
    best_batch_loss : 1.7659006726355782
    Epoch 3, Loss: 1.7659006726355782, fit: 0.08788333333333333
      batch rate adapted learning_rate :              0.001
      loss :              1.7659006726355782
      loss_factor :              0.1765900672635578
      loss adapted learning_rate :              3.1184051856147867e-05
    epoch : 4 ; learning_rate : 3.1184051856147867e-05 ; fit : 0.08788333333333333
    batch_size :          60000
    best_batch_loss : 1.765847132856488
    Epoch 4, Loss: 1.765847132856488, fit: 0.0879
      batch rate adapted learning_rate :              0.001
      loss :              1.765847132856488
      loss_factor :              0.1765847132856488
      loss adapted learning_rate :              3.1182160966174784e-05
    epoch : 5 ; learning_rate : 3.1182160966174784e-05 ; fit : 0.0879
    batch_size :          60000
    best_batch_loss : 1.765793564058021
    Epoch 5, Loss: 1.765793564058021, fit: 0.088
      batch rate adapted learning_rate :              0.001
      loss :              1.765793564058021
      loss_factor :              0.17657935640580208
      loss adapted learning_rate :              3.1180269108687275e-05
    epoch : 6 ; learning_rate : 3.1180269108687275e-05 ; fit : 0.088
    batch_size :          60000
    best_batch_loss : 1.765739965794373
    Epoch 6, Loss: 1.765739965794373, fit: 0.088
      batch rate adapted learning_rate :              0.001
      loss :              1.765739965794373
      loss_factor :              0.17657399657943731
      loss adapted learning_rate :              3.117837626803514e-05
    epoch : 7 ; learning_rate : 3.117837626803514e-05 ; fit : 0.088
    batch_size :          60000
    best_batch_loss : 1.7656863376311556
    Epoch 7, Loss: 1.7656863376311556, fit: 0.088
      batch rate adapted learning_rate :              0.001
      loss :              1.7656863376311556
      loss_factor :              0.17656863376311555
      loss adapted learning_rate :              3.117648242897323e-05
    epoch : 8 ; learning_rate : 3.117648242897323e-05 ; fit : 0.088
    batch_size :          60000
    best_batch_loss : 1.765632679145664
    Epoch 8, Loss: 1.765632679145664, fit: 0.08801666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.765632679145664_fit_0.08801666666666667_2024-01-03_145642
  self.fit : 0.08801666666666667
  self.loss : 1.765632679145664
  current_accuracy : 0.0859
   Accuracy mean: 0.086
   Accuracy mean: 0.0859
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.02
    epoch : 0 ; learning_rate : 0.02 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7011656826364139
    Epoch 0, Loss: 1.7011656826364139, fit: 0.12978333333333333
    Epoch 0, Loss: 1.7011656826364139
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7011656826364139_fit_0.12978333333333333_2024-01-03_145645
  self.fit : 0.12978333333333333
  self.loss : 1.7011656826364139
  current_accuracy : 0.1452
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.02
      loss :              1.7011656826364139
      loss_factor :              0.17011656826364138
      loss adapted learning_rate :              0.0005787929359559632
    epoch : 0 ; learning_rate : 0.0005787929359559632 ; fit : 0.12978333333333333
    batch_size :          60000
    best_batch_loss : 1.63901810520008
    Epoch 0, Loss: 1.63901810520008, fit: 0.14963333333333334
    Epoch 0, Loss: 1.63901810520008
      batch rate adapted learning_rate :              0.02
      loss :              1.63901810520008
      loss_factor :              0.16390181052000802
      loss adapted learning_rate :              0.0005372760698347322
    epoch : 1 ; learning_rate : 0.0005372760698347322 ; fit : 0.14963333333333334
    batch_size :          60000
    best_batch_loss : 1.6381069904554129
    Epoch 1, Loss: 1.6381069904554129, fit: 0.15021666666666667
      batch rate adapted learning_rate :              0.02
      loss :              1.6381069904554129
      loss_factor :              0.1638106990455413
      loss adapted learning_rate :              0.0005366789024357781
    epoch : 2 ; learning_rate : 0.0005366789024357781 ; fit : 0.15021666666666667
    batch_size :          60000
    best_batch_loss : 1.6372939188255056
    Epoch 2, Loss: 1.6372939188255056, fit: 0.15056666666666665
      batch rate adapted learning_rate :              0.02
      loss :              1.6372939188255056
      loss_factor :              0.16372939188255056
      loss adapted learning_rate :              0.0005361462753245963
    epoch : 3 ; learning_rate : 0.0005361462753245963 ; fit : 0.15056666666666665
    batch_size :          60000
    best_batch_loss : 1.6365134444803617
    Epoch 3, Loss: 1.6365134444803617, fit: 0.15098333333333333
      batch rate adapted learning_rate :              0.02
      loss :              1.6365134444803617
      loss_factor :              0.16365134444803617
      loss adapted learning_rate :              0.0005356352507929955
    epoch : 4 ; learning_rate : 0.0005356352507929955 ; fit : 0.15098333333333333
    batch_size :          60000
    best_batch_loss : 1.6357615221205737
    Epoch 4, Loss: 1.6357615221205737, fit: 0.15115
      batch rate adapted learning_rate :              0.02
      loss :              1.6357615221205737
      loss_factor :              0.16357615221205737
      loss adapted learning_rate :              0.0005351431514500433
    epoch : 5 ; learning_rate : 0.0005351431514500433 ; fit : 0.15115
    batch_size :          60000
    best_batch_loss : 1.6350313519389332
    Epoch 5, Loss: 1.6350313519389332, fit: 0.1516
      batch rate adapted learning_rate :              0.02
      loss :              1.6350313519389332
      loss_factor :              0.1635031351938933
      loss adapted learning_rate :              0.0005346655043646511
    epoch : 6 ; learning_rate : 0.0005346655043646511 ; fit : 0.1516
    batch_size :          60000
    best_batch_loss : 1.6343142665473573
    Epoch 6, Loss: 1.6343142665473573, fit: 0.15205
      batch rate adapted learning_rate :              0.02
      loss :              1.6343142665473573
      loss_factor :              0.16343142665473573
      loss adapted learning_rate :              0.0005341966243680453
    epoch : 7 ; learning_rate : 0.0005341966243680453 ; fit : 0.15205
    batch_size :          60000
    best_batch_loss : 1.6336005948071797
    Epoch 7, Loss: 1.6336005948071797, fit: 0.1523
      batch rate adapted learning_rate :              0.02
      loss :              1.6336005948071797
      loss_factor :              0.16336005948071797
      loss adapted learning_rate :              0.0005337301806708743
    epoch : 8 ; learning_rate : 0.0005337301806708743 ; fit : 0.1523
    batch_size :          60000
    best_batch_loss : 1.6328814874598678
    Epoch 8, Loss: 1.6328814874598678, fit: 0.15236666666666668
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.6328814874598678_fit_0.15236666666666668_2024-01-03_145706
  self.fit : 0.15236666666666668
  self.loss : 1.6328814874598678
  current_accuracy : 0.1479
   Accuracy mean: 0.1452
   Accuracy mean: 0.1479
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.025
    epoch : 0 ; learning_rate : 0.025 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7749634376496777
    Epoch 0, Loss: 1.7749634376496777, fit: 0.08878333333333334
    Epoch 0, Loss: 1.7749634376496777
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7749634376496777_fit_0.08878333333333334_2024-01-03_145708
  self.fit : 0.08878333333333334
  self.loss : 1.7749634376496777
  current_accuracy : 0.1157
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.025
      loss :              1.7749634376496777
      loss_factor :              0.17749634376496776
      loss adapted learning_rate :              0.0007876238012482903
    epoch : 0 ; learning_rate : 0.0007876238012482903 ; fit : 0.08878333333333334
    batch_size :          60000
    best_batch_loss : 1.7231382389905923
    Epoch 0, Loss: 1.7231382389905923, fit: 0.10846666666666667
    Epoch 0, Loss: 1.7231382389905923
      batch rate adapted learning_rate :              0.025
      loss :              1.7231382389905923
      loss_factor :              0.17231382389905922
      loss adapted learning_rate :              0.0007423013476678999
    epoch : 1 ; learning_rate : 0.0007423013476678999 ; fit : 0.10846666666666667
    batch_size :          60000
    best_batch_loss : 1.7219341054810442
    Epoch 1, Loss: 1.7219341054810442, fit: 0.10881666666666667
      batch rate adapted learning_rate :              0.025
      loss :              1.7219341054810442
      loss_factor :              0.17219341054810441
      loss adapted learning_rate :              0.000741264265904701
    epoch : 2 ; learning_rate : 0.000741264265904701 ; fit : 0.10881666666666667
    batch_size :          60000
    best_batch_loss : 1.7207650753308668
    Epoch 2, Loss: 1.7207650753308668, fit: 0.10921666666666667
      batch rate adapted learning_rate :              0.025
      loss :              1.7207650753308668
      loss_factor :              0.17207650753308668
      loss adapted learning_rate :              0.000740258111119611
    epoch : 3 ; learning_rate : 0.000740258111119611 ; fit : 0.10921666666666667
    batch_size :          60000
    best_batch_loss : 1.7195577829964699
    Epoch 3, Loss: 1.7195577829964699, fit: 0.10966666666666666
      batch rate adapted learning_rate :              0.025
      loss :              1.7195577829964699
      loss_factor :              0.17195577829964698
      loss adapted learning_rate :              0.0007392197422659337
    epoch : 4 ; learning_rate : 0.0007392197422659337 ; fit : 0.10966666666666666
    batch_size :          60000
    best_batch_loss : 1.7183144129919865
    Epoch 4, Loss: 1.7183144129919865, fit: 0.11038333333333333
      batch rate adapted learning_rate :              0.025
      loss :              1.7183144129919865
      loss_factor :              0.17183144129919864
      loss adapted learning_rate :              0.0007381511054739987
    epoch : 5 ; learning_rate : 0.0007381511054739987 ; fit : 0.11038333333333333
    batch_size :          60000
    best_batch_loss : 1.7170447847098729
    Epoch 5, Loss: 1.7170447847098729, fit: 0.11081666666666666
      batch rate adapted learning_rate :              0.025
      loss :              1.7170447847098729
      loss_factor :              0.1717044784709873
      loss adapted learning_rate :              0.0007370606981748436
    epoch : 6 ; learning_rate : 0.0007370606981748436 ; fit : 0.11081666666666666
    batch_size :          60000
    best_batch_loss : 1.7157644528625158
    Epoch 6, Loss: 1.7157644528625158, fit: 0.11141666666666666
      batch rate adapted learning_rate :              0.025
      loss :              1.7157644528625158
      loss_factor :              0.17157644528625157
      loss adapted learning_rate :              0.0007359619144266521
    epoch : 7 ; learning_rate : 0.0007359619144266521 ; fit : 0.11141666666666666
    batch_size :          60000
    best_batch_loss : 1.7144889972221213
    Epoch 7, Loss: 1.7144889972221213, fit: 0.11208333333333333
      batch rate adapted learning_rate :              0.025
      loss :              1.7144889972221213
      loss_factor :              0.17144889972221214
      loss adapted learning_rate :              0.0007348681303989289
    epoch : 8 ; learning_rate : 0.0007348681303989289 ; fit : 0.11208333333333333
    batch_size :          60000
    best_batch_loss : 1.7132281260637992
    Epoch 8, Loss: 1.7132281260637992, fit: 0.11255
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7132281260637992_fit_0.11255_2024-01-03_145729
  self.fit : 0.11255
  self.loss : 1.7132281260637992
  current_accuracy : 0.1219
   Accuracy mean: 0.1157
   Accuracy mean: 0.1219
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.03
    epoch : 0 ; learning_rate : 0.03 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7314955183721492
    Epoch 0, Loss: 1.7314955183721492, fit: 0.10773333333333333
    Epoch 0, Loss: 1.7314955183721492
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7314955183721492_fit_0.10773333333333333_2024-01-03_145731
  self.fit : 0.10773333333333333
  self.loss : 1.7314955183721492
  current_accuracy : 0.1236
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.03
      loss :              1.7314955183721492
      loss_factor :              0.17314955183721492
      loss adapted learning_rate :              0.0008994230190428513
    epoch : 0 ; learning_rate : 0.0008994230190428513 ; fit : 0.10773333333333333
    batch_size :          60000
    best_batch_loss : 1.6887245819275782
    Epoch 0, Loss: 1.6887245819275782, fit: 0.12443333333333334
    Epoch 0, Loss: 1.6887245819275782
      batch rate adapted learning_rate :              0.03
      loss :              1.6887245819275782
      loss_factor :              0.1688724581927578
      loss adapted learning_rate :              0.000855537214081942
    epoch : 1 ; learning_rate : 0.000855537214081942 ; fit : 0.12443333333333334
    batch_size :          60000
    best_batch_loss : 1.6876662826527662
    Epoch 1, Loss: 1.6876662826527662, fit: 0.12508333333333332
      batch rate adapted learning_rate :              0.03
      loss :              1.6876662826527662
      loss_factor :              0.1687666282652766
      loss adapted learning_rate :              0.0008544652444809018
    epoch : 2 ; learning_rate : 0.0008544652444809018 ; fit : 0.12508333333333332
    batch_size :          60000
    best_batch_loss : 1.6866669101753153
    Epoch 2, Loss: 1.6866669101753153, fit: 0.12553333333333333
      batch rate adapted learning_rate :              0.03
      loss :              1.6866669101753153
      loss_factor :              0.16866669101753154
      loss adapted learning_rate :              0.0008534535797641036
    epoch : 3 ; learning_rate : 0.0008534535797641036 ; fit : 0.12553333333333333
    batch_size :          60000
    best_batch_loss : 1.6856764603452168
    Epoch 3, Loss: 1.6856764603452168, fit: 0.12625
      batch rate adapted learning_rate :              0.03
      loss :              1.6856764603452168
      loss_factor :              0.16856764603452168
      loss adapted learning_rate :              0.0008524515386885938
    epoch : 4 ; learning_rate : 0.0008524515386885938 ; fit : 0.12625
    batch_size :          60000
    best_batch_loss : 1.6846927607747924
    Epoch 4, Loss: 1.6846927607747924, fit: 0.12656666666666666
      batch rate adapted learning_rate :              0.03
      loss :              1.6846927607747924
      loss_factor :              0.16846927607747925
      loss adapted learning_rate :              0.0008514569094620976
    epoch : 5 ; learning_rate : 0.0008514569094620976 ; fit : 0.12656666666666666
    batch_size :          60000
    best_batch_loss : 1.6837155935196026
    Epoch 5, Loss: 1.6837155935196026, fit: 0.12721666666666667
      batch rate adapted learning_rate :              0.03
      loss :              1.6837155935196026
      loss_factor :              0.16837155935196027
      loss adapted learning_rate :              0.0008504694599583203
    epoch : 6 ; learning_rate : 0.0008504694599583203 ; fit : 0.12721666666666667
    batch_size :          60000
    best_batch_loss : 1.6827486494365775
    Epoch 6, Loss: 1.6827486494365775, fit: 0.12761666666666666
      batch rate adapted learning_rate :              0.03
      loss :              1.6827486494365775
      loss_factor :              0.16827486494365776
      loss adapted learning_rate :              0.0008494929051541878
    epoch : 7 ; learning_rate : 0.0008494929051541878 ; fit : 0.12761666666666666
    batch_size :          60000
    best_batch_loss : 1.681796983907615
    Epoch 7, Loss: 1.681796983907615, fit: 0.1279
      batch rate adapted learning_rate :              0.03
      loss :              1.681796983907615
      loss_factor :              0.1681796983907615
      loss adapted learning_rate :              0.0008485323285242251
    epoch : 8 ; learning_rate : 0.0008485323285242251 ; fit : 0.1279
    batch_size :          60000
    best_batch_loss : 1.6808640529222145
    Epoch 8, Loss: 1.6808640529222145, fit: 0.12825
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6808640529222145_fit_0.12825_2024-01-03_145752
  self.fit : 0.12825
  self.loss : 1.6808640529222145
  current_accuracy : 0.1261
   Accuracy mean: 0.1236
   Accuracy mean: 0.1261
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.1
    epoch : 0 ; learning_rate : 0.1 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7324381224980392
    Epoch 0, Loss: 1.7324381224980392, fit: 0.10503333333333334
    Epoch 0, Loss: 1.7324381224980392
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7324381224980392_fit_0.10503333333333334_2024-01-03_145755
  self.fit : 0.10503333333333334
  self.loss : 1.7324381224980392
  current_accuracy : 0.1341
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.1
      loss :              1.7324381224980392
      loss_factor :              0.17324381224980392
      loss adapted learning_rate :              0.0030013418482845314
    epoch : 0 ; learning_rate : 0.0030013418482845314 ; fit : 0.10503333333333334
    batch_size :          60000
    best_batch_loss : 1.6520746797588268
    Epoch 0, Loss: 1.6520746797588268, fit: 0.14168333333333333
    Epoch 0, Loss: 1.6520746797588268
      batch rate adapted learning_rate :              0.1
      loss :              1.6520746797588268
      loss_factor :              0.16520746797588268
      loss adapted learning_rate :              0.0027293507475002303
    epoch : 1 ; learning_rate : 0.0027293507475002303 ; fit : 0.14168333333333333
    batch_size :          60000
    best_batch_loss : 1.6470922148737737
    Epoch 1, Loss: 1.6470922148737737, fit: 0.14361666666666667
      batch rate adapted learning_rate :              0.1
      loss :              1.6470922148737737
      loss_factor :              0.16470922148737738
      loss adapted learning_rate :              0.002712912764297794
    epoch : 2 ; learning_rate : 0.002712912764297794 ; fit : 0.14361666666666667
    batch_size :          60000
    best_batch_loss : 1.6428974976757322
    Epoch 2, Loss: 1.6428974976757322, fit: 0.14605
      batch rate adapted learning_rate :              0.1
      loss :              1.6428974976757322
      loss_factor :              0.1642897497675732
      loss adapted learning_rate :              0.0026991121878691825
    epoch : 3 ; learning_rate : 0.0026991121878691825 ; fit : 0.14605
    batch_size :          60000
    best_batch_loss : 1.6391747925117228
    Epoch 3, Loss: 1.6391747925117228, fit: 0.1477
      batch rate adapted learning_rate :              0.1
      loss :              1.6391747925117228
      loss_factor :              0.16391747925117228
      loss adapted learning_rate :              0.0026868940004058494
    epoch : 4 ; learning_rate : 0.0026868940004058494 ; fit : 0.1477
    batch_size :          60000
    best_batch_loss : 1.6355994369332583
    Epoch 4, Loss: 1.6355994369332583, fit: 0.14888333333333334
      batch rate adapted learning_rate :              0.1
      loss :              1.6355994369332583
      loss_factor :              0.16355994369332583
      loss adapted learning_rate :              0.002675185518096392
    epoch : 5 ; learning_rate : 0.002675185518096392 ; fit : 0.14888333333333334
    batch_size :          60000
    best_batch_loss : 1.6320951137142006
    Epoch 5, Loss: 1.6320951137142006, fit: 0.15061666666666668
      batch rate adapted learning_rate :              0.1
      loss :              1.6320951137142006
      loss_factor :              0.16320951137142006
      loss adapted learning_rate :              0.0026637344602097697
    epoch : 6 ; learning_rate : 0.0026637344602097697 ; fit : 0.15061666666666668
    batch_size :          60000
    best_batch_loss : 1.6286903208733423
    Epoch 6, Loss: 1.6286903208733423, fit: 0.1522
      batch rate adapted learning_rate :              0.1
      loss :              1.6286903208733423
      loss_factor :              0.16286903208733422
      loss adapted learning_rate :              0.0026526321613065105
    epoch : 7 ; learning_rate : 0.0026526321613065105 ; fit : 0.1522
    batch_size :          60000
    best_batch_loss : 1.6253750149345418
    Epoch 7, Loss: 1.6253750149345418, fit: 0.15366666666666667
      batch rate adapted learning_rate :              0.1
      loss :              1.6253750149345418
      loss_factor :              0.1625375014934542
      loss adapted learning_rate :              0.0026418439391734626
    epoch : 8 ; learning_rate : 0.0026418439391734626 ; fit : 0.15366666666666667
    batch_size :          60000
    best_batch_loss : 1.6221827149047636
    Epoch 8, Loss: 1.6221827149047636, fit: 0.15498333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6221827149047636_fit_0.15498333333333333_2024-01-03_145816
  self.fit : 0.15498333333333333
  self.loss : 1.6221827149047636
  current_accuracy : 0.1484
   Accuracy mean: 0.1341
   Accuracy mean: 0.1484
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.9
    epoch : 0 ; learning_rate : 0.9 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7089447806515838
    Epoch 0, Loss: 1.7089447806515838, fit: 0.12266666666666666
    Epoch 0, Loss: 1.7089447806515838
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7089447806515838_fit_0.12266666666666666_2024-01-03_145819
  self.fit : 0.12266666666666666
  self.loss : 1.7089447806515838
  current_accuracy : 0.2019
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.9
      loss :              1.7089447806515838
      loss_factor :              0.1708944780651584
      loss adapted learning_rate :              0.026284430369846616
    epoch : 0 ; learning_rate : 0.026284430369846616 ; fit : 0.12266666666666666
    batch_size :          60000
    best_batch_loss : 1.5469895793761397
    Epoch 0, Loss: 1.5469895793761397, fit: 0.2072
    Epoch 0, Loss: 1.5469895793761397
      batch rate adapted learning_rate :              0.9
      loss :              1.5469895793761397
      loss_factor :              0.15469895793761396
      loss adapted learning_rate :              0.021538590828285287
    epoch : 1 ; learning_rate : 0.021538590828285287 ; fit : 0.2072
    batch_size :          60000
    best_batch_loss : 1.498308475192824
    Epoch 1, Loss: 1.498308475192824, fit: 0.22888333333333333
      batch rate adapted learning_rate :              0.9
      loss :              1.498308475192824
      loss_factor :              0.14983084751928238
      loss adapted learning_rate :              0.020204354581511805
    epoch : 2 ; learning_rate : 0.020204354581511805 ; fit : 0.22888333333333333
    batch_size :          60000
    best_batch_loss : 1.4524746117692353
    Epoch 2, Loss: 1.4524746117692353, fit: 0.24991666666666668
      batch rate adapted learning_rate :              0.9
      loss :              1.4524746117692353
      loss_factor :              0.14524746117692353
      loss adapted learning_rate :              0.01898714248050772
    epoch : 3 ; learning_rate : 0.01898714248050772 ; fit : 0.24991666666666668
    batch_size :          60000
    best_batch_loss : 1.4072258345368898
    Epoch 3, Loss: 1.4072258345368898, fit: 0.2712833333333333
      batch rate adapted learning_rate :              0.9
      loss :              1.4072258345368898
      loss_factor :              0.140722583453689
      loss adapted learning_rate :              0.017822560944492416
    epoch : 4 ; learning_rate : 0.017822560944492416 ; fit : 0.2712833333333333
    batch_size :          60000
    best_batch_loss : 1.3678279291208497
    Epoch 4, Loss: 1.3678279291208497, fit: 0.2911
      batch rate adapted learning_rate :              0.9
      loss :              1.3678279291208497
      loss_factor :              0.13678279291208498
      loss adapted learning_rate :              0.016838579193147293
    epoch : 5 ; learning_rate : 0.016838579193147293 ; fit : 0.2911
    batch_size :          60000
    best_batch_loss : 1.3308051630891065
    Epoch 5, Loss: 1.3308051630891065, fit: 0.30851666666666666
      batch rate adapted learning_rate :              0.9
      loss :              1.3308051630891065
      loss_factor :              0.13308051630891066
      loss adapted learning_rate :              0.01593938143894161
    epoch : 6 ; learning_rate : 0.01593938143894161 ; fit : 0.30851666666666666
    batch_size :          60000
    best_batch_loss : 1.2980246841985672
    Epoch 6, Loss: 1.2980246841985672, fit: 0.3242833333333333
      batch rate adapted learning_rate :              0.9
      loss :              1.2980246841985672
      loss_factor :              0.1298024684198567
      loss adapted learning_rate :              0.015163812727099107
    epoch : 7 ; learning_rate : 0.015163812727099107 ; fit : 0.3242833333333333
    batch_size :          60000
    best_batch_loss : 1.2701912732332261
    Epoch 7, Loss: 1.2701912732332261, fit: 0.33766666666666667
      batch rate adapted learning_rate :              0.9
      loss :              1.2701912732332261
      loss_factor :              0.1270191273233226
      loss adapted learning_rate :              0.014520472835380592
    epoch : 8 ; learning_rate : 0.014520472835380592 ; fit : 0.33766666666666667
    batch_size :          60000
    best_batch_loss : 1.2442050698642924
    Epoch 8, Loss: 1.2442050698642924, fit: 0.35028333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.2442050698642924_fit_0.35028333333333334_2024-01-03_145840
  self.fit : 0.35028333333333334
  self.loss : 1.2442050698642924
  current_accuracy : 0.3588
   Accuracy mean: 0.2019
   Accuracy mean: 0.3588
  Results saved to test_combinations_results20240103145840
  normalized_accuracies :      [0.20935252 0.20935252 0.         0.         0.06294964 0.06258993
   0.01870504 0.01834532 0.23165468 0.24136691 0.12553957 0.14784173
   0.15395683 0.16294964 0.19172662 0.24316547 0.43561151 1.        ]
batch_rate :  0.5
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-11
    epoch : 0 ; learning_rate : 5e-11 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7127454509155464
    Epoch 0, Loss: 1.7135277957498776, fit: 0.1136
    Epoch 0, Loss: 1.7135277957498776
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.714310140584209_fit_0.1136_2024-01-03_145843
  self.fit : 0.1136
  self.loss : 1.714310140584209
  current_accuracy : 0.1107
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-11
      loss :              1.714310140584209
      loss_factor :              0.1714310140584209
      loss adapted learning_rate :              1.4694296290549253e-12
    epoch : 0 ; learning_rate : 1.4694296290549253e-12 ; fit : 0.1136
    batch_size :          30000
    best_batch_loss : 1.7091360593683897
    Epoch 0, Loss: 1.7135277956807045, fit: 0.11113333333333333
    Epoch 0, Loss: 1.7135277956807045
      batch rate adapted learning_rate :              5e-11
      loss :              1.7179195319930194
      loss_factor :              0.17179195319930193
      loss adapted learning_rate :              1.4756237592015572e-12
    epoch : 1 ; learning_rate : 1.4756237592015572e-12 ; fit : 0.11113333333333333
    batch_size :          30000
    best_batch_loss : 1.7103338881522858
    Epoch 1, Loss: 1.7135277956780017, fit: 0.1153
      batch rate adapted learning_rate :              5e-11
      loss :              1.7103338881522858
      loss_factor :              0.1710333888152286
      loss adapted learning_rate :              1.462621004481058e-12
    epoch : 2 ; learning_rate : 1.462621004481058e-12 ; fit : 0.1153
    batch_size :          30000
    best_batch_loss : 1.71000213026156
    Epoch 2, Loss: 1.713527795675426, fit: 0.11193333333333333
      batch rate adapted learning_rate :              5e-11
      loss :              1.7170534610892922
      loss_factor :              0.17170534610892924
      loss adapted learning_rate :              1.474136294119359e-12
    epoch : 3 ; learning_rate : 1.474136294119359e-12 ; fit : 0.11193333333333333
    batch_size :          30000
    best_batch_loss : 1.711691425362649
    Epoch 3, Loss: 1.713527795672484, fit: 0.1141
      batch rate adapted learning_rate :              5e-11
      loss :              1.711691425362649
      loss_factor :              0.1711691425362649
      loss adapted learning_rate :              1.4649437678300085e-12
    epoch : 4 ; learning_rate : 1.4649437678300085e-12 ; fit : 0.1141
    batch_size :          30000
    best_batch_loss : 1.7106632515367557
    Epoch 4, Loss: 1.7135277956698394, fit: 0.11463333333333334
      batch rate adapted learning_rate :              5e-11
      loss :              1.7106632515367557
      loss_factor :              0.17106632515367556
      loss adapted learning_rate :              1.4631843800791528e-12
    epoch : 5 ; learning_rate : 1.4631843800791528e-12 ; fit : 0.11463333333333334
    batch_size :          30000
    best_batch_loss : 1.7121826076854176
    Epoch 5, Loss: 1.7135277956671906, fit: 0.11313333333333334
      batch rate adapted learning_rate :              5e-11
      loss :              1.7148729836489636
      loss_factor :              0.17148729836489635
      loss adapted learning_rate :              1.470394675024549e-12
    epoch : 6 ; learning_rate : 1.470394675024549e-12 ; fit : 0.11313333333333334
    batch_size :          30000
    best_batch_loss : 1.7125044195293917
    Epoch 6, Loss: 1.7135277956644617, fit: 0.1145
      batch rate adapted learning_rate :              5e-11
      loss :              1.7125044195293917
      loss_factor :              0.17125044195293918
      loss adapted learning_rate :              1.4663356934538495e-12
    epoch : 7 ; learning_rate : 1.4663356934538495e-12 ; fit : 0.1145
    batch_size :          30000
    best_batch_loss : 1.7069989221524362
    Epoch 7, Loss: 1.7135277956616894, fit: 0.11676666666666667
      batch rate adapted learning_rate :              5e-11
      loss :              1.7069989221524362
      loss_factor :              0.17069989221524362
      loss adapted learning_rate :              1.4569226601147895e-12
    epoch : 8 ; learning_rate : 1.4569226601147895e-12 ; fit : 0.11676666666666667
    batch_size :          30000
    best_batch_loss : 1.712871288358838
    Epoch 8, Loss: 1.7135277956590649, fit: 0.11366666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.714184302959292_fit_0.11366666666666667_2024-01-03_145903
  self.fit : 0.11366666666666667
  self.loss : 1.714184302959292
  current_accuracy : 0.1107
   Accuracy mean: 0.1107
   Accuracy mean: 0.1107
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-08
    epoch : 0 ; learning_rate : 5e-08 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7738727608446352
    Epoch 0, Loss: 1.7749505373208034, fit: 0.08246666666666666
    Epoch 0, Loss: 1.7749505373208034
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7760283137969715_fit_0.08246666666666666_2024-01-03_145906
  self.fit : 0.08246666666666666
  self.loss : 1.7760283137969715
  current_accuracy : 0.08
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-08
      loss :              1.7760283137969715
      loss_factor :              0.17760283137969715
      loss adapted learning_rate :              1.577138285704257e-09
    epoch : 0 ; learning_rate : 1.577138285704257e-09 ; fit : 0.08246666666666666
    batch_size :          30000
    best_batch_loss : 1.7732607544256156
    Epoch 0, Loss: 1.7749504109452854, fit: 0.08293333333333333
    Epoch 0, Loss: 1.7749504109452854
      batch rate adapted learning_rate :              5e-08
      loss :              1.776640067464955
      loss_factor :              0.1776640067464955
      loss adapted learning_rate :              1.5782249646609395e-09
    epoch : 1 ; learning_rate : 1.5782249646609395e-09 ; fit : 0.08293333333333333
    batch_size :          30000
    best_batch_loss : 1.7719847001494733
    Epoch 1, Loss: 1.7749504057364907, fit: 0.0848
      batch rate adapted learning_rate :              5e-08
      loss :              1.7719847001494733
      loss_factor :              0.17719847001494732
      loss adapted learning_rate :              1.5699648887819092e-09
    epoch : 2 ; learning_rate : 1.5699648887819092e-09 ; fit : 0.0848
    batch_size :          30000
    best_batch_loss : 1.7748952436920993
    Epoch 2, Loss: 1.7749504003505432, fit: 0.0831
      batch rate adapted learning_rate :              5e-08
      loss :              1.775005557008987
      loss_factor :              0.1775005557008987
      loss adapted learning_rate :              1.5753223637063923e-09
    epoch : 3 ; learning_rate : 1.5753223637063923e-09 ; fit : 0.0831
    batch_size :          30000
    best_batch_loss : 1.7729935872262752
    Epoch 3, Loss: 1.7749503950469037, fit: 0.08246666666666666
      batch rate adapted learning_rate :              5e-08
      loss :              1.7769072028675321
      loss_factor :              0.1776907202867532
      loss adapted learning_rate :              1.5786996038012583e-09
    epoch : 4 ; learning_rate : 1.5786996038012583e-09 ; fit : 0.08246666666666666
    batch_size :          30000
    best_batch_loss : 1.7694106892834742
    Epoch 4, Loss: 1.7749503898557644, fit: 0.0804
      batch rate adapted learning_rate :              5e-08
      loss :              1.7804900904280545
      loss_factor :              0.17804900904280546
      loss adapted learning_rate :              1.5850724810562508e-09
    epoch : 5 ; learning_rate : 1.5850724810562508e-09 ; fit : 0.0804
    batch_size :          30000
    best_batch_loss : 1.7738068751626326
    Epoch 5, Loss: 1.7749503845616714, fit: 0.08393333333333333
      batch rate adapted learning_rate :              5e-08
      loss :              1.7738068751626326
      loss_factor :              0.17738068751626326
      loss adapted learning_rate :              1.5731954151871114e-09
    epoch : 6 ; learning_rate : 1.5731954151871114e-09 ; fit : 0.08393333333333333
    batch_size :          30000
    best_batch_loss : 1.773636669910584
    Epoch 6, Loss: 1.7749503789999679, fit: 0.08373333333333334
      batch rate adapted learning_rate :              5e-08
      loss :              1.773636669910584
      loss_factor :              0.1773636669910584
      loss adapted learning_rate :              1.5728935184257528e-09
    epoch : 7 ; learning_rate : 1.5728935184257528e-09 ; fit : 0.08373333333333334
    batch_size :          30000
    best_batch_loss : 1.7741888431166284
    Epoch 7, Loss: 1.7749503737435477, fit: 0.08406666666666666
      batch rate adapted learning_rate :              5e-08
      loss :              1.7741888431166284
      loss_factor :              0.17741888431166283
      loss adapted learning_rate :              1.5738730255197598e-09
    epoch : 8 ; learning_rate : 1.5738730255197598e-09 ; fit : 0.08406666666666666
    batch_size :          30000
    best_batch_loss : 1.7740377446542377
    Epoch 8, Loss: 1.774950368494389, fit: 0.08363333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7740377446542377_fit_0.08363333333333334_2024-01-03_145926
  self.fit : 0.08363333333333334
  self.loss : 1.7740377446542377
  current_accuracy : 0.08
   Accuracy mean: 0.08
   Accuracy mean: 0.08
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-05
    epoch : 0 ; learning_rate : 5e-05 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.6880210135293152
    Epoch 0, Loss: 1.6902513457779764, fit: 0.1287
    Epoch 0, Loss: 1.6902513457779764
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6924816780266378_fit_0.1287_2024-01-03_145928
  self.fit : 0.1287
  self.loss : 1.6924816780266378
  current_accuracy : 0.1304
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-05
      loss :              1.6924816780266378
      loss_factor :              0.16924816780266377
      loss adapted learning_rate :              1.4322471152279318e-06
    epoch : 0 ; learning_rate : 1.4322471152279318e-06 ; fit : 0.1287
    batch_size :          30000
    best_batch_loss : 1.6878376038491516
    Epoch 0, Loss: 1.6901594951722858, fit: 0.131
    Epoch 0, Loss: 1.6901594951722858
      batch rate adapted learning_rate :              5e-05
      loss :              1.6878376038491516
      loss_factor :              0.16878376038491516
      loss adapted learning_rate :              1.4243978884836228e-06
    epoch : 1 ; learning_rate : 1.4243978884836228e-06 ; fit : 0.131
    batch_size :          30000
    best_batch_loss : 1.687972514655608
    Epoch 1, Loss: 1.690155944994154, fit: 0.13026666666666667
      batch rate adapted learning_rate :              5e-05
      loss :              1.687972514655608
      loss_factor :              0.1687972514655608
      loss adapted learning_rate :              1.4246256051163884e-06
    epoch : 2 ; learning_rate : 1.4246256051163884e-06 ; fit : 0.13026666666666667
    batch_size :          30000
    best_batch_loss : 1.686107655832558
    Epoch 2, Loss: 1.6901525483714481, fit: 0.13193333333333335
      batch rate adapted learning_rate :              5e-05
      loss :              1.686107655832558
      loss_factor :              0.1686107655832558
      loss adapted learning_rate :              1.421479513528582e-06
    epoch : 3 ; learning_rate : 1.421479513528582e-06 ; fit : 0.13193333333333335
    batch_size :          30000
    best_batch_loss : 1.6893581947091696
    Epoch 3, Loss: 1.6901491736137544, fit: 0.13056666666666666
      batch rate adapted learning_rate :              5e-05
      loss :              1.6893581947091696
      loss_factor :              0.16893581947091696
      loss adapted learning_rate :              1.4269655550155123e-06
    epoch : 4 ; learning_rate : 1.4269655550155123e-06 ; fit : 0.13056666666666666
    batch_size :          30000
    best_batch_loss : 1.6837381050708042
    Epoch 4, Loss: 1.690145519428426, fit: 0.1327
      batch rate adapted learning_rate :              5e-05
      loss :              1.6837381050708042
      loss_factor :              0.16837381050708042
      loss adapted learning_rate :              1.4174870032337112e-06
    epoch : 5 ; learning_rate : 1.4174870032337112e-06 ; fit : 0.1327
    batch_size :          30000
    best_batch_loss : 1.6891273889724874
    Epoch 5, Loss: 1.690142137234642, fit: 0.13096666666666668
      batch rate adapted learning_rate :              5e-05
      loss :              1.6891273889724874
      loss_factor :              0.16891273889724873
      loss adapted learning_rate :              1.4265756680885061e-06
    epoch : 6 ; learning_rate : 1.4265756680885061e-06 ; fit : 0.13096666666666668
    batch_size :          30000
    best_batch_loss : 1.687514997624746
    Epoch 6, Loss: 1.6901387366063574, fit: 0.12853333333333333
      batch rate adapted learning_rate :              5e-05
      loss :              1.6927624755879687
      loss_factor :              0.16927624755879686
      loss adapted learning_rate :              1.4327223993793542e-06
    epoch : 7 ; learning_rate : 1.4327223993793542e-06 ; fit : 0.12853333333333333
    batch_size :          30000
    best_batch_loss : 1.6884203972688945
    Epoch 7, Loss: 1.6901354313754178, fit: 0.13113333333333332
      batch rate adapted learning_rate :              5e-05
      loss :              1.6884203972688945
      loss_factor :              0.16884203972688944
      loss adapted learning_rate :              1.4253817189568257e-06
    epoch : 8 ; learning_rate : 1.4253817189568257e-06 ; fit : 0.13113333333333332
    batch_size :          30000
    best_batch_loss : 1.6875588453805792
    Epoch 8, Loss: 1.6901316738889363, fit: 0.12773333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6927045023972935_fit_0.12773333333333334_2024-01-03_145949
  self.fit : 0.12773333333333334
  self.loss : 1.6927045023972935
  current_accuracy : 0.1305
   Accuracy mean: 0.1304
   Accuracy mean: 0.1305
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.0005
    epoch : 0 ; learning_rate : 0.0005 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7220273721464778
    Epoch 0, Loss: 1.722740570745185, fit: 0.11406666666666666
    Epoch 0, Loss: 1.722740570745185
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7234537693438923_fit_0.11406666666666666_2024-01-03_145952
  self.fit : 0.11406666666666666
  self.loss : 1.7234537693438923
  current_accuracy : 0.1142
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.0005
      loss :              1.7234537693438923
      loss_factor :              0.17234537693438923
      loss adapted learning_rate :              1.4851464475328352e-05
    epoch : 0 ; learning_rate : 1.4851464475328352e-05 ; fit : 0.11406666666666666
    batch_size :          30000
    best_batch_loss : 1.720342491968881
    Epoch 0, Loss: 1.7223336849895223, fit: 0.11616666666666667
    Epoch 0, Loss: 1.7223336849895223
      batch rate adapted learning_rate :              0.0005
      loss :              1.720342491968881
      loss_factor :              0.1720342491968881
      loss adapted learning_rate :              1.4797891448368496e-05
    epoch : 1 ; learning_rate : 1.4797891448368496e-05 ; fit : 0.11616666666666667
    batch_size :          30000
    best_batch_loss : 1.7191245857705517
    Epoch 1, Loss: 1.722317787567798, fit: 0.1135
      batch rate adapted learning_rate :              0.0005
      loss :              1.7255109893650442
      loss_factor :              0.17255109893650442
      loss adapted learning_rate :              1.4886940872097668e-05
    epoch : 2 ; learning_rate : 1.4886940872097668e-05 ; fit : 0.1135
    batch_size :          30000
    best_batch_loss : 1.7219648611678855
    Epoch 2, Loss: 1.7223009416796002, fit: 0.11473333333333334
      batch rate adapted learning_rate :              0.0005
      loss :              1.7226370221913152
      loss_factor :              0.1722637022191315
      loss adapted learning_rate :              1.4837391551120808e-05
    epoch : 3 ; learning_rate : 1.4837391551120808e-05 ; fit : 0.11473333333333334
    batch_size :          30000
    best_batch_loss : 1.7197819555497684
    Epoch 3, Loss: 1.72228435460549, fit: 0.11586666666666667
      batch rate adapted learning_rate :              0.0005
      loss :              1.7197819555497684
      loss_factor :              0.17197819555497684
      loss adapted learning_rate :              1.4788249873172929e-05
    epoch : 4 ; learning_rate : 1.4788249873172929e-05 ; fit : 0.11586666666666667
    batch_size :          30000
    best_batch_loss : 1.7206653221561274
    Epoch 4, Loss: 1.7222706458642991, fit: 0.11576666666666667
      batch rate adapted learning_rate :              0.0005
      loss :              1.7206653221561274
      loss_factor :              0.17206653221561274
      loss adapted learning_rate :              1.480344575435325e-05
    epoch : 5 ; learning_rate : 1.480344575435325e-05 ; fit : 0.11576666666666667
    batch_size :          30000
    best_batch_loss : 1.722129927468111
    Epoch 5, Loss: 1.7222527909028913, fit: 0.11446666666666666
      batch rate adapted learning_rate :              0.0005
      loss :              1.7223756543376716
      loss_factor :              0.17223756543376717
      loss adapted learning_rate :              1.4832889473275614e-05
    epoch : 6 ; learning_rate : 1.4832889473275614e-05 ; fit : 0.11446666666666666
    batch_size :          30000
    best_batch_loss : 1.72219577855966
    Epoch 6, Loss: 1.722233555856545, fit: 0.1151
      batch rate adapted learning_rate :              0.0005
      loss :              1.7222713331534298
      loss_factor :              0.17222713331534298
      loss adapted learning_rate :              1.4831092725010462e-05
    epoch : 7 ; learning_rate : 1.4831092725010462e-05 ; fit : 0.1151
    batch_size :          30000
    best_batch_loss : 1.7222069807294742
    Epoch 7, Loss: 1.7222201505043233, fit: 0.1148
      batch rate adapted learning_rate :              0.0005
      loss :              1.7222069807294742
      loss_factor :              0.17222069807294743
      loss adapted learning_rate :              1.482998442236666e-05
    epoch : 8 ; learning_rate : 1.482998442236666e-05 ; fit : 0.1148
    batch_size :          30000
    best_batch_loss : 1.7177917543334835
    Epoch 8, Loss: 1.7222051717078772, fit: 0.11743333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7177917543334835_fit_0.11743333333333333_2024-01-03_150013
  self.fit : 0.11743333333333333
  self.loss : 1.7177917543334835
  current_accuracy : 0.114
   Accuracy mean: 0.1142
   Accuracy mean: 0.114
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.01
    epoch : 0 ; learning_rate : 0.01 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7875827262847235
    Epoch 0, Loss: 1.7934313696533182, fit: 0.07626666666666666
    Epoch 0, Loss: 1.7934313696533182
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7875827262847235_fit_0.07626666666666666_2024-01-03_150015
  self.fit : 0.07626666666666666
  self.loss : 1.7875827262847235
  current_accuracy : 0.0733
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.01
      loss :              1.7875827262847235
      loss_factor :              0.17875827262847235
      loss adapted learning_rate :              0.00031954520033115246
    epoch : 0 ; learning_rate : 0.00031954520033115246 ; fit : 0.07626666666666666
    batch_size :          30000
    best_batch_loss : 1.7825219480090664
    Epoch 0, Loss: 1.7851823516756573, fit: 0.07786666666666667
    Epoch 0, Loss: 1.7851823516756573
      batch rate adapted learning_rate :              0.01
      loss :              1.7825219480090664
      loss_factor :              0.17825219480090665
      loss adapted learning_rate :              0.0003177384495134037
    epoch : 1 ; learning_rate : 0.0003177384495134037 ; fit : 0.07786666666666667
    batch_size :          30000
    best_batch_loss : 1.7840612780668594
    Epoch 1, Loss: 1.7847438139670775, fit: 0.07563333333333333
      batch rate adapted learning_rate :              0.01
      loss :              1.7854263498672953
      loss_factor :              0.17854263498672954
      loss adapted learning_rate :              0.0003187747250800454
    epoch : 2 ; learning_rate : 0.0003187747250800454 ; fit : 0.07563333333333333
    batch_size :          30000
    best_batch_loss : 1.7811167664791017
    Epoch 2, Loss: 1.7843087437493343, fit: 0.07853333333333333
      batch rate adapted learning_rate :              0.01
      loss :              1.7811167664791017
      loss_factor :              0.17811167664791017
      loss adapted learning_rate :              0.0003172376935832971
    epoch : 3 ; learning_rate : 0.0003172376935832971 ; fit : 0.07853333333333333
    batch_size :          30000
    best_batch_loss : 1.7831007094448765
    Epoch 3, Loss: 1.7838791771731581, fit: 0.07636666666666667
      batch rate adapted learning_rate :              0.01
      loss :              1.7846576449014397
      loss_factor :              0.17846576449014398
      loss adapted learning_rate :              0.0003185002909505154
    epoch : 4 ; learning_rate : 0.0003185002909505154 ; fit : 0.07636666666666667
    batch_size :          30000
    best_batch_loss : 1.7828760819985479
    Epoch 4, Loss: 1.7833927133062522, fit: 0.07636666666666667
      batch rate adapted learning_rate :              0.01
      loss :              1.7839093446139564
      loss_factor :              0.17839093446139564
      loss adapted learning_rate :              0.0003182332549800996
    epoch : 5 ; learning_rate : 0.0003182332549800996 ; fit : 0.07636666666666667
    batch_size :          30000
    best_batch_loss : 1.7824435073365392
    Epoch 5, Loss: 1.7829647958290797, fit: 0.07716666666666666
      batch rate adapted learning_rate :              0.01
      loss :              1.7824435073365392
      loss_factor :              0.1782443507336539
      loss adapted learning_rate :              0.0003177104856846183
    epoch : 6 ; learning_rate : 0.0003177104856846183 ; fit : 0.07716666666666666
    batch_size :          30000
    best_batch_loss : 1.7800557606229375
    Epoch 6, Loss: 1.7825221008149927, fit: 0.07596666666666667
      batch rate adapted learning_rate :              0.01
      loss :              1.784988441007048
      loss_factor :              0.1784988441007048
      loss adapted learning_rate :              0.00031861837345287713
    epoch : 7 ; learning_rate : 0.00031861837345287713 ; fit : 0.07596666666666667
    batch_size :          30000
    best_batch_loss : 1.7810299604345738
    Epoch 7, Loss: 1.7820837219579913, fit: 0.0766
      batch rate adapted learning_rate :              0.01
      loss :              1.7831374834814087
      loss_factor :              0.17831374834814087
      loss adapted learning_rate :              0.0003179579284996411
    epoch : 8 ; learning_rate : 0.0003179579284996411 ; fit : 0.0766
    batch_size :          30000
    best_batch_loss : 1.7810020713958368
    Epoch 8, Loss: 1.7815987836593883, fit: 0.07676666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7821954959229396_fit_0.07676666666666666_2024-01-03_150036
  self.fit : 0.07676666666666666
  self.loss : 1.7821954959229396
  current_accuracy : 0.076
   Accuracy mean: 0.0733
   Accuracy mean: 0.076
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.0125
    epoch : 0 ; learning_rate : 0.0125 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7822179449368105
    Epoch 0, Loss: 1.7901190241709024, fit: 0.0776
    Epoch 0, Loss: 1.7901190241709024
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7822179449368105_fit_0.0776_2024-01-03_150038
  self.fit : 0.0776
  self.loss : 1.7822179449368105
  current_accuracy : 0.0807
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.0125
      loss :              1.7822179449368105
      loss_factor :              0.17822179449368106
      loss adapted learning_rate :              0.0003970376004068486
    epoch : 0 ; learning_rate : 0.0003970376004068486 ; fit : 0.0776
    batch_size :          30000
    best_batch_loss : 1.7705808664334832
    Epoch 0, Loss: 1.7720752908747248, fit: 0.07996666666666667
    Epoch 0, Loss: 1.7720752908747248
      batch rate adapted learning_rate :              0.0125
      loss :              1.7735697153159662
      loss_factor :              0.17735697153159663
      loss adapted learning_rate :              0.00039319369188574473
    epoch : 1 ; learning_rate : 0.00039319369188574473 ; fit : 0.07996666666666667
    batch_size :          30000
    best_batch_loss : 1.7678541245487476
    Epoch 1, Loss: 1.771509417754217, fit: 0.0788
      batch rate adapted learning_rate :              0.0125
      loss :              1.7751647109596866
      loss_factor :              0.17751647109596866
      loss adapted learning_rate :              0.0003939012188795735
    epoch : 2 ; learning_rate : 0.0003939012188795735 ; fit : 0.0788
    batch_size :          30000
    best_batch_loss : 1.7692036765746109
    Epoch 2, Loss: 1.7709336887895164, fit: 0.0808
      batch rate adapted learning_rate :              0.0125
      loss :              1.7726637010044222
      loss_factor :              0.17726637010044222
      loss adapted learning_rate :              0.00039279207460733695
    epoch : 3 ; learning_rate : 0.00039279207460733695 ; fit : 0.0808
    batch_size :          30000
    best_batch_loss : 1.7680846835321298
    Epoch 3, Loss: 1.770320420596553, fit: 0.083
      batch rate adapted learning_rate :              0.0125
      loss :              1.7680846835321298
      loss_factor :              0.17680846835321298
      loss adapted learning_rate :              0.000390765431017614
    epoch : 4 ; learning_rate : 0.000390765431017614 ; fit : 0.083
    batch_size :          30000
    best_batch_loss : 1.7691264686607948
    Epoch 4, Loss: 1.769678930972502, fit: 0.08146666666666667
      batch rate adapted learning_rate :              0.0125
      loss :              1.770231393284209
      loss_factor :              0.1770231393284209
      loss adapted learning_rate :              0.000391714898221119
    epoch : 5 ; learning_rate : 0.000391714898221119 ; fit : 0.08146666666666667
    batch_size :          30000
    best_batch_loss : 1.7668904450242413
    Epoch 5, Loss: 1.7690176833621325, fit: 0.083
      batch rate adapted learning_rate :              0.0125
      loss :              1.7668904450242413
      loss_factor :              0.17668904450242412
      loss adapted learning_rate :              0.00039023773058974515
    epoch : 6 ; learning_rate : 0.00039023773058974515 ; fit : 0.083
    batch_size :          30000
    best_batch_loss : 1.7652354407211919
    Epoch 6, Loss: 1.7683681274614873, fit: 0.08376666666666667
      batch rate adapted learning_rate :              0.0125
      loss :              1.7652354407211919
      loss_factor :              0.17652354407211918
      loss adapted learning_rate :              0.0003895070201472675
    epoch : 7 ; learning_rate : 0.0003895070201472675 ; fit : 0.08376666666666667
    batch_size :          30000
    best_batch_loss : 1.7672716739562042
    Epoch 7, Loss: 1.7676702057995861, fit: 0.08316666666666667
      batch rate adapted learning_rate :              0.0125
      loss :              1.7672716739562042
      loss_factor :              0.17672716739562042
      loss adapted learning_rate :              0.0003904061461959955
    epoch : 8 ; learning_rate : 0.0003904061461959955 ; fit : 0.08316666666666667
    batch_size :          30000
    best_batch_loss : 1.760654959444394
    Epoch 8, Loss: 1.7669816558024494, fit: 0.08563333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.760654959444394_fit_0.08563333333333334_2024-01-03_150059
  self.fit : 0.08563333333333334
  self.loss : 1.760654959444394
  current_accuracy : 0.0825
   Accuracy mean: 0.0807
   Accuracy mean: 0.0825
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.015
    epoch : 0 ; learning_rate : 0.015 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7097560805608378
    Epoch 0, Loss: 1.7190188618193578, fit: 0.1132
    Epoch 0, Loss: 1.7190188618193578
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7097560805608378_fit_0.1132_2024-01-03_150101
  self.fit : 0.1132
  self.loss : 1.7097560805608378
  current_accuracy : 0.1281
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.015
      loss :              1.7097560805608378
      loss_factor :              0.17097560805608378
      loss adapted learning_rate :              0.00043848987825221365
    epoch : 0 ; learning_rate : 0.00043848987825221365 ; fit : 0.1132
    batch_size :          30000
    best_batch_loss : 1.6855307967704023
    Epoch 0, Loss: 1.68972956966183, fit: 0.12283333333333334
    Epoch 0, Loss: 1.68972956966183
      batch rate adapted learning_rate :              0.015
      loss :              1.6855307967704023
      loss_factor :              0.16855307967704022
      loss adapted learning_rate :              0.00042615211002922006
    epoch : 1 ; learning_rate : 0.00042615211002922006 ; fit : 0.12283333333333334
    batch_size :          30000
    best_batch_loss : 1.687473693750439
    Epoch 1, Loss: 1.6887376760521988, fit: 0.1217
      batch rate adapted learning_rate :              0.015
      loss :              1.687473693750439
      loss_factor :              0.16874736937504392
      loss adapted learning_rate :              0.0004271351200649626
    epoch : 2 ; learning_rate : 0.0004271351200649626 ; fit : 0.1217
    batch_size :          30000
    best_batch_loss : 1.6871689583644653
    Epoch 2, Loss: 1.6878357253679979, fit: 0.12173333333333333
      batch rate adapted learning_rate :              0.015
      loss :              1.6871689583644653
      loss_factor :              0.16871689583644653
      loss adapted learning_rate :              0.00042698086411029523
    epoch : 3 ; learning_rate : 0.00042698086411029523 ; fit : 0.12173333333333333
    batch_size :          30000
    best_batch_loss : 1.6858257595618362
    Epoch 3, Loss: 1.6868674021707692, fit: 0.1214
      batch rate adapted learning_rate :              0.015
      loss :              1.6879090447797023
      loss_factor :              0.16879090447797024
      loss adapted learning_rate :              0.0004273555415173691
    epoch : 4 ; learning_rate : 0.0004273555415173691 ; fit : 0.1214
    batch_size :          30000
    best_batch_loss : 1.6850902233774987
    Epoch 4, Loss: 1.6859573376583161, fit: 0.12236666666666667
      batch rate adapted learning_rate :              0.015
      loss :              1.6868244519391338
      loss_factor :              0.16868244519391337
      loss adapted learning_rate :              0.00042680650974896386
    epoch : 5 ; learning_rate : 0.00042680650974896386 ; fit : 0.12236666666666667
    batch_size :          30000
    best_batch_loss : 1.6841125101465968
    Epoch 5, Loss: 1.6850206484857169, fit: 0.12386666666666667
      batch rate adapted learning_rate :              0.015
      loss :              1.6841125101465968
      loss_factor :              0.16841125101465967
      loss adapted learning_rate :              0.0004254352420248406
    epoch : 6 ; learning_rate : 0.0004254352420248406 ; fit : 0.12386666666666667
    batch_size :          30000
    best_batch_loss : 1.6819933775295874
    Epoch 6, Loss: 1.6841038593247584, fit: 0.12506666666666666
      batch rate adapted learning_rate :              0.015
      loss :              1.6819933775295874
      loss_factor :              0.16819933775295876
      loss adapted learning_rate :              0.0004243652583080084
    epoch : 7 ; learning_rate : 0.0004243652583080084 ; fit : 0.12506666666666666
    batch_size :          30000
    best_batch_loss : 1.681416960944729
    Epoch 7, Loss: 1.6832496764236178, fit: 0.12376666666666666
      batch rate adapted learning_rate :              0.015
      loss :              1.6850823919025066
      loss_factor :              0.16850823919025065
      loss adapted learning_rate :              0.0004259254001249809
    epoch : 8 ; learning_rate : 0.0004259254001249809 ; fit : 0.12376666666666666
    batch_size :          30000
    best_batch_loss : 1.6816240352356728
    Epoch 8, Loss: 1.6823453745262702, fit: 0.1247
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6816240352356728_fit_0.1247_2024-01-03_150122
  self.fit : 0.1247
  self.loss : 1.6816240352356728
  current_accuracy : 0.1305
   Accuracy mean: 0.1281
   Accuracy mean: 0.1305
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.05
    epoch : 0 ; learning_rate : 0.05 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.6416288604868052
    Epoch 0, Loss: 1.667454916182836, fit: 0.14493333333333333
    Epoch 0, Loss: 1.667454916182836
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6416288604868052_fit_0.14493333333333333_2024-01-03_150125
  self.fit : 0.14493333333333333
  self.loss : 1.6416288604868052
  current_accuracy : 0.1785
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.05
      loss :              1.6416288604868052
      loss_factor :              0.1641628860486805
      loss adapted learning_rate :              0.0013474726577916033
    epoch : 0 ; learning_rate : 0.0013474726577916033 ; fit : 0.14493333333333333
    batch_size :          30000
    best_batch_loss : 1.5725256403384864
    Epoch 0, Loss: 1.5754268965209937, fit: 0.1781
    Epoch 0, Loss: 1.5754268965209937
      batch rate adapted learning_rate :              0.05
      loss :              1.5725256403384864
      loss_factor :              0.15725256403384863
      loss adapted learning_rate :              0.0012364184447609832
    epoch : 1 ; learning_rate : 0.0012364184447609832 ; fit : 0.1781
    batch_size :          30000
    best_batch_loss : 1.5666214567091015
    Epoch 1, Loss: 1.572275467652947, fit: 0.17456666666666668
      batch rate adapted learning_rate :              0.05
      loss :              1.5779294785967921
      loss_factor :              0.1577929478596792
      loss adapted learning_rate :              0.0012449307197123722
    epoch : 2 ; learning_rate : 0.0012449307197123722 ; fit : 0.17456666666666668
    batch_size :          30000
    best_batch_loss : 1.5664012313004847
    Epoch 2, Loss: 1.569284789719539, fit: 0.17806666666666668
      batch rate adapted learning_rate :              0.05
      loss :              1.5721683481385933
      loss_factor :              0.15721683481385934
      loss adapted learning_rate :              0.0012358566574444169
    epoch : 3 ; learning_rate : 0.0012358566574444169 ; fit : 0.17806666666666668
    batch_size :          30000
    best_batch_loss : 1.5657146757709952
    Epoch 3, Loss: 1.566312142862179, fit: 0.1816
      batch rate adapted learning_rate :              0.05
      loss :              1.5657146757709952
      loss_factor :              0.15657146757709953
      loss adapted learning_rate :              0.0012257312229623366
    epoch : 4 ; learning_rate : 0.0012257312229623366 ; fit : 0.1816
    batch_size :          30000
    best_batch_loss : 1.561351278099334
    Epoch 4, Loss: 1.5632738512799584, fit: 0.1825
      batch rate adapted learning_rate :              0.05
      loss :              1.561351278099334
      loss_factor :              0.15613512780993338
      loss adapted learning_rate :              0.0012189089068112117
    epoch : 5 ; learning_rate : 0.0012189089068112117 ; fit : 0.1825
    batch_size :          30000
    best_batch_loss : 1.5598909192109958
    Epoch 5, Loss: 1.5602566263727247, fit: 0.18406666666666666
      batch rate adapted learning_rate :              0.05
      loss :              1.5598909192109958
      loss_factor :              0.15598909192109958
      loss adapted learning_rate :              0.0012166298399184628
    epoch : 6 ; learning_rate : 0.0012166298399184628 ; fit : 0.18406666666666666
    batch_size :          30000
    best_batch_loss : 1.55690543038302
    Epoch 6, Loss: 1.5573245668287081, fit: 0.18586666666666668
      batch rate adapted learning_rate :              0.05
      loss :              1.55690543038302
      loss_factor :              0.15569054303830202
      loss adapted learning_rate :              0.0012119772595780686
    epoch : 7 ; learning_rate : 0.0012119772595780686 ; fit : 0.18586666666666668
    batch_size :          30000
    best_batch_loss : 1.5537890343086944
    Epoch 7, Loss: 1.5543195411069508, fit: 0.1863
      batch rate adapted learning_rate :              0.05
      loss :              1.554850047905207
      loss_factor :              0.1554850047905207
      loss adapted learning_rate :              0.001208779335735412
    epoch : 8 ; learning_rate : 0.001208779335735412 ; fit : 0.1863
    batch_size :          30000
    best_batch_loss : 1.5505127977702993
    Epoch 8, Loss: 1.5513278334647773, fit: 0.18903333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.5505127977702993_fit_0.18903333333333333_2024-01-03_150145
  self.fit : 0.18903333333333333
  self.loss : 1.5505127977702993
  current_accuracy : 0.1922
   Accuracy mean: 0.1785
   Accuracy mean: 0.1922
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.45
    epoch : 0 ; learning_rate : 0.45 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.4476114335298456
    Epoch 0, Loss: 1.5525940736726898, fit: 0.258
    Epoch 0, Loss: 1.5525940736726898
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.4476114335298456_fit_0.258_2024-01-03_150148
  self.fit : 0.258
  self.loss : 1.4476114335298456
  current_accuracy : 0.2262
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.45
      loss :              1.4476114335298456
      loss_factor :              0.14476114335298457
      loss adapted learning_rate :              0.009430104881188507
    epoch : 0 ; learning_rate : 0.009430104881188507 ; fit : 0.258
    batch_size :          30000
    best_batch_loss : 1.484316296081756
    Epoch 0, Loss: 1.4924738010111473, fit: 0.24306666666666665
    Epoch 0, Loss: 1.4924738010111473
      batch rate adapted learning_rate :              0.45
      loss :              1.484316296081756
      loss_factor :              0.1484316296081756
      loss adapted learning_rate :              0.009914376900662386
    epoch : 1 ; learning_rate : 0.009914376900662386 ; fit : 0.24306666666666665
    batch_size :          30000
    best_batch_loss : 1.4484301766251264
    Epoch 1, Loss: 1.4562291260317504, fit: 0.2596
      batch rate adapted learning_rate :              0.45
      loss :              1.4484301766251264
      loss_factor :              0.14484301766251265
      loss adapted learning_rate :              0.009440774894512328
    epoch : 2 ; learning_rate : 0.009440774894512328 ; fit : 0.2596
    batch_size :          30000
    best_batch_loss : 1.4198907812904014
    Epoch 2, Loss: 1.4269284251963512, fit: 0.27303333333333335
      batch rate adapted learning_rate :              0.45
      loss :              1.4198907812904014
      loss_factor :              0.14198907812904013
      loss adapted learning_rate :              0.009072404238570598
    epoch : 3 ; learning_rate : 0.009072404238570598 ; fit : 0.27303333333333335
    batch_size :          30000
    best_batch_loss : 1.3984358154592147
    Epoch 3, Loss: 1.3997912010818636, fit: 0.28353333333333336
      batch rate adapted learning_rate :              0.45
      loss :              1.3984358154592147
      loss_factor :              0.13984358154592147
      loss adapted learning_rate :              0.008800302284815854
    epoch : 4 ; learning_rate : 0.008800302284815854 ; fit : 0.28353333333333336
    batch_size :          30000
    best_batch_loss : 1.369696824826358
    Epoch 4, Loss: 1.3746640422960281, fit: 0.2969333333333333
      batch rate adapted learning_rate :              0.45
      loss :              1.369696824826358
      loss_factor :              0.1369696824826358
      loss adapted learning_rate :              0.008442312263727332
    epoch : 5 ; learning_rate : 0.008442312263727332 ; fit : 0.2969333333333333
    batch_size :          30000
    best_batch_loss : 1.3466411513573797
    Epoch 5, Loss: 1.3514264770029156, fit: 0.30793333333333334
      batch rate adapted learning_rate :              0.45
      loss :              1.3466411513573797
      loss_factor :              0.13466411513573798
      loss adapted learning_rate :              0.008160490757381083
    epoch : 6 ; learning_rate : 0.008160490757381083 ; fit : 0.30793333333333334
    batch_size :          30000
    best_batch_loss : 1.3231186808256092
    Epoch 6, Loss: 1.3278763308209647, fit: 0.31993333333333335
      batch rate adapted learning_rate :              0.45
      loss :              1.3231186808256092
      loss_factor :              0.13231186808256093
      loss adapted learning_rate :              0.007877893695973652
    epoch : 7 ; learning_rate : 0.007877893695973652 ; fit : 0.31993333333333335
    batch_size :          30000
    best_batch_loss : 1.295483997629449
    Epoch 7, Loss: 1.3034830141164977, fit: 0.3325
      batch rate adapted learning_rate :              0.45
      loss :              1.295483997629449
      loss_factor :              0.1295483997629449
      loss adapted learning_rate :              0.007552254546512904
    epoch : 8 ; learning_rate : 0.007552254546512904 ; fit : 0.3325
    batch_size :          30000
    best_batch_loss : 1.275116317807961
    Epoch 8, Loss: 1.2784848603505212, fit: 0.34103333333333335
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.275116317807961_fit_0.34103333333333335_2024-01-03_150209
  self.fit : 0.34103333333333335
  self.loss : 1.275116317807961
  current_accuracy : 0.34
   Accuracy mean: 0.2262
   Accuracy mean: 0.34
  Results saved to test_combinations_results20240103150209
  normalized_accuracies :      [0.14023247 0.14023247 0.02512186 0.02512186 0.21409824 0.21447319
   0.15335583 0.15260592 0.         0.01012373 0.02774653 0.03449569
   0.20547432 0.21447319 0.39445069 0.44581927 0.57330334 1.        ]
batch_rate :  0.2
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.2
      batch rate adapted learning_rate :              2.0000000000000002e-11
    epoch : 0 ; learning_rate : 2.0000000000000002e-11 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.6913351269225647
    Epoch 0, Loss: 1.6968792104265993, fit: 0.12308333333333334
    Epoch 0, Loss: 1.6968792104265993
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.6913351269225647_fit_0.12308333333333334_2024-01-03_150213
  self.fit : 0.12308333333333334
  self.loss : 1.6913351269225647
  current_accuracy : 0.1235
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.2
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.6913351269225647
      loss_factor :              0.16913351269225646
      loss adapted learning_rate :              5.721229023124336e-13
    epoch : 0 ; learning_rate : 5.721229023124336e-13 ; fit : 0.12308333333333334
    batch_size :          12000
    best_batch_loss : 1.692376531364225
    Epoch 0, Loss: 1.6968792104013461, fit: 0.12375
    Epoch 0, Loss: 1.6968792104013461
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.6939540367989443
      loss_factor :              0.16939540367989442
      loss adapted learning_rate :              5.738960557574878e-13
    epoch : 1 ; learning_rate : 5.738960557574878e-13 ; fit : 0.12375
    batch_size :          12000
    best_batch_loss : 1.6891433462572898
    Epoch 1, Loss: 1.696879210400057, fit: 0.12175
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.695544172918839
      loss_factor :              0.1695544172918839
      loss adapted learning_rate :              5.749740084638059e-13
    epoch : 2 ; learning_rate : 5.749740084638059e-13 ; fit : 0.12175
    batch_size :          12000
    best_batch_loss : 1.6936690139393258
    Epoch 2, Loss: 1.6968792103988326, fit: 0.11925
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.7023266186307686
      loss_factor :              0.17023266186307687
      loss adapted learning_rate :              5.795831832997734e-13
    epoch : 3 ; learning_rate : 5.795831832997734e-13 ; fit : 0.11925
    batch_size :          12000
    best_batch_loss : 1.6933640322750385
    Epoch 3, Loss: 1.696879210397575, fit: 0.12108333333333333
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.698063292169643
      loss_factor :              0.1698063292169643
      loss adapted learning_rate :              5.766837888428014e-13
    epoch : 4 ; learning_rate : 5.766837888428014e-13 ; fit : 0.12108333333333333
    batch_size :          12000
    best_batch_loss : 1.6888007527979434
    Epoch 4, Loss: 1.6968792103964185, fit: 0.11758333333333333
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.7043607285731759
      loss_factor :              0.1704360728573176
      loss adapted learning_rate :              5.809690986204975e-13
    epoch : 5 ; learning_rate : 5.809690986204975e-13 ; fit : 0.11758333333333333
    batch_size :          12000
    best_batch_loss : 1.6807695003277852
    Epoch 5, Loss: 1.6968792103950556, fit: 0.12941666666666668
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.6807695003277852
      loss_factor :              0.1680769500327785
      loss adapted learning_rate :              5.649972226464225e-13
    epoch : 6 ; learning_rate : 5.649972226464225e-13 ; fit : 0.12941666666666668
    batch_size :          12000
    best_batch_loss : 1.6923578984768877
    Epoch 6, Loss: 1.6968792103938788, fit: 0.12108333333333333
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.6984555896003855
      loss_factor :              0.16984555896003856
      loss adapted learning_rate :              5.769502779689587e-13
    epoch : 7 ; learning_rate : 5.769502779689587e-13 ; fit : 0.12108333333333333
    batch_size :          12000
    best_batch_loss : 1.6951296899318113
    Epoch 7, Loss: 1.6968792103926789, fit: 0.12233333333333334
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.695241528329384
      loss_factor :              0.1695241528329384
      loss adapted learning_rate :              5.747687678745092e-13
    epoch : 8 ; learning_rate : 5.747687678745092e-13 ; fit : 0.12233333333333334
    batch_size :          12000
    best_batch_loss : 1.6885712614972748
    Epoch 8, Loss: 1.6968792103915797, fit: 0.11658333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7052572965933699_fit_0.11658333333333333_2024-01-03_150233
  self.fit : 0.11658333333333333
  self.loss : 1.7052572965933699
  current_accuracy : 0.1235
   Accuracy mean: 0.1235
   Accuracy mean: 0.1235
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.2
      batch rate adapted learning_rate :              2e-08
    epoch : 0 ; learning_rate : 2e-08 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7941003652665373
    Epoch 0, Loss: 1.7963924648117302, fit: 0.07275
    Epoch 0, Loss: 1.7963924648117302
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7957363663535877_fit_0.07275_2024-01-03_150236
  self.fit : 0.07275
  self.loss : 1.7957363663535877
  current_accuracy : 0.067
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.2
      batch rate adapted learning_rate :              2e-08
      loss :              1.7957363663535877
      loss_factor :              0.17957363663535877
      loss adapted learning_rate :              6.449338194889573e-10
    epoch : 0 ; learning_rate : 6.449338194889573e-10 ; fit : 0.07275
    batch_size :          12000
    best_batch_loss : 1.7889680756589474
    Epoch 0, Loss: 1.7963923785288032, fit: 0.072
    Epoch 0, Loss: 1.7963923785288032
      batch rate adapted learning_rate :              2e-08
      loss :              1.7959383665527564
      loss_factor :              0.17959383665527565
      loss adapted learning_rate :              6.450789232912365e-10
    epoch : 1 ; learning_rate : 6.450789232912365e-10 ; fit : 0.072
    batch_size :          12000
    best_batch_loss : 1.7945670899573916
    Epoch 1, Loss: 1.7963923740752836, fit: 0.073
      batch rate adapted learning_rate :              2e-08
      loss :              1.7945670899573916
      loss_factor :              0.17945670899573915
      loss adapted learning_rate :              6.440942080716281e-10
    epoch : 2 ; learning_rate : 6.440942080716281e-10 ; fit : 0.073
    batch_size :          12000
    best_batch_loss : 1.792255189486336
    Epoch 2, Loss: 1.7963923695441026, fit: 0.0715
      batch rate adapted learning_rate :              2e-08
      loss :              1.8010493816376703
      loss_factor :              0.18010493816376702
      loss adapted learning_rate :              6.487557750194869e-10
    epoch : 3 ; learning_rate : 6.487557750194869e-10 ; fit : 0.0715
    batch_size :          12000
    best_batch_loss : 1.7954569807984626
    Epoch 3, Loss: 1.7963923651280758, fit: 0.07375
      batch rate adapted learning_rate :              2e-08
      loss :              1.7954569807984626
      loss_factor :              0.17954569807984627
      loss adapted learning_rate :              6.447331539795863e-10
    epoch : 4 ; learning_rate : 6.447331539795863e-10 ; fit : 0.07375
    batch_size :          12000
    best_batch_loss : 1.7916087970609875
    Epoch 4, Loss: 1.796392360103222, fit: 0.07058333333333333
      batch rate adapted learning_rate :              2e-08
      loss :              1.795724237010121
      loss_factor :              0.1795724237010121
      loss adapted learning_rate :              6.449251070771162e-10
    epoch : 5 ; learning_rate : 6.449251070771162e-10 ; fit : 0.07058333333333333
    batch_size :          12000
    best_batch_loss : 1.7926721192449038
    Epoch 5, Loss: 1.7963923557315236, fit: 0.07108333333333333
      batch rate adapted learning_rate :              2e-08
      loss :              1.8000366077360985
      loss_factor :              0.18000366077360985
      loss adapted learning_rate :              6.480263578380162e-10
    epoch : 6 ; learning_rate : 6.480263578380162e-10 ; fit : 0.07108333333333333
    batch_size :          12000
    best_batch_loss : 1.7918254137956768
    Epoch 6, Loss: 1.7963923515257012, fit: 0.071
      batch rate adapted learning_rate :              2e-08
      loss :              1.7989186002484636
      loss_factor :              0.17989186002484636
      loss adapted learning_rate :              6.472216260639784e-10
    epoch : 7 ; learning_rate : 6.472216260639784e-10 ; fit : 0.071
    batch_size :          12000
    best_batch_loss : 1.7866429853573815
    Epoch 7, Loss: 1.7963923468655065, fit: 0.06983333333333333
      batch rate adapted learning_rate :              2e-08
      loss :              1.7995612841304773
      loss_factor :              0.17995612841304773
      loss adapted learning_rate :              6.476841630682664e-10
    epoch : 8 ; learning_rate : 6.476841630682664e-10 ; fit : 0.06983333333333333
    batch_size :          12000
    best_batch_loss : 1.7947999855811458
    Epoch 8, Loss: 1.7963923421046486, fit: 0.072
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.795394114973922_fit_0.072_2024-01-03_150257
  self.fit : 0.072
  self.loss : 1.795394114973922
  current_accuracy : 0.067
   Accuracy mean: 0.067
   Accuracy mean: 0.067
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.2
      batch rate adapted learning_rate :              2e-05
    epoch : 0 ; learning_rate : 2e-05 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7233336010806104
    Epoch 0, Loss: 1.7362534780670429, fit: 0.10433333333333333
    Epoch 0, Loss: 1.7362534780670429
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.73006916616925_fit_0.10433333333333333_2024-01-03_150259
  self.fit : 0.10433333333333333
  self.loss : 1.73006916616925
  current_accuracy : 0.094
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.2
      batch rate adapted learning_rate :              2e-05
      loss :              1.73006916616925
      loss_factor :              0.173006916616925
      loss adapted learning_rate :              5.986278639459129e-07
    epoch : 0 ; learning_rate : 5.986278639459129e-07 ; fit : 0.10433333333333333
    batch_size :          12000
    best_batch_loss : 1.728822953885933
    Epoch 0, Loss: 1.7362254807228963, fit: 0.10475
    Epoch 0, Loss: 1.7362254807228963
      batch rate adapted learning_rate :              2e-05
      loss :              1.728822953885933
      loss_factor :              0.1728822953885933
      loss adapted learning_rate :              5.977657611765765e-07
    epoch : 1 ; learning_rate : 5.977657611765765e-07 ; fit : 0.10475
    batch_size :          12000
    best_batch_loss : 1.7285188552063935
    Epoch 1, Loss: 1.7362243091569662, fit: 0.10266666666666667
      batch rate adapted learning_rate :              2e-05
      loss :              1.7314371803059658
      loss_factor :              0.17314371803059658
      loss adapted learning_rate :              5.995749418691748e-07
    epoch : 2 ; learning_rate : 5.995749418691748e-07 ; fit : 0.10266666666666667
    batch_size :          12000
    best_batch_loss : 1.731584720900908
    Epoch 2, Loss: 1.736223135901884, fit: 0.09691666666666666
      batch rate adapted learning_rate :              2e-05
      loss :              1.7449797296982814
      loss_factor :              0.17449797296982814
      loss adapted learning_rate :              6.089908514115775e-07
    epoch : 3 ; learning_rate : 6.089908514115775e-07 ; fit : 0.09691666666666666
    batch_size :          12000
    best_batch_loss : 1.7320401017198852
    Epoch 3, Loss: 1.736222067589745, fit: 0.10091666666666667
      batch rate adapted learning_rate :              2e-05
      loss :              1.7362108451636853
      loss_factor :              0.17362108451636854
      loss adapted learning_rate :              6.028856197727998e-07
    epoch : 4 ; learning_rate : 6.028856197727998e-07 ; fit : 0.10091666666666667
    batch_size :          12000
    best_batch_loss : 1.7281598827412268
    Epoch 4, Loss: 1.7362209646703253, fit: 0.10183333333333333
      batch rate adapted learning_rate :              2e-05
      loss :              1.7348886213133676
      loss_factor :              0.17348886213133677
      loss adapted learning_rate :              6.019677056725197e-07
    epoch : 5 ; learning_rate : 6.019677056725197e-07 ; fit : 0.10183333333333333
    batch_size :          12000
    best_batch_loss : 1.7274798902117074
    Epoch 5, Loss: 1.7362196867051551, fit: 0.09866666666666667
      batch rate adapted learning_rate :              2e-05
      loss :              1.7398196185714296
      loss_factor :              0.17398196185714296
      loss adapted learning_rate :              6.053944610332071e-07
    epoch : 6 ; learning_rate : 6.053944610332071e-07 ; fit : 0.09866666666666667
    batch_size :          12000
    best_batch_loss : 1.7329785650098648
    Epoch 6, Loss: 1.7362184422794735, fit: 0.102
      batch rate adapted learning_rate :              2e-05
      loss :              1.7365326347827055
      loss_factor :              0.17365326347827054
      loss adapted learning_rate :              6.03109118333073e-07
    epoch : 7 ; learning_rate : 6.03109118333073e-07 ; fit : 0.102
    batch_size :          12000
    best_batch_loss : 1.7271002765471213
    Epoch 7, Loss: 1.73621727181976, fit: 0.1005
      batch rate adapted learning_rate :              2e-05
      loss :              1.7374425609224573
      loss_factor :              0.17374425609224572
      loss adapted learning_rate :              6.037413305009573e-07
    epoch : 8 ; learning_rate : 6.037413305009573e-07 ; fit : 0.1005
    batch_size :          12000
    best_batch_loss : 1.7274011661157729
    Epoch 8, Loss: 1.7362161786309476, fit: 0.10175
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7351612550547773_fit_0.10175_2024-01-03_150319
  self.fit : 0.10175
  self.loss : 1.7351612550547773
  current_accuracy : 0.094
   Accuracy mean: 0.094
   Accuracy mean: 0.094
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.0002
    epoch : 0 ; learning_rate : 0.0002 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7787085265593647
    Epoch 0, Loss: 1.7875981396077116, fit: 0.07483333333333334
    Epoch 0, Loss: 1.7875981396077116
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7961014003238884_fit_0.07483333333333334_2024-01-03_150322
  self.fit : 0.07483333333333334
  self.loss : 1.7961014003238884
  current_accuracy : 0.0814
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.0002
      loss :              1.7961014003238884
      loss_factor :              0.17961014003238884
      loss adapted learning_rate :              6.451960480490867e-06
    epoch : 0 ; learning_rate : 6.451960480490867e-06 ; fit : 0.07483333333333334
    batch_size :          12000
    best_batch_loss : 1.7780617324701606
    Epoch 0, Loss: 1.7871322364698494, fit: 0.0825
    Epoch 0, Loss: 1.7871322364698494
      batch rate adapted learning_rate :              0.0002
      loss :              1.7780617324701606
      loss_factor :              0.17780617324701606
      loss adapted learning_rate :              6.323007048949578e-06
    epoch : 1 ; learning_rate : 6.323007048949578e-06 ; fit : 0.0825
    batch_size :          12000
    best_batch_loss : 1.7791243428775576
    Epoch 1, Loss: 1.787105144585381, fit: 0.07641666666666666
      batch rate adapted learning_rate :              0.0002
      loss :              1.791536438180173
      loss_factor :              0.1791536438180173
      loss adapted learning_rate :              6.419205618654602e-06
    epoch : 2 ; learning_rate : 6.419205618654602e-06 ; fit : 0.07641666666666666
    batch_size :          12000
    best_batch_loss : 1.7823914479838776
    Epoch 2, Loss: 1.7870798324814383, fit: 0.07825
      batch rate adapted learning_rate :              0.0002
      loss :              1.7895471707826818
      loss_factor :              0.17895471707826818
      loss adapted learning_rate :              6.404958152912602e-06
    epoch : 3 ; learning_rate : 6.404958152912602e-06 ; fit : 0.07825
    batch_size :          12000
    best_batch_loss : 1.7768237126320685
    Epoch 3, Loss: 1.7870568164746727, fit: 0.08275
      batch rate adapted learning_rate :              0.0002
      loss :              1.7768237126320685
      loss_factor :              0.17768237126320685
      loss adapted learning_rate :              6.314205011543216e-06
    epoch : 4 ; learning_rate : 6.314205011543216e-06 ; fit : 0.08275
    batch_size :          12000
    best_batch_loss : 1.7829210357329905
    Epoch 4, Loss: 1.7870322334704736, fit: 0.08075
      batch rate adapted learning_rate :              0.0002
      loss :              1.786231450685531
      loss_factor :              0.1786231450685531
      loss adapted learning_rate :              6.381245590836273e-06
    epoch : 5 ; learning_rate : 6.381245590836273e-06 ; fit : 0.08075
    batch_size :          12000
    best_batch_loss : 1.781038660265763
    Epoch 5, Loss: 1.7870089998035394, fit: 0.07508333333333334
      batch rate adapted learning_rate :              0.0002
      loss :              1.7912736976929977
      loss_factor :              0.17912736976929977
      loss adapted learning_rate :              6.417322920093491e-06
    epoch : 6 ; learning_rate : 6.417322920093491e-06 ; fit : 0.07508333333333334
    batch_size :          12000
    best_batch_loss : 1.7768601638198873
    Epoch 6, Loss: 1.7869854478243123, fit: 0.08066666666666666
      batch rate adapted learning_rate :              0.0002
      loss :              1.783498279232074
      loss_factor :              0.1783498279232074
      loss adapted learning_rate :              6.361732224047537e-06
    epoch : 7 ; learning_rate : 6.361732224047537e-06 ; fit : 0.08066666666666666
    batch_size :          12000
    best_batch_loss : 1.780015759328116
    Epoch 7, Loss: 1.7869594272442073, fit: 0.08175
      batch rate adapted learning_rate :              0.0002
      loss :              1.780015759328116
      loss_factor :              0.1780015759328116
      loss adapted learning_rate :              6.3369122069129e-06
    epoch : 8 ; learning_rate : 6.3369122069129e-06 ; fit : 0.08175
    batch_size :          12000
    best_batch_loss : 1.7812181125814188
    Epoch 8, Loss: 1.7869376057603512, fit: 0.08016666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7826882455045352_fit_0.08016666666666666_2024-01-03_150342
  self.fit : 0.08016666666666666
  self.loss : 1.7826882455045352
  current_accuracy : 0.0815
   Accuracy mean: 0.0814
   Accuracy mean: 0.0815
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.004
    epoch : 0 ; learning_rate : 0.004 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7235304413615025
    Epoch 0, Loss: 1.730449877717441, fit: 0.1065
    Epoch 0, Loss: 1.730449877717441
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7235304413615025_fit_0.1065_2024-01-03_150345
  self.fit : 0.1065
  self.loss : 1.7235304413615025
  current_accuracy : 0.1094
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.004
      loss :              1.7235304413615025
      loss_factor :              0.17235304413615024
      loss adapted learning_rate :              0.00011882228729199101
    epoch : 0 ; learning_rate : 0.00011882228729199101 ; fit : 0.1065
    batch_size :          12000
    best_batch_loss : 1.7146016190952815
    Epoch 0, Loss: 1.7216715379562406, fit: 0.10458333333333333
    Epoch 0, Loss: 1.7216715379562406
      batch rate adapted learning_rate :              0.004
      loss :              1.7261233593355791
      loss_factor :              0.1726123359335579
      loss adapted learning_rate :              0.0001191800740657578
    epoch : 1 ; learning_rate : 0.0001191800740657578 ; fit : 0.10458333333333333
    batch_size :          12000
    best_batch_loss : 1.713727049286932
    Epoch 1, Loss: 1.7212197198576655, fit: 0.11075
      batch rate adapted learning_rate :              0.004
      loss :              1.713727049286932
      loss_factor :              0.17137270492869322
      loss adapted learning_rate :              0.00011747441597830781
    epoch : 2 ; learning_rate : 0.00011747441597830781 ; fit : 0.11075
    batch_size :          12000
    best_batch_loss : 1.7169716637467578
    Epoch 2, Loss: 1.720783068612005, fit: 0.10616666666666667
      batch rate adapted learning_rate :              0.004
      loss :              1.720737115818834
      loss_factor :              0.1720737115818834
      loss adapted learning_rate :              0.00011843744887026078
    epoch : 3 ; learning_rate : 0.00011843744887026078 ; fit : 0.10616666666666667
    batch_size :          12000
    best_batch_loss : 1.7098121326858566
    Epoch 3, Loss: 1.7203720110879916, fit: 0.11325
      batch rate adapted learning_rate :              0.004
      loss :              1.7098121326858566
      loss_factor :              0.17098121326858567
      loss adapted learning_rate :              0.00011693830116319032
    epoch : 4 ; learning_rate : 0.00011693830116319032 ; fit : 0.11325
    batch_size :          12000
    best_batch_loss : 1.7145816724588894
    Epoch 4, Loss: 1.7199792735878838, fit: 0.106
      batch rate adapted learning_rate :              0.004
      loss :              1.720542090968742
      loss_factor :              0.17205420909687422
      loss adapted learning_rate :              0.00011841060347180366
    epoch : 5 ; learning_rate : 0.00011841060347180366 ; fit : 0.106
    batch_size :          12000
    best_batch_loss : 1.7133397516781452
    Epoch 5, Loss: 1.7195278366331426, fit: 0.11166666666666666
      batch rate adapted learning_rate :              0.004
      loss :              1.7133397516781452
      loss_factor :              0.1713339751678145
      loss adapted learning_rate :              0.00011742132418722112
    epoch : 6 ; learning_rate : 0.00011742132418722112 ; fit : 0.11166666666666666
    batch_size :          12000
    best_batch_loss : 1.707123734277616
    Epoch 6, Loss: 1.7191286824257053, fit: 0.10716666666666666
      batch rate adapted learning_rate :              0.004
      loss :              1.7201695112211546
      loss_factor :              0.17201695112211546
      loss adapted learning_rate :              0.00011835932589339302
    epoch : 7 ; learning_rate : 0.00011835932589339302 ; fit : 0.10716666666666666
    batch_size :          12000
    best_batch_loss : 1.712642553450018
    Epoch 7, Loss: 1.718733009406178, fit: 0.1075
      batch rate adapted learning_rate :              0.004
      loss :              1.7184438864317237
      loss_factor :              0.17184438864317236
      loss adapted learning_rate :              0.00011812197563258267
    epoch : 8 ; learning_rate : 0.00011812197563258267 ; fit : 0.1075
    batch_size :          12000
    best_batch_loss : 1.7095625639034848
    Epoch 8, Loss: 1.718307362031026, fit: 0.10491666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.727259692438212_fit_0.10491666666666667_2024-01-03_150405
  self.fit : 0.10491666666666667
  self.loss : 1.727259692438212
  current_accuracy : 0.1121
   Accuracy mean: 0.1094
   Accuracy mean: 0.1121
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.005000000000000001
    epoch : 0 ; learning_rate : 0.005000000000000001 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7299660010998623
    Epoch 0, Loss: 1.7421364115936697, fit: 0.1015
    Epoch 0, Loss: 1.7421364115936697
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7299660010998623_fit_0.1015_2024-01-03_150407
  self.fit : 0.1015
  self.loss : 1.7299660010998623
  current_accuracy : 0.1028
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.7299660010998623
      loss_factor :              0.17299660010998624
      loss adapted learning_rate :              0.0001496391182480725
    epoch : 0 ; learning_rate : 0.0001496391182480725 ; fit : 0.1015
    batch_size :          12000
    best_batch_loss : 1.7246385232711685
    Epoch 0, Loss: 1.726662443003967, fit: 0.10683333333333334
    Epoch 0, Loss: 1.726662443003967
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.7250825494133544
      loss_factor :              0.17250825494133543
      loss adapted learning_rate :              0.00014879549011452393
    epoch : 1 ; learning_rate : 0.00014879549011452393 ; fit : 0.10683333333333334
    batch_size :          12000
    best_batch_loss : 1.7200377209486015
    Epoch 1, Loss: 1.7259985129752073, fit: 0.10066666666666667
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.730938050776339
      loss_factor :              0.1730938050776339
      loss adapted learning_rate :              0.00014980732678126962
    epoch : 2 ; learning_rate : 0.00014980732678126962 ; fit : 0.10066666666666667
    batch_size :          12000
    best_batch_loss : 1.7200906430415352
    Epoch 2, Loss: 1.7253022740202848, fit: 0.10183333333333333
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.7311084458204689
      loss_factor :              0.1731108445820469
      loss adapted learning_rate :              0.000149836822559548
    epoch : 3 ; learning_rate : 0.000149836822559548 ; fit : 0.10183333333333333
    batch_size :          12000
    best_batch_loss : 1.7158640453492404
    Epoch 3, Loss: 1.7246224585277603, fit: 0.10341666666666667
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.727741551029512
      loss_factor :              0.1727741551029512
      loss adapted learning_rate :              0.00014925454335769318
    epoch : 4 ; learning_rate : 0.00014925454335769318 ; fit : 0.10341666666666667
    batch_size :          12000
    best_batch_loss : 1.7203239242912731
    Epoch 4, Loss: 1.7239200283929144, fit: 0.10566666666666667
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.7203239242912731
      loss_factor :              0.17203239242912732
      loss adapted learning_rate :              0.00014797572022444633
    epoch : 5 ; learning_rate : 0.00014797572022444633 ; fit : 0.10566666666666667
    batch_size :          12000
    best_batch_loss : 1.7151400190741042
    Epoch 5, Loss: 1.7232834655424005, fit: 0.10516666666666667
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.7213967593846722
      loss_factor :              0.1721396759384672
      loss adapted learning_rate :              0.00014816034016100257
    epoch : 6 ; learning_rate : 0.00014816034016100257 ; fit : 0.10516666666666667
    batch_size :          12000
    best_batch_loss : 1.7147973004740564
    Epoch 6, Loss: 1.7225589713418565, fit: 0.10483333333333333
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.7235006865223377
      loss_factor :              0.17235006865223376
      loss adapted learning_rate :              0.00014852273082214848
    epoch : 7 ; learning_rate : 0.00014852273082214848 ; fit : 0.10483333333333333
    batch_size :          12000
    best_batch_loss : 1.7186826208742487
    Epoch 7, Loss: 1.7217916302814487, fit: 0.10625
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.7186826208742487
      loss_factor :              0.17186826208742487
      loss adapted learning_rate :              0.00014769349756475883
    epoch : 8 ; learning_rate : 0.00014769349756475883 ; fit : 0.10625
    batch_size :          12000
    best_batch_loss : 1.7175190353020047
    Epoch 8, Loss: 1.7211163049926617, fit: 0.10591666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7199478069751237_fit_0.10591666666666667_2024-01-03_150427
  self.fit : 0.10591666666666667
  self.loss : 1.7199478069751237
  current_accuracy : 0.107
   Accuracy mean: 0.1028
   Accuracy mean: 0.107
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.006
    epoch : 0 ; learning_rate : 0.006 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.641672046506377
    Epoch 0, Loss: 1.6699345925778342, fit: 0.14666666666666667
    Epoch 0, Loss: 1.6699345925778342
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.641672046506377_fit_0.14666666666666667_2024-01-03_150430
  self.fit : 0.14666666666666667
  self.loss : 1.641672046506377
  current_accuracy : 0.1551
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.006
      loss :              1.641672046506377
      loss_factor :              0.1641672046506377
      loss adapted learning_rate :              0.00016170522649682615
    epoch : 0 ; learning_rate : 0.00016170522649682615 ; fit : 0.14666666666666667
    batch_size :          12000
    best_batch_loss : 1.621731752390742
    Epoch 0, Loss: 1.6306916893353236, fit: 0.15158333333333332
    Epoch 0, Loss: 1.6306916893353236
      batch rate adapted learning_rate :              0.006
      loss :              1.6322772740451321
      loss_factor :              0.16322772740451322
      loss adapted learning_rate :              0.00015985974596185244
    epoch : 1 ; learning_rate : 0.00015985974596185244 ; fit : 0.15158333333333332
    batch_size :          12000
    best_batch_loss : 1.6249753673876113
    Epoch 1, Loss: 1.629264161472968, fit: 0.15466666666666667
      batch rate adapted learning_rate :              0.006
      loss :              1.6249753673876113
      loss_factor :              0.16249753673876113
      loss adapted learning_rate :              0.00015843269667699015
    epoch : 2 ; learning_rate : 0.00015843269667699015 ; fit : 0.15466666666666667
    batch_size :          12000
    best_batch_loss : 1.6223813216649423
    Epoch 2, Loss: 1.627860236493379, fit: 0.15266666666666667
      batch rate adapted learning_rate :              0.006
      loss :              1.6283684662030373
      loss_factor :              0.16283684662030373
      loss adapted learning_rate :              0.00015909503170346594
    epoch : 3 ; learning_rate : 0.00015909503170346594 ; fit : 0.15266666666666667
    batch_size :          12000
    best_batch_loss : 1.61642558358037
    Epoch 3, Loss: 1.6264473013689114, fit: 0.156
      batch rate adapted learning_rate :              0.006
      loss :              1.6221086505674047
      loss_factor :              0.16221086505674048
      loss adapted learning_rate :              0.00015787418845473642
    epoch : 4 ; learning_rate : 0.00015787418845473642 ; fit : 0.156
    batch_size :          12000
    best_batch_loss : 1.6163375138797682
    Epoch 4, Loss: 1.625073698733132, fit: 0.15316666666666667
      batch rate adapted learning_rate :              0.006
      loss :              1.6277157149046464
      loss_factor :              0.16277157149046464
      loss adapted learning_rate :              0.00015896750691285266
    epoch : 5 ; learning_rate : 0.00015896750691285266 ; fit : 0.15316666666666667
    batch_size :          12000
    best_batch_loss : 1.6170331031477088
    Epoch 5, Loss: 1.6237030514973374, fit: 0.15866666666666668
      batch rate adapted learning_rate :              0.006
      loss :              1.6170331031477088
      loss_factor :              0.16170331031477087
      loss adapted learning_rate :              0.00015688776340053048
    epoch : 6 ; learning_rate : 0.00015688776340053048 ; fit : 0.15866666666666668
    batch_size :          12000
    best_batch_loss : 1.613046164134466
    Epoch 6, Loss: 1.6223455552060662, fit: 0.15791666666666668
      batch rate adapted learning_rate :              0.006
      loss :              1.6178287555779618
      loss_factor :              0.1617828755577962
      loss adapted learning_rate :              0.00015704219294249621
    epoch : 7 ; learning_rate : 0.00015704219294249621 ; fit : 0.15791666666666668
    batch_size :          12000
    best_batch_loss : 1.6159508767451198
    Epoch 7, Loss: 1.6210118221508774, fit: 0.15383333333333332
      batch rate adapted learning_rate :              0.006
      loss :              1.6236579580319133
      loss_factor :              0.16236579580319133
      loss adapted learning_rate :              0.00015817590988082174
    epoch : 8 ; learning_rate : 0.00015817590988082174 ; fit : 0.15383333333333332
    batch_size :          12000
    best_batch_loss : 1.6134956779106078
    Epoch 8, Loss: 1.6196213530609185, fit: 0.15683333333333332
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6167706272738949_fit_0.15683333333333332_2024-01-03_150450
  self.fit : 0.15683333333333332
  self.loss : 1.6167706272738949
  current_accuracy : 0.162
   Accuracy mean: 0.1551
   Accuracy mean: 0.162
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.020000000000000004
    epoch : 0 ; learning_rate : 0.020000000000000004 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.6749417251951924
    Epoch 0, Loss: 1.732052447073755, fit: 0.13166666666666665
    Epoch 0, Loss: 1.732052447073755
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6749417251951924_fit_0.13166666666666665_2024-01-03_150453
  self.fit : 0.13166666666666665
  self.loss : 1.6749417251951924
  current_accuracy : 0.1407
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.6749417251951924
      loss_factor :              0.16749417251951923
      loss adapted learning_rate :              0.0005610859565599695
    epoch : 0 ; learning_rate : 0.0005610859565599695 ; fit : 0.13166666666666665
    batch_size :          12000
    best_batch_loss : 1.6425815991310655
    Epoch 0, Loss: 1.6523877048780087, fit: 0.14075
    Epoch 0, Loss: 1.6523877048780087
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.652052716942939
      loss_factor :              0.16520527169429391
      loss adapted learning_rate :              0.0005458556359117095
    epoch : 1 ; learning_rate : 0.0005458556359117095 ; fit : 0.14075
    batch_size :          12000
    best_batch_loss : 1.6422183210514862
    Epoch 1, Loss: 1.6491344979042093, fit: 0.14166666666666666
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.64810601747974
      loss_factor :              0.164810601747974
      loss adapted learning_rate :              0.0005432506889705859
    epoch : 2 ; learning_rate : 0.0005432506889705859 ; fit : 0.14166666666666666
    batch_size :          12000
    best_batch_loss : 1.6352257895020035
    Epoch 2, Loss: 1.6459453501076786, fit: 0.14983333333333335
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.6352257895020035
      loss_factor :              0.16352257895020034
      loss adapted learning_rate :              0.0005347926765304901
    epoch : 3 ; learning_rate : 0.0005347926765304901 ; fit : 0.14983333333333335
    batch_size :          12000
    best_batch_loss : 1.6396451038494517
    Epoch 3, Loss: 1.6427098692740765, fit: 0.145
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.6420955684148921
      loss_factor :              0.16420955684148922
      loss adapted learning_rate :              0.0005392955711615657
    epoch : 4 ; learning_rate : 0.0005392955711615657 ; fit : 0.145
    batch_size :          12000
    best_batch_loss : 1.6317753930930237
    Epoch 4, Loss: 1.6394729198811047, fit: 0.14983333333333335
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.6317753930930237
      loss_factor :              0.16317753930930237
      loss adapted learning_rate :              0.0005325381867007785
    epoch : 5 ; learning_rate : 0.0005325381867007785 ; fit : 0.14983333333333335
    batch_size :          12000
    best_batch_loss : 1.628427156869482
    Epoch 5, Loss: 1.6362583203534, fit: 0.15208333333333332
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.628427156869482
      loss_factor :              0.1628427156869482
      loss adapted learning_rate :              0.000530355001046005
    epoch : 6 ; learning_rate : 0.000530355001046005 ; fit : 0.15208333333333332
    batch_size :          12000
    best_batch_loss : 1.6232937274230044
    Epoch 6, Loss: 1.6330053069725239, fit: 0.15541666666666668
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.6232937274230044
      loss_factor :              0.16232937274230044
      loss adapted learning_rate :              0.0005270165050981744
    epoch : 7 ; learning_rate : 0.0005270165050981744 ; fit : 0.15541666666666668
    batch_size :          12000
    best_batch_loss : 1.625460337037415
    Epoch 7, Loss: 1.6298312384356533, fit: 0.1535
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.625460337037415
      loss_factor :              0.1625460337037415
      loss adapted learning_rate :              0.0005284242614563574
    epoch : 8 ; learning_rate : 0.0005284242614563574 ; fit : 0.1535
    batch_size :          12000
    best_batch_loss : 1.6178911202056685
    Epoch 8, Loss: 1.6267740898632863, fit: 0.15441666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6247455844552146_fit_0.15441666666666667_2024-01-03_150513
  self.fit : 0.15441666666666667
  self.loss : 1.6247455844552146
  current_accuracy : 0.1554
   Accuracy mean: 0.1407
   Accuracy mean: 0.1554
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.18000000000000002
    epoch : 0 ; learning_rate : 0.18000000000000002 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.062741241643939
    Epoch 0, Loss: 1.385098563177119, fit: 0.44083333333333335
    Epoch 0, Loss: 1.385098563177119
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.062741241643939_fit_0.44083333333333335_2024-01-03_150516
  self.fit : 0.44083333333333335
  self.loss : 1.062741241643939
  current_accuracy : 0.5143
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              1.062741241643939
      loss_factor :              0.1062741241643939
      loss adapted learning_rate :              0.0020329541040436223
    epoch : 0 ; learning_rate : 0.0020329541040436223 ; fit : 0.44083333333333335
    batch_size :          12000
    best_batch_loss : 0.9224190861967508
    Epoch 0, Loss: 0.9338516195869624, fit: 0.515
    Epoch 0, Loss: 0.9338516195869624
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              0.9224190861967508
      loss_factor :              0.09224190861967509
      loss adapted learning_rate :              0.0015315425470440883
    epoch : 1 ; learning_rate : 0.0015315425470440883 ; fit : 0.515
    batch_size :          12000
    best_batch_loss : 0.906644585141304
    Epoch 1, Loss: 0.9209027960585232, fit: 0.5224166666666666
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              0.906644585141304
      loss_factor :              0.0906644585141304
      loss adapted learning_rate :              0.001479607926778885
    epoch : 2 ; learning_rate : 0.001479607926778885 ; fit : 0.5224166666666666
    batch_size :          12000
    best_batch_loss : 0.902364026282736
    Epoch 2, Loss: 0.9125256989245849, fit: 0.5209166666666667
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              0.910408193211293
      loss_factor :              0.0910408193211293
      loss adapted learning_rate :              0.0014919175408792521
    epoch : 3 ; learning_rate : 0.0014919175408792521 ; fit : 0.5209166666666667
    batch_size :          12000
    best_batch_loss : 0.8981038648490942
    Epoch 3, Loss: 0.9054179812434324, fit: 0.52375
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              0.9007039838432571
      loss_factor :              0.0900703983843257
      loss adapted learning_rate :              0.0014602817997200058
    epoch : 4 ; learning_rate : 0.0014602817997200058 ; fit : 0.52375
    batch_size :          12000
    best_batch_loss : 0.8831462040605751
    Epoch 4, Loss: 0.898794777327734, fit: 0.523
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              0.9038380812602821
      loss_factor :              0.09038380812602821
      loss adapted learning_rate :              0.0014704618988452833
    epoch : 5 ; learning_rate : 0.0014704618988452833 ; fit : 0.523
    batch_size :          12000
    best_batch_loss : 0.8766834283497373
    Epoch 5, Loss: 0.8931103674130713, fit: 0.5316666666666666
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              0.8855951522136202
      loss_factor :              0.08855951522136202
      loss adapted learning_rate :              0.0014117017925236773
    epoch : 6 ; learning_rate : 0.0014117017925236773 ; fit : 0.5316666666666666
    batch_size :          12000
    best_batch_loss : 0.8789106928342071
    Epoch 6, Loss: 0.8882310838360618, fit: 0.5333333333333333
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              0.8831901326250375
      loss_factor :              0.08831901326250376
      loss adapted learning_rate :              0.0014040446586592167
    epoch : 7 ; learning_rate : 0.0014040446586592167 ; fit : 0.5333333333333333
    batch_size :          12000
    best_batch_loss : 0.870330027181574
    Epoch 7, Loss: 0.8840341104287042, fit: 0.52775
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              0.8922277517863325
      loss_factor :              0.08922277517863325
      loss adapted learning_rate :              0.0014329266499038484
    epoch : 8 ; learning_rate : 0.0014329266499038484 ; fit : 0.52775
    batch_size :          12000
    best_batch_loss : 0.8721763518584484
    Epoch 8, Loss: 0.8802792049144135, fit: 0.5305
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.889055275907543_fit_0.5305_2024-01-03_150536
  self.fit : 0.5305
  self.loss : 0.889055275907543
  current_accuracy : 0.5442
   Accuracy mean: 0.5143
   Accuracy mean: 0.5442
  Results saved to test_combinations_results20240103150536
  normalized_accuracies :      [0.11839899 0.11839899 0.         0.         0.05658005 0.05658005
   0.03017603 0.03038558 0.08885163 0.09450964 0.07502096 0.0838223
   0.18461861 0.19907795 0.15444258 0.18524728 0.93734283 1.        ]
batch_rate :  0.1
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.1
      batch rate adapted learning_rate :              1.0000000000000001e-11
    epoch : 0 ; learning_rate : 1.0000000000000001e-11 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.773995186793804
    Epoch 0, Loss: 1.7827853754392824, fit: 0.08
    Epoch 0, Loss: 1.7827853754392824
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7766567340290962_fit_0.08_2024-01-03_150540
  self.fit : 0.08
  self.loss : 1.7766567340290962
  current_accuracy : 0.0717
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.1
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.7766567340290962
      loss_factor :              0.17766567340290962
      loss adapted learning_rate :              3.156509150570935e-13
    epoch : 0 ; learning_rate : 3.156509150570935e-13 ; fit : 0.08
    batch_size :          6000
    best_batch_loss : 1.7754116328060012
    Epoch 0, Loss: 1.7827853753876508, fit: 0.08066666666666666
    Epoch 0, Loss: 1.7827853753876508
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.7754116328060012
      loss_factor :              0.17754116328060013
      loss adapted learning_rate :              3.152086465902872e-13
    epoch : 1 ; learning_rate : 3.152086465902872e-13 ; fit : 0.08066666666666666
    batch_size :          6000
    best_batch_loss : 1.7757489457887563
    Epoch 1, Loss: 1.7827853753845646, fit: 0.07716666666666666
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.7827921660260484
      loss_factor :              0.17827921660260485
      loss adapted learning_rate :              3.1783479072438497e-13
    epoch : 2 ; learning_rate : 3.1783479072438497e-13 ; fit : 0.07716666666666666
    batch_size :          6000
    best_batch_loss : 1.771365883575039
    Epoch 2, Loss: 1.782785375381552, fit: 0.07716666666666666
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.7815389734333045
      loss_factor :              0.17815389734333045
      loss adapted learning_rate :              3.1738811138617934e-13
    epoch : 3 ; learning_rate : 3.1738811138617934e-13 ; fit : 0.07716666666666666
    batch_size :          6000
    best_batch_loss : 1.769055387756937
    Epoch 3, Loss: 1.7827853753785554, fit: 0.08166666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.769055387756937
      loss_factor :              0.1769055387756937
      loss adapted learning_rate :              3.129556964951847e-13
    epoch : 4 ; learning_rate : 3.129556964951847e-13 ; fit : 0.08166666666666667
    batch_size :          6000
    best_batch_loss : 1.7707439792502018
    Epoch 4, Loss: 1.7827853753754375, fit: 0.07583333333333334
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.78409834457069
      loss_factor :              0.17840983445706898
      loss adapted learning_rate :              3.1830069030998763e-13
    epoch : 5 ; learning_rate : 3.1830069030998763e-13 ; fit : 0.07583333333333334
    batch_size :          6000
    best_batch_loss : 1.774471034205972
    Epoch 5, Loss: 1.7827853753723952, fit: 0.08166666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.774471034205972
      loss_factor :              0.1774471034205972
      loss adapted learning_rate :              3.1487474512360117e-13
    epoch : 6 ; learning_rate : 3.1487474512360117e-13 ; fit : 0.08166666666666667
    batch_size :          6000
    best_batch_loss : 1.773901890987753
    Epoch 6, Loss: 1.7827853753692677, fit: 0.082
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.773901890987753
      loss_factor :              0.1773901890987753
      loss adapted learning_rate :              3.1467279188499263e-13
    epoch : 7 ; learning_rate : 3.1467279188499263e-13 ; fit : 0.082
    batch_size :          6000
    best_batch_loss : 1.7680895692221559
    Epoch 7, Loss: 1.7827853753663971, fit: 0.0785
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.779652307604294
      loss_factor :              0.17796523076042942
      loss adapted learning_rate :              3.1671623359612896e-13
    epoch : 8 ; learning_rate : 3.1671623359612896e-13 ; fit : 0.0785
    batch_size :          6000
    best_batch_loss : 1.7631075045062352
    Epoch 8, Loss: 1.7827853753631882, fit: 0.07533333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7844994364181654_fit_0.07533333333333334_2024-01-03_150559
  self.fit : 0.07533333333333334
  self.loss : 1.7844994364181654
  current_accuracy : 0.0717
   Accuracy mean: 0.0717
   Accuracy mean: 0.0717
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.1
      batch rate adapted learning_rate :              1e-08
    epoch : 0 ; learning_rate : 1e-08 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7941440136159674
    Epoch 0, Loss: 1.803286141661578, fit: 0.07283333333333333
    Epoch 0, Loss: 1.803286141661578
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7964392375400962_fit_0.07283333333333333_2024-01-03_150602
  self.fit : 0.07283333333333333
  self.loss : 1.7964392375400962
  current_accuracy : 0.0768
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.1
      batch rate adapted learning_rate :              1e-08
      loss :              1.7964392375400962
      loss_factor :              0.17964392375400962
      loss adapted learning_rate :              3.227193934173642e-10
    epoch : 0 ; learning_rate : 3.227193934173642e-10 ; fit : 0.07283333333333333
    batch_size :          6000
    best_batch_loss : 1.7928484325412404
    Epoch 0, Loss: 1.8032861119401633, fit: 0.068
    Epoch 0, Loss: 1.8032861119401633
      batch rate adapted learning_rate :              1e-08
      loss :              1.8082805172598109
      loss_factor :              0.1808280517259811
      loss adapted learning_rate :              3.26987842910141e-10
    epoch : 1 ; learning_rate : 3.26987842910141e-10 ; fit : 0.068
    batch_size :          6000
    best_batch_loss : 1.791459082875789
    Epoch 1, Loss: 1.8032861107053453, fit: 0.06933333333333333
      batch rate adapted learning_rate :              1e-08
      loss :              1.8054314976103776
      loss_factor :              0.18054314976103775
      loss adapted learning_rate :              3.2595828925636506e-10
    epoch : 2 ; learning_rate : 3.2595828925636506e-10 ; fit : 0.06933333333333333
    batch_size :          6000
    best_batch_loss : 1.7845176459492307
    Epoch 2, Loss: 1.803286109218029, fit: 0.0735
      batch rate adapted learning_rate :              1e-08
      loss :              1.7995396416723306
      loss_factor :              0.17995396416723305
      loss adapted learning_rate :              3.23834292195018e-10
    epoch : 3 ; learning_rate : 3.23834292195018e-10 ; fit : 0.0735
    batch_size :          6000
    best_batch_loss : 1.7969725818145998
    Epoch 3, Loss: 1.8032861079254154, fit: 0.0715
      batch rate adapted learning_rate :              1e-08
      loss :              1.8022465338322775
      loss_factor :              0.18022465338322774
      loss adapted learning_rate :              3.248092568710458e-10
    epoch : 4 ; learning_rate : 3.248092568710458e-10 ; fit : 0.0715
    batch_size :          6000
    best_batch_loss : 1.7889408907171649
    Epoch 4, Loss: 1.8032861067897998, fit: 0.06683333333333333
      batch rate adapted learning_rate :              1e-08
      loss :              1.8148503192601377
      loss_factor :              0.18148503192601378
      loss adapted learning_rate :              3.293681681318624e-10
    epoch : 5 ; learning_rate : 3.293681681318624e-10 ; fit : 0.06683333333333333
    batch_size :          6000
    best_batch_loss : 1.7929118305675729
    Epoch 5, Loss: 1.803286105547794, fit: 0.07066666666666667
      batch rate adapted learning_rate :              1e-08
      loss :              1.8063203292132546
      loss_factor :              0.18063203292132546
      loss adapted learning_rate :              3.2627931317290804e-10
    epoch : 6 ; learning_rate : 3.2627931317290804e-10 ; fit : 0.07066666666666667
    batch_size :          6000
    best_batch_loss : 1.7909874884024832
    Epoch 6, Loss: 1.803286104265329, fit: 0.0675
      batch rate adapted learning_rate :              1e-08
      loss :              1.8096623874146425
      loss_factor :              0.18096623874146425
      loss adapted learning_rate :              3.274877956423264e-10
    epoch : 7 ; learning_rate : 3.274877956423264e-10 ; fit : 0.0675
    batch_size :          6000
    best_batch_loss : 1.7910174714213087
    Epoch 7, Loss: 1.803286102820002, fit: 0.07366666666666667
      batch rate adapted learning_rate :              1e-08
      loss :              1.802230723427594
      loss_factor :              0.18022307234275942
      loss adapted learning_rate :              3.248035580466349e-10
    epoch : 8 ; learning_rate : 3.248035580466349e-10 ; fit : 0.07366666666666667
    batch_size :          6000
    best_batch_loss : 1.7952375063184118
    Epoch 8, Loss: 1.8032861016039858, fit: 0.07333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7994435547797123_fit_0.07333333333333333_2024-01-03_150622
  self.fit : 0.07333333333333333
  self.loss : 1.7994435547797123
  current_accuracy : 0.0768
   Accuracy mean: 0.0768
   Accuracy mean: 0.0768
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.1
      batch rate adapted learning_rate :              1e-05
    epoch : 0 ; learning_rate : 1e-05 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.6667532419621502
    Epoch 0, Loss: 1.690073272051698, fit: 0.1275
    Epoch 0, Loss: 1.690073272051698
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6872202350124292_fit_0.1275_2024-01-03_150624
  self.fit : 0.1275
  self.loss : 1.6872202350124292
  current_accuracy : 0.1247
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.1
      batch rate adapted learning_rate :              1e-05
      loss :              1.6872202350124292
      loss_factor :              0.16872202350124293
      loss adapted learning_rate :              2.8467121214353976e-07
    epoch : 0 ; learning_rate : 2.8467121214353976e-07 ; fit : 0.1275
    batch_size :          6000
    best_batch_loss : 1.6794718005721723
    Epoch 0, Loss: 1.6899971051111269, fit: 0.1265
    Epoch 0, Loss: 1.6899971051111269
      batch rate adapted learning_rate :              1e-05
      loss :              1.6902917257987728
      loss_factor :              0.1690291725798773
      loss adapted learning_rate :              2.8570861183037946e-07
    epoch : 1 ; learning_rate : 2.8570861183037946e-07 ; fit : 0.1265
    batch_size :          6000
    best_batch_loss : 1.677778881456907
    Epoch 1, Loss: 1.6899930017139004, fit: 0.12783333333333333
      batch rate adapted learning_rate :              1e-05
      loss :              1.6857590159533602
      loss_factor :              0.16857590159533603
      loss adapted learning_rate :              2.841783459868042e-07
    epoch : 2 ; learning_rate : 2.841783459868042e-07 ; fit : 0.12783333333333333
    batch_size :          6000
    best_batch_loss : 1.6772755523647644
    Epoch 2, Loss: 1.6899892881121146, fit: 0.1245
      batch rate adapted learning_rate :              1e-05
      loss :              1.6950490481026452
      loss_factor :              0.16950490481026453
      loss adapted learning_rate :              2.8731912754736843e-07
    epoch : 3 ; learning_rate : 2.8731912754736843e-07 ; fit : 0.1245
    batch_size :          6000
    best_batch_loss : 1.6789932064447968
    Epoch 3, Loss: 1.6899850632487308, fit: 0.131
      batch rate adapted learning_rate :              1e-05
      loss :              1.6789932064447968
      loss_factor :              0.16789932064447968
      loss adapted learning_rate :              2.819018187287781e-07
    epoch : 4 ; learning_rate : 2.819018187287781e-07 ; fit : 0.131
    batch_size :          6000
    best_batch_loss : 1.6774232929266666
    Epoch 4, Loss: 1.6899812813183503, fit: 0.1215
      batch rate adapted learning_rate :              1e-05
      loss :              1.6983874546835838
      loss_factor :              0.16983874546835837
      loss adapted learning_rate :              2.884519946226582e-07
    epoch : 5 ; learning_rate : 2.884519946226582e-07 ; fit : 0.1215
    batch_size :          6000
    best_batch_loss : 1.6832875597185062
    Epoch 5, Loss: 1.6899771630712017, fit: 0.12966666666666668
      batch rate adapted learning_rate :              1e-05
      loss :              1.6858217305395105
      loss_factor :              0.16858217305395104
      loss adapted learning_rate :              2.8419949071592296e-07
    epoch : 6 ; learning_rate : 2.8419949071592296e-07 ; fit : 0.12966666666666668
    batch_size :          6000
    best_batch_loss : 1.682095726796034
    Epoch 6, Loss: 1.6899733343941417, fit: 0.12866666666666668
      batch rate adapted learning_rate :              1e-05
      loss :              1.6822307336692344
      loss_factor :              0.16822307336692344
      loss adapted learning_rate :              2.8299002413013306e-07
    epoch : 7 ; learning_rate : 2.8299002413013306e-07 ; fit : 0.12866666666666668
    batch_size :          6000
    best_batch_loss : 1.6798079914812991
    Epoch 7, Loss: 1.6899699221792333, fit: 0.12433333333333334
      batch rate adapted learning_rate :              1e-05
      loss :              1.6924066594323925
      loss_factor :              0.16924066594323925
      loss adapted learning_rate :              2.8642403008911104e-07
    epoch : 8 ; learning_rate : 2.8642403008911104e-07 ; fit : 0.12433333333333334
    batch_size :          6000
    best_batch_loss : 1.681423686703895
    Epoch 8, Loss: 1.6899658484553237, fit: 0.13016666666666668
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.681423686703895_fit_0.13016666666666668_2024-01-03_150644
  self.fit : 0.13016666666666668
  self.loss : 1.681423686703895
  current_accuracy : 0.1246
   Accuracy mean: 0.1247
   Accuracy mean: 0.1246
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.6560549837584555
    Epoch 0, Loss: 1.66736546306421, fit: 0.13033333333333333
    Epoch 0, Loss: 1.66736546306421
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.6717403845306555_fit_0.13033333333333333_2024-01-03_150646
  self.fit : 0.13033333333333333
  self.loss : 1.6717403845306555
  current_accuracy : 0.1357
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.0001
      loss :              1.6717403845306555
      loss_factor :              0.16717403845306555
      loss adapted learning_rate :              2.7947159132707042e-06
    epoch : 0 ; learning_rate : 2.7947159132707042e-06 ; fit : 0.13033333333333333
    batch_size :          6000
    best_batch_loss : 1.6598724871628443
    Epoch 0, Loss: 1.6668294592274424, fit: 0.13683333333333333
    Epoch 0, Loss: 1.6668294592274424
      batch rate adapted learning_rate :              0.0001
      loss :              1.6598724871628443
      loss_factor :              0.16598724871628442
      loss adapted learning_rate :              2.7551766736401666e-06
    epoch : 1 ; learning_rate : 2.7551766736401666e-06 ; fit : 0.13683333333333333
    batch_size :          6000
    best_batch_loss : 1.6536450366284758
    Epoch 1, Loss: 1.666803600232764, fit: 0.13566666666666666
      batch rate adapted learning_rate :              0.0001
      loss :              1.6587437090759514
      loss_factor :              0.16587437090759513
      loss adapted learning_rate :              2.7514306923990444e-06
    epoch : 2 ; learning_rate : 2.7514306923990444e-06 ; fit : 0.13566666666666666
    batch_size :          6000
    best_batch_loss : 1.6557951423101624
    Epoch 2, Loss: 1.666775343938314, fit: 0.13083333333333333
      batch rate adapted learning_rate :              0.0001
      loss :              1.674666249820254
      loss_factor :              0.1674666249820254
      loss adapted learning_rate :              2.804507048287034e-06
    epoch : 3 ; learning_rate : 2.804507048287034e-06 ; fit : 0.13083333333333333
    batch_size :          6000
    best_batch_loss : 1.6527092916569073
    Epoch 3, Loss: 1.6667465574796125, fit: 0.14083333333333334
      batch rate adapted learning_rate :              0.0001
      loss :              1.6527092916569073
      loss_factor :              0.16527092916569072
      loss adapted learning_rate :              2.731448002729076e-06
    epoch : 4 ; learning_rate : 2.731448002729076e-06 ; fit : 0.14083333333333334
    batch_size :          6000
    best_batch_loss : 1.6588112801665653
    Epoch 4, Loss: 1.6667188370631838, fit: 0.13633333333333333
      batch rate adapted learning_rate :              0.0001
      loss :              1.664688520998767
      loss_factor :              0.1664688520998767
      loss adapted learning_rate :              2.771187871945062e-06
    epoch : 5 ; learning_rate : 2.771187871945062e-06 ; fit : 0.13633333333333333
    batch_size :          6000
    best_batch_loss : 1.656237419939726
    Epoch 5, Loss: 1.6666919187611131, fit: 0.13316666666666666
      batch rate adapted learning_rate :              0.0001
      loss :              1.664693433391735
      loss_factor :              0.1664693433391735
      loss adapted learning_rate :              2.7712042271775627e-06
    epoch : 6 ; learning_rate : 2.7712042271775627e-06 ; fit : 0.13316666666666666
    batch_size :          6000
    best_batch_loss : 1.6505144376032181
    Epoch 6, Loss: 1.666664220073824, fit: 0.13133333333333333
      batch rate adapted learning_rate :              0.0001
      loss :              1.6721312562737634
      loss_factor :              0.16721312562737634
      loss adapted learning_rate :              2.7960229382076743e-06
    epoch : 7 ; learning_rate : 2.7960229382076743e-06 ; fit : 0.13133333333333333
    batch_size :          6000
    best_batch_loss : 1.6467420464727056
    Epoch 7, Loss: 1.6666369550157902, fit: 0.13116666666666665
      batch rate adapted learning_rate :              0.0001
      loss :              1.6746504170003411
      loss_factor :              0.16746504170003412
      loss adapted learning_rate :              2.804454019159417e-06
    epoch : 8 ; learning_rate : 2.804454019159417e-06 ; fit : 0.13116666666666665
    batch_size :          6000
    best_batch_loss : 1.6536501096702376
    Epoch 8, Loss: 1.6666081816172194, fit: 0.12983333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.674266080888864_fit_0.12983333333333333_2024-01-03_150706
  self.fit : 0.12983333333333333
  self.loss : 1.674266080888864
  current_accuracy : 0.1358
   Accuracy mean: 0.1357
   Accuracy mean: 0.1358
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.002
    epoch : 0 ; learning_rate : 0.002 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.6660747422819373
    Epoch 0, Loss: 1.6831127948501254, fit: 0.134
    Epoch 0, Loss: 1.6831127948501254
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.6684194497674019_fit_0.134_2024-01-03_150709
  self.fit : 0.134
  self.loss : 1.6684194497674019
  current_accuracy : 0.1272
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.002
      loss :              1.6684194497674019
      loss_factor :              0.16684194497674018
      loss adapted learning_rate :              5.56724692072432e-05
    epoch : 0 ; learning_rate : 5.56724692072432e-05 ; fit : 0.134
    batch_size :          6000
    best_batch_loss : 1.6612308601911363
    Epoch 0, Loss: 1.673521093421428, fit: 0.12766666666666668
    Epoch 0, Loss: 1.673521093421428
      batch rate adapted learning_rate :              0.002
      loss :              1.6758735084018082
      loss_factor :              0.1675873508401808
      loss adapted learning_rate :              5.617104032325971e-05
    epoch : 1 ; learning_rate : 5.617104032325971e-05 ; fit : 0.12766666666666668
    batch_size :          6000
    best_batch_loss : 1.6600900715864313
    Epoch 1, Loss: 1.6730858288835466, fit: 0.12983333333333333
      batch rate adapted learning_rate :              0.002
      loss :              1.6726745749170229
      loss_factor :              0.1672674574917023
      loss adapted learning_rate :              5.5956804671476875e-05
    epoch : 2 ; learning_rate : 5.5956804671476875e-05 ; fit : 0.12983333333333333
    batch_size :          6000
    best_batch_loss : 1.657254253721093
    Epoch 2, Loss: 1.6726397137980002, fit: 0.12533333333333332
      batch rate adapted learning_rate :              0.002
      loss :              1.6853148588307685
      loss_factor :              0.16853148588307684
      loss adapted learning_rate :              5.680572346791546e-05
    epoch : 3 ; learning_rate : 5.680572346791546e-05 ; fit : 0.12533333333333332
    batch_size :          6000
    best_batch_loss : 1.6627961403972138
    Epoch 3, Loss: 1.6722061542213527, fit: 0.131
      batch rate adapted learning_rate :              0.002
      loss :              1.6701779491335862
      loss_factor :              0.1670177949133586
      loss adapted learning_rate :              5.578988763544143e-05
    epoch : 4 ; learning_rate : 5.578988763544143e-05 ; fit : 0.131
    batch_size :          6000
    best_batch_loss : 1.6524848258473803
    Epoch 4, Loss: 1.6717888538810675, fit: 0.13266666666666665
      batch rate adapted learning_rate :              0.002
      loss :              1.6677203686417004
      loss_factor :              0.16677203686417003
      loss adapted learning_rate :              5.562582455964817e-05
    epoch : 5 ; learning_rate : 5.562582455964817e-05 ; fit : 0.13266666666666665
    batch_size :          6000
    best_batch_loss : 1.6643409939589884
    Epoch 5, Loss: 1.6713709793207163, fit: 0.12866666666666668
      batch rate adapted learning_rate :              0.002
      loss :              1.677387132140358
      loss_factor :              0.1677387132140358
      loss adapted learning_rate :              5.62725518214011e-05
    epoch : 6 ; learning_rate : 5.62725518214011e-05 ; fit : 0.12866666666666668
    batch_size :          6000
    best_batch_loss : 1.6534422641702238
    Epoch 6, Loss: 1.6710075549702577, fit: 0.136
      batch rate adapted learning_rate :              0.002
      loss :              1.6626826721091674
      loss_factor :              0.16626826721091675
      loss adapted learning_rate :              5.529027336264163e-05
    epoch : 7 ; learning_rate : 5.529027336264163e-05 ; fit : 0.136
    batch_size :          6000
    best_batch_loss : 1.6630941028462434
    Epoch 7, Loss: 1.6706207868340937, fit: 0.13033333333333333
      batch rate adapted learning_rate :              0.002
      loss :              1.6747982531314791
      loss_factor :              0.16747982531314792
      loss adapted learning_rate :              5.609898377384509e-05
    epoch : 8 ; learning_rate : 5.609898377384509e-05 ; fit : 0.13033333333333333
    batch_size :          6000
    best_batch_loss : 1.644706827072155
    Epoch 8, Loss: 1.6702905480082455, fit: 0.12816666666666668
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.6775772168002598_fit_0.12816666666666668_2024-01-03_150729
  self.fit : 0.12816666666666668
  self.loss : 1.6775772168002598
  current_accuracy : 0.1292
   Accuracy mean: 0.1272
   Accuracy mean: 0.1292
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.0025000000000000005
    epoch : 0 ; learning_rate : 0.0025000000000000005 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.6040502272745483
    Epoch 0, Loss: 1.624454458467032, fit: 0.1675
    Epoch 0, Loss: 1.624454458467032
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.6040502272745483_fit_0.1675_2024-01-03_150731
  self.fit : 0.1675
  self.loss : 1.6040502272745483
  current_accuracy : 0.169
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.6040502272745483
      loss_factor :              0.16040502272745483
      loss adapted learning_rate :              6.432442829048827e-05
    epoch : 0 ; learning_rate : 6.432442829048827e-05 ; fit : 0.1675
    batch_size :          6000
    best_batch_loss : 1.5891277338507852
    Epoch 0, Loss: 1.6026710362120342, fit: 0.1645
    Epoch 0, Loss: 1.6026710362120342
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.6056676208496117
      loss_factor :              0.16056676208496118
      loss adapted learning_rate :              6.445421271612133e-05
    epoch : 1 ; learning_rate : 6.445421271612133e-05 ; fit : 0.1645
    batch_size :          6000
    best_batch_loss : 1.584171604722644
    Epoch 1, Loss: 1.6018441200558275, fit: 0.17216666666666666
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.5898275644066369
      loss_factor :              0.15898275644066368
      loss adapted learning_rate :              6.318879211367849e-05
    epoch : 2 ; learning_rate : 6.318879211367849e-05 ; fit : 0.17216666666666666
    batch_size :          6000
    best_batch_loss : 1.5860220288923705
    Epoch 2, Loss: 1.600976448210212, fit: 0.16133333333333333
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.6128194712182466
      loss_factor :              0.16128194712182467
      loss adapted learning_rate :              6.502966616851764e-05
    epoch : 3 ; learning_rate : 6.502966616851764e-05 ; fit : 0.16133333333333333
    batch_size :          6000
    best_batch_loss : 1.5900982395684942
    Epoch 3, Loss: 1.6001766065972955, fit: 0.16783333333333333
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.6027721327949502
      loss_factor :              0.160277213279495
      loss adapted learning_rate :              6.422196274160185e-05
    epoch : 4 ; learning_rate : 6.422196274160185e-05 ; fit : 0.16783333333333333
    batch_size :          6000
    best_batch_loss : 1.5869374408477637
    Epoch 4, Loss: 1.5993199014848358, fit: 0.1715
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.5955993736929504
      loss_factor :              0.15955993736929502
      loss adapted learning_rate :              6.364843403323338e-05
    epoch : 5 ; learning_rate : 6.364843403323338e-05 ; fit : 0.1715
    batch_size :          6000
    best_batch_loss : 1.5877126817308902
    Epoch 5, Loss: 1.5984672408613556, fit: 0.16316666666666665
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.6071900969983888
      loss_factor :              0.16071900969983888
      loss adapted learning_rate :              6.457650019724227e-05
    epoch : 6 ; learning_rate : 6.457650019724227e-05 ; fit : 0.16316666666666665
    batch_size :          6000
    best_batch_loss : 1.5874739845455603
    Epoch 6, Loss: 1.5976439262071045, fit: 0.16516666666666666
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.6055295841364314
      loss_factor :              0.16055295841364314
      loss adapted learning_rate :              6.444313113843257e-05
    epoch : 7 ; learning_rate : 6.444313113843257e-05 ; fit : 0.16516666666666666
    batch_size :          6000
    best_batch_loss : 1.574478417722827
    Epoch 7, Loss: 1.5968065959002347, fit: 0.17483333333333334
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.5862894438767472
      loss_factor :              0.15862894438767472
      loss adapted learning_rate :              6.290785499387001e-05
    epoch : 8 ; learning_rate : 6.290785499387001e-05 ; fit : 0.17483333333333334
    batch_size :          6000
    best_batch_loss : 1.580418399926803
    Epoch 8, Loss: 1.595967023831059, fit: 0.169
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.5993372085789348_fit_0.169_2024-01-03_150751
  self.fit : 0.169
  self.loss : 1.5993372085789348
  current_accuracy : 0.1714
   Accuracy mean: 0.169
   Accuracy mean: 0.1714
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.003
    epoch : 0 ; learning_rate : 0.003 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7189857323087299
    Epoch 0, Loss: 1.7527310332378034, fit: 0.109
    Epoch 0, Loss: 1.7527310332378034
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7189857323087299_fit_0.109_2024-01-03_150753
  self.fit : 0.109
  self.loss : 1.7189857323087299
  current_accuracy : 0.1025
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.003
      loss :              1.7189857323087299
      loss_factor :              0.171898573230873
      loss adapted learning_rate :              8.864735843642942e-05
    epoch : 0 ; learning_rate : 8.864735843642942e-05 ; fit : 0.109
    batch_size :          6000
    best_batch_loss : 1.7183416608648692
    Epoch 0, Loss: 1.7276709560098018, fit: 0.10733333333333334
    Epoch 0, Loss: 1.7276709560098018
      batch rate adapted learning_rate :              0.003
      loss :              1.724452323261003
      loss_factor :              0.1724452323261003
      loss adapted learning_rate :              8.921207445600812e-05
    epoch : 1 ; learning_rate : 8.921207445600812e-05 ; fit : 0.10733333333333334
    batch_size :          6000
    best_batch_loss : 1.718936504022695
    Epoch 1, Loss: 1.7264259106644988, fit: 0.10833333333333334
      batch rate adapted learning_rate :              0.003
      loss :              1.718983259213665
      loss_factor :              0.1718983259213665
      loss adapted learning_rate :              8.864710336370503e-05
    epoch : 2 ; learning_rate : 8.864710336370503e-05 ; fit : 0.10833333333333334
    batch_size :          6000
    best_batch_loss : 1.7121273384785063
    Epoch 2, Loss: 1.7252526249728755, fit: 0.10233333333333333
      batch rate adapted learning_rate :              0.003
      loss :              1.7305034263954904
      loss_factor :              0.17305034263954905
      loss adapted learning_rate :              8.983926326299599e-05
    epoch : 3 ; learning_rate : 8.983926326299599e-05 ; fit : 0.10233333333333333
    batch_size :          6000
    best_batch_loss : 1.707189238954236
    Epoch 3, Loss: 1.7240512426935828, fit: 0.10316666666666667
      batch rate adapted learning_rate :              0.003
      loss :              1.7317239409425742
      loss_factor :              0.17317239409425741
      loss adapted learning_rate :              8.99660342290104e-05
    epoch : 4 ; learning_rate : 8.99660342290104e-05 ; fit : 0.10316666666666667
    batch_size :          6000
    best_batch_loss : 1.7009843004327838
    Epoch 4, Loss: 1.7229051344515993, fit: 0.118
      batch rate adapted learning_rate :              0.003
      loss :              1.7009843004327838
      loss_factor :              0.17009843004327838
      loss adapted learning_rate :              8.68004277095642e-05
    epoch : 5 ; learning_rate : 8.68004277095642e-05 ; fit : 0.118
    batch_size :          6000
    best_batch_loss : 1.7085212634709448
    Epoch 5, Loss: 1.7217570110089717, fit: 0.11383333333333333
      batch rate adapted learning_rate :              0.003
      loss :              1.7091657045331832
      loss_factor :              0.17091657045331832
      loss adapted learning_rate :              8.763742216657237e-05
    epoch : 6 ; learning_rate : 8.763742216657237e-05 ; fit : 0.11383333333333333
    batch_size :          6000
    best_batch_loss : 1.7077027339447264
    Epoch 6, Loss: 1.7205739564991434, fit: 0.09916666666666667
      batch rate adapted learning_rate :              0.003
      loss :              1.7384276002036863
      loss_factor :              0.17384276002036864
      loss adapted learning_rate :              9.066391563449845e-05
    epoch : 7 ; learning_rate : 9.066391563449845e-05 ; fit : 0.09916666666666667
    batch_size :          6000
    best_batch_loss : 1.6933314460716407
    Epoch 7, Loss: 1.7193315334031394, fit: 0.106
      batch rate adapted learning_rate :              0.003
      loss :              1.7221839936779837
      loss_factor :              0.17221839936779837
      loss adapted learning_rate :              8.897753124241949e-05
    epoch : 8 ; learning_rate : 8.897753124241949e-05 ; fit : 0.106
    batch_size :          6000
    best_batch_loss : 1.7050457164998425
    Epoch 8, Loss: 1.718090867511835, fit: 0.11083333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7151976048740856_fit_0.11083333333333334_2024-01-03_150813
  self.fit : 0.11083333333333334
  self.loss : 1.7151976048740856
  current_accuracy : 0.1089
   Accuracy mean: 0.1025
   Accuracy mean: 0.1089
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.010000000000000002
    epoch : 0 ; learning_rate : 0.010000000000000002 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.6399560133691677
    Epoch 0, Loss: 1.6833265432020543, fit: 0.14383333333333334
    Epoch 0, Loss: 1.6833265432020543
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6399560133691677_fit_0.14383333333333334_2024-01-03_150815
  self.fit : 0.14383333333333334
  self.loss : 1.6399560133691677
  current_accuracy : 0.1533
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.6399560133691677
      loss_factor :              0.16399560133691676
      loss adapted learning_rate :              0.00026894557257856944
    epoch : 0 ; learning_rate : 0.00026894557257856944 ; fit : 0.14383333333333334
    batch_size :          6000
    best_batch_loss : 1.5988875012542734
    Epoch 0, Loss: 1.6323029414144197, fit: 0.1635
    Epoch 0, Loss: 1.6323029414144197
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.5988875012542734
      loss_factor :              0.15988875012542733
      loss adapted learning_rate :              0.00025564412416671344
    epoch : 1 ; learning_rate : 0.00025564412416671344 ; fit : 0.1635
    batch_size :          6000
    best_batch_loss : 1.6115946452648713
    Epoch 1, Loss: 1.6300657026803016, fit: 0.15333333333333332
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.6242766417876
      loss_factor :              0.16242766417876
      loss adapted learning_rate :              0.00026382746090568044
    epoch : 2 ; learning_rate : 0.00026382746090568044 ; fit : 0.15333333333333332
    batch_size :          6000
    best_batch_loss : 1.6208578147023205
    Epoch 2, Loss: 1.6278541015961652, fit: 0.1545
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.6213138030097272
      loss_factor :              0.1621313803009727
      loss adapted learning_rate :              0.0002628658447829865
    epoch : 3 ; learning_rate : 0.0002628658447829865 ; fit : 0.1545
    batch_size :          6000
    best_batch_loss : 1.6170100868693673
    Epoch 3, Loss: 1.6256959793198582, fit: 0.15366666666666667
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.617695848438589
      loss_factor :              0.1617695848438589
      loss adapted learning_rate :              0.0002616939858055447
    epoch : 4 ; learning_rate : 0.0002616939858055447 ; fit : 0.15366666666666667
    batch_size :          6000
    best_batch_loss : 1.609365746928079
    Epoch 4, Loss: 1.6235400937250413, fit: 0.156
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.6139614496446817
      loss_factor :              0.16139614496446816
      loss adapted learning_rate :              0.0002604871560939163
    epoch : 5 ; learning_rate : 0.0002604871560939163 ; fit : 0.156
    batch_size :          6000
    best_batch_loss : 1.6104468786183423
    Epoch 5, Loss: 1.6214137895006995, fit: 0.15566666666666668
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.6134003534137051
      loss_factor :              0.1613400353413705
      loss adapted learning_rate :              0.0002603060700395469
    epoch : 6 ; learning_rate : 0.0002603060700395469 ; fit : 0.15566666666666668
    batch_size :          6000
    best_batch_loss : 1.5962417690492885
    Epoch 6, Loss: 1.6193360670516674, fit: 0.1495
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.6271087836551865
      loss_factor :              0.16271087836551865
      loss adapted learning_rate :              0.0002647482993847861
    epoch : 7 ; learning_rate : 0.0002647482993847861 ; fit : 0.1495
    batch_size :          6000
    best_batch_loss : 1.605402917347146
    Epoch 7, Loss: 1.6171809585021564, fit: 0.1605
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.6080783353303274
      loss_factor :              0.16080783353303274
      loss adapted learning_rate :              0.00025859159325587576
    epoch : 8 ; learning_rate : 0.00025859159325587576 ; fit : 0.1605
    batch_size :          6000
    best_batch_loss : 1.600760187088982
    Epoch 8, Loss: 1.6151449262832813, fit: 0.153
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.62086495677947_fit_0.153_2024-01-03_150835
  self.fit : 0.153
  self.loss : 1.62086495677947
  current_accuracy : 0.1647
   Accuracy mean: 0.1533
   Accuracy mean: 0.1647
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.09000000000000001
    epoch : 0 ; learning_rate : 0.09000000000000001 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.0063860523706745
    Epoch 0, Loss: 1.3554974682561483, fit: 0.4656666666666667
    Epoch 0, Loss: 1.3554974682561483
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.0063860523706745_fit_0.4656666666666667_2024-01-03_150838
  self.fit : 0.4656666666666667
  self.loss : 1.0063860523706745
  current_accuracy : 0.4888
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              1.0063860523706745
      loss_factor :              0.10063860523706744
      loss adapted learning_rate :              0.0009115315977656069
    epoch : 0 ; learning_rate : 0.0009115315977656069 ; fit : 0.4656666666666667
    batch_size :          6000
    best_batch_loss : 0.9514471959650602
    Epoch 0, Loss: 0.9714141224569035, fit: 0.4736666666666667
    Epoch 0, Loss: 0.9714141224569035
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              0.9898923885388207
      loss_factor :              0.09898923885388207
      loss adapted learning_rate :              0.0008818982467983826
    epoch : 1 ; learning_rate : 0.0008818982467983826 ; fit : 0.4736666666666667
    batch_size :          6000
    best_batch_loss : 0.9380941356951142
    Epoch 1, Loss: 0.9672922067088482, fit: 0.485
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              0.9742113012953793
      loss_factor :              0.09742113012953793
      loss adapted learning_rate :              0.0008541788936144728
    epoch : 2 ; learning_rate : 0.0008541788936144728 ; fit : 0.485
    batch_size :          6000
    best_batch_loss : 0.9460233686284036
    Epoch 2, Loss: 0.9633924677349146, fit: 0.49216666666666664
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              0.9611944112603398
      loss_factor :              0.09611944112603397
      loss adapted learning_rate :              0.0008315052266143002
    epoch : 3 ; learning_rate : 0.0008315052266143002 ; fit : 0.49216666666666664
    batch_size :          6000
    best_batch_loss : 0.943391814293017
    Epoch 3, Loss: 0.959749637013241, fit: 0.4995
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              0.9468368454568837
      loss_factor :              0.09468368454568837
      loss adapted learning_rate :              0.0008068500107232685
    epoch : 4 ; learning_rate : 0.0008068500107232685 ; fit : 0.4995
    batch_size :          6000
    best_batch_loss : 0.9313244858924311
    Epoch 4, Loss: 0.9563281856174319, fit: 0.4945
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              0.9590078082531539
      loss_factor :              0.09590078082531539
      loss adapted learning_rate :              0.0008277263786614662
    epoch : 5 ; learning_rate : 0.0008277263786614662 ; fit : 0.4945
    batch_size :          6000
    best_batch_loss : 0.9301606923359566
    Epoch 5, Loss: 0.9528927849888094, fit: 0.49433333333333335
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              0.9566181213713468
      loss_factor :              0.09566181213713468
      loss adapted learning_rate :              0.0008236064071224404
    epoch : 6 ; learning_rate : 0.0008236064071224404 ; fit : 0.49433333333333335
    batch_size :          6000
    best_batch_loss : 0.9364552196445993
    Epoch 6, Loss: 0.9494957475279914, fit: 0.5041666666666667
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              0.9369969718988234
      loss_factor :              0.09369969718988233
      loss adapted learning_rate :              0.000790166992812808
    epoch : 7 ; learning_rate : 0.000790166992812808 ; fit : 0.5041666666666667
    batch_size :          6000
    best_batch_loss : 0.9337752144291198
    Epoch 7, Loss: 0.9462794772728612, fit: 0.4905
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              0.9604068775719934
      loss_factor :              0.09604068775719934
      loss adapted learning_rate :              0.0008301432334388275
    epoch : 8 ; learning_rate : 0.0008301432334388275 ; fit : 0.4905
    batch_size :          6000
    best_batch_loss : 0.9220499002464391
    Epoch 8, Loss: 0.9430987594455009, fit: 0.5078333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.9254445456503335_fit_0.5078333333333334_2024-01-03_150858
  self.fit : 0.5078333333333334
  self.loss : 0.9254445456503335
  current_accuracy : 0.5062
   Accuracy mean: 0.4888
   Accuracy mean: 0.5062
  Results saved to test_combinations_results20240103150858
  normalized_accuracies :      [0.         0.         0.01173763 0.01173763 0.12197929 0.12174914
   0.14729574 0.14752589 0.12773303 0.13233602 0.22393556 0.22945915
   0.07088608 0.08561565 0.18780207 0.21403913 0.95995397 1.        ]
batch_rate :  0.01
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-12
    epoch : 0 ; learning_rate : 1e-12 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6756905275462657
    Epoch 0, Loss: 1.7456183808015395, fit: 0.11166666666666666
    Epoch 0, Loss: 1.7456183808015395
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7209339069976262_fit_0.11166666666666666_2024-01-03_150903
  self.fit : 0.11166666666666666
  self.loss : 1.7209339069976262
  current_accuracy : 0.1071
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-12
      loss :              1.7209339069976262
      loss_factor :              0.17209339069976262
      loss adapted learning_rate :              2.961613512254115e-14
    epoch : 0 ; learning_rate : 2.961613512254115e-14 ; fit : 0.11166666666666666
    batch_size :          600
    best_batch_loss : 1.7041591736746988
    Epoch 0, Loss: 1.7456183807390278, fit: 0.11
    Epoch 0, Loss: 1.7456183807390278
      batch rate adapted learning_rate :              1e-12
      loss :              1.736850870482875
      loss_factor :              0.1736850870482875
      loss adapted learning_rate :              3.016650946297121e-14
    epoch : 1 ; learning_rate : 3.016650946297121e-14 ; fit : 0.11
    batch_size :          600
    best_batch_loss : 1.6850235614574074
    Epoch 1, Loss: 1.745618380733716, fit: 0.09833333333333333
      batch rate adapted learning_rate :              1e-12
      loss :              1.753827304807006
      loss_factor :              0.1753827304807006
      loss adapted learning_rate :              3.075910215086607e-14
    epoch : 2 ; learning_rate : 3.075910215086607e-14 ; fit : 0.09833333333333333
    batch_size :          600
    best_batch_loss : 1.6954385673058343
    Epoch 2, Loss: 1.7456183807284602, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-12
      loss :              1.7858359755031479
      loss_factor :              0.17858359755031478
      loss adapted learning_rate :              3.1892101314012796e-14
    epoch : 3 ; learning_rate : 3.1892101314012796e-14 ; fit : 0.08333333333333333
    batch_size :          600
    best_batch_loss : 1.6841464985430097
    Epoch 3, Loss: 1.7456183807233792, fit: 0.11
      batch rate adapted learning_rate :              1e-12
      loss :              1.7309994359552274
      loss_factor :              0.17309994359552275
      loss adapted learning_rate :              2.9963590472773153e-14
    epoch : 4 ; learning_rate : 2.9963590472773153e-14 ; fit : 0.11
    batch_size :          600
    best_batch_loss : 1.6814614483265389
    Epoch 4, Loss: 1.7456183807184154, fit: 0.135
      batch rate adapted learning_rate :              1e-12
      loss :              1.6814614483265389
      loss_factor :              0.1681461448326539
      loss adapted learning_rate :              2.8273126022083815e-14
    epoch : 5 ; learning_rate : 2.8273126022083815e-14 ; fit : 0.135
    batch_size :          600
    best_batch_loss : 1.6647851731539474
    Epoch 5, Loss: 1.7456183807133163, fit: 0.12833333333333333
      batch rate adapted learning_rate :              1e-12
      loss :              1.691804249258544
      loss_factor :              0.1691804249258544
      loss adapted learning_rate :              2.862201617809265e-14
    epoch : 6 ; learning_rate : 2.862201617809265e-14 ; fit : 0.12833333333333333
    batch_size :          600
    best_batch_loss : 1.6881335864339049
    Epoch 6, Loss: 1.7456183807086523, fit: 0.08166666666666667
      batch rate adapted learning_rate :              1e-12
      loss :              1.7829176645420595
      loss_factor :              0.17829176645420594
      loss adapted learning_rate :              3.178795398536111e-14
    epoch : 7 ; learning_rate : 3.178795398536111e-14 ; fit : 0.08166666666666667
    batch_size :          600
    best_batch_loss : 1.6756952764853223
    Epoch 7, Loss: 1.7456183807033578, fit: 0.09333333333333334
      batch rate adapted learning_rate :              1e-12
      loss :              1.746434215335227
      loss_factor :              0.1746434215335227
      loss adapted learning_rate :              3.0500324684935695e-14
    epoch : 8 ; learning_rate : 3.0500324684935695e-14 ; fit : 0.09333333333333334
    batch_size :          600
    best_batch_loss : 1.6929345765596755
    Epoch 8, Loss: 1.7456183806979908, fit: 0.09
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7747333097253857_fit_0.09_2024-01-03_150926
  self.fit : 0.09
  self.loss : 1.7747333097253857
  current_accuracy : 0.1071
   Accuracy mean: 0.1071
   Accuracy mean: 0.1071
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-09
    epoch : 0 ; learning_rate : 1e-09 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.5522243261200364
    Epoch 0, Loss: 1.6252434580857882, fit: 0.15166666666666667
    Epoch 0, Loss: 1.6252434580857882
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.6269192634728962_fit_0.15166666666666667_2024-01-03_150928
  self.fit : 0.15166666666666667
  self.loss : 1.6269192634728962
  current_accuracy : 0.1649
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-09
      loss :              1.6269192634728962
      loss_factor :              0.16269192634728963
      loss adapted learning_rate :              2.6468662898591916e-11
    epoch : 0 ; learning_rate : 2.6468662898591916e-11 ; fit : 0.15166666666666667
    batch_size :          600
    best_batch_loss : 1.5493744605242237
    Epoch 0, Loss: 1.6252434568015321, fit: 0.15333333333333332
    Epoch 0, Loss: 1.6252434568015321
      batch rate adapted learning_rate :              1e-09
      loss :              1.6404498026571215
      loss_factor :              0.16404498026571215
      loss adapted learning_rate :              2.691075555037789e-11
    epoch : 1 ; learning_rate : 2.691075555037789e-11 ; fit : 0.15333333333333332
    batch_size :          600
    best_batch_loss : 1.5575735881003956
    Epoch 1, Loss: 1.6252434565475518, fit: 0.18
      batch rate adapted learning_rate :              1e-09
      loss :              1.584066463418336
      loss_factor :              0.1584066463418336
      loss adapted learning_rate :              2.5092665605266744e-11
    epoch : 2 ; learning_rate : 2.5092665605266744e-11 ; fit : 0.18
    batch_size :          600
    best_batch_loss : 1.5413967284765642
    Epoch 2, Loss: 1.625243456216654, fit: 0.125
      batch rate adapted learning_rate :              1e-09
      loss :              1.6852468719485945
      loss_factor :              0.16852468719485944
      loss adapted learning_rate :              2.8400570194125224e-11
    epoch : 3 ; learning_rate : 2.8400570194125224e-11 ; fit : 0.125
    batch_size :          600
    best_batch_loss : 1.554740548511935
    Epoch 3, Loss: 1.6252434559243105, fit: 0.16833333333333333
      batch rate adapted learning_rate :              1e-09
      loss :              1.5970847209243118
      loss_factor :              0.15970847209243116
      loss adapted learning_rate :              2.5506796058098865e-11
    epoch : 4 ; learning_rate : 2.5506796058098865e-11 ; fit : 0.16833333333333333
    batch_size :          600
    best_batch_loss : 1.5357232179246463
    Epoch 4, Loss: 1.6252434555815598, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1e-09
      loss :              1.6633583784255899
      loss_factor :              0.16633583784255898
      loss adapted learning_rate :              2.766761095078608e-11
    epoch : 5 ; learning_rate : 2.766761095078608e-11 ; fit : 0.13333333333333333
    batch_size :          600
    best_batch_loss : 1.5516101920668417
    Epoch 5, Loss: 1.6252434553022326, fit: 0.15833333333333333
      batch rate adapted learning_rate :              1e-09
      loss :              1.6195073491164647
      loss_factor :              0.16195073491164647
      loss adapted learning_rate :              2.6228040538422387e-11
    epoch : 6 ; learning_rate : 2.6228040538422387e-11 ; fit : 0.15833333333333333
    batch_size :          600
    best_batch_loss : 1.5473208721320277
    Epoch 6, Loss: 1.6252434550785564, fit: 0.15833333333333333
      batch rate adapted learning_rate :              1e-09
      loss :              1.6112416245942767
      loss_factor :              0.16112416245942768
      loss adapted learning_rate :              2.5960995728252043e-11
    epoch : 7 ; learning_rate : 2.5960995728252043e-11 ; fit : 0.15833333333333333
    batch_size :          600
    best_batch_loss : 1.5387287648529386
    Epoch 7, Loss: 1.625243454565894, fit: 0.14333333333333334
      batch rate adapted learning_rate :              1e-09
      loss :              1.654826248997593
      loss_factor :              0.1654826248997593
      loss adapted learning_rate :              2.7384499143714433e-11
    epoch : 8 ; learning_rate : 2.7384499143714433e-11 ; fit : 0.14333333333333334
    batch_size :          600
    best_batch_loss : 1.5172632046700771
    Epoch 8, Loss: 1.6252434544366803, fit: 0.15833333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.6108279191562962_fit_0.15833333333333333_2024-01-03_150951
  self.fit : 0.15833333333333333
  self.loss : 1.6108279191562962
  current_accuracy : 0.1649
   Accuracy mean: 0.1649
   Accuracy mean: 0.1649
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.01
      batch rate adapted learning_rate :              1.0000000000000002e-06
    epoch : 0 ; learning_rate : 1.0000000000000002e-06 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.7361774838687385
    Epoch 0, Loss: 1.772794701984335, fit: 0.09333333333333334
    Epoch 0, Loss: 1.772794701984335
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.758894482045766_fit_0.09333333333333334_2024-01-03_150953
  self.fit : 0.09333333333333334
  self.loss : 1.758894482045766
  current_accuracy : 0.0869
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.01
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.758894482045766
      loss_factor :              0.17588944820457658
      loss adapted learning_rate :              3.0937097989710435e-08
    epoch : 0 ; learning_rate : 3.0937097989710435e-08 ; fit : 0.09333333333333334
    batch_size :          600
    best_batch_loss : 1.720012549483915
    Epoch 0, Loss: 1.7727219151571898, fit: 0.08166666666666667
    Epoch 0, Loss: 1.7727219151571898
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.782693448135131
      loss_factor :              0.17826934481351311
      loss adapted learning_rate :              3.177995930023924e-08
    epoch : 1 ; learning_rate : 3.177995930023924e-08 ; fit : 0.08166666666666667
    batch_size :          600
    best_batch_loss : 1.6870124508178042
    Epoch 1, Loss: 1.7727178122065153, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.7739946269989495
      loss_factor :              0.17739946269989496
      loss adapted learning_rate :              3.147056936621142e-08
    epoch : 2 ; learning_rate : 3.147056936621142e-08 ; fit : 0.08333333333333333
    batch_size :          600
    best_batch_loss : 1.717490660410256
    Epoch 2, Loss: 1.7727133023494552, fit: 0.06833333333333333
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.8031996613884222
      loss_factor :              0.1803199661388422
      loss adapted learning_rate :              3.251529018831321e-08
    epoch : 3 ; learning_rate : 3.251529018831321e-08 ; fit : 0.06833333333333333
    batch_size :          600
    best_batch_loss : 1.7164431835709437
    Epoch 3, Loss: 1.7727091677900495, fit: 0.06833333333333333
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.8079332449333008
      loss_factor :              0.18079332449333008
      loss adapted learning_rate :              3.268622618135055e-08
    epoch : 4 ; learning_rate : 3.268622618135055e-08 ; fit : 0.06833333333333333
    batch_size :          600
    best_batch_loss : 1.7208702342996025
    Epoch 4, Loss: 1.7727052289603034, fit: 0.07166666666666667
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.7970882904240058
      loss_factor :              0.17970882904240057
      loss adapted learning_rate :              3.229526323579076e-08
    epoch : 5 ; learning_rate : 3.229526323579076e-08 ; fit : 0.07166666666666667
    batch_size :          600
    best_batch_loss : 1.710476816226344
    Epoch 5, Loss: 1.7727008250052732, fit: 0.09
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.7587673260335377
      loss_factor :              0.17587673260335376
      loss adapted learning_rate :              3.0932625071231606e-08
    epoch : 6 ; learning_rate : 3.0932625071231606e-08 ; fit : 0.09
    batch_size :          600
    best_batch_loss : 1.7002853951489856
    Epoch 6, Loss: 1.7726965547898252, fit: 0.08666666666666667
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.7641863467608834
      loss_factor :              0.17641863467608834
      loss adapted learning_rate :              3.1123534660975124e-08
    epoch : 7 ; learning_rate : 3.1123534660975124e-08 ; fit : 0.08666666666666667
    batch_size :          600
    best_batch_loss : 1.7011173511149464
    Epoch 7, Loss: 1.7726925419960993, fit: 0.07833333333333334
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.7892182066964433
      loss_factor :              0.17892182066964432
      loss adapted learning_rate :              3.201301791174037e-08
    epoch : 8 ; learning_rate : 3.201301791174037e-08 ; fit : 0.07833333333333334
    batch_size :          600
    best_batch_loss : 1.7200818316321405
    Epoch 8, Loss: 1.772688204133405, fit: 0.10833333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7217716830550354_fit_0.10833333333333334_2024-01-03_151016
  self.fit : 0.10833333333333334
  self.loss : 1.7217716830550354
  current_accuracy : 0.0869
   Accuracy mean: 0.0869
   Accuracy mean: 0.0869
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-05
    epoch : 0 ; learning_rate : 1e-05 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6641271904325998
    Epoch 0, Loss: 1.72360912212719, fit: 0.11666666666666667
    Epoch 0, Loss: 1.72360912212719
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.713676457534187_fit_0.11666666666666667_2024-01-03_151019
  self.fit : 0.11666666666666667
  self.loss : 1.713676457534187
  current_accuracy : 0.1103
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-05
      loss :              1.713676457534187
      loss_factor :              0.1713676457534187
      loss adapted learning_rate :              2.9366870011069207e-07
    epoch : 0 ; learning_rate : 2.9366870011069207e-07 ; fit : 0.11666666666666667
    batch_size :          600
    best_batch_loss : 1.6535997828786275
    Epoch 0, Loss: 1.722821316543901, fit: 0.10666666666666667
    Epoch 0, Loss: 1.722821316543901
      batch rate adapted learning_rate :              1e-05
      loss :              1.7254148602197903
      loss_factor :              0.17254148602197902
      loss adapted learning_rate :              2.9770564398672785e-07
    epoch : 1 ; learning_rate : 2.9770564398672785e-07 ; fit : 0.10666666666666667
    batch_size :          600
    best_batch_loss : 1.6417434233336343
    Epoch 1, Loss: 1.722774714866602, fit: 0.10666666666666667
      batch rate adapted learning_rate :              1e-05
      loss :              1.7224284717654161
      loss_factor :              0.17224284717654162
      loss adapted learning_rate :              2.966759840348147e-07
    epoch : 2 ; learning_rate : 2.966759840348147e-07 ; fit : 0.10666666666666667
    batch_size :          600
    best_batch_loss : 1.643345216282028
    Epoch 2, Loss: 1.7227270886596302, fit: 0.095
      batch rate adapted learning_rate :              1e-05
      loss :              1.7396485193625184
      loss_factor :              0.17396485193625183
      loss adapted learning_rate :              3.0263769709202026e-07
    epoch : 3 ; learning_rate : 3.0263769709202026e-07 ; fit : 0.095
    batch_size :          600
    best_batch_loss : 1.6520803629286853
    Epoch 3, Loss: 1.7226778990416032, fit: 0.09833333333333333
      batch rate adapted learning_rate :              1e-05
      loss :              1.762878969719615
      loss_factor :              0.1762878969719615
      loss adapted learning_rate :              3.107742261879692e-07
    epoch : 4 ; learning_rate : 3.107742261879692e-07 ; fit : 0.09833333333333333
    batch_size :          600
    best_batch_loss : 1.6651093707630358
    Epoch 4, Loss: 1.722632488240262, fit: 0.095
      batch rate adapted learning_rate :              1e-05
      loss :              1.749135448696506
      loss_factor :              0.1749135448696506
      loss adapted learning_rate :              3.059474817886728e-07
    epoch : 5 ; learning_rate : 3.059474817886728e-07 ; fit : 0.095
    batch_size :          600
    best_batch_loss : 1.6616254166920816
    Epoch 5, Loss: 1.7225827631920236, fit: 0.12333333333333334
      batch rate adapted learning_rate :              1e-05
      loss :              1.703433288362189
      loss_factor :              0.1703433288362189
      loss adapted learning_rate :              2.9016849679004204e-07
    epoch : 6 ; learning_rate : 2.9016849679004204e-07 ; fit : 0.12333333333333334
    batch_size :          600
    best_batch_loss : 1.6620268994575207
    Epoch 6, Loss: 1.7225360662754674, fit: 0.11833333333333333
      batch rate adapted learning_rate :              1e-05
      loss :              1.7166016323562905
      loss_factor :              0.17166016323562905
      loss adapted learning_rate :              2.9467211642082813e-07
    epoch : 7 ; learning_rate : 2.9467211642082813e-07 ; fit : 0.11833333333333333
    batch_size :          600
    best_batch_loss : 1.6475917826010695
    Epoch 7, Loss: 1.7224910589891917, fit: 0.105
      batch rate adapted learning_rate :              1e-05
      loss :              1.7198070988592553
      loss_factor :              0.17198070988592554
      loss adapted learning_rate :              2.957736457286689e-07
    epoch : 8 ; learning_rate : 2.957736457286689e-07 ; fit : 0.105
    batch_size :          600
    best_batch_loss : 1.6497845679178544
    Epoch 8, Loss: 1.7224436739306455, fit: 0.14
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.6711447882286958_fit_0.14_2024-01-03_151041
  self.fit : 0.14
  self.loss : 1.6711447882286958
  current_accuracy : 0.1106
   Accuracy mean: 0.1103
   Accuracy mean: 0.1106
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.0002
    epoch : 0 ; learning_rate : 0.0002 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.675564920966005
    Epoch 0, Loss: 1.7602522719232805, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7602522719232805
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7661350418746857_fit_0.08333333333333333_2024-01-03_151044
  self.fit : 0.08333333333333333
  self.loss : 1.7661350418746857
  current_accuracy : 0.0895
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.0002
      loss :              1.7661350418746857
      loss_factor :              0.17661350418746857
      loss adapted learning_rate :              6.238465972275396e-06
    epoch : 0 ; learning_rate : 6.238465972275396e-06 ; fit : 0.08333333333333333
    batch_size :          600
    best_batch_loss : 1.6883407501677175
    Epoch 0, Loss: 1.748794129560692, fit: 0.095
    Epoch 0, Loss: 1.748794129560692
      batch rate adapted learning_rate :              0.0002
      loss :              1.7415862737599983
      loss_factor :              0.17415862737599982
      loss adapted learning_rate :              6.066245497898471e-06
    epoch : 1 ; learning_rate : 6.066245497898471e-06 ; fit : 0.095
    batch_size :          600
    best_batch_loss : 1.6905157421573143
    Epoch 1, Loss: 1.7481550180314718, fit: 0.11
      batch rate adapted learning_rate :              0.0002
      loss :              1.717793668620609
      loss_factor :              0.1717793668620609
      loss adapted learning_rate :              5.901630175906103e-06
    epoch : 2 ; learning_rate : 5.901630175906103e-06 ; fit : 0.11
    batch_size :          600
    best_batch_loss : 1.6951265128643827
    Epoch 2, Loss: 1.7475332993533774, fit: 0.10666666666666667
      batch rate adapted learning_rate :              0.0002
      loss :              1.7279114004003304
      loss_factor :              0.17279114004003304
      loss adapted learning_rate :              5.9713556152668625e-06
    epoch : 3 ; learning_rate : 5.9713556152668625e-06 ; fit : 0.10666666666666667
    batch_size :          600
    best_batch_loss : 1.6903702711910304
    Epoch 3, Loss: 1.7469059421816937, fit: 0.095
      batch rate adapted learning_rate :              0.0002
      loss :              1.753863317967852
      loss_factor :              0.1753863317967852
      loss adapted learning_rate :              6.152073076226405e-06
    epoch : 4 ; learning_rate : 6.152073076226405e-06 ; fit : 0.095
    batch_size :          600
    best_batch_loss : 1.6761515983469455
    Epoch 4, Loss: 1.746220513741091, fit: 0.09166666666666666
      batch rate adapted learning_rate :              0.0002
      loss :              1.7535772053459247
      loss_factor :              0.17535772053459248
      loss adapted learning_rate :              6.150066030217647e-06
    epoch : 5 ; learning_rate : 6.150066030217647e-06 ; fit : 0.09166666666666666
    batch_size :          600
    best_batch_loss : 1.6834574874755446
    Epoch 5, Loss: 1.7455778455065463, fit: 0.08666666666666667
      batch rate adapted learning_rate :              0.0002
      loss :              1.763117297116384
      loss_factor :              0.1763117297116384
      loss adapted learning_rate :              6.217165206781966e-06
    epoch : 6 ; learning_rate : 6.217165206781966e-06 ; fit : 0.08666666666666667
    batch_size :          600
    best_batch_loss : 1.6822425098031022
    Epoch 6, Loss: 1.7449057663178296, fit: 0.095
      batch rate adapted learning_rate :              0.0002
      loss :              1.746492190537156
      loss_factor :              0.1746492190537156
      loss adapted learning_rate :              6.1004699432145486e-06
    epoch : 7 ; learning_rate : 6.1004699432145486e-06 ; fit : 0.095
    batch_size :          600
    best_batch_loss : 1.6858338868857672
    Epoch 7, Loss: 1.7442666311829222, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0002
      loss :              1.7793187091276683
      loss_factor :              0.17793187091276683
      loss adapted learning_rate :              6.331950137303503e-06
    epoch : 8 ; learning_rate : 6.331950137303503e-06 ; fit : 0.08333333333333333
    batch_size :          600
    best_batch_loss : 1.698681401373129
    Epoch 8, Loss: 1.7435779252753476, fit: 0.08833333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.760627422772442_fit_0.08833333333333333_2024-01-03_151106
  self.fit : 0.08833333333333333
  self.loss : 1.760627422772442
  current_accuracy : 0.0914
   Accuracy mean: 0.0895
   Accuracy mean: 0.0914
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.00025
    epoch : 0 ; learning_rate : 0.00025 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6829583061530582
    Epoch 0, Loss: 1.7381457520270378, fit: 0.115
    Epoch 0, Loss: 1.7381457520270378
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7063347858479838_fit_0.115_2024-01-03_151109
  self.fit : 0.115
  self.loss : 1.7063347858479838
  current_accuracy : 0.1023
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.00025
      loss :              1.7063347858479838
      loss_factor :              0.17063347858479838
      loss adapted learning_rate :              7.2789460034872125e-06
    epoch : 0 ; learning_rate : 7.2789460034872125e-06 ; fit : 0.115
    batch_size :          600
    best_batch_loss : 1.665346799012921
    Epoch 0, Loss: 1.7329744836297363, fit: 0.10666666666666667
    Epoch 0, Loss: 1.7329744836297363
      batch rate adapted learning_rate :              0.00025
      loss :              1.7172284644768243
      loss_factor :              0.17172284644768243
      loss adapted learning_rate :              7.37218399802358e-06
    epoch : 1 ; learning_rate : 7.37218399802358e-06 ; fit : 0.10666666666666667
    batch_size :          600
    best_batch_loss : 1.6764096097657524
    Epoch 1, Loss: 1.7326017999633094, fit: 0.095
      batch rate adapted learning_rate :              0.00025
      loss :              1.7369147898571669
      loss_factor :              0.17369147898571669
      loss adapted learning_rate :              7.542182468061415e-06
    epoch : 2 ; learning_rate : 7.542182468061415e-06 ; fit : 0.095
    batch_size :          600
    best_batch_loss : 1.6722961247169978
    Epoch 2, Loss: 1.732241188667153, fit: 0.07833333333333334
      batch rate adapted learning_rate :              0.00025
      loss :              1.7754698076850688
      loss_factor :              0.1775469807685069
      loss adapted learning_rate :              7.880732595003139e-06
    epoch : 3 ; learning_rate : 7.880732595003139e-06 ; fit : 0.07833333333333334
    batch_size :          600
    best_batch_loss : 1.677011004608298
    Epoch 3, Loss: 1.7318329722776966, fit: 0.11333333333333333
      batch rate adapted learning_rate :              0.00025
      loss :              1.6992581022900477
      loss_factor :              0.16992581022900477
      loss adapted learning_rate :              7.218695245495936e-06
    epoch : 4 ; learning_rate : 7.218695245495936e-06 ; fit : 0.11333333333333333
    batch_size :          600
    best_batch_loss : 1.6556992843162985
    Epoch 4, Loss: 1.7314532960115994, fit: 0.10166666666666667
      batch rate adapted learning_rate :              0.00025
      loss :              1.733367135044682
      loss_factor :              0.1733367135044682
      loss adapted learning_rate :              7.511404062132522e-06
    epoch : 5 ; learning_rate : 7.511404062132522e-06 ; fit : 0.10166666666666667
    batch_size :          600
    best_batch_loss : 1.6721495800474853
    Epoch 5, Loss: 1.7310931992508154, fit: 0.125
      batch rate adapted learning_rate :              0.00025
      loss :              1.6843360388855155
      loss_factor :              0.16843360388855155
      loss adapted learning_rate :              7.092469729721372e-06
    epoch : 6 ; learning_rate : 7.092469729721372e-06 ; fit : 0.125
    batch_size :          600
    best_batch_loss : 1.6820285910795996
    Epoch 6, Loss: 1.7306455918503414, fit: 0.11333333333333333
      batch rate adapted learning_rate :              0.00025
      loss :              1.7078034159239155
      loss_factor :              0.17078034159239155
      loss adapted learning_rate :              7.291481268603487e-06
    epoch : 7 ; learning_rate : 7.291481268603487e-06 ; fit : 0.11333333333333333
    batch_size :          600
    best_batch_loss : 1.6566572767786356
    Epoch 7, Loss: 1.7302329818539, fit: 0.11333333333333333
      batch rate adapted learning_rate :              0.00025
      loss :              1.7090175910900007
      loss_factor :              0.17090175910900007
      loss adapted learning_rate :              7.301852816637673e-06
    epoch : 8 ; learning_rate : 7.301852816637673e-06 ; fit : 0.11333333333333333
    batch_size :          600
    best_batch_loss : 1.6714069147558936
    Epoch 8, Loss: 1.7297663040398756, fit: 0.10666666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7213566215623022_fit_0.10666666666666667_2024-01-03_151132
  self.fit : 0.10666666666666667
  self.loss : 1.7213566215623022
  current_accuracy : 0.1044
   Accuracy mean: 0.1023
   Accuracy mean: 0.1044
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.0003
    epoch : 0 ; learning_rate : 0.0003 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.7199031155976445
    Epoch 0, Loss: 1.7774069740103593, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7774069740103593
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.776972179928553_fit_0.08333333333333333_2024-01-03_151134
  self.fit : 0.08333333333333333
  self.loss : 1.776972179928553
  current_accuracy : 0.0855
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.0003
      loss :              1.776972179928553
      loss_factor :              0.1776972179928553
      loss adapted learning_rate :              9.4728903847201e-06
    epoch : 0 ; learning_rate : 9.4728903847201e-06 ; fit : 0.08333333333333333
    batch_size :          600
    best_batch_loss : 1.7006918759330256
    Epoch 0, Loss: 1.7644139296920904, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7644139296920904
      batch rate adapted learning_rate :              0.0003
      loss :              1.773258234940027
      loss_factor :              0.1773258234940027
      loss adapted learning_rate :              9.433334303347858e-06
    epoch : 1 ; learning_rate : 9.433334303347858e-06 ; fit : 0.08333333333333333
    batch_size :          600
    best_batch_loss : 1.7003763799832081
    Epoch 1, Loss: 1.7634405557910522, fit: 0.08
      batch rate adapted learning_rate :              0.0003
      loss :              1.7792037648443557
      loss_factor :              0.17792037648443557
      loss adapted learning_rate :              9.496698110508987e-06
    epoch : 2 ; learning_rate : 9.496698110508987e-06 ; fit : 0.08
    batch_size :          600
    best_batch_loss : 1.69877982843643
    Epoch 2, Loss: 1.7625156177277916, fit: 0.09166666666666666
      batch rate adapted learning_rate :              0.0003
      loss :              1.7486374213202789
      loss_factor :              0.17486374213202788
      loss adapted learning_rate :              9.173198493724902e-06
    epoch : 3 ; learning_rate : 9.173198493724902e-06 ; fit : 0.09166666666666666
    batch_size :          600
    best_batch_loss : 1.70686403938809
    Epoch 3, Loss: 1.761589607669551, fit: 0.095
      batch rate adapted learning_rate :              0.0003
      loss :              1.7524856217278875
      loss_factor :              0.17524856217278875
      loss adapted learning_rate :              9.21361756308894e-06
    epoch : 4 ; learning_rate : 9.21361756308894e-06 ; fit : 0.095
    batch_size :          600
    best_batch_loss : 1.6907020888026654
    Epoch 4, Loss: 1.7606576897277817, fit: 0.09666666666666666
      batch rate adapted learning_rate :              0.0003
      loss :              1.7398435142287894
      loss_factor :              0.17398435142287894
      loss adapted learning_rate :              9.08116636201195e-06
    epoch : 5 ; learning_rate : 9.08116636201195e-06 ; fit : 0.09666666666666666
    batch_size :          600
    best_batch_loss : 1.7110891656317893
    Epoch 5, Loss: 1.7596454260451995, fit: 0.09
      batch rate adapted learning_rate :              0.0003
      loss :              1.7504685159372537
      loss_factor :              0.17504685159372538
      loss adapted learning_rate :              9.192420075862714e-06
    epoch : 6 ; learning_rate : 9.192420075862714e-06 ; fit : 0.09
    batch_size :          600
    best_batch_loss : 1.6920848757510114
    Epoch 6, Loss: 1.7587624302280713, fit: 0.09166666666666666
      batch rate adapted learning_rate :              0.0003
      loss :              1.7441758064717503
      loss_factor :              0.17441758064717502
      loss adapted learning_rate :              9.12644773164414e-06
    epoch : 7 ; learning_rate : 9.12644773164414e-06 ; fit : 0.09166666666666666
    batch_size :          600
    best_batch_loss : 1.7065074706017866
    Epoch 7, Loss: 1.7577878804438798, fit: 0.09833333333333333
      batch rate adapted learning_rate :              0.0003
      loss :              1.734394189457365
      loss_factor :              0.1734394189457365
      loss adapted learning_rate :              9.02436961327041e-06
    epoch : 8 ; learning_rate : 9.02436961327041e-06 ; fit : 0.09833333333333333
    batch_size :          600
    best_batch_loss : 1.6785097085569445
    Epoch 8, Loss: 1.756740953679036, fit: 0.095
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7501743572810506_fit_0.095_2024-01-03_151157
  self.fit : 0.095
  self.loss : 1.7501743572810506
  current_accuracy : 0.0899
   Accuracy mean: 0.0855
   Accuracy mean: 0.0899
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.001
    epoch : 0 ; learning_rate : 0.001 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6338383993176904
    Epoch 0, Loss: 1.7342174058425612, fit: 0.14
    Epoch 0, Loss: 1.7342174058425612
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6338383993176904_fit_0.14_2024-01-03_151159
  self.fit : 0.14
  self.loss : 1.6338383993176904
  current_accuracy : 0.1244
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.001
      loss :              1.6338383993176904
      loss_factor :              0.16338383993176903
      loss adapted learning_rate :              2.6694279150849926e-05
    epoch : 0 ; learning_rate : 2.6694279150849926e-05 ; fit : 0.14
    batch_size :          600
    best_batch_loss : 1.609620849487077
    Epoch 0, Loss: 1.6716351850600035, fit: 0.13166666666666665
    Epoch 0, Loss: 1.6716351850600035
      batch rate adapted learning_rate :              0.001
      loss :              1.6706035395865468
      loss_factor :              0.16706035395865468
      loss adapted learning_rate :              2.7909161864790987e-05
    epoch : 1 ; learning_rate : 2.7909161864790987e-05 ; fit : 0.13166666666666665
    batch_size :          600
    best_batch_loss : 1.6116885152379612
    Epoch 1, Loss: 1.6681392494943328, fit: 0.13666666666666666
      batch rate adapted learning_rate :              0.001
      loss :              1.6612370863494228
      loss_factor :              0.16612370863494227
      loss adapted learning_rate :              2.7597086570627196e-05
    epoch : 2 ; learning_rate : 2.7597086570627196e-05 ; fit : 0.13666666666666666
    batch_size :          600
    best_batch_loss : 1.6045875861836254
    Epoch 2, Loss: 1.6645169147900485, fit: 0.13666666666666666
      batch rate adapted learning_rate :              0.001
      loss :              1.655903190439104
      loss_factor :              0.1655903190439104
      loss adapted learning_rate :              2.742015376106403e-05
    epoch : 3 ; learning_rate : 2.742015376106403e-05 ; fit : 0.13666666666666666
    batch_size :          600
    best_batch_loss : 1.6191634707906448
    Epoch 3, Loss: 1.660907799401754, fit: 0.12
      batch rate adapted learning_rate :              0.001
      loss :              1.683562557913479
      loss_factor :              0.1683562557913479
      loss adapted learning_rate :              2.834382886408176e-05
    epoch : 4 ; learning_rate : 2.834382886408176e-05 ; fit : 0.12
    batch_size :          600
    best_batch_loss : 1.5691495831250248
    Epoch 4, Loss: 1.6573208190847848, fit: 0.14333333333333334
      batch rate adapted learning_rate :              0.001
      loss :              1.645070124891606
      loss_factor :              0.1645070124891606
      loss adapted learning_rate :              2.7062557158108843e-05
    epoch : 5 ; learning_rate : 2.7062557158108843e-05 ; fit : 0.14333333333333334
    batch_size :          600
    best_batch_loss : 1.5779206946491584
    Epoch 5, Loss: 1.6535799430711753, fit: 0.14833333333333334
      batch rate adapted learning_rate :              0.001
      loss :              1.637852206925095
      loss_factor :              0.1637852206925095
      loss adapted learning_rate :              2.6825598517294043e-05
    epoch : 6 ; learning_rate : 2.6825598517294043e-05 ; fit : 0.14833333333333334
    batch_size :          600
    best_batch_loss : 1.5921351128468286
    Epoch 6, Loss: 1.6500418722634291, fit: 0.13833333333333334
      batch rate adapted learning_rate :              0.001
      loss :              1.6572999767885843
      loss_factor :              0.16572999767885843
      loss adapted learning_rate :              2.746643213063442e-05
    epoch : 7 ; learning_rate : 2.746643213063442e-05 ; fit : 0.13833333333333334
    batch_size :          600
    best_batch_loss : 1.5898758813424643
    Epoch 7, Loss: 1.646427564760324, fit: 0.11833333333333333
      batch rate adapted learning_rate :              0.001
      loss :              1.6952806239871712
      loss_factor :              0.16952806239871712
      loss adapted learning_rate :              2.8739763940663326e-05
    epoch : 8 ; learning_rate : 2.8739763940663326e-05 ; fit : 0.11833333333333333
    batch_size :          600
    best_batch_loss : 1.5801224916142698
    Epoch 8, Loss: 1.6426874303352341, fit: 0.16166666666666665
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6117793489530519_fit_0.16166666666666665_2024-01-03_151223
  self.fit : 0.16166666666666665
  self.loss : 1.6117793489530519
  current_accuracy : 0.141
   Accuracy mean: 0.1244
   Accuracy mean: 0.141
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.009000000000000001
    epoch : 0 ; learning_rate : 0.009000000000000001 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 0.9650896438489857
    Epoch 0, Loss: 1.2932854672171823, fit: 0.475
    Epoch 0, Loss: 1.2932854672171823
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.9844810449011565_fit_0.475_2024-01-03_151225
  self.fit : 0.475
  self.loss : 0.9844810449011565
  current_accuracy : 0.4843
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              0.9844810449011565
      loss_factor :              0.09844810449011565
      loss adapted learning_rate :              8.722826349927058e-05
    epoch : 0 ; learning_rate : 8.722826349927058e-05 ; fit : 0.475
    batch_size :          600
    best_batch_loss : 0.8824671774614787
    Epoch 0, Loss: 0.9881818456519762, fit: 0.4816666666666667
    Epoch 0, Loss: 0.9881818456519762
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              0.9820421497668637
      loss_factor :              0.09820421497668637
      loss adapted learning_rate :              8.679661055268508e-05
    epoch : 1 ; learning_rate : 8.679661055268508e-05 ; fit : 0.4816666666666667
    batch_size :          600
    best_batch_loss : 0.9017243603766656
    Epoch 1, Loss: 0.9846852322229191, fit: 0.45
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              1.0541857628142701
      loss_factor :              0.105418576281427
      loss adapted learning_rate :              0.00010001768602682742
    epoch : 2 ; learning_rate : 0.00010001768602682742 ; fit : 0.45
    batch_size :          600
    best_batch_loss : 0.8825896054447439
    Epoch 2, Loss: 0.9811057330472477, fit: 0.52
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              0.914726862530416
      loss_factor :              0.0914726862530416
      loss adapted learning_rate :              7.530527097312648e-05
    epoch : 3 ; learning_rate : 7.530527097312648e-05 ; fit : 0.52
    batch_size :          600
    best_batch_loss : 0.8768056959494409
    Epoch 3, Loss: 0.9776180245601059, fit: 0.49333333333333335
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              0.9529537254729613
      loss_factor :              0.09529537254729613
      loss adapted learning_rate :              8.173087226035165e-05
    epoch : 4 ; learning_rate : 8.173087226035165e-05 ; fit : 0.49333333333333335
    batch_size :          600
    best_batch_loss : 0.8576694831681457
    Epoch 4, Loss: 0.9746132341159074, fit: 0.475
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              1.004457879771823
      loss_factor :              0.10044578797718232
      loss adapted learning_rate :              9.080420690121359e-05
    epoch : 5 ; learning_rate : 9.080420690121359e-05 ; fit : 0.475
    batch_size :          600
    best_batch_loss : 0.8549465684833901
    Epoch 5, Loss: 0.9712620080473132, fit: 0.485
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              0.9694358711801594
      loss_factor :              0.09694358711801594
      loss adapted learning_rate :              8.458253174977512e-05
    epoch : 6 ; learning_rate : 8.458253174977512e-05 ; fit : 0.485
    batch_size :          600
    best_batch_loss : 0.8436503634714753
    Epoch 6, Loss: 0.9679672138140133, fit: 0.45666666666666667
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              1.0122069435055485
      loss_factor :              0.10122069435055485
      loss adapted learning_rate :              9.221066068327603e-05
    epoch : 7 ; learning_rate : 9.221066068327603e-05 ; fit : 0.45666666666666667
    batch_size :          600
    best_batch_loss : 0.8882019677492997
    Epoch 7, Loss: 0.9647024847535455, fit: 0.5083333333333333
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              0.9306424256112901
      loss_factor :              0.09306424256112901
      loss adapted learning_rate :              7.794857919128992e-05
    epoch : 8 ; learning_rate : 7.794857919128992e-05 ; fit : 0.5083333333333333
    batch_size :          600
    best_batch_loss : 0.8575877060006587
    Epoch 8, Loss: 0.9614816522345753, fit: 0.5166666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.9168685593863468_fit_0.5166666666666667_2024-01-03_151248
  self.fit : 0.5166666666666667
  self.loss : 0.9168685593863468
  current_accuracy : 0.5011
   Accuracy mean: 0.4843
   Accuracy mean: 0.5011
  Results saved to test_combinations_results20240103151248
  normalized_accuracies :      [0.05197305 0.05197305 0.19104909 0.19104909 0.00336862 0.00336862
   0.05967276 0.06039461 0.00962464 0.01419634 0.04042348 0.04547642
   0.         0.0105871  0.09359962 0.13354187 0.95957652 1.        ]
batch_rate :  0.001
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-13
    epoch : 0 ; learning_rate : 1e-13 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5205833686374919
    Epoch 0, Loss: 1.7640871111658643, fit: 0.13333333333333333
    Epoch 0, Loss: 1.7640871111658643
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.6960093161201633_fit_0.13333333333333333_2024-01-03_151257
  self.fit : 0.13333333333333333
  self.loss : 1.6960093161201633
  current_accuracy : 0.0833
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-13
      loss :              1.6960093161201633
      loss_factor :              0.16960093161201634
      loss adapted learning_rate :              2.876447600366384e-15
    epoch : 0 ; learning_rate : 2.876447600366384e-15 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.530370860804139
    Epoch 0, Loss: 1.76408711112657, fit: 0.05
    Epoch 0, Loss: 1.76408711112657
      batch rate adapted learning_rate :              1e-13
      loss :              1.7820104506681502
      loss_factor :              0.17820104506681503
      loss adapted learning_rate :              3.1755612462905043e-15
    epoch : 1 ; learning_rate : 3.1755612462905043e-15 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5111375649602639
    Epoch 1, Loss: 1.7640871111242258, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-13
      loss :              1.759149592569431
      loss_factor :              0.1759149592569431
      loss adapted learning_rate :              3.094607289037195e-15
    epoch : 2 ; learning_rate : 3.094607289037195e-15 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4291709210386192
    Epoch 2, Loss: 1.7640871111213774, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1e-13
      loss :              1.8150763475659228
      loss_factor :              0.1815076347565923
      loss adapted learning_rate :              3.2945021474932507e-15
    epoch : 3 ; learning_rate : 3.2945021474932507e-15 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4779930010718743
    Epoch 3, Loss: 1.7640871111187608, fit: 0.1
      batch rate adapted learning_rate :              1e-13
      loss :              1.7207723452652497
      loss_factor :              0.17207723452652496
      loss adapted learning_rate :              2.9610574642296676e-15
    epoch : 4 ; learning_rate : 2.9610574642296676e-15 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4612796264402712
    Epoch 4, Loss: 1.7640871111158736, fit: 0.1
      batch rate adapted learning_rate :              1e-13
      loss :              1.7435367287360173
      loss_factor :              0.17435367287360173
      loss adapted learning_rate :              3.0399203244514926e-15
    epoch : 5 ; learning_rate : 3.0399203244514926e-15 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5074729233463204
    Epoch 5, Loss: 1.764087111113578, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1e-13
      loss :              1.8432093844831727
      loss_factor :              0.18432093844831726
      loss adapted learning_rate :              3.3974208350468364e-15
    epoch : 6 ; learning_rate : 3.3974208350468364e-15 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4974473084792104
    Epoch 6, Loss: 1.7640871111108443, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-13
      loss :              1.6682614478730693
      loss_factor :              0.16682614478730692
      loss adapted learning_rate :              2.7830962584595496e-15
    epoch : 7 ; learning_rate : 2.7830962584595496e-15 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4497054868230885
    Epoch 7, Loss: 1.7640871111081633, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1e-13
      loss :              1.7701115400021241
      loss_factor :              0.17701115400021242
      loss adapted learning_rate :              3.133294864048692e-15
    epoch : 8 ; learning_rate : 3.133294864048692e-15 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.476073832750618
    Epoch 8, Loss: 1.764087111105723, fit: 0.08333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7985546501900234_fit_0.08333333333333333_2024-01-03_151359
  self.fit : 0.08333333333333333
  self.loss : 1.7985546501900234
  current_accuracy : 0.0833
   Accuracy mean: 0.0833
   Accuracy mean: 0.0833
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-10
    epoch : 0 ; learning_rate : 1e-10 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4471828364959065
    Epoch 0, Loss: 1.7231106711744806, fit: 0.1
    Epoch 0, Loss: 1.7231106711744806
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.747065450831782_fit_0.1_2024-01-03_151407
  self.fit : 0.1
  self.loss : 1.747065450831782
  current_accuracy : 0.1133
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-10
      loss :              1.747065450831782
      loss_factor :              0.1747065450831782
      loss adapted learning_rate :              3.052237689490058e-12
    epoch : 0 ; learning_rate : 3.052237689490058e-12 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.445515142881965
    Epoch 0, Loss: 1.7231105922530316, fit: 0.18333333333333332
    Epoch 0, Loss: 1.7231105922530316
      batch rate adapted learning_rate :              1e-10
      loss :              1.54459004065917
      loss_factor :              0.154459004065917
      loss adapted learning_rate :              2.3857583937034967e-12
    epoch : 1 ; learning_rate : 2.3857583937034967e-12 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.435805405492176
    Epoch 1, Loss: 1.7231105882733457, fit: 0.016666666666666666
      batch rate adapted learning_rate :              1e-10
      loss :              1.9039742283045518
      loss_factor :              0.1903974228304552
      loss adapted learning_rate :              3.6251178620479137e-12
    epoch : 2 ; learning_rate : 3.6251178620479137e-12 ; fit : 0.016666666666666666
    batch_size :          60
    best_batch_loss : 1.428139525609825
    Epoch 2, Loss: 1.723110583622402, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss :              1.6936537506801217
      loss_factor :              0.16936537506801216
      loss adapted learning_rate :              2.868463027192844e-12
    epoch : 3 ; learning_rate : 2.868463027192844e-12 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4323813379065655
    Epoch 3, Loss: 1.7231105785471068, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-10
      loss :              1.7311107151213283
      loss_factor :              0.17311107151213284
      loss adapted learning_rate :              2.996744308007877e-12
    epoch : 4 ; learning_rate : 2.996744308007877e-12 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4411399148805826
    Epoch 4, Loss: 1.7231105741812593, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss :              1.7080151528287346
      loss_factor :              0.17080151528287346
      loss adapted learning_rate :              2.9173157622925658e-12
    epoch : 5 ; learning_rate : 2.9173157622925658e-12 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4430726694683103
    Epoch 5, Loss: 1.723110569328578, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss :              1.7896144767431628
      loss_factor :              0.17896144767431627
      loss adapted learning_rate :              3.2027199753687035e-12
    epoch : 6 ; learning_rate : 3.2027199753687035e-12 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4186821550324438
    Epoch 6, Loss: 1.723110564660596, fit: 0.21666666666666667
      batch rate adapted learning_rate :              1e-10
      loss :              1.5380477435573676
      loss_factor :              0.15380477435573675
      loss adapted learning_rate :              2.36559086146191e-12
    epoch : 7 ; learning_rate : 2.36559086146191e-12 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.3886997106524424
    Epoch 7, Loss: 1.723110560253476, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1e-10
      loss :              1.782163829133163
      loss_factor :              0.1782163829133163
      loss adapted learning_rate :              3.1761079138705776e-12
    epoch : 8 ; learning_rate : 3.1761079138705776e-12 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4070985255361246
    Epoch 8, Loss: 1.723110556047809, fit: 0.06666666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7674664821131283_fit_0.06666666666666667_2024-01-03_151510
  self.fit : 0.06666666666666667
  self.loss : 1.7674664821131283
  current_accuracy : 0.1133
   Accuracy mean: 0.1133
   Accuracy mean: 0.1133
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-07
    epoch : 0 ; learning_rate : 1.0000000000000001e-07 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4496930651625912
    Epoch 0, Loss: 1.7375296054095934, fit: 0.15
    Epoch 0, Loss: 1.7375296054095934
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6318878623788085_fit_0.15_2024-01-03_151517
  self.fit : 0.15
  self.loss : 1.6318878623788085
  current_accuracy : 0.1035
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.6318878623788085
      loss_factor :              0.16318878623788086
      loss adapted learning_rate :              2.6630579953792776e-09
    epoch : 0 ; learning_rate : 2.6630579953792776e-09 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4520587287709363
    Epoch 0, Loss: 1.7374824686174681, fit: 0.18333333333333332
    Epoch 0, Loss: 1.7374824686174681
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.5853446318100832
      loss_factor :              0.15853446318100833
      loss adapted learning_rate :              2.513317601609049e-09
    epoch : 1 ; learning_rate : 2.513317601609049e-09 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.4525264944050003
    Epoch 1, Loss: 1.7374801422699073, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.7250445635699156
      loss_factor :              0.17250445635699158
      loss adapted learning_rate :              2.9757787463021216e-09
    epoch : 2 ; learning_rate : 2.9757787463021216e-09 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.497238999331475
    Epoch 2, Loss: 1.7374775658062653, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.7623859509699698
      loss_factor :              0.17623859509699696
      loss adapted learning_rate :              3.106004240176325e-09
    epoch : 3 ; learning_rate : 3.106004240176325e-09 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4620262303631661
    Epoch 3, Loss: 1.7374750365240095, fit: 0.05
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.8335310589572862
      loss_factor :              0.18335310589572862
      loss adapted learning_rate :              3.3618361441610277e-09
    epoch : 4 ; learning_rate : 3.3618361441610277e-09 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4459570597833793
    Epoch 4, Loss: 1.7374721290187676, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.815060402718722
      loss_factor :              0.1815060402718722
      loss adapted learning_rate :              3.2944442655174498e-09
    epoch : 5 ; learning_rate : 3.2944442655174498e-09 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4205056296734166
    Epoch 5, Loss: 1.737469327607248, fit: 0.2
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.5554103907387875
      loss_factor :              0.15554103907387876
      loss adapted learning_rate :              2.419301483618188e-09
    epoch : 6 ; learning_rate : 2.419301483618188e-09 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.3881618543771899
    Epoch 6, Loss: 1.737466779013318, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.7069277182800893
      loss_factor :              0.17069277182800893
      loss adapted learning_rate :              2.9136022354328724e-09
    epoch : 7 ; learning_rate : 2.9136022354328724e-09 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4172765107810865
    Epoch 7, Loss: 1.7374645053517173, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.7379354987833413
      loss_factor :              0.17379354987833412
      loss adapted learning_rate :              3.020419797931301e-09
    epoch : 8 ; learning_rate : 3.020419797931301e-09 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4656899623748516
    Epoch 8, Loss: 1.7374620207042093, fit: 0.1
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7604083836258055_fit_0.1_2024-01-03_151620
  self.fit : 0.1
  self.loss : 1.7604083836258055
  current_accuracy : 0.1034
   Accuracy mean: 0.1035
   Accuracy mean: 0.1034
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-06
    epoch : 0 ; learning_rate : 1e-06 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5098050144422908
    Epoch 0, Loss: 1.7430330023952452, fit: 0.06666666666666667
    Epoch 0, Loss: 1.7430330023952452
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.813425949159874_fit_0.06666666666666667_2024-01-03_151627
  self.fit : 0.06666666666666667
  self.loss : 1.813425949159874
  current_accuracy : 0.0952
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-06
      loss :              1.813425949159874
      loss_factor :              0.18134259491598742
      loss adapted learning_rate :              3.2885136730863904e-08
    epoch : 0 ; learning_rate : 3.2885136730863904e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4726237802498923
    Epoch 0, Loss: 1.7427561590451923, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7427561590451923
      batch rate adapted learning_rate :              1e-06
      loss :              1.750703472728624
      loss_factor :              0.1750703472728624
      loss adapted learning_rate :              3.0649626494240634e-08
    epoch : 1 ; learning_rate : 3.0649626494240634e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4674012899383884
    Epoch 1, Loss: 1.7427398208188611, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-06
      loss :              1.7806311002410797
      loss_factor :              0.17806311002410796
      loss adapted learning_rate :              3.170647115145757e-08
    epoch : 2 ; learning_rate : 3.170647115145757e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.435147384606182
    Epoch 2, Loss: 1.742724412996445, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1e-06
      loss :              1.779916645341237
      loss_factor :              0.17799166453412368
      loss adapted learning_rate :              3.1681032643628025e-08
    epoch : 3 ; learning_rate : 3.1681032643628025e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4882876032561139
    Epoch 3, Loss: 1.742706268321564, fit: 0.1
      batch rate adapted learning_rate :              1e-06
      loss :              1.7623969266595563
      loss_factor :              0.17623969266595563
      loss adapted learning_rate :              3.106042927099049e-08
    epoch : 4 ; learning_rate : 3.106042927099049e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4827546516278283
    Epoch 4, Loss: 1.7426914260406585, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-06
      loss :              1.694864262278741
      loss_factor :              0.1694864262278741
      loss adapted learning_rate :              2.8725648675496608e-08
    epoch : 5 ; learning_rate : 2.8725648675496608e-08 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5043646090223264
    Epoch 5, Loss: 1.7426745925741922, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-06
      loss :              1.762863335012532
      loss_factor :              0.1762863335012532
      loss adapted learning_rate :              3.107687137931506e-08
    epoch : 6 ; learning_rate : 3.107687137931506e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4282509201558145
    Epoch 6, Loss: 1.742660240476155, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1e-06
      loss :              1.6818606155293117
      loss_factor :              0.16818606155293117
      loss adapted learning_rate :              2.828655130068635e-08
    epoch : 7 ; learning_rate : 2.828655130068635e-08 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.475594171713204
    Epoch 7, Loss: 1.7426449673786477, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-06
      loss :              1.7784566401765654
      loss_factor :              0.17784566401765653
      loss adapted learning_rate :              3.162908020988117e-08
    epoch : 8 ; learning_rate : 3.162908020988117e-08 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4768779605989044
    Epoch 8, Loss: 1.7426294602673937, fit: 0.11666666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.728494124884228_fit_0.11666666666666667_2024-01-03_151731
  self.fit : 0.11666666666666667
  self.loss : 1.728494124884228
  current_accuracy : 0.0952
   Accuracy mean: 0.0952
   Accuracy mean: 0.0952
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.001
      batch rate adapted learning_rate :              2e-05
    epoch : 0 ; learning_rate : 2e-05 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.551176994841982
    Epoch 0, Loss: 1.7926644757010048, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7926644757010048
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.769306269831685_fit_0.08333333333333333_2024-01-03_151738
  self.fit : 0.08333333333333333
  self.loss : 1.769306269831685
  current_accuracy : 0.0742
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.001
      batch rate adapted learning_rate :              2e-05
      loss :              1.769306269831685
      loss_factor :              0.1769306269831685
      loss adapted learning_rate :              6.260889352931423e-07
    epoch : 0 ; learning_rate : 6.260889352931423e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.51746850321752
    Epoch 0, Loss: 1.7849870528666127, fit: 0.1
    Epoch 0, Loss: 1.7849870528666127
      batch rate adapted learning_rate :              2e-05
      loss :              1.7558365429304004
      loss_factor :              0.17558365429304004
      loss adapted learning_rate :              6.165923930979561e-07
    epoch : 1 ; learning_rate : 6.165923930979561e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5420806427698563
    Epoch 1, Loss: 1.784344314537962, fit: 0.03333333333333333
      batch rate adapted learning_rate :              2e-05
      loss :              1.909373540564231
      loss_factor :              0.1909373540564231
      loss adapted learning_rate :              7.291414634813575e-07
    epoch : 2 ; learning_rate : 7.291414634813575e-07 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.52833973302983
    Epoch 2, Loss: 1.783742012118329, fit: 0.1
      batch rate adapted learning_rate :              2e-05
      loss :              1.7423071262694032
      loss_factor :              0.17423071262694032
      loss adapted learning_rate :              6.071268244498292e-07
    epoch : 3 ; learning_rate : 6.071268244498292e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5361799843939432
    Epoch 3, Loss: 1.7830572005285314, fit: 0.1
      batch rate adapted learning_rate :              2e-05
      loss :              1.7739526935173535
      loss_factor :              0.17739526935173536
      loss adapted learning_rate :              6.293816317674949e-07
    epoch : 4 ; learning_rate : 6.293816317674949e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5281509349141327
    Epoch 4, Loss: 1.7825573118931846, fit: 0.11666666666666667
      batch rate adapted learning_rate :              2e-05
      loss :              1.7227018902459443
      loss_factor :              0.17227018902459443
      loss adapted learning_rate :              5.935403605313899e-07
    epoch : 5 ; learning_rate : 5.935403605313899e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5284966538922233
    Epoch 5, Loss: 1.7819878410801107, fit: 0.05
      batch rate adapted learning_rate :              2e-05
      loss :              1.8332752841993238
      loss_factor :              0.18332752841993238
      loss adapted learning_rate :              6.721796535312223e-07
    epoch : 6 ; learning_rate : 6.721796535312223e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4902492358289972
    Epoch 6, Loss: 1.781453625384167, fit: 0.06666666666666667
      batch rate adapted learning_rate :              2e-05
      loss :              1.7924439375768997
      loss_factor :              0.17924439375768997
      loss adapted learning_rate :              6.425710538712363e-07
    epoch : 7 ; learning_rate : 6.425710538712363e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5275166537889833
    Epoch 7, Loss: 1.7809354425864834, fit: 0.016666666666666666
      batch rate adapted learning_rate :              2e-05
      loss :              1.9141492421762976
      loss_factor :              0.19141492421762976
      loss adapted learning_rate :              7.327934642648189e-07
    epoch : 8 ; learning_rate : 7.327934642648189e-07 ; fit : 0.016666666666666666
    batch_size :          60
    best_batch_loss : 1.5314145567539859
    Epoch 8, Loss: 1.7804031882718487, fit: 0.11666666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7003556771768453_fit_0.11666666666666667_2024-01-03_151840
  self.fit : 0.11666666666666667
  self.loss : 1.7003556771768453
  current_accuracy : 0.0756
   Accuracy mean: 0.0742
   Accuracy mean: 0.0756
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.001
      batch rate adapted learning_rate :              2.5e-05
    epoch : 0 ; learning_rate : 2.5e-05 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5013828263798383
    Epoch 0, Loss: 1.7612880774146833, fit: 0.1
    Epoch 0, Loss: 1.7612880774146833
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.713830559519632_fit_0.1_2024-01-03_151847
  self.fit : 0.1
  self.loss : 1.713830559519632
  current_accuracy : 0.0941
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.001
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.713830559519632
      loss_factor :              0.17138305595196318
      loss adapted learning_rate :              7.343037966858435e-07
    epoch : 0 ; learning_rate : 7.343037966858435e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4748642185705265
    Epoch 0, Loss: 1.7392894196988495, fit: 0.15
    Epoch 0, Loss: 1.7392894196988495
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.6180342413581186
      loss_factor :              0.16180342413581186
      loss adapted learning_rate :              6.545087015518357e-07
    epoch : 1 ; learning_rate : 6.545087015518357e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4951602001629885
    Epoch 1, Loss: 1.7382456730469422, fit: 0.05
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.793124852149783
      loss_factor :              0.17931248521497828
      loss adapted learning_rate :              8.03824183849295e-07
    epoch : 2 ; learning_rate : 8.03824183849295e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4553255057595365
    Epoch 2, Loss: 1.7371459298604894, fit: 0.08333333333333333
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.7548889565765406
      loss_factor :              0.17548889565765408
      loss adapted learning_rate :              7.69908812478575e-07
    epoch : 3 ; learning_rate : 7.69908812478575e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4577010779929396
    Epoch 3, Loss: 1.735972234630266, fit: 0.08333333333333333
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.7730238337884967
      loss_factor :              0.17730238337884968
      loss adapted learning_rate :              7.859033787955149e-07
    epoch : 4 ; learning_rate : 7.859033787955149e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.468165168861195
    Epoch 4, Loss: 1.7347647477283832, fit: 0.06666666666666667
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.786289226550322
      loss_factor :              0.17862892265503222
      loss adapted learning_rate :              7.977073002224371e-07
    epoch : 5 ; learning_rate : 7.977073002224371e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.416662859686675
    Epoch 5, Loss: 1.7335447695234374, fit: 0.11666666666666667
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.7252222263588228
      loss_factor :              0.1725222226358823
      loss adapted learning_rate :              7.440979325806234e-07
    epoch : 6 ; learning_rate : 7.440979325806234e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4677622660196215
    Epoch 6, Loss: 1.7323944951859391, fit: 0.11666666666666667
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.679523420147155
      loss_factor :              0.16795234201471548
      loss adapted learning_rate :              7.051997297056992e-07
    epoch : 7 ; learning_rate : 7.051997297056992e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4720881838585362
    Epoch 7, Loss: 1.731283771410379, fit: 0.11666666666666667
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.7285306425979567
      loss_factor :              0.17285306425979569
      loss adapted learning_rate :              7.469545456000265e-07
    epoch : 8 ; learning_rate : 7.469545456000265e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4383476322372692
    Epoch 8, Loss: 1.7301861414922803, fit: 0.11666666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.6722106985641816_fit_0.11666666666666667_2024-01-03_151949
  self.fit : 0.11666666666666667
  self.loss : 1.6722106985641816
  current_accuracy : 0.0982
   Accuracy mean: 0.0941
   Accuracy mean: 0.0982
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              3e-05
    epoch : 0 ; learning_rate : 3e-05 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.378665390869638
    Epoch 0, Loss: 1.6930610108761235, fit: 0.13333333333333333
    Epoch 0, Loss: 1.6930610108761235
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6837954688314762_fit_0.13333333333333333_2024-01-03_151956
  self.fit : 0.13333333333333333
  self.loss : 1.6837954688314762
  current_accuracy : 0.1443
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              3e-05
      loss :              1.6837954688314762
      loss_factor :              0.16837954688314763
      loss adapted learning_rate :              8.505501542572232e-07
    epoch : 0 ; learning_rate : 8.505501542572232e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.3577837232854022
    Epoch 0, Loss: 1.663366537392816, fit: 0.13333333333333333
    Epoch 0, Loss: 1.663366537392816
      batch rate adapted learning_rate :              3e-05
      loss :              1.6726533671950417
      loss_factor :              0.16726533671950417
      loss adapted learning_rate :              8.393307860366733e-07
    epoch : 1 ; learning_rate : 8.393307860366733e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.3904581605103263
    Epoch 1, Loss: 1.661913692123928, fit: 0.15
      batch rate adapted learning_rate :              3e-05
      loss :              1.638852668468562
      loss_factor :              0.1638852668468562
      loss adapted learning_rate :              8.05751420683958e-07
    epoch : 2 ; learning_rate : 8.05751420683958e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.375805259697689
    Epoch 2, Loss: 1.6604251726689994, fit: 0.13333333333333333
      batch rate adapted learning_rate :              3e-05
      loss :              1.6760926201925175
      loss_factor :              0.16760926201925175
      loss adapted learning_rate :              8.427859414391456e-07
    epoch : 3 ; learning_rate : 8.427859414391456e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.374253678601595
    Epoch 3, Loss: 1.6589501801416258, fit: 0.11666666666666667
      batch rate adapted learning_rate :              3e-05
      loss :              1.6977350675477971
      loss_factor :              0.1697735067547797
      loss adapted learning_rate :              8.646913078744569e-07
    epoch : 4 ; learning_rate : 8.646913078744569e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4086232104951142
    Epoch 4, Loss: 1.6574759242724004, fit: 0.2
      batch rate adapted learning_rate :              3e-05
      loss :              1.5509391100420873
      loss_factor :              0.15509391100420872
      loss adapted learning_rate :              7.216236369174424e-07
    epoch : 5 ; learning_rate : 7.216236369174424e-07 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.3761508019932573
    Epoch 5, Loss: 1.6560706893617234, fit: 0.15
      batch rate adapted learning_rate :              3e-05
      loss :              1.625739301468679
      loss_factor :              0.16257393014686788
      loss adapted learning_rate :              7.929084829019603e-07
    epoch : 6 ; learning_rate : 7.929084829019603e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.3410404136637546
    Epoch 6, Loss: 1.6547637490568299, fit: 0.16666666666666666
      batch rate adapted learning_rate :              3e-05
      loss :              1.581711577751974
      loss_factor :              0.15817115777519739
      loss adapted learning_rate :              7.505434545583915e-07
    epoch : 7 ; learning_rate : 7.505434545583915e-07 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.348089875000468
    Epoch 7, Loss: 1.6533990309085072, fit: 0.16666666666666666
      batch rate adapted learning_rate :              3e-05
      loss :              1.6174481891197958
      loss_factor :              0.1617448189119796
      loss adapted learning_rate :              7.848415933460721e-07
    epoch : 8 ; learning_rate : 7.848415933460721e-07 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.2700218042996523
    Epoch 8, Loss: 1.6520692981526615, fit: 0.1
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7050195725614412_fit_0.1_2024-01-03_152059
  self.fit : 0.1
  self.loss : 1.7050195725614412
  current_accuracy : 0.1495
   Accuracy mean: 0.1443
   Accuracy mean: 0.1495
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4476200140718256
    Epoch 0, Loss: 1.749678182639496, fit: 0.11666666666666667
    Epoch 0, Loss: 1.749678182639496
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7108873509638225_fit_0.11666666666666667_2024-01-03_152106
  self.fit : 0.11666666666666667
  self.loss : 1.7108873509638225
  current_accuracy : 0.1141
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0001
      loss :              1.7108873509638225
      loss_factor :              0.17108873509638226
      loss adapted learning_rate :              2.9271355276880064e-06
    epoch : 0 ; learning_rate : 2.9271355276880064e-06 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.3984027748246628
    Epoch 0, Loss: 1.7058502230349357, fit: 0.2
    Epoch 0, Loss: 1.7058502230349357
      batch rate adapted learning_rate :              0.0001
      loss :              1.547280162674628
      loss_factor :              0.1547280162674628
      loss adapted learning_rate :              2.3940759018064234e-06
    epoch : 1 ; learning_rate : 2.3940759018064234e-06 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.3487702875744996
    Epoch 1, Loss: 1.703723371188585, fit: 0.15
      batch rate adapted learning_rate :              0.0001
      loss :              1.63811841399674
      loss_factor :              0.163811841399674
      loss adapted learning_rate :              2.683431938275195e-06
    epoch : 2 ; learning_rate : 2.683431938275195e-06 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4253353545282197
    Epoch 2, Loss: 1.7015919324666817, fit: 0.0
      batch rate adapted learning_rate :              0.0001
      loss :              1.8986600144576642
      loss_factor :              0.18986600144576643
      loss adapted learning_rate :              3.6049098505003787e-06
    epoch : 3 ; learning_rate : 3.6049098505003787e-06 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4533209332320198
    Epoch 3, Loss: 1.6989452222560748, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.0001
      loss :              1.6283601049439929
      loss_factor :              0.16283601049439927
      loss adapted learning_rate :              2.6515566313732113e-06
    epoch : 4 ; learning_rate : 2.6515566313732113e-06 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.359345298244825
    Epoch 4, Loss: 1.6961583912237406, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss :              1.735894158546207
      loss_factor :              0.1735894158546207
      loss adapted learning_rate :              3.0133285296748443e-06
    epoch : 5 ; learning_rate : 3.0133285296748443e-06 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.390960362509649
    Epoch 5, Loss: 1.693547838628389, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.0001
      loss :              1.7700405722643144
      loss_factor :              0.17700405722643145
      loss adapted learning_rate :              3.133043627461782e-06
    epoch : 6 ; learning_rate : 3.133043627461782e-06 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4255613930978015
    Epoch 6, Loss: 1.6907520650791228, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss :              1.7203349721709391
      loss_factor :              0.1720334972170939
      loss adapted learning_rate :              2.9595524164743857e-06
    epoch : 7 ; learning_rate : 2.9595524164743857e-06 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.3928824231827803
    Epoch 7, Loss: 1.6877846402991445, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0001
      loss :              1.6674260534597336
      loss_factor :              0.16674260534597335
      loss adapted learning_rate :              2.780309643756302e-06
    epoch : 8 ; learning_rate : 2.780309643756302e-06 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.33583270943065
    Epoch 8, Loss: 1.6850817518199597, fit: 0.16666666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.643157089126811_fit_0.16666666666666666_2024-01-03_152208
  self.fit : 0.16666666666666666
  self.loss : 1.643157089126811
  current_accuracy : 0.1273
   Accuracy mean: 0.1141
   Accuracy mean: 0.1273
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0009000000000000001
    epoch : 0 ; learning_rate : 0.0009000000000000001 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 0.7280732518166341
    Epoch 0, Loss: 1.3142668844629963, fit: 0.5166666666666667
    Epoch 0, Loss: 1.3142668844629963
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.9182219252921456_fit_0.5166666666666667_2024-01-03_152215
  self.fit : 0.5166666666666667
  self.loss : 0.9182219252921456
  current_accuracy : 0.4778
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              0.9182219252921456
      loss_factor :              0.09182219252921456
      loss adapted learning_rate :              7.588183536784932e-06
    epoch : 0 ; learning_rate : 7.588183536784932e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.6064055093834948
    Epoch 0, Loss: 0.9917138444382865, fit: 0.55
    Epoch 0, Loss: 0.9917138444382865
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              0.8378137296339003
      loss_factor :              0.08378137296339003
      loss adapted learning_rate :              6.317386610067596e-06
    epoch : 1 ; learning_rate : 6.317386610067596e-06 ; fit : 0.55
    batch_size :          60
    best_batch_loss : 0.6258917387263342
    Epoch 1, Loss: 0.988431715480382, fit: 0.48333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              0.9885810489711738
      loss_factor :              0.09885810489711738
      loss adapted learning_rate :              8.795632413464519e-06
    epoch : 2 ; learning_rate : 8.795632413464519e-06 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.5478280536427006
    Epoch 2, Loss: 0.9848761616714017, fit: 0.35
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              1.19910131968066
      loss_factor :              0.11991013196806599
      loss adapted learning_rate :              1.2940595773739102e-05
    epoch : 3 ; learning_rate : 1.2940595773739102e-05 ; fit : 0.35
    batch_size :          60
    best_batch_loss : 0.6037065775294218
    Epoch 3, Loss: 0.979841599403908, fit: 0.5166666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              0.9540303213757204
      loss_factor :              0.09540303213757204
      loss adapted learning_rate :              8.191564686938343e-06
    epoch : 4 ; learning_rate : 8.191564686938343e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.5994702430231824
    Epoch 4, Loss: 0.9749043746848978, fit: 0.48333333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              0.9984479518895113
      loss_factor :              0.09984479518895113
      loss adapted learning_rate :              8.972084813691241e-06
    epoch : 5 ; learning_rate : 8.972084813691241e-06 ; fit : 0.48333333333333334
    batch_size :          60
    best_batch_loss : 0.6186685751572222
    Epoch 5, Loss: 0.9710425367893455, fit: 0.5833333333333334
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              0.8284261970704746
      loss_factor :              0.08284261970704745
      loss adapted learning_rate :              6.176609675933839e-06
    epoch : 6 ; learning_rate : 6.176609675933839e-06 ; fit : 0.5833333333333334
    batch_size :          60
    best_batch_loss : 0.5755794833226192
    Epoch 6, Loss: 0.9676333845192046, fit: 0.45
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              1.0384018179083259
      loss_factor :              0.10384018179083258
      loss adapted learning_rate :              9.704505018917844e-06
    epoch : 7 ; learning_rate : 9.704505018917844e-06 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.600729984961122
    Epoch 7, Loss: 0.9641488575958881, fit: 0.55
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              0.8400736992101305
      loss_factor :              0.08400736992101306
      loss adapted learning_rate :              6.351514380941337e-06
    epoch : 8 ; learning_rate : 6.351514380941337e-06 ; fit : 0.55
    batch_size :          60
    best_batch_loss : 0.5476739185618356
    Epoch 8, Loss: 0.9606812244063461, fit: 0.55
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.8108571873082501_fit_0.55_2024-01-03_152317
  self.fit : 0.55
  self.loss : 0.8108571873082501
  current_accuracy : 0.4952
   Accuracy mean: 0.4778
   Accuracy mean: 0.4952
  Results saved to test_combinations_results20240103152317
  normalized_accuracies :      [0.0216152  0.0216152  0.09287411 0.09287411 0.0695962  0.06935867
   0.04988124 0.04988124 0.         0.00332542 0.04726841 0.05700713
   0.16650831 0.17885986 0.09477435 0.12612827 0.95866983 1.        ]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  1.0
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-17
    epoch : 0 ; learning_rate : 1e-17 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.728713075966079
    Epoch 0, Loss: 1.728713075966079, fit: 0.10826666666666666
    Epoch 0, Loss: 1.728713075966079
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.728713075966079_fit_0.10826666666666666_2024-01-03_163819
  self.fit : 0.10826666666666666
  self.loss : 1.728713075966079
  current_accuracy : 0.108
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-17
      loss :              1.728713075966079
      loss_factor :              0.1728713075966079
      loss adapted learning_rate :              2.988448899016103e-19
    epoch : 0 ; learning_rate : 2.988448899016103e-19 ; fit : 0.10826666666666666
    batch_size :          60000
    best_batch_loss : 1.7287130759660798
    Epoch 0, Loss: 1.7287130759660798, fit: 0.10826666666666666
    Epoch 0, Loss: 1.7287130759660798
      batch rate adapted learning_rate :              1e-17
      loss :              1.7287130759660798
      loss_factor :              0.172871307596608
      loss adapted learning_rate :              2.9884488990161057e-19
    epoch : 1 ; learning_rate : 2.9884488990161057e-19 ; fit : 0.10826666666666666
    batch_size :          60000
    best_batch_loss : 1.7287130759660785
    Epoch 1, Loss: 1.7287130759660785, fit: 0.10826666666666666
      batch rate adapted learning_rate :              1e-17
      loss :              1.7287130759660785
      loss_factor :              0.17287130759660785
      loss adapted learning_rate :              2.988448899016101e-19
    epoch : 2 ; learning_rate : 2.988448899016101e-19 ; fit : 0.10826666666666666
    batch_size :          60000
    best_batch_loss : 1.728713075966079
    Epoch 2, Loss: 1.728713075966079, fit: 0.10826666666666666
      batch rate adapted learning_rate :              1e-17
      loss :              1.728713075966079
      loss_factor :              0.1728713075966079
      loss adapted learning_rate :              2.988448899016103e-19
    epoch : 3 ; learning_rate : 2.988448899016103e-19 ; fit : 0.10826666666666666
    batch_size :          60000
    best_batch_loss : 1.728713075966079
    Epoch 3, Loss: 1.728713075966079, fit: 0.10826666666666666
      batch rate adapted learning_rate :              1e-17
      loss :              1.728713075966079
      loss_factor :              0.1728713075966079
      loss adapted learning_rate :              2.988448899016103e-19
    epoch : 4 ; learning_rate : 2.988448899016103e-19 ; fit : 0.10826666666666666
    batch_size :          60000
    best_batch_loss : 1.7287130759660798
    Epoch 4, Loss: 1.7287130759660798, fit: 0.10826666666666666
      batch rate adapted learning_rate :              1e-17
      loss :              1.7287130759660798
      loss_factor :              0.172871307596608
      loss adapted learning_rate :              2.9884488990161057e-19
    epoch : 5 ; learning_rate : 2.9884488990161057e-19 ; fit : 0.10826666666666666
    batch_size :          60000
    best_batch_loss : 1.7287130759660796
    Epoch 5, Loss: 1.7287130759660796, fit: 0.10826666666666666
      batch rate adapted learning_rate :              1e-17
      loss :              1.7287130759660796
      loss_factor :              0.17287130759660796
      loss adapted learning_rate :              2.9884488990161047e-19
    epoch : 6 ; learning_rate : 2.9884488990161047e-19 ; fit : 0.10826666666666666
    batch_size :          60000
    best_batch_loss : 1.728713075966079
    Epoch 6, Loss: 1.728713075966079, fit: 0.10826666666666666
      batch rate adapted learning_rate :              1e-17
      loss :              1.728713075966079
      loss_factor :              0.1728713075966079
      loss adapted learning_rate :              2.988448899016103e-19
    epoch : 7 ; learning_rate : 2.988448899016103e-19 ; fit : 0.10826666666666666
    batch_size :          60000
    best_batch_loss : 1.7287130759660796
    Epoch 7, Loss: 1.7287130759660796, fit: 0.10826666666666666
      batch rate adapted learning_rate :              1e-17
      loss :              1.7287130759660796
      loss_factor :              0.17287130759660796
      loss adapted learning_rate :              2.9884488990161047e-19
    epoch : 8 ; learning_rate : 2.9884488990161047e-19 ; fit : 0.10826666666666666
    batch_size :          60000
    best_batch_loss : 1.7287130759660791
    Epoch 8, Loss: 1.7287130759660791, fit: 0.10826666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7287130759660791_fit_0.10826666666666666_2024-01-03_163842
  self.fit : 0.10826666666666666
  self.loss : 1.7287130759660791
  current_accuracy : 0.108
   Accuracy mean: 0.108
   Accuracy mean: 0.108
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-10
    epoch : 0 ; learning_rate : 1e-10 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7109031510286166
    Epoch 0, Loss: 1.7109031510286166, fit: 0.11588333333333334
    Epoch 0, Loss: 1.7109031510286166
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7109031510286166_fit_0.11588333333333334_2024-01-03_163845
  self.fit : 0.11588333333333334
  self.loss : 1.7109031510286166
  current_accuracy : 0.1148
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-10
      loss :              1.7109031510286166
      loss_factor :              0.17109031510286166
      loss adapted learning_rate :              2.9271895921996492e-12
    epoch : 0 ; learning_rate : 2.9271895921996492e-12 ; fit : 0.11588333333333334
    batch_size :          60000
    best_batch_loss : 1.7109031508843102
    Epoch 0, Loss: 1.7109031508843102, fit: 0.11588333333333334
    Epoch 0, Loss: 1.7109031508843102
      batch rate adapted learning_rate :              1e-10
      loss :              1.7109031508843102
      loss_factor :              0.17109031508843103
      loss adapted learning_rate :              2.927189591705861e-12
    epoch : 1 ; learning_rate : 2.927189591705861e-12 ; fit : 0.11588333333333334
    batch_size :          60000
    best_batch_loss : 1.7109031508800865
    Epoch 1, Loss: 1.7109031508800865, fit: 0.11588333333333334
      batch rate adapted learning_rate :              1e-10
      loss :              1.7109031508800865
      loss_factor :              0.17109031508800865
      loss adapted learning_rate :              2.927189591691408e-12
    epoch : 2 ; learning_rate : 2.927189591691408e-12 ; fit : 0.11588333333333334
    batch_size :          60000
    best_batch_loss : 1.710903150875861
    Epoch 2, Loss: 1.710903150875861, fit: 0.11588333333333334
      batch rate adapted learning_rate :              1e-10
      loss :              1.710903150875861
      loss_factor :              0.1710903150875861
      loss adapted learning_rate :              2.9271895916769494e-12
    epoch : 3 ; learning_rate : 2.9271895916769494e-12 ; fit : 0.11588333333333334
    batch_size :          60000
    best_batch_loss : 1.7109031508716375
    Epoch 3, Loss: 1.7109031508716375, fit: 0.11588333333333334
      batch rate adapted learning_rate :              1e-10
      loss :              1.7109031508716375
      loss_factor :              0.17109031508716374
      loss adapted learning_rate :              2.927189591662497e-12
    epoch : 4 ; learning_rate : 2.927189591662497e-12 ; fit : 0.11588333333333334
    batch_size :          60000
    best_batch_loss : 1.7109031508674137
    Epoch 4, Loss: 1.7109031508674137, fit: 0.11588333333333334
      batch rate adapted learning_rate :              1e-10
      loss :              1.7109031508674137
      loss_factor :              0.17109031508674138
      loss adapted learning_rate :              2.9271895916480445e-12
    epoch : 5 ; learning_rate : 2.9271895916480445e-12 ; fit : 0.11588333333333334
    batch_size :          60000
    best_batch_loss : 1.7109031508631893
    Epoch 5, Loss: 1.7109031508631893, fit: 0.11588333333333334
      batch rate adapted learning_rate :              1e-10
      loss :              1.7109031508631893
      loss_factor :              0.17109031508631894
      loss adapted learning_rate :              2.9271895916335895e-12
    epoch : 6 ; learning_rate : 2.9271895916335895e-12 ; fit : 0.11588333333333334
    batch_size :          60000
    best_batch_loss : 1.710903150858964
    Epoch 6, Loss: 1.710903150858964, fit: 0.11588333333333334
      batch rate adapted learning_rate :              1e-10
      loss :              1.710903150858964
      loss_factor :              0.1710903150858964
      loss adapted learning_rate :              2.9271895916191308e-12
    epoch : 7 ; learning_rate : 2.9271895916191308e-12 ; fit : 0.11588333333333334
    batch_size :          60000
    best_batch_loss : 1.7109031508547414
    Epoch 7, Loss: 1.7109031508547414, fit: 0.11588333333333334
      batch rate adapted learning_rate :              1e-10
      loss :              1.7109031508547414
      loss_factor :              0.17109031508547415
      loss adapted learning_rate :              2.9271895916046826e-12
    epoch : 8 ; learning_rate : 2.9271895916046826e-12 ; fit : 0.11588333333333334
    batch_size :          60000
    best_batch_loss : 1.7109031508505175
    Epoch 8, Loss: 1.7109031508505175, fit: 0.11588333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7109031508505175_fit_0.11588333333333334_2024-01-03_163906
  self.fit : 0.11588333333333334
  self.loss : 1.7109031508505175
  current_accuracy : 0.1148
   Accuracy mean: 0.1148
   Accuracy mean: 0.1148
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-07
    epoch : 0 ; learning_rate : 1e-07 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7054685287315663
    Epoch 0, Loss: 1.7054685287315663, fit: 0.12156666666666667
    Epoch 0, Loss: 1.7054685287315663
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7054685287315663_fit_0.12156666666666667_2024-01-03_163909
  self.fit : 0.12156666666666667
  self.loss : 1.7054685287315663
  current_accuracy : 0.1233
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-07
      loss :              1.7054685287315663
      loss_factor :              0.17054685287315663
      loss adapted learning_rate :              2.908622902493813e-09
    epoch : 0 ; learning_rate : 2.908622902493813e-09 ; fit : 0.12156666666666667
    batch_size :          60000
    best_batch_loss : 1.7054684001664273
    Epoch 0, Loss: 1.7054684001664273, fit: 0.12156666666666667
    Epoch 0, Loss: 1.7054684001664273
      batch rate adapted learning_rate :              1e-07
      loss :              1.7054684001664273
      loss_factor :              0.17054684001664272
      loss adapted learning_rate :              2.9086224639662326e-09
    epoch : 1 ; learning_rate : 2.9086224639662326e-09 ; fit : 0.12156666666666667
    batch_size :          60000
    best_batch_loss : 1.7054683964269441
    Epoch 1, Loss: 1.7054683964269441, fit: 0.12156666666666667
      batch rate adapted learning_rate :              1e-07
      loss :              1.7054683964269441
      loss_factor :              0.1705468396426944
      loss adapted learning_rate :              2.908622451211092e-09
    epoch : 2 ; learning_rate : 2.908622451211092e-09 ; fit : 0.12156666666666667
    batch_size :          60000
    best_batch_loss : 1.7054683926874614
    Epoch 2, Loss: 1.7054683926874614, fit: 0.12156666666666667
      batch rate adapted learning_rate :              1e-07
      loss :              1.7054683926874614
      loss_factor :              0.17054683926874614
      loss adapted learning_rate :              2.9086224384559528e-09
    epoch : 3 ; learning_rate : 2.9086224384559528e-09 ; fit : 0.12156666666666667
    batch_size :          60000
    best_batch_loss : 1.7054683889479771
    Epoch 3, Loss: 1.7054683889479771, fit: 0.12156666666666667
      batch rate adapted learning_rate :              1e-07
      loss :              1.7054683889479771
      loss_factor :              0.17054683889479771
      loss adapted learning_rate :              2.9086224257008083e-09
    epoch : 4 ; learning_rate : 2.9086224257008083e-09 ; fit : 0.12156666666666667
    batch_size :          60000
    best_batch_loss : 1.7054683852084922
    Epoch 4, Loss: 1.7054683852084922, fit: 0.12156666666666667
      batch rate adapted learning_rate :              1e-07
      loss :              1.7054683852084922
      loss_factor :              0.17054683852084923
      loss adapted learning_rate :              2.908622412945662e-09
    epoch : 5 ; learning_rate : 2.908622412945662e-09 ; fit : 0.12156666666666667
    batch_size :          60000
    best_batch_loss : 1.705468381469008
    Epoch 5, Loss: 1.705468381469008, fit: 0.12156666666666667
      batch rate adapted learning_rate :              1e-07
      loss :              1.705468381469008
      loss_factor :              0.1705468381469008
      loss adapted learning_rate :              2.9086224001905177e-09
    epoch : 6 ; learning_rate : 2.9086224001905177e-09 ; fit : 0.12156666666666667
    batch_size :          60000
    best_batch_loss : 1.7054683777295223
    Epoch 6, Loss: 1.7054683777295223, fit: 0.12156666666666667
      batch rate adapted learning_rate :              1e-07
      loss :              1.7054683777295223
      loss_factor :              0.17054683777295224
      loss adapted learning_rate :              2.9086223874353686e-09
    epoch : 7 ; learning_rate : 2.9086223874353686e-09 ; fit : 0.12156666666666667
    batch_size :          60000
    best_batch_loss : 1.705468373990036
    Epoch 7, Loss: 1.705468373990036, fit: 0.12156666666666667
      batch rate adapted learning_rate :              1e-07
      loss :              1.705468373990036
      loss_factor :              0.17054683739900361
      loss adapted learning_rate :              2.9086223746802175e-09
    epoch : 8 ; learning_rate : 2.9086223746802175e-09 ; fit : 0.12156666666666667
    batch_size :          60000
    best_batch_loss : 1.7054683702505502
    Epoch 8, Loss: 1.7054683702505502, fit: 0.12156666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7054683702505502_fit_0.12156666666666667_2024-01-03_163931
  self.fit : 0.12156666666666667
  self.loss : 1.7054683702505502
  current_accuracy : 0.1233
   Accuracy mean: 0.1233
   Accuracy mean: 0.1233
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6822044430326226
    Epoch 0, Loss: 1.6822044430326226, fit: 0.13148333333333334
    Epoch 0, Loss: 1.6822044430326226
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6822044430326226_fit_0.13148333333333334_2024-01-03_163934
  self.fit : 0.13148333333333334
  self.loss : 1.6822044430326226
  current_accuracy : 0.126
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.0001
      loss :              1.6822044430326226
      loss_factor :              0.16822044430326227
      loss adapted learning_rate :              2.8298117881586967e-06
    epoch : 0 ; learning_rate : 2.8298117881586967e-06 ; fit : 0.13148333333333334
    batch_size :          60000
    best_batch_loss : 1.6820460865397082
    Epoch 0, Loss: 1.6820460865397082, fit: 0.1316
    Epoch 0, Loss: 1.6820460865397082
      batch rate adapted learning_rate :              0.0001
      loss :              1.6820460865397082
      loss_factor :              0.16820460865397083
      loss adapted learning_rate :              2.8292790372435476e-06
    epoch : 1 ; learning_rate : 2.8292790372435476e-06 ; fit : 0.1316
    batch_size :          60000
    best_batch_loss : 1.6820416010594224
    Epoch 1, Loss: 1.6820416010594224, fit: 0.1316
      batch rate adapted learning_rate :              0.0001
      loss :              1.6820416010594224
      loss_factor :              0.16820416010594225
      loss adapted learning_rate :              2.8292639476945454e-06
    epoch : 2 ; learning_rate : 2.8292639476945454e-06 ; fit : 0.1316
    batch_size :          60000
    best_batch_loss : 1.6820371161294572
    Epoch 2, Loss: 1.6820371161294572, fit: 0.1316
      batch rate adapted learning_rate :              0.0001
      loss :              1.6820371161294572
      loss_factor :              0.16820371161294573
      loss adapted learning_rate :              2.8292488600371015e-06
    epoch : 3 ; learning_rate : 2.8292488600371015e-06 ; fit : 0.1316
    batch_size :          60000
    best_batch_loss : 1.6820326309297762
    Epoch 3, Loss: 1.6820326309297762, fit: 0.13158333333333333
      batch rate adapted learning_rate :              0.0001
      loss :              1.6820326309297762
      loss_factor :              0.16820326309297762
      loss adapted learning_rate :              2.829233771512545e-06
    epoch : 4 ; learning_rate : 2.829233771512545e-06 ; fit : 0.13158333333333333
    batch_size :          60000
    best_batch_loss : 1.68202814546085
    Epoch 4, Loss: 1.68202814546085, fit: 0.1316
      batch rate adapted learning_rate :              0.0001
      loss :              1.68202814546085
      loss_factor :              0.16820281454608502
      loss adapted learning_rate :              2.829218682122467e-06
    epoch : 5 ; learning_rate : 2.829218682122467e-06 ; fit : 0.1316
    batch_size :          60000
    best_batch_loss : 1.6820236597231526
    Epoch 5, Loss: 1.6820236597231526, fit: 0.13158333333333333
      batch rate adapted learning_rate :              0.0001
      loss :              1.6820236597231526
      loss_factor :              0.16820236597231525
      loss adapted learning_rate :              2.8292035918684675e-06
    epoch : 6 ; learning_rate : 2.8292035918684675e-06 ; fit : 0.13158333333333333
    batch_size :          60000
    best_batch_loss : 1.6820191737171573
    Epoch 6, Loss: 1.6820191737171573, fit: 0.13158333333333333
      batch rate adapted learning_rate :              0.0001
      loss :              1.6820191737171573
      loss_factor :              0.16820191737171572
      loss adapted learning_rate :              2.8291885007521485e-06
    epoch : 7 ; learning_rate : 2.8291885007521485e-06 ; fit : 0.13158333333333333
    batch_size :          60000
    best_batch_loss : 1.682014687443333
    Epoch 7, Loss: 1.682014687443333, fit: 0.13156666666666667
      batch rate adapted learning_rate :              0.0001
      loss :              1.682014687443333
      loss_factor :              0.1682014687443333
      loss adapted learning_rate :              2.8291734087750936e-06
    epoch : 8 ; learning_rate : 2.8291734087750936e-06 ; fit : 0.13156666666666667
    batch_size :          60000
    best_batch_loss : 1.68201020090215
    Epoch 8, Loss: 1.68201020090215, fit: 0.13156666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.68201020090215_fit_0.13156666666666667_2024-01-03_163956
  self.fit : 0.13156666666666667
  self.loss : 1.68201020090215
  current_accuracy : 0.126
   Accuracy mean: 0.126
   Accuracy mean: 0.126
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.001
    epoch : 0 ; learning_rate : 0.001 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.72475849762426
    Epoch 0, Loss: 1.72475849762426, fit: 0.10506666666666667
    Epoch 0, Loss: 1.72475849762426
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.72475849762426_fit_0.10506666666666667_2024-01-03_163959
  self.fit : 0.10506666666666667
  self.loss : 1.72475849762426
  current_accuracy : 0.1104
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.001
      loss :              1.72475849762426
      loss_factor :              0.172475849762426
      loss adapted learning_rate :              2.9747918751270942e-05
    epoch : 0 ; learning_rate : 2.9747918751270942e-05 ; fit : 0.10506666666666667
    batch_size :          60000
    best_batch_loss : 1.7239213203985362
    Epoch 0, Loss: 1.7239213203985362, fit: 0.10523333333333333
    Epoch 0, Loss: 1.7239213203985362
      batch rate adapted learning_rate :              0.001
      loss :              1.7239213203985362
      loss_factor :              0.17239213203985362
      loss adapted learning_rate :              2.9719047189246327e-05
    epoch : 1 ; learning_rate : 2.9719047189246327e-05 ; fit : 0.10523333333333333
    batch_size :          60000
    best_batch_loss : 1.7238961859095054
    Epoch 1, Loss: 1.7238961859095054, fit: 0.10523333333333333
      batch rate adapted learning_rate :              0.001
      loss :              1.7238961859095054
      loss_factor :              0.17238961859095053
      loss adapted learning_rate :              2.9718180597933397e-05
    epoch : 2 ; learning_rate : 2.9718180597933397e-05 ; fit : 0.10523333333333333
    batch_size :          60000
    best_batch_loss : 1.7238710591955801
    Epoch 2, Loss: 1.7238710591955801, fit: 0.10523333333333333
      batch rate adapted learning_rate :              0.001
      loss :              1.7238710591955801
      loss_factor :              0.17238710591955803
      loss adapted learning_rate :              2.971731428732092e-05
    epoch : 3 ; learning_rate : 2.971731428732092e-05 ; fit : 0.10523333333333333
    batch_size :          60000
    best_batch_loss : 1.7238459166718998
    Epoch 3, Loss: 1.7238459166718998, fit: 0.10523333333333333
      batch rate adapted learning_rate :              0.001
      loss :              1.7238459166718998
      loss_factor :              0.17238459166718997
      loss adapted learning_rate :              2.9716447444263825e-05
    epoch : 4 ; learning_rate : 2.9716447444263825e-05 ; fit : 0.10523333333333333
    batch_size :          60000
    best_batch_loss : 1.7238207584122738
    Epoch 4, Loss: 1.7238207584122738, fit: 0.10526666666666666
      batch rate adapted learning_rate :              0.001
      loss :              1.7238207584122738
      loss_factor :              0.1723820758412274
      loss adapted learning_rate :              2.9715580071330673e-05
    epoch : 5 ; learning_rate : 2.9715580071330673e-05 ; fit : 0.10526666666666666
    batch_size :          60000
    best_batch_loss : 1.7237955844942117
    Epoch 5, Loss: 1.7237955844942117, fit: 0.10528333333333334
      batch rate adapted learning_rate :              0.001
      loss :              1.7237955844942117
      loss_factor :              0.17237955844942116
      loss adapted learning_rate :              2.9714712171217405e-05
    epoch : 6 ; learning_rate : 2.9714712171217405e-05 ; fit : 0.10528333333333334
    batch_size :          60000
    best_batch_loss : 1.7237703949982375
    Epoch 6, Loss: 1.7237703949982375, fit: 0.1053
      batch rate adapted learning_rate :              0.001
      loss :              1.7237703949982375
      loss_factor :              0.17237703949982375
      loss adapted learning_rate :              2.9713843746723797e-05
    epoch : 7 ; learning_rate : 2.9713843746723797e-05 ; fit : 0.1053
    batch_size :          60000
    best_batch_loss : 1.7237451900079002
    Epoch 7, Loss: 1.7237451900079002, fit: 0.10531666666666667
      batch rate adapted learning_rate :              0.001
      loss :              1.7237451900079002
      loss_factor :              0.17237451900079
      loss adapted learning_rate :              2.9712974800753717e-05
    epoch : 8 ; learning_rate : 2.9712974800753717e-05 ; fit : 0.10531666666666667
    batch_size :          60000
    best_batch_loss : 1.7237199696097771
    Epoch 8, Loss: 1.7237199696097771, fit: 0.10531666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7237199696097771_fit_0.10531666666666667_2024-01-03_164021
  self.fit : 0.10531666666666667
  self.loss : 1.7237199696097771
  current_accuracy : 0.1103
   Accuracy mean: 0.1104
   Accuracy mean: 0.1103
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.02
    epoch : 0 ; learning_rate : 0.02 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7235199625688318
    Epoch 0, Loss: 1.7235199625688318, fit: 0.10828333333333333
    Epoch 0, Loss: 1.7235199625688318
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7235199625688318_fit_0.10828333333333333_2024-01-03_164023
  self.fit : 0.10828333333333333
  self.loss : 1.7235199625688318
  current_accuracy : 0.1271
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.02
      loss :              1.7235199625688318
      loss_factor :              0.1723519962568832
      loss adapted learning_rate :              0.0005941042122746535
    epoch : 0 ; learning_rate : 0.0005941042122746535 ; fit : 0.10828333333333333
    batch_size :          60000
    best_batch_loss : 1.6983028046429898
    Epoch 0, Loss: 1.6983028046429898, fit: 0.11766666666666667
    Epoch 0, Loss: 1.6983028046429898
      batch rate adapted learning_rate :              0.02
      loss :              1.6983028046429898
      loss_factor :              0.169830280464299
      loss adapted learning_rate :              0.0005768464832516491
    epoch : 1 ; learning_rate : 0.0005768464832516491 ; fit : 0.11766666666666667
    batch_size :          60000
    best_batch_loss : 1.6975949441205964
    Epoch 1, Loss: 1.6975949441205964, fit: 0.11818333333333333
      batch rate adapted learning_rate :              0.02
      loss :              1.6975949441205964
      loss_factor :              0.16975949441205965
      loss adapted learning_rate :              0.0005763657188607623
    epoch : 2 ; learning_rate : 0.0005763657188607623 ; fit : 0.11818333333333333
    batch_size :          60000
    best_batch_loss : 1.696896763322377
    Epoch 2, Loss: 1.696896763322377, fit: 0.11855
      batch rate adapted learning_rate :              0.02
      loss :              1.696896763322377
      loss_factor :              0.1696896763322377
      loss adapted learning_rate :              0.0005758917250747918
    epoch : 3 ; learning_rate : 0.0005758917250747918 ; fit : 0.11855
    batch_size :          60000
    best_batch_loss : 1.6961896456376921
    Epoch 3, Loss: 1.6961896456376921, fit: 0.11906666666666667
      batch rate adapted learning_rate :              0.02
      loss :              1.6961896456376921
      loss_factor :              0.1696189645637692
      loss adapted learning_rate :              0.000575411862793704
    epoch : 4 ; learning_rate : 0.000575411862793704 ; fit : 0.11906666666666667
    batch_size :          60000
    best_batch_loss : 1.6954747292024923
    Epoch 4, Loss: 1.6954747292024923, fit: 0.11946666666666667
      batch rate adapted learning_rate :              0.02
      loss :              1.6954747292024923
      loss_factor :              0.16954747292024924
      loss adapted learning_rate :              0.000574926911472853
    epoch : 5 ; learning_rate : 0.000574926911472853 ; fit : 0.11946666666666667
    batch_size :          60000
    best_batch_loss : 1.6947528361423256
    Epoch 5, Loss: 1.6947528361423256, fit: 0.11988333333333333
      batch rate adapted learning_rate :              0.02
      loss :              1.6947528361423256
      loss_factor :              0.16947528361423256
      loss adapted learning_rate :              0.0005744374351224912
    epoch : 6 ; learning_rate : 0.0005744374351224912 ; fit : 0.11988333333333333
    batch_size :          60000
    best_batch_loss : 1.6940243573232068
    Epoch 6, Loss: 1.6940243573232068, fit: 0.12018333333333334
      batch rate adapted learning_rate :              0.02
      loss :              1.6940243573232068
      loss_factor :              0.16940243573232067
      loss adapted learning_rate :              0.0005739437046408607
    epoch : 7 ; learning_rate : 0.0005739437046408607 ; fit : 0.12018333333333334
    batch_size :          60000
    best_batch_loss : 1.6932892293127073
    Epoch 7, Loss: 1.6932892293127073, fit: 0.1204
      batch rate adapted learning_rate :              0.02
      loss :              1.6932892293127073
      loss_factor :              0.16932892293127072
      loss adapted learning_rate :              0.0005734456828212843
    epoch : 8 ; learning_rate : 0.0005734456828212843 ; fit : 0.1204
    batch_size :          60000
    best_batch_loss : 1.6925471526007072
    Epoch 8, Loss: 1.6925471526007072, fit: 0.12088333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.6925471526007072_fit_0.12088333333333333_2024-01-03_164045
  self.fit : 0.12088333333333333
  self.loss : 1.6925471526007072
  current_accuracy : 0.1305
   Accuracy mean: 0.1271
   Accuracy mean: 0.1305
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.025
    epoch : 0 ; learning_rate : 0.025 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.8224475171660344
    Epoch 0, Loss: 1.8224475171660344, fit: 0.058866666666666664
    Epoch 0, Loss: 1.8224475171660344
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.8224475171660344_fit_0.058866666666666664_2024-01-03_164047
  self.fit : 0.058866666666666664
  self.loss : 1.8224475171660344
  current_accuracy : 0.0653
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.025
      loss :              1.8224475171660344
      loss_factor :              0.18224475171660343
      loss adapted learning_rate :              0.0008303287382061608
    epoch : 0 ; learning_rate : 0.0008303287382061608 ; fit : 0.058866666666666664
    batch_size :          60000
    best_batch_loss : 1.7996801723603526
    Epoch 0, Loss: 1.7996801723603526, fit: 0.06791666666666667
    Epoch 0, Loss: 1.7996801723603526
      batch rate adapted learning_rate :              0.025
      loss :              1.7996801723603526
      loss_factor :              0.17996801723603525
      loss adapted learning_rate :              0.0008097121806967471
    epoch : 1 ; learning_rate : 0.0008097121806967471 ; fit : 0.06791666666666667
    batch_size :          60000
    best_batch_loss : 1.7989853999790801
    Epoch 1, Loss: 1.7989853999790801, fit: 0.06835
      batch rate adapted learning_rate :              0.025
      loss :              1.7989853999790801
      loss_factor :              0.179898539997908
      loss adapted learning_rate :              0.0008090871173344728
    epoch : 2 ; learning_rate : 0.0008090871173344728 ; fit : 0.06835
    batch_size :          60000
    best_batch_loss : 1.7983178968358196
    Epoch 2, Loss: 1.7983178968358196, fit: 0.0686
      batch rate adapted learning_rate :              0.025
      loss :              1.7983178968358196
      loss_factor :              0.17983178968358196
      loss adapted learning_rate :              0.0008084868145200014
    epoch : 3 ; learning_rate : 0.0008084868145200014 ; fit : 0.0686
    batch_size :          60000
    best_batch_loss : 1.7976589315298763
    Epoch 3, Loss: 1.7976589315298763, fit: 0.06881666666666666
      batch rate adapted learning_rate :              0.025
      loss :              1.7976589315298763
      loss_factor :              0.17976589315298763
      loss adapted learning_rate :              0.0008078944085272842
    epoch : 4 ; learning_rate : 0.0008078944085272842 ; fit : 0.06881666666666666
    batch_size :          60000
    best_batch_loss : 1.7970054179049575
    Epoch 4, Loss: 1.7970054179049575, fit: 0.06923333333333333
      batch rate adapted learning_rate :              0.025
      loss :              1.7970054179049575
      loss_factor :              0.17970054179049574
      loss adapted learning_rate :              0.0008073071179949426
    epoch : 5 ; learning_rate : 0.0008073071179949426 ; fit : 0.06923333333333333
    batch_size :          60000
    best_batch_loss : 1.7963529109981986
    Epoch 5, Loss: 1.7963529109981986, fit: 0.06958333333333333
      batch rate adapted learning_rate :              0.025
      loss :              1.7963529109981986
      loss_factor :              0.17963529109981985
      loss adapted learning_rate :              0.0008067209452129255
    epoch : 6 ; learning_rate : 0.0008067209452129255 ; fit : 0.06958333333333333
    batch_size :          60000
    best_batch_loss : 1.7956963686728915
    Epoch 6, Loss: 1.7956963686728915, fit: 0.06981666666666667
      batch rate adapted learning_rate :              0.025
      loss :              1.7956963686728915
      loss_factor :              0.17956963686728916
      loss adapted learning_rate :              0.0008061313621162523
    epoch : 7 ; learning_rate : 0.0008061313621162523 ; fit : 0.06981666666666667
    batch_size :          60000
    best_batch_loss : 1.7950317218398897
    Epoch 7, Loss: 1.7950317218398897, fit: 0.07006666666666667
      batch rate adapted learning_rate :              0.025
      loss :              1.7950317218398897
      loss_factor :              0.17950317218398898
      loss adapted learning_rate :              0.0008055347206028699
    epoch : 8 ; learning_rate : 0.0008055347206028699 ; fit : 0.07006666666666667
    batch_size :          60000
    best_batch_loss : 1.794357030967348
    Epoch 8, Loss: 1.794357030967348, fit: 0.07025
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.794357030967348_fit_0.07025_2024-01-03_164109
  self.fit : 0.07025
  self.loss : 1.794357030967348
  current_accuracy : 0.0693
   Accuracy mean: 0.0653
   Accuracy mean: 0.0693
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.03
    epoch : 0 ; learning_rate : 0.03 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7345866240262406
    Epoch 0, Loss: 1.7345866240262406, fit: 0.099
    Epoch 0, Loss: 1.7345866240262406
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7345866240262406_fit_0.099_2024-01-03_164111
  self.fit : 0.099
  self.loss : 1.7345866240262406
  current_accuracy : 0.1216
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.03
      loss :              1.7345866240262406
      loss_factor :              0.17345866240262406
      loss adapted learning_rate :              0.0009026372268752252
    epoch : 0 ; learning_rate : 0.0009026372268752252 ; fit : 0.099
    batch_size :          60000
    best_batch_loss : 1.6946632670755002
    Epoch 0, Loss: 1.6946632670755002, fit: 0.11793333333333333
    Epoch 0, Loss: 1.6946632670755002
      batch rate adapted learning_rate :              0.03
      loss :              1.6946632670755002
      loss_factor :              0.16946632670755002
      loss adapted learning_rate :              0.0008615650766325023
    epoch : 1 ; learning_rate : 0.0008615650766325023 ; fit : 0.11793333333333333
    batch_size :          60000
    best_batch_loss : 1.693622975479041
    Epoch 1, Loss: 1.693622975479041, fit: 0.11835
      batch rate adapted learning_rate :              0.03
      loss :              1.693622975479041
      loss_factor :              0.1693622975479041
      loss adapted learning_rate :              0.000860507634921144
    epoch : 2 ; learning_rate : 0.000860507634921144 ; fit : 0.11835
    batch_size :          60000
    best_batch_loss : 1.692623131926064
    Epoch 2, Loss: 1.692623131926064, fit: 0.11865
      batch rate adapted learning_rate :              0.03
      loss :              1.692623131926064
      loss_factor :              0.1692623131926064
      loss adapted learning_rate :              0.0008594919200193593
    epoch : 3 ; learning_rate : 0.0008594919200193593 ; fit : 0.11865
    batch_size :          60000
    best_batch_loss : 1.691618904821626
    Epoch 3, Loss: 1.691618904821626, fit: 0.11915
      batch rate adapted learning_rate :              0.03
      loss :              1.691618904821626
      loss_factor :              0.1691618904821626
      loss adapted learning_rate :              0.0008584723557449751
    epoch : 4 ; learning_rate : 0.0008584723557449751 ; fit : 0.11915
    batch_size :          60000
    best_batch_loss : 1.6906120992773328
    Epoch 4, Loss: 1.6906120992773328, fit: 0.1197
      batch rate adapted learning_rate :              0.03
      loss :              1.6906120992773328
      loss_factor :              0.16906120992773327
      loss adapted learning_rate :              0.0008574507810668729
    epoch : 5 ; learning_rate : 0.0008574507810668729 ; fit : 0.1197
    batch_size :          60000
    best_batch_loss : 1.6896047977778803
    Epoch 5, Loss: 1.6896047977778803, fit: 0.12006666666666667
      batch rate adapted learning_rate :              0.03
      loss :              1.6896047977778803
      loss_factor :              0.16896047977778803
      loss adapted learning_rate :              0.0008564293118022094
    epoch : 6 ; learning_rate : 0.0008564293118022094 ; fit : 0.12006666666666667
    batch_size :          60000
    best_batch_loss : 1.688598872932083
    Epoch 6, Loss: 1.688598872932083, fit: 0.1206
      batch rate adapted learning_rate :              0.03
      loss :              1.688598872932083
      loss_factor :              0.1688598872932083
      loss adapted learning_rate :              0.0008554098461002503
    epoch : 7 ; learning_rate : 0.0008554098461002503 ; fit : 0.1206
    batch_size :          60000
    best_batch_loss : 1.6875956382240773
    Epoch 7, Loss: 1.6875956382240773, fit: 0.12116666666666667
      batch rate adapted learning_rate :              0.03
      loss :              1.6875956382240773
      loss_factor :              0.16875956382240773
      loss adapted learning_rate :              0.0008543937114458792
    epoch : 8 ; learning_rate : 0.0008543937114458792 ; fit : 0.12116666666666667
    batch_size :          60000
    best_batch_loss : 1.6865955890213418
    Epoch 8, Loss: 1.6865955890213418, fit: 0.1217
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6865955890213418_fit_0.1217_2024-01-03_164132
  self.fit : 0.1217
  self.loss : 1.6865955890213418
  current_accuracy : 0.1273
   Accuracy mean: 0.1216
   Accuracy mean: 0.1273
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.1
    epoch : 0 ; learning_rate : 0.1 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6889887336069624
    Epoch 0, Loss: 1.6889887336069624, fit: 0.12803333333333333
    Epoch 0, Loss: 1.6889887336069624
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6889887336069624_fit_0.12803333333333333_2024-01-03_164135
  self.fit : 0.12803333333333333
  self.loss : 1.6889887336069624
  current_accuracy : 0.1561
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.1
      loss :              1.6889887336069624
      loss_factor :              0.16889887336069626
      loss adapted learning_rate :              0.0028526829422512515
    epoch : 0 ; learning_rate : 0.0028526829422512515 ; fit : 0.12803333333333333
    batch_size :          60000
    best_batch_loss : 1.6278473334311625
    Epoch 0, Loss: 1.6278473334311625, fit: 0.15385
    Epoch 0, Loss: 1.6278473334311625
      batch rate adapted learning_rate :              0.1
      loss :              1.6278473334311625
      loss_factor :              0.16278473334311624
      loss adapted learning_rate :              0.0026498869409589463
    epoch : 1 ; learning_rate : 0.0026498869409589463 ; fit : 0.15385
    batch_size :          60000
    best_batch_loss : 1.6252246192128983
    Epoch 1, Loss: 1.6252246192128983, fit: 0.15491666666666667
      batch rate adapted learning_rate :              0.1
      loss :              1.6252246192128983
      loss_factor :              0.16252246192128983
      loss adapted learning_rate :              0.0026413550628957105
    epoch : 2 ; learning_rate : 0.0026413550628957105 ; fit : 0.15491666666666667
    batch_size :          60000
    best_batch_loss : 1.6226542282191576
    Epoch 2, Loss: 1.6226542282191576, fit: 0.1564
      batch rate adapted learning_rate :              0.1
      loss :              1.6226542282191576
      loss_factor :              0.16226542282191575
      loss adapted learning_rate :              0.00263300674435751
    epoch : 3 ; learning_rate : 0.00263300674435751 ; fit : 0.1564
    batch_size :          60000
    best_batch_loss : 1.6198993546003924
    Epoch 3, Loss: 1.6198993546003924, fit: 0.1574
      batch rate adapted learning_rate :              0.1
      loss :              1.6198993546003924
      loss_factor :              0.16198993546003923
      loss adapted learning_rate :              0.0026240739190347678
    epoch : 4 ; learning_rate : 0.0026240739190347678 ; fit : 0.1574
    batch_size :          60000
    best_batch_loss : 1.6170075453023642
    Epoch 4, Loss: 1.6170075453023642, fit: 0.15898333333333334
      batch rate adapted learning_rate :              0.1
      loss :              1.6170075453023642
      loss_factor :              0.1617007545302364
      loss adapted learning_rate :              0.0026147134015647774
    epoch : 5 ; learning_rate : 0.0026147134015647774 ; fit : 0.15898333333333334
    batch_size :          60000
    best_batch_loss : 1.6140866498509936
    Epoch 5, Loss: 1.6140866498509936, fit: 0.16
      batch rate adapted learning_rate :              0.1
      loss :              1.6140866498509936
      loss_factor :              0.16140866498509937
      loss adapted learning_rate :              0.0026052757132272045
    epoch : 6 ; learning_rate : 0.0026052757132272045 ; fit : 0.16
    batch_size :          60000
    best_batch_loss : 1.611214223437359
    Epoch 6, Loss: 1.611214223437359, fit: 0.16165
      batch rate adapted learning_rate :              0.1
      loss :              1.611214223437359
      loss_factor :              0.1611214223437359
      loss adapted learning_rate :              0.0025960112738068514
    epoch : 7 ; learning_rate : 0.0025960112738068514 ; fit : 0.16165
    batch_size :          60000
    best_batch_loss : 1.608413779695242
    Epoch 7, Loss: 1.608413779695242, fit: 0.16323333333333334
      batch rate adapted learning_rate :              0.1
      loss :              1.608413779695242
      loss_factor :              0.1608413779695242
      loss adapted learning_rate :              0.0025869948867135346
    epoch : 8 ; learning_rate : 0.0025869948867135346 ; fit : 0.16323333333333334
    batch_size :          60000
    best_batch_loss : 1.6056559720508752
    Epoch 8, Loss: 1.6056559720508752, fit: 0.16408333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6056559720508752_fit_0.16408333333333333_2024-01-03_164157
  self.fit : 0.16408333333333333
  self.loss : 1.6056559720508752
  current_accuracy : 0.1664
   Accuracy mean: 0.1561
   Accuracy mean: 0.1664
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.9
    epoch : 0 ; learning_rate : 0.9 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.8245261436107387
    Epoch 0, Loss: 1.8245261436107387, fit: 0.05998333333333333
    Epoch 0, Loss: 1.8245261436107387
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.8245261436107387_fit_0.05998333333333333_2024-01-03_164159
  self.fit : 0.05998333333333333
  self.loss : 1.8245261436107387
  current_accuracy : 0.413
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.9
      loss :              1.8245261436107387
      loss_factor :              0.18245261436107388
      loss adapted learning_rate :              0.029960060838471667
    epoch : 0 ; learning_rate : 0.029960060838471667 ; fit : 0.05998333333333333
    batch_size :          60000
    best_batch_loss : 1.1357469243557614
    Epoch 0, Loss: 1.1357469243557614, fit: 0.41401666666666664
    Epoch 0, Loss: 1.1357469243557614
      batch rate adapted learning_rate :              0.9
      loss :              1.1357469243557614
      loss_factor :              0.11357469243557614
      loss adapted learning_rate :              0.011609289685652145
    epoch : 1 ; learning_rate : 0.011609289685652145 ; fit : 0.41401666666666664
    batch_size :          60000
    best_batch_loss : 1.0830975739724882
    Epoch 1, Loss: 1.0830975739724882, fit: 0.4391833333333333
      batch rate adapted learning_rate :              0.9
      loss :              1.0830975739724882
      loss_factor :              0.10830975739724882
      loss adapted learning_rate :              0.010557903192705807
    epoch : 2 ; learning_rate : 0.010557903192705807 ; fit : 0.4391833333333333
    batch_size :          60000
    best_batch_loss : 1.0666843483136177
    Epoch 2, Loss: 1.0666843483136177, fit: 0.44708333333333333
      batch rate adapted learning_rate :              0.9
      loss :              1.0666843483136177
      loss_factor :              0.10666843483136176
      loss adapted learning_rate :              0.010240339490435225
    epoch : 3 ; learning_rate : 0.010240339490435225 ; fit : 0.44708333333333333
    batch_size :          60000
    best_batch_loss : 1.053303006176748
    Epoch 3, Loss: 1.053303006176748, fit: 0.4537833333333333
      batch rate adapted learning_rate :              0.9
      loss :              1.053303006176748
      loss_factor :              0.10533030061767482
      loss adapted learning_rate :              0.009985025005388773
    epoch : 4 ; learning_rate : 0.009985025005388773 ; fit : 0.4537833333333333
    batch_size :          60000
    best_batch_loss : 1.0415091660191962
    Epoch 4, Loss: 1.0415091660191962, fit: 0.45993333333333336
      batch rate adapted learning_rate :              0.9
      loss :              1.0415091660191962
      loss_factor :              0.10415091660191962
      loss adapted learning_rate :              0.009762672086118014
    epoch : 5 ; learning_rate : 0.009762672086118014 ; fit : 0.45993333333333336
    batch_size :          60000
    best_batch_loss : 1.0301620132338853
    Epoch 5, Loss: 1.0301620132338853, fit: 0.46496666666666664
      batch rate adapted learning_rate :              0.9
      loss :              1.0301620132338853
      loss_factor :              0.10301620132338854
      loss adapted learning_rate :              0.009551103961590828
    epoch : 6 ; learning_rate : 0.009551103961590828 ; fit : 0.46496666666666664
    batch_size :          60000
    best_batch_loss : 1.0200334271915286
    Epoch 6, Loss: 1.0200334271915286, fit: 0.4702
      batch rate adapted learning_rate :              0.9
      loss :              1.0200334271915286
      loss_factor :              0.10200334271915286
      loss adapted learning_rate :              0.009364213733292858
    epoch : 7 ; learning_rate : 0.009364213733292858 ; fit : 0.4702
    batch_size :          60000
    best_batch_loss : 1.0107681827417796
    Epoch 7, Loss: 1.0107681827417796, fit: 0.4740666666666667
      batch rate adapted learning_rate :              0.9
      loss :              1.0107681827417796
      loss_factor :              0.10107681827417796
      loss adapted learning_rate :              0.009194870873188075
    epoch : 8 ; learning_rate : 0.009194870873188075 ; fit : 0.4740666666666667
    batch_size :          60000
    best_batch_loss : 1.0019792081117622
    Epoch 8, Loss: 1.0019792081117622, fit: 0.4783833333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.0019792081117622_fit_0.4783833333333333_2024-01-03_164222
  self.fit : 0.4783833333333333
  self.loss : 1.0019792081117622
  current_accuracy : 0.4861
   Accuracy mean: 0.413
   Accuracy mean: 0.4861
  Error saving file: doc/out/test_combinations_results/20240103164222.
  normalized_accuracies :      [0.10147338 0.10147338 0.11763308 0.11763308 0.1378327  0.1378327
   0.14424905 0.14424905 0.10717681 0.10693916 0.14686312 0.15494297
   0.         0.0095057  0.13379278 0.1473384  0.21577947 0.24025665
   0.82628327 1.        ]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  1.0
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-17
    epoch : 0 ; learning_rate : 1e-17 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.8000177027895774
    Epoch 0, Loss: 1.8000177027895774, fit: 0.0697
    Epoch 0, Loss: 1.8000177027895774
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.8000177027895774_fit_0.0697_2024-01-03_164453
  self.fit : 0.0697
  self.loss : 1.8000177027895774
  current_accuracy : 0.0665
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-17
      loss :              1.8000177027895774
      loss_factor :              0.18000177027895775
      loss adapted learning_rate :              3.2400637303558683e-19
    epoch : 0 ; learning_rate : 3.2400637303558683e-19 ; fit : 0.0697
    batch_size :          60000
    best_batch_loss : 1.8000177027895774
    Epoch 0, Loss: 1.8000177027895774, fit: 0.0697
    Epoch 0, Loss: 1.8000177027895774
      batch rate adapted learning_rate :              1e-17
      loss :              1.8000177027895774
      loss_factor :              0.18000177027895775
      loss adapted learning_rate :              3.2400637303558683e-19
    epoch : 1 ; learning_rate : 3.2400637303558683e-19 ; fit : 0.0697
    batch_size :          60000
    best_batch_loss : 1.800017702789577
    Epoch 1, Loss: 1.800017702789577, fit: 0.0697
      batch rate adapted learning_rate :              1e-17
      loss :              1.800017702789577
      loss_factor :              0.1800017702789577
      loss adapted learning_rate :              3.2400637303558664e-19
    epoch : 2 ; learning_rate : 3.2400637303558664e-19 ; fit : 0.0697
    batch_size :          60000
    best_batch_loss : 1.8000177027895772
    Epoch 2, Loss: 1.8000177027895772, fit: 0.0697
      batch rate adapted learning_rate :              1e-17
      loss :              1.8000177027895772
      loss_factor :              0.18000177027895772
      loss adapted learning_rate :              3.240063730355867e-19
    epoch : 3 ; learning_rate : 3.240063730355867e-19 ; fit : 0.0697
    batch_size :          60000
    best_batch_loss : 1.8000177027895772
    Epoch 3, Loss: 1.8000177027895772, fit: 0.0697
      batch rate adapted learning_rate :              1e-17
      loss :              1.8000177027895772
      loss_factor :              0.18000177027895772
      loss adapted learning_rate :              3.240063730355867e-19
    epoch : 4 ; learning_rate : 3.240063730355867e-19 ; fit : 0.0697
    batch_size :          60000
    best_batch_loss : 1.8000177027895774
    Epoch 4, Loss: 1.8000177027895774, fit: 0.0697
      batch rate adapted learning_rate :              1e-17
      loss :              1.8000177027895774
      loss_factor :              0.18000177027895775
      loss adapted learning_rate :              3.2400637303558683e-19
    epoch : 5 ; learning_rate : 3.2400637303558683e-19 ; fit : 0.0697
    batch_size :          60000
    best_batch_loss : 1.8000177027895774
    Epoch 5, Loss: 1.8000177027895774, fit: 0.0697
      batch rate adapted learning_rate :              1e-17
      loss :              1.8000177027895774
      loss_factor :              0.18000177027895775
      loss adapted learning_rate :              3.2400637303558683e-19
    epoch : 6 ; learning_rate : 3.2400637303558683e-19 ; fit : 0.0697
    batch_size :          60000
    best_batch_loss : 1.8000177027895774
    Epoch 6, Loss: 1.8000177027895774, fit: 0.0697
      batch rate adapted learning_rate :              1e-17
      loss :              1.8000177027895774
      loss_factor :              0.18000177027895775
      loss adapted learning_rate :              3.2400637303558683e-19
    epoch : 7 ; learning_rate : 3.2400637303558683e-19 ; fit : 0.0697
    batch_size :          60000
    best_batch_loss : 1.8000177027895767
    Epoch 7, Loss: 1.8000177027895767, fit: 0.0697
      batch rate adapted learning_rate :              1e-17
      loss :              1.8000177027895767
      loss_factor :              0.18000177027895767
      loss adapted learning_rate :              3.240063730355865e-19
    epoch : 8 ; learning_rate : 3.240063730355865e-19 ; fit : 0.0697
    batch_size :          60000
    best_batch_loss : 1.8000177027895767
    Epoch 8, Loss: 1.8000177027895767, fit: 0.0697
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.8000177027895767_fit_0.0697_2024-01-03_164514
  self.fit : 0.0697
  self.loss : 1.8000177027895767
  current_accuracy : 0.0665
   Accuracy mean: 0.0665
   Accuracy mean: 0.0665
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-10
    epoch : 0 ; learning_rate : 1e-10 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.8041562701692204
    Epoch 0, Loss: 1.8041562701692204, fit: 0.07153333333333334
    Epoch 0, Loss: 1.8041562701692204
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.8041562701692204_fit_0.07153333333333334_2024-01-03_164517
  self.fit : 0.07153333333333334
  self.loss : 1.8041562701692204
  current_accuracy : 0.0722
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-10
      loss :              1.8041562701692204
      loss_factor :              0.18041562701692204
      loss adapted learning_rate :              3.254979847190913e-12
    epoch : 0 ; learning_rate : 3.254979847190913e-12 ; fit : 0.07153333333333334
    batch_size :          60000
    best_batch_loss : 1.8041562700734075
    Epoch 0, Loss: 1.8041562700734075, fit: 0.07153333333333334
    Epoch 0, Loss: 1.8041562700734075
      batch rate adapted learning_rate :              1e-10
      loss :              1.8041562700734075
      loss_factor :              0.18041562700734076
      loss adapted learning_rate :              3.2549798468451907e-12
    epoch : 1 ; learning_rate : 3.2549798468451907e-12 ; fit : 0.07153333333333334
    batch_size :          60000
    best_batch_loss : 1.804156270070289
    Epoch 1, Loss: 1.804156270070289, fit: 0.07153333333333334
      batch rate adapted learning_rate :              1e-10
      loss :              1.804156270070289
      loss_factor :              0.1804156270070289
      loss adapted learning_rate :              3.254979846833938e-12
    epoch : 2 ; learning_rate : 3.254979846833938e-12 ; fit : 0.07153333333333334
    batch_size :          60000
    best_batch_loss : 1.804156270067172
    Epoch 2, Loss: 1.804156270067172, fit: 0.07153333333333334
      batch rate adapted learning_rate :              1e-10
      loss :              1.804156270067172
      loss_factor :              0.1804156270067172
      loss adapted learning_rate :              3.254979846822691e-12
    epoch : 3 ; learning_rate : 3.254979846822691e-12 ; fit : 0.07153333333333334
    batch_size :          60000
    best_batch_loss : 1.8041562700640519
    Epoch 3, Loss: 1.8041562700640519, fit: 0.07153333333333334
      batch rate adapted learning_rate :              1e-10
      loss :              1.8041562700640519
      loss_factor :              0.18041562700640518
      loss adapted learning_rate :              3.254979846811432e-12
    epoch : 4 ; learning_rate : 3.254979846811432e-12 ; fit : 0.07153333333333334
    batch_size :          60000
    best_batch_loss : 1.8041562700609342
    Epoch 4, Loss: 1.8041562700609342, fit: 0.07153333333333334
      batch rate adapted learning_rate :              1e-10
      loss :              1.8041562700609342
      loss_factor :              0.18041562700609343
      loss adapted learning_rate :              3.254979846800183e-12
    epoch : 5 ; learning_rate : 3.254979846800183e-12 ; fit : 0.07153333333333334
    batch_size :          60000
    best_batch_loss : 1.8041562700578153
    Epoch 5, Loss: 1.8041562700578153, fit: 0.07153333333333334
      batch rate adapted learning_rate :              1e-10
      loss :              1.8041562700578153
      loss_factor :              0.18041562700578154
      loss adapted learning_rate :              3.2549798467889287e-12
    epoch : 6 ; learning_rate : 3.2549798467889287e-12 ; fit : 0.07153333333333334
    batch_size :          60000
    best_batch_loss : 1.8041562700546954
    Epoch 6, Loss: 1.8041562700546954, fit: 0.07153333333333334
      batch rate adapted learning_rate :              1e-10
      loss :              1.8041562700546954
      loss_factor :              0.18041562700546954
      loss adapted learning_rate :              3.2549798467776713e-12
    epoch : 7 ; learning_rate : 3.2549798467776713e-12 ; fit : 0.07153333333333334
    batch_size :          60000
    best_batch_loss : 1.8041562700515776
    Epoch 7, Loss: 1.8041562700515776, fit: 0.07153333333333334
      batch rate adapted learning_rate :              1e-10
      loss :              1.8041562700515776
      loss_factor :              0.18041562700515776
      loss adapted learning_rate :              3.254979846766421e-12
    epoch : 8 ; learning_rate : 3.254979846766421e-12 ; fit : 0.07153333333333334
    batch_size :          60000
    best_batch_loss : 1.8041562700484588
    Epoch 8, Loss: 1.8041562700484588, fit: 0.07153333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.8041562700484588_fit_0.07153333333333334_2024-01-03_164537
  self.fit : 0.07153333333333334
  self.loss : 1.8041562700484588
  current_accuracy : 0.0722
   Accuracy mean: 0.0722
   Accuracy mean: 0.0722
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-07
    epoch : 0 ; learning_rate : 1e-07 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7726112869789596
    Epoch 0, Loss: 1.7726112869789596, fit: 0.08195
    Epoch 0, Loss: 1.7726112869789596
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7726112869789596_fit_0.08195_2024-01-03_164540
  self.fit : 0.08195
  self.loss : 1.7726112869789596
  current_accuracy : 0.0811
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-07
      loss :              1.7726112869789596
      loss_factor :              0.17726112869789595
      loss adapted learning_rate :              3.142150774725203e-09
    epoch : 0 ; learning_rate : 3.142150774725203e-09 ; fit : 0.08195
    batch_size :          60000
    best_batch_loss : 1.772611183723635
    Epoch 0, Loss: 1.772611183723635, fit: 0.08195
    Epoch 0, Loss: 1.772611183723635
      batch rate adapted learning_rate :              1e-07
      loss :              1.772611183723635
      loss_factor :              0.1772611183723635
      loss adapted learning_rate :              3.142150408662106e-09
    epoch : 1 ; learning_rate : 3.142150408662106e-09 ; fit : 0.08195
    batch_size :          60000
    best_batch_loss : 1.7726111804791982
    Epoch 1, Loss: 1.7726111804791982, fit: 0.08195
      batch rate adapted learning_rate :              1e-07
      loss :              1.7726111804791982
      loss_factor :              0.17726111804791983
      loss adapted learning_rate :              3.142150397159857e-09
    epoch : 2 ; learning_rate : 3.142150397159857e-09 ; fit : 0.08195
    batch_size :          60000
    best_batch_loss : 1.7726111772347612
    Epoch 2, Loss: 1.7726111772347612, fit: 0.08195
      batch rate adapted learning_rate :              1e-07
      loss :              1.7726111772347612
      loss_factor :              0.17726111772347614
      loss adapted learning_rate :              3.142150385657606e-09
    epoch : 3 ; learning_rate : 3.142150385657606e-09 ; fit : 0.08195
    batch_size :          60000
    best_batch_loss : 1.7726111739903248
    Epoch 3, Loss: 1.7726111739903248, fit: 0.08195
      batch rate adapted learning_rate :              1e-07
      loss :              1.7726111739903248
      loss_factor :              0.17726111739903247
      loss adapted learning_rate :              3.142150374155357e-09
    epoch : 4 ; learning_rate : 3.142150374155357e-09 ; fit : 0.08195
    batch_size :          60000
    best_batch_loss : 1.772611170745889
    Epoch 4, Loss: 1.772611170745889, fit: 0.08195
      batch rate adapted learning_rate :              1e-07
      loss :              1.772611170745889
      loss_factor :              0.1772611170745889
      loss adapted learning_rate :              3.1421503626531106e-09
    epoch : 5 ; learning_rate : 3.1421503626531106e-09 ; fit : 0.08195
    batch_size :          60000
    best_batch_loss : 1.7726111675014515
    Epoch 5, Loss: 1.7726111675014515, fit: 0.08195
      batch rate adapted learning_rate :              1e-07
      loss :              1.7726111675014515
      loss_factor :              0.17726111675014516
      loss adapted learning_rate :              3.1421503511508593e-09
    epoch : 6 ; learning_rate : 3.1421503511508593e-09 ; fit : 0.08195
    batch_size :          60000
    best_batch_loss : 1.7726111642570153
    Epoch 6, Loss: 1.7726111642570153, fit: 0.08195
      batch rate adapted learning_rate :              1e-07
      loss :              1.7726111642570153
      loss_factor :              0.17726111642570153
      loss adapted learning_rate :              3.142150339648611e-09
    epoch : 7 ; learning_rate : 3.142150339648611e-09 ; fit : 0.08195
    batch_size :          60000
    best_batch_loss : 1.7726111610125783
    Epoch 7, Loss: 1.7726111610125783, fit: 0.08195
      batch rate adapted learning_rate :              1e-07
      loss :              1.7726111610125783
      loss_factor :              0.17726111610125783
      loss adapted learning_rate :              3.1421503281463606e-09
    epoch : 8 ; learning_rate : 3.1421503281463606e-09 ; fit : 0.08195
    batch_size :          60000
    best_batch_loss : 1.772611157768142
    Epoch 8, Loss: 1.772611157768142, fit: 0.08195
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.772611157768142_fit_0.08195_2024-01-03_164601
  self.fit : 0.08195
  self.loss : 1.772611157768142
  current_accuracy : 0.0811
   Accuracy mean: 0.0811
   Accuracy mean: 0.0811
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7718908224710406
    Epoch 0, Loss: 1.7718908224710406, fit: 0.08833333333333333
    Epoch 0, Loss: 1.7718908224710406
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7718908224710406_fit_0.08833333333333333_2024-01-03_164603
  self.fit : 0.08833333333333333
  self.loss : 1.7718908224710406
  current_accuracy : 0.0944
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.0001
      loss :              1.7718908224710406
      loss_factor :              0.17718908224710406
      loss adapted learning_rate :              3.1395970867571003e-06
    epoch : 0 ; learning_rate : 3.1395970867571003e-06 ; fit : 0.08833333333333333
    batch_size :          60000
    best_batch_loss : 1.7717146260371326
    Epoch 0, Loss: 1.7717146260371326, fit: 0.08846666666666667
    Epoch 0, Loss: 1.7717146260371326
      batch rate adapted learning_rate :              0.0001
      loss :              1.7717146260371326
      loss_factor :              0.17717146260371325
      loss adapted learning_rate :              3.1389727161138963e-06
    epoch : 1 ; learning_rate : 3.1389727161138963e-06 ; fit : 0.08846666666666667
    batch_size :          60000
    best_batch_loss : 1.771709076334117
    Epoch 1, Loss: 1.771709076334117, fit: 0.08848333333333333
      batch rate adapted learning_rate :              0.0001
      loss :              1.771709076334117
      loss_factor :              0.1771709076334117
      loss adapted learning_rate :              3.13895305116469e-06
    epoch : 2 ; learning_rate : 3.13895305116469e-06 ; fit : 0.08848333333333333
    batch_size :          60000
    best_batch_loss : 1.7717035265212595
    Epoch 2, Loss: 1.7717035265212595, fit: 0.08846666666666667
      batch rate adapted learning_rate :              0.0001
      loss :              1.7717035265212595
      loss_factor :              0.17717035265212594
      loss adapted learning_rate :              3.138933385887867e-06
    epoch : 3 ; learning_rate : 3.138933385887867e-06 ; fit : 0.08846666666666667
    batch_size :          60000
    best_batch_loss : 1.771697975531466
    Epoch 3, Loss: 1.771697975531466, fit: 0.08848333333333333
      batch rate adapted learning_rate :              0.0001
      loss :              1.771697975531466
      loss_factor :              0.1771697975531466
      loss adapted learning_rate :              3.138913716502295e-06
    epoch : 4 ; learning_rate : 3.138913716502295e-06 ; fit : 0.08848333333333333
    batch_size :          60000
    best_batch_loss : 1.7716924233664493
    Epoch 4, Loss: 1.7716924233664493, fit: 0.08848333333333333
      batch rate adapted learning_rate :              0.0001
      loss :              1.7716924233664493
      loss_factor :              0.17716924233664494
      loss adapted learning_rate :              3.1388940430140826e-06
    epoch : 5 ; learning_rate : 3.1388940430140826e-06 ; fit : 0.08848333333333333
    batch_size :          60000
    best_batch_loss : 1.7716868700279227
    Epoch 5, Loss: 1.7716868700279227, fit: 0.08846666666666667
      batch rate adapted learning_rate :              0.0001
      loss :              1.7716868700279227
      loss_factor :              0.17716868700279226
      loss adapted learning_rate :              3.1388743654293375e-06
    epoch : 6 ; learning_rate : 3.1388743654293375e-06 ; fit : 0.08846666666666667
    batch_size :          60000
    best_batch_loss : 1.7716813155176014
    Epoch 6, Loss: 1.7716813155176014, fit: 0.08846666666666667
      batch rate adapted learning_rate :              0.0001
      loss :              1.7716813155176014
      loss_factor :              0.17716813155176014
      loss adapted learning_rate :              3.1388546837541793e-06
    epoch : 7 ; learning_rate : 3.1388546837541793e-06 ; fit : 0.08846666666666667
    batch_size :          60000
    best_batch_loss : 1.771675759837196
    Epoch 7, Loss: 1.771675759837196, fit: 0.08846666666666667
      batch rate adapted learning_rate :              0.0001
      loss :              1.771675759837196
      loss_factor :              0.1771675759837196
      loss adapted learning_rate :              3.1388349979947056e-06
    epoch : 8 ; learning_rate : 3.1388349979947056e-06 ; fit : 0.08846666666666667
    batch_size :          60000
    best_batch_loss : 1.7716702029884148
    Epoch 8, Loss: 1.7716702029884148, fit: 0.08846666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7716702029884148_fit_0.08846666666666667_2024-01-03_164624
  self.fit : 0.08846666666666667
  self.loss : 1.7716702029884148
  current_accuracy : 0.0944
   Accuracy mean: 0.0944
   Accuracy mean: 0.0944
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.001
    epoch : 0 ; learning_rate : 0.001 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7512506014419402
    Epoch 0, Loss: 1.7512506014419402, fit: 0.09586666666666667
    Epoch 0, Loss: 1.7512506014419402
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7512506014419402_fit_0.09586666666666667_2024-01-03_164627
  self.fit : 0.09586666666666667
  self.loss : 1.7512506014419402
  current_accuracy : 0.0964
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.001
      loss :              1.7512506014419402
      loss_factor :              0.17512506014419402
      loss adapted learning_rate :              3.066878669050757e-05
    epoch : 0 ; learning_rate : 3.066878669050757e-05 ; fit : 0.09586666666666667
    batch_size :          60000
    best_batch_loss : 1.7500796388996422
    Epoch 0, Loss: 1.7500796388996422, fit: 0.09628333333333333
    Epoch 0, Loss: 1.7500796388996422
      batch rate adapted learning_rate :              0.001
      loss :              1.7500796388996422
      loss_factor :              0.17500796388996423
      loss adapted learning_rate :              3.0627787424911025e-05
    epoch : 1 ; learning_rate : 3.0627787424911025e-05 ; fit : 0.09628333333333333
    batch_size :          60000
    best_batch_loss : 1.7500443161690118
    Epoch 1, Loss: 1.7500443161690118, fit: 0.09628333333333333
      batch rate adapted learning_rate :              0.001
      loss :              1.7500443161690118
      loss_factor :              0.17500443161690118
      loss adapted learning_rate :              3.062655108555464e-05
    epoch : 2 ; learning_rate : 3.062655108555464e-05 ; fit : 0.09628333333333333
    batch_size :          60000
    best_batch_loss : 1.7500090615495447
    Epoch 2, Loss: 1.7500090615495447, fit: 0.0963
      batch rate adapted learning_rate :              0.001
      loss :              1.7500090615495447
      loss_factor :              0.17500090615495448
      loss adapted learning_rate :              3.062531715505518e-05
    epoch : 3 ; learning_rate : 3.062531715505518e-05 ; fit : 0.0963
    batch_size :          60000
    best_batch_loss : 1.7499738294048588
    Epoch 3, Loss: 1.7499738294048588, fit: 0.0963
      batch rate adapted learning_rate :              0.001
      loss :              1.7499738294048588
      loss_factor :              0.1749973829404859
      loss adapted learning_rate :              3.062408403601906e-05
    epoch : 4 ; learning_rate : 3.062408403601906e-05 ; fit : 0.0963
    batch_size :          60000
    best_batch_loss : 1.7499386199212417
    Epoch 4, Loss: 1.7499386199212417, fit: 0.09631666666666666
      batch rate adapted learning_rate :              0.001
      loss :              1.7499386199212417
      loss_factor :              0.17499386199212416
      loss adapted learning_rate :              3.0622851734918596e-05
    epoch : 5 ; learning_rate : 3.0622851734918596e-05 ; fit : 0.09631666666666666
    batch_size :          60000
    best_batch_loss : 1.7499034332914971
    Epoch 5, Loss: 1.7499034332914971, fit: 0.09633333333333334
      batch rate adapted learning_rate :              0.001
      loss :              1.7499034332914971
      loss_factor :              0.1749903433291497
      loss adapted learning_rate :              3.0621620258453686e-05
    epoch : 6 ; learning_rate : 3.0621620258453686e-05 ; fit : 0.09633333333333334
    batch_size :          60000
    best_batch_loss : 1.749868269713008
    Epoch 6, Loss: 1.749868269713008, fit: 0.09635
      batch rate adapted learning_rate :              0.001
      loss :              1.749868269713008
      loss_factor :              0.1749868269713008
      loss adapted learning_rate :              3.062038961348397e-05
    epoch : 7 ; learning_rate : 3.062038961348397e-05 ; fit : 0.09635
    batch_size :          60000
    best_batch_loss : 1.749833129387634
    Epoch 7, Loss: 1.749833129387634, fit: 0.09635
      batch rate adapted learning_rate :              0.001
      loss :              1.749833129387634
      loss_factor :              0.1749833129387634
      loss adapted learning_rate :              3.0619159807025204e-05
    epoch : 8 ; learning_rate : 3.0619159807025204e-05 ; fit : 0.09635
    batch_size :          60000
    best_batch_loss : 1.7497980125216053
    Epoch 8, Loss: 1.7497980125216053, fit: 0.09641666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7497980125216053_fit_0.09641666666666666_2024-01-03_164648
  self.fit : 0.09641666666666666
  self.loss : 1.7497980125216053
  current_accuracy : 0.0966
   Accuracy mean: 0.0964
   Accuracy mean: 0.0966
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.02
    epoch : 0 ; learning_rate : 0.02 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7068102142410306
    Epoch 0, Loss: 1.7068102142410306, fit: 0.11731666666666667
    Epoch 0, Loss: 1.7068102142410306
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7068102142410306_fit_0.11731666666666667_2024-01-03_164650
  self.fit : 0.11731666666666667
  self.loss : 1.7068102142410306
  current_accuracy : 0.112
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.02
      loss :              1.7068102142410306
      loss_factor :              0.17068102142410307
      loss adapted learning_rate :              0.0005826402214875026
    epoch : 0 ; learning_rate : 0.0005826402214875026 ; fit : 0.11731666666666667
    batch_size :          60000
    best_batch_loss : 1.6941947826592518
    Epoch 0, Loss: 1.6941947826592518, fit: 0.1205
    Epoch 0, Loss: 1.6941947826592518
      batch rate adapted learning_rate :              0.02
      loss :              1.6941947826592518
      loss_factor :              0.16941947826592518
      loss adapted learning_rate :              0.0005740591923179659
    epoch : 1 ; learning_rate : 0.0005740591923179659 ; fit : 0.1205
    batch_size :          60000
    best_batch_loss : 1.6936926603771574
    Epoch 1, Loss: 1.6936926603771574, fit: 0.1206
      batch rate adapted learning_rate :              0.02
      loss :              1.6936926603771574
      loss_factor :              0.16936926603771574
      loss adapted learning_rate :              0.0005737189655630906
    epoch : 2 ; learning_rate : 0.0005737189655630906 ; fit : 0.1206
    batch_size :          60000
    best_batch_loss : 1.6931942240592048
    Epoch 2, Loss: 1.6931942240592048, fit: 0.1209
      batch rate adapted learning_rate :              0.02
      loss :              1.6931942240592048
      loss_factor :              0.1693194224059205
      loss adapted learning_rate :              0.0005733813360774906
    epoch : 3 ; learning_rate : 0.0005733813360774906 ; fit : 0.1209
    batch_size :          60000
    best_batch_loss : 1.6926949026268436
    Epoch 3, Loss: 1.6926949026268436, fit: 0.12105
      batch rate adapted learning_rate :              0.02
      loss :              1.6926949026268436
      loss_factor :              0.16926949026268437
      loss adapted learning_rate :              0.00057304320667578
    epoch : 4 ; learning_rate : 0.00057304320667578 ; fit : 0.12105
    batch_size :          60000
    best_batch_loss : 1.6921968295262444
    Epoch 4, Loss: 1.6921968295262444, fit: 0.12136666666666666
      batch rate adapted learning_rate :              0.02
      loss :              1.6921968295262444
      loss_factor :              0.16921968295262443
      loss adapted learning_rate :              0.0005727060219717346
    epoch : 5 ; learning_rate : 0.0005727060219717346 ; fit : 0.12136666666666666
    batch_size :          60000
    best_batch_loss : 1.6917016775450604
    Epoch 5, Loss: 1.6917016775450604, fit: 0.12156666666666667
      batch rate adapted learning_rate :              0.02
      loss :              1.6917016775450604
      loss_factor :              0.16917016775450605
      loss adapted learning_rate :              0.0005723709131617544
    epoch : 6 ; learning_rate : 0.0005723709131617544 ; fit : 0.12156666666666667
    batch_size :          60000
    best_batch_loss : 1.6912106393654078
    Epoch 6, Loss: 1.6912106393654078, fit: 0.12185
      batch rate adapted learning_rate :              0.02
      loss :              1.6912106393654078
      loss_factor :              0.16912106393654078
      loss adapted learning_rate :              0.0005720386853405502
    epoch : 7 ; learning_rate : 0.0005720386853405502 ; fit : 0.12185
    batch_size :          60000
    best_batch_loss : 1.6907243660257494
    Epoch 7, Loss: 1.6907243660257494, fit: 0.12216666666666667
      batch rate adapted learning_rate :              0.02
      loss :              1.6907243660257494
      loss_factor :              0.16907243660257493
      loss adapted learning_rate :              0.0005717097763746344
    epoch : 8 ; learning_rate : 0.0005717097763746344 ; fit : 0.12216666666666667
    batch_size :          60000
    best_batch_loss : 1.6902429091820532
    Epoch 8, Loss: 1.6902429091820532, fit: 0.12246666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.6902429091820532_fit_0.12246666666666667_2024-01-03_164712
  self.fit : 0.12246666666666667
  self.loss : 1.6902429091820532
  current_accuracy : 0.1141
   Accuracy mean: 0.112
   Accuracy mean: 0.1141
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.025
    epoch : 0 ; learning_rate : 0.025 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.72578639348418
    Epoch 0, Loss: 1.72578639348418, fit: 0.10648333333333333
    Epoch 0, Loss: 1.72578639348418
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.72578639348418_fit_0.10648333333333333_2024-01-03_164714
  self.fit : 0.10648333333333333
  self.loss : 1.72578639348418
  current_accuracy : 0.1231
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.025
      loss :              1.72578639348418
      loss_factor :              0.172578639348418
      loss adapted learning_rate :              0.0007445846689837833
    epoch : 0 ; learning_rate : 0.0007445846689837833 ; fit : 0.10648333333333333
    batch_size :          60000
    best_batch_loss : 1.6890041745431348
    Epoch 0, Loss: 1.6890041745431348, fit: 0.12185
    Epoch 0, Loss: 1.6890041745431348
      batch rate adapted learning_rate :              0.025
      loss :              1.6890041745431348
      loss_factor :              0.16890041745431347
      loss adapted learning_rate :              0.000713183775406034
    epoch : 1 ; learning_rate : 0.000713183775406034 ; fit : 0.12185
    batch_size :          60000
    best_batch_loss : 1.6880057424255832
    Epoch 1, Loss: 1.6880057424255832, fit: 0.1222
      batch rate adapted learning_rate :              0.025
      loss :              1.6880057424255832
      loss_factor :              0.16880057424255832
      loss adapted learning_rate :              0.0007123408466154361
    epoch : 2 ; learning_rate : 0.0007123408466154361 ; fit : 0.1222
    batch_size :          60000
    best_batch_loss : 1.6870499382356412
    Epoch 2, Loss: 1.6870499382356412, fit: 0.12281666666666667
      batch rate adapted learning_rate :              0.025
      loss :              1.6870499382356412
      loss_factor :              0.16870499382356413
      loss adapted learning_rate :              0.0007115343735252203
    epoch : 3 ; learning_rate : 0.0007115343735252203 ; fit : 0.12281666666666667
    batch_size :          60000
    best_batch_loss : 1.6860967242222609
    Epoch 3, Loss: 1.6860967242222609, fit: 0.12331666666666667
      batch rate adapted learning_rate :              0.025
      loss :              1.6860967242222609
      loss_factor :              0.16860967242222608
      loss adapted learning_rate :              0.0007107305408582597
    epoch : 4 ; learning_rate : 0.0007107305408582597 ; fit : 0.12331666666666667
    batch_size :          60000
    best_batch_loss : 1.6851477469025815
    Epoch 4, Loss: 1.6851477469025815, fit: 0.12388333333333333
      batch rate adapted learning_rate :              0.025
      loss :              1.6851477469025815
      loss_factor :              0.16851477469025816
      loss adapted learning_rate :              0.0007099307322227118
    epoch : 5 ; learning_rate : 0.0007099307322227118 ; fit : 0.12388333333333333
    batch_size :          60000
    best_batch_loss : 1.6842055890238328
    Epoch 5, Loss: 1.6842055890238328, fit: 0.12418333333333334
      batch rate adapted learning_rate :              0.025
      loss :              1.6842055890238328
      loss_factor :              0.16842055890238328
      loss adapted learning_rate :              0.0007091371165247789
    epoch : 6 ; learning_rate : 0.0007091371165247789 ; fit : 0.12418333333333334
    batch_size :          60000
    best_batch_loss : 1.6832728720561445
    Epoch 6, Loss: 1.6832728720561445, fit: 0.12476666666666666
      batch rate adapted learning_rate :              0.025
      loss :              1.6832728720561445
      loss_factor :              0.16832728720561446
      loss adapted learning_rate :              0.0007083518904500355
    epoch : 7 ; learning_rate : 0.0007083518904500355 ; fit : 0.12476666666666666
    batch_size :          60000
    best_batch_loss : 1.6823509818050517
    Epoch 7, Loss: 1.6823509818050517, fit: 0.12518333333333334
      batch rate adapted learning_rate :              0.025
      loss :              1.6823509818050517
      loss_factor :              0.16823509818050517
      loss adapted learning_rate :              0.0007075762064951054
    epoch : 8 ; learning_rate : 0.0007075762064951054 ; fit : 0.12518333333333334
    batch_size :          60000
    best_batch_loss : 1.6814395658467252
    Epoch 8, Loss: 1.6814395658467252, fit: 0.1255
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.6814395658467252_fit_0.1255_2024-01-03_164736
  self.fit : 0.1255
  self.loss : 1.6814395658467252
  current_accuracy : 0.1266
   Accuracy mean: 0.1231
   Accuracy mean: 0.1266
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.03
    epoch : 0 ; learning_rate : 0.03 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7717532829768976
    Epoch 0, Loss: 1.7717532829768976, fit: 0.09155
    Epoch 0, Loss: 1.7717532829768976
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7717532829768976_fit_0.09155_2024-01-03_164738
  self.fit : 0.09155
  self.loss : 1.7717532829768976
  current_accuracy : 0.1164
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.03
      loss :              1.7717532829768976
      loss_factor :              0.17717532829768975
      loss adapted learning_rate :              0.0009417329087218243
    epoch : 0 ; learning_rate : 0.0009417329087218243 ; fit : 0.09155
    batch_size :          60000
    best_batch_loss : 1.7118453196767836
    Epoch 0, Loss: 1.7118453196767836, fit: 0.11146666666666667
    Epoch 0, Loss: 1.7118453196767836
      batch rate adapted learning_rate :              0.03
      loss :              1.7118453196767836
      loss_factor :              0.17118453196767836
      loss adapted learning_rate :              0.0008791243195497927
    epoch : 1 ; learning_rate : 0.0008791243195497927 ; fit : 0.11146666666666667
    batch_size :          60000
    best_batch_loss : 1.7108274196729376
    Epoch 1, Loss: 1.7108274196729376, fit: 0.11183333333333334
      batch rate adapted learning_rate :              0.03
      loss :              1.7108274196729376
      loss_factor :              0.17108274196729376
      loss adapted learning_rate :              0.0008780791379714285
    epoch : 2 ; learning_rate : 0.0008780791379714285 ; fit : 0.11183333333333334
    batch_size :          60000
    best_batch_loss : 1.7099022208976753
    Epoch 2, Loss: 1.7099022208976753, fit: 0.112
      batch rate adapted learning_rate :              0.03
      loss :              1.7099022208976753
      loss_factor :              0.17099022208976752
      loss adapted learning_rate :              0.0008771296815092407
    epoch : 3 ; learning_rate : 0.0008771296815092407 ; fit : 0.112
    batch_size :          60000
    best_batch_loss : 1.7090062032776336
    Epoch 3, Loss: 1.7090062032776336, fit: 0.11243333333333333
      batch rate adapted learning_rate :              0.03
      loss :              1.7090062032776336
      loss_factor :              0.17090062032776338
      loss adapted learning_rate :              0.0008762106608524298
    epoch : 4 ; learning_rate : 0.0008762106608524298 ; fit : 0.11243333333333333
    batch_size :          60000
    best_batch_loss : 1.7081342491198106
    Epoch 4, Loss: 1.7081342491198106, fit: 0.11286666666666667
      batch rate adapted learning_rate :              0.03
      loss :              1.7081342491198106
      loss_factor :              0.17081342491198107
      loss adapted learning_rate :              0.0008753167839048297
    epoch : 5 ; learning_rate : 0.0008753167839048297 ; fit : 0.11286666666666667
    batch_size :          60000
    best_batch_loss : 1.707275591693287
    Epoch 5, Loss: 1.707275591693287, fit: 0.11325
      batch rate adapted learning_rate :              0.03
      loss :              1.707275591693287
      loss_factor :              0.1707275591693287
      loss adapted learning_rate :              0.0008744369837974989
    epoch : 6 ; learning_rate : 0.0008744369837974989 ; fit : 0.11325
    batch_size :          60000
    best_batch_loss : 1.7064171337662208
    Epoch 6, Loss: 1.7064171337662208, fit: 0.1136
      batch rate adapted learning_rate :              0.03
      loss :              1.7064171337662208
      loss_factor :              0.1706417133766221
      loss adapted learning_rate :              0.0008735578303232774
    epoch : 7 ; learning_rate : 0.0008735578303232774 ; fit : 0.1136
    batch_size :          60000
    best_batch_loss : 1.7055469225992894
    Epoch 7, Loss: 1.7055469225992894, fit: 0.11395
      batch rate adapted learning_rate :              0.03
      loss :              1.7055469225992894
      loss_factor :              0.17055469225992895
      loss adapted learning_rate :              0.000872667091556372
    epoch : 8 ; learning_rate : 0.000872667091556372 ; fit : 0.11395
    batch_size :          60000
    best_batch_loss : 1.704659662276082
    Epoch 8, Loss: 1.704659662276082, fit: 0.1144
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.704659662276082_fit_0.1144_2024-01-03_164800
  self.fit : 0.1144
  self.loss : 1.704659662276082
  current_accuracy : 0.1197
   Accuracy mean: 0.1164
   Accuracy mean: 0.1197
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.1
    epoch : 0 ; learning_rate : 0.1 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7650844164640032
    Epoch 0, Loss: 1.7650844164640032, fit: 0.08943333333333334
    Epoch 0, Loss: 1.7650844164640032
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7650844164640032_fit_0.08943333333333334_2024-01-03_164803
  self.fit : 0.08943333333333334
  self.loss : 1.7650844164640032
  current_accuracy : 0.1396
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.1
      loss :              1.7650844164640032
      loss_factor :              0.1765084416464003
      loss adapted learning_rate :              0.0031155229972440707
    epoch : 0 ; learning_rate : 0.0031155229972440707 ; fit : 0.08943333333333334
    batch_size :          60000
    best_batch_loss : 1.6579949549289612
    Epoch 0, Loss: 1.6579949549289612, fit: 0.13701666666666668
    Epoch 0, Loss: 1.6579949549289612
      batch rate adapted learning_rate :              0.1
      loss :              1.6579949549289612
      loss_factor :              0.16579949549289613
      loss adapted learning_rate :              0.002748947270569889
    epoch : 1 ; learning_rate : 0.002748947270569889 ; fit : 0.13701666666666668
    batch_size :          60000
    best_batch_loss : 1.656816331703483
    Epoch 1, Loss: 1.656816331703483, fit: 0.13751666666666668
      batch rate adapted learning_rate :              0.1
      loss :              1.656816331703483
      loss_factor :              0.16568163317034829
      loss adapted learning_rate :              0.0027450403569993856
    epoch : 2 ; learning_rate : 0.0027450403569993856 ; fit : 0.13751666666666668
    batch_size :          60000
    best_batch_loss : 1.6557725069942502
    Epoch 2, Loss: 1.6557725069942502, fit: 0.13811666666666667
      batch rate adapted learning_rate :              0.1
      loss :              1.6557725069942502
      loss_factor :              0.16557725069942503
      loss adapted learning_rate :              0.002741582594918025
    epoch : 3 ; learning_rate : 0.002741582594918025 ; fit : 0.13811666666666667
    batch_size :          60000
    best_batch_loss : 1.6546908903061208
    Epoch 3, Loss: 1.6546908903061208, fit: 0.13876666666666668
      batch rate adapted learning_rate :              0.1
      loss :              1.6546908903061208
      loss_factor :              0.16546908903061208
      loss adapted learning_rate :              0.002738001942462063
    epoch : 4 ; learning_rate : 0.002738001942462063 ; fit : 0.13876666666666668
    batch_size :          60000
    best_batch_loss : 1.6535586486225615
    Epoch 4, Loss: 1.6535586486225615, fit: 0.13915
      batch rate adapted learning_rate :              0.1
      loss :              1.6535586486225615
      loss_factor :              0.16535586486225615
      loss adapted learning_rate :              0.002734256204434472
    epoch : 5 ; learning_rate : 0.002734256204434472 ; fit : 0.13915
    batch_size :          60000
    best_batch_loss : 1.6523934752415668
    Epoch 5, Loss: 1.6523934752415668, fit: 0.1396
      batch rate adapted learning_rate :              0.1
      loss :              1.6523934752415668
      loss_factor :              0.16523934752415667
      loss adapted learning_rate :              0.0027304041970209023
    epoch : 6 ; learning_rate : 0.0027304041970209023 ; fit : 0.1396
    batch_size :          60000
    best_batch_loss : 1.6511930164618205
    Epoch 6, Loss: 1.6511930164618205, fit: 0.14026666666666668
      batch rate adapted learning_rate :              0.1
      loss :              1.6511930164618205
      loss_factor :              0.16511930164618205
      loss adapted learning_rate :              0.002726438377612286
    epoch : 7 ; learning_rate : 0.002726438377612286 ; fit : 0.14026666666666668
    batch_size :          60000
    best_batch_loss : 1.649973021042585
    Epoch 7, Loss: 1.649973021042585, fit: 0.14083333333333334
      batch rate adapted learning_rate :              0.1
      loss :              1.649973021042585
      loss_factor :              0.1649973021042585
      loss adapted learning_rate :              0.002722410970168395
    epoch : 8 ; learning_rate : 0.002722410970168395 ; fit : 0.14083333333333334
    batch_size :          60000
    best_batch_loss : 1.64874546763429
    Epoch 8, Loss: 1.64874546763429, fit: 0.14155
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.64874546763429_fit_0.14155_2024-01-03_164824
  self.fit : 0.14155
  self.loss : 1.64874546763429
  current_accuracy : 0.1436
   Accuracy mean: 0.1396
   Accuracy mean: 0.1436
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.9
    epoch : 0 ; learning_rate : 0.9 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7333685957585638
    Epoch 0, Loss: 1.7333685957585638, fit: 0.10531666666666667
    Epoch 0, Loss: 1.7333685957585638
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7333685957585638_fit_0.10531666666666667_2024-01-03_164827
  self.fit : 0.10531666666666667
  self.loss : 1.7333685957585638
  current_accuracy : 0.3827
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.9
      loss :              1.7333685957585638
      loss_factor :              0.17333685957585637
      loss adapted learning_rate :              0.027041100198858137
    epoch : 0 ; learning_rate : 0.027041100198858137 ; fit : 0.10531666666666667
    batch_size :          60000
    best_batch_loss : 1.2110507777201054
    Epoch 0, Loss: 1.2110507777201054, fit: 0.37538333333333335
    Epoch 0, Loss: 1.2110507777201054
      batch rate adapted learning_rate :              0.9
      loss :              1.2110507777201054
      loss_factor :              0.12110507777201054
      loss adapted learning_rate :              0.013199795875948252
    epoch : 1 ; learning_rate : 0.013199795875948252 ; fit : 0.37538333333333335
    batch_size :          60000
    best_batch_loss : 1.1854604709282675
    Epoch 1, Loss: 1.1854604709282675, fit: 0.3884166666666667
      batch rate adapted learning_rate :              0.9
      loss :              1.1854604709282675
      loss_factor :              0.11854604709282675
      loss adapted learning_rate :              0.012647848753201227
    epoch : 2 ; learning_rate : 0.012647848753201227 ; fit : 0.3884166666666667
    batch_size :          60000
    best_batch_loss : 1.175197135544983
    Epoch 2, Loss: 1.175197135544983, fit: 0.3933333333333333
      batch rate adapted learning_rate :              0.9
      loss :              1.175197135544983
      loss_factor :              0.1175197135544983
      loss adapted learning_rate :              0.0124297947665382
    epoch : 3 ; learning_rate : 0.0124297947665382 ; fit : 0.3933333333333333
    batch_size :          60000
    best_batch_loss : 1.1652088300704937
    Epoch 3, Loss: 1.1652088300704937, fit: 0.3984
      batch rate adapted learning_rate :              0.9
      loss :              1.1652088300704937
      loss_factor :              0.11652088300704937
      loss adapted learning_rate :              0.012219404559068238
    epoch : 4 ; learning_rate : 0.012219404559068238 ; fit : 0.3984
    batch_size :          60000
    best_batch_loss : 1.1563815333973317
    Epoch 4, Loss: 1.1563815333973317, fit: 0.4027
      batch rate adapted learning_rate :              0.9
      loss :              1.1563815333973317
      loss_factor :              0.11563815333973318
      loss adapted learning_rate :              0.01203496425704128
    epoch : 5 ; learning_rate : 0.01203496425704128 ; fit : 0.4027
    batch_size :          60000
    best_batch_loss : 1.1476535573897921
    Epoch 5, Loss: 1.1476535573897921, fit: 0.4076
      batch rate adapted learning_rate :              0.9
      loss :              1.1476535573897921
      loss_factor :              0.11476535573897921
      loss adapted learning_rate :              0.011853978190105004
    epoch : 6 ; learning_rate : 0.011853978190105004 ; fit : 0.4076
    batch_size :          60000
    best_batch_loss : 1.1396047053225407
    Epoch 6, Loss: 1.1396047053225407, fit: 0.4114333333333333
      batch rate adapted learning_rate :              0.9
      loss :              1.1396047053225407
      loss_factor :              0.11396047053225407
      loss adapted learning_rate :              0.011688289959539473
    epoch : 7 ; learning_rate : 0.011688289959539473 ; fit : 0.4114333333333333
    batch_size :          60000
    best_batch_loss : 1.1317155224823798
    Epoch 7, Loss: 1.1317155224823798, fit: 0.4151666666666667
      batch rate adapted learning_rate :              0.9
      loss :              1.1317155224823798
      loss_factor :              0.11317155224823798
      loss adapted learning_rate :              0.011527020214448094
    epoch : 8 ; learning_rate : 0.011527020214448094 ; fit : 0.4151666666666667
    batch_size :          60000
    best_batch_loss : 1.1242238976411014
    Epoch 8, Loss: 1.1242238976411014, fit: 0.41878333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.1242238976411014_fit_0.41878333333333334_2024-01-03_164848
  self.fit : 0.41878333333333334
  self.loss : 1.1242238976411014
  current_accuracy : 0.4329
   Accuracy mean: 0.3827
   Accuracy mean: 0.4329
  Error saving file: doc/out/test_combinations_results/20240103164849.
  normalized_accuracies :      [0.         0.         0.01555677 0.01555677 0.03984716 0.03984716
   0.07614629 0.07614629 0.0816048  0.08215066 0.12418122 0.12991266
   0.15447598 0.16402838 0.13618996 0.14519651 0.19950873 0.21042576
   0.86299127 1.        ]
batch_rate :  0.5
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-18
    epoch : 0 ; learning_rate : 5e-18 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7594191949912728
    Epoch 0, Loss: 1.760633279346773, fit: 0.09453333333333333
    Epoch 0, Loss: 1.760633279346773
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7594191949912728_fit_0.09453333333333333_2024-01-03_164852
  self.fit : 0.09453333333333333
  self.loss : 1.7594191949912728
  current_accuracy : 0.0962
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-18
      loss :              1.7594191949912728
      loss_factor :              0.17594191949912727
      loss adapted learning_rate :              1.5477779518518692e-19
    epoch : 0 ; learning_rate : 1.5477779518518692e-19 ; fit : 0.09453333333333333
    batch_size :          30000
    best_batch_loss : 1.7561843523042848
    Epoch 0, Loss: 1.7606332793467727, fit: 0.0963
    Epoch 0, Loss: 1.7606332793467727
      batch rate adapted learning_rate :              5e-18
      loss :              1.7561843523042848
      loss_factor :              0.17561843523042847
      loss adapted learning_rate :              1.5420917396392103e-19
    epoch : 1 ; learning_rate : 1.5420917396392103e-19 ; fit : 0.0963
    batch_size :          30000
    best_batch_loss : 1.7602615389457463
    Epoch 1, Loss: 1.7606332793467727, fit: 0.094
      batch rate adapted learning_rate :              5e-18
      loss :              1.7610050197477989
      loss_factor :              0.1761005019747799
      loss adapted learning_rate :              1.550569339788473e-19
    epoch : 2 ; learning_rate : 1.550569339788473e-19 ; fit : 0.094
    batch_size :          30000
    best_batch_loss : 1.7582052863171425
    Epoch 2, Loss: 1.7606332793467723, fit: 0.09303333333333333
      batch rate adapted learning_rate :              5e-18
      loss :              1.7630612723764023
      loss_factor :              0.17630612723764022
      loss adapted learning_rate :              1.5541925250767492e-19
    epoch : 3 ; learning_rate : 1.5541925250767492e-19 ; fit : 0.09303333333333333
    batch_size :          30000
    best_batch_loss : 1.7592234223524004
    Epoch 3, Loss: 1.7606332793467727, fit: 0.09356666666666667
      batch rate adapted learning_rate :              5e-18
      loss :              1.762043136341145
      loss_factor :              0.1762043136341145
      loss adapted learning_rate :              1.5523980071634698e-19
    epoch : 4 ; learning_rate : 1.5523980071634698e-19 ; fit : 0.09356666666666667
    batch_size :          30000
    best_batch_loss : 1.7541564577677968
    Epoch 4, Loss: 1.7606332793467725, fit: 0.0915
      batch rate adapted learning_rate :              5e-18
      loss :              1.7671101009257482
      loss_factor :              0.1767110100925748
      loss adapted learning_rate :              1.561339054396904e-19
    epoch : 5 ; learning_rate : 1.561339054396904e-19 ; fit : 0.0915
    batch_size :          30000
    best_batch_loss : 1.759744566122594
    Epoch 5, Loss: 1.7606332793467727, fit: 0.0937
      batch rate adapted learning_rate :              5e-18
      loss :              1.7615219925709515
      loss_factor :              0.17615219925709516
      loss adapted learning_rate :              1.551479865155568e-19
    epoch : 6 ; learning_rate : 1.551479865155568e-19 ; fit : 0.0937
    batch_size :          30000
    best_batch_loss : 1.7588042750699222
    Epoch 6, Loss: 1.7606332793467727, fit: 0.09493333333333333
      batch rate adapted learning_rate :              5e-18
      loss :              1.7588042750699222
      loss_factor :              0.17588042750699223
      loss adapted learning_rate :              1.5466962390021175e-19
    epoch : 7 ; learning_rate : 1.5466962390021175e-19 ; fit : 0.09493333333333333
    batch_size :          30000
    best_batch_loss : 1.760538372458044
    Epoch 7, Loss: 1.7606332793467727, fit: 0.09436666666666667
      batch rate adapted learning_rate :              5e-18
      loss :              1.760538372458044
      loss_factor :              0.1760538372458044
      loss adapted learning_rate :              1.5497476804486093e-19
    epoch : 8 ; learning_rate : 1.5497476804486093e-19 ; fit : 0.09436666666666667
    batch_size :          30000
    best_batch_loss : 1.7584429860408046
    Epoch 8, Loss: 1.7606332793467727, fit: 0.09323333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7628235726527408_fit_0.09323333333333333_2024-01-03_164914
  self.fit : 0.09323333333333333
  self.loss : 1.7628235726527408
  current_accuracy : 0.0962
   Accuracy mean: 0.0962
   Accuracy mean: 0.0962
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-11
    epoch : 0 ; learning_rate : 5e-11 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7255463298670422
    Epoch 0, Loss: 1.7266798463796387, fit: 0.10656666666666667
    Epoch 0, Loss: 1.7266798463796387
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7255463298670422_fit_0.10656666666666667_2024-01-03_164917
  self.fit : 0.10656666666666667
  self.loss : 1.7255463298670422
  current_accuracy : 0.1018
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-11
      loss :              1.7255463298670422
      loss_factor :              0.1725546329867042
      loss adapted learning_rate :              1.4887550682588094e-12
    epoch : 0 ; learning_rate : 1.4887550682588094e-12 ; fit : 0.10656666666666667
    batch_size :          30000
    best_batch_loss : 1.7241802196982943
    Epoch 0, Loss: 1.7266798462415687, fit: 0.1077
    Epoch 0, Loss: 1.7266798462415687
      batch rate adapted learning_rate :              5e-11
      loss :              1.7241802196982943
      loss_factor :              0.17241802196982942
      loss adapted learning_rate :              1.486398714999429e-12
    epoch : 1 ; learning_rate : 1.486398714999429e-12 ; fit : 0.1077
    batch_size :          30000
    best_batch_loss : 1.7252781497860104
    Epoch 1, Loss: 1.7266798462359607, fit: 0.10646666666666667
      batch rate adapted learning_rate :              5e-11
      loss :              1.7280815426859113
      loss_factor :              0.17280815426859114
      loss adapted learning_rate :              1.4931329090858597e-12
    epoch : 2 ; learning_rate : 1.4931329090858597e-12 ; fit : 0.10646666666666667
    batch_size :          30000
    best_batch_loss : 1.726666228568467
    Epoch 2, Loss: 1.7266798462305497, fit: 0.1065
      batch rate adapted learning_rate :              5e-11
      loss :              1.7266934638926323
      loss_factor :              0.17266934638926323
      loss adapted learning_rate :              1.4907351591247687e-12
    epoch : 3 ; learning_rate : 1.4907351591247687e-12 ; fit : 0.1065
    batch_size :          30000
    best_batch_loss : 1.7255965896903094
    Epoch 3, Loss: 1.7266798462254105, fit: 0.10656666666666667
      batch rate adapted learning_rate :              5e-11
      loss :              1.7255965896903094
      loss_factor :              0.17255965896903094
      loss adapted learning_rate :              1.4888417951754131e-12
    epoch : 4 ; learning_rate : 1.4888417951754131e-12 ; fit : 0.10656666666666667
    batch_size :          30000
    best_batch_loss : 1.725321187594297
    Epoch 4, Loss: 1.726679846219932, fit: 0.1074
      batch rate adapted learning_rate :              5e-11
      loss :              1.725321187594297
      loss_factor :              0.1725321187594297
      loss adapted learning_rate :              1.4883666001808978e-12
    epoch : 5 ; learning_rate : 1.4883666001808978e-12 ; fit : 0.1074
    batch_size :          30000
    best_batch_loss : 1.7242121965718091
    Epoch 5, Loss: 1.7266798462143849, fit: 0.1052
      batch rate adapted learning_rate :              5e-11
      loss :              1.7291474958569606
      loss_factor :              0.17291474958569605
      loss adapted learning_rate :              1.4949755312141986e-12
    epoch : 6 ; learning_rate : 1.4949755312141986e-12 ; fit : 0.1052
    batch_size :          30000
    best_batch_loss : 1.725733826114214
    Epoch 6, Loss: 1.726679846209045, fit: 0.10716666666666666
      batch rate adapted learning_rate :              5e-11
      loss :              1.725733826114214
      loss_factor :              0.1725733826114214
      loss adapted learning_rate :              1.4890786192974025e-12
    epoch : 7 ; learning_rate : 1.4890786192974025e-12 ; fit : 0.10716666666666666
    batch_size :          30000
    best_batch_loss : 1.7229810946285682
    Epoch 7, Loss: 1.72667984620344, fit: 0.1085
      batch rate adapted learning_rate :              5e-11
      loss :              1.7229810946285682
      loss_factor :              0.1722981094628568
      loss adapted learning_rate :              1.4843319262237296e-12
    epoch : 8 ; learning_rate : 1.4843319262237296e-12 ; fit : 0.1085
    batch_size :          30000
    best_batch_loss : 1.7253768691287403
    Epoch 8, Loss: 1.7266798461981199, fit: 0.1061
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7279828232674992_fit_0.1061_2024-01-03_164937
  self.fit : 0.1061
  self.loss : 1.7279828232674992
  current_accuracy : 0.1018
   Accuracy mean: 0.1018
   Accuracy mean: 0.1018
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-08
    epoch : 0 ; learning_rate : 5e-08 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7409307088103176
    Epoch 0, Loss: 1.7411268298929095, fit: 0.0965
    Epoch 0, Loss: 1.7411268298929095
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7413229509755013_fit_0.0965_2024-01-03_164940
  self.fit : 0.0965
  self.loss : 1.7413229509755013
  current_accuracy : 0.1034
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-08
      loss :              1.7413229509755013
      loss_factor :              0.17413229509755013
      loss adapted learning_rate :              1.516102809797014e-09
    epoch : 0 ; learning_rate : 1.516102809797014e-09 ; fit : 0.0965
    batch_size :          30000
    best_batch_loss : 1.7390734603637645
    Epoch 0, Loss: 1.741126772540796, fit: 0.09773333333333334
    Epoch 0, Loss: 1.741126772540796
      batch rate adapted learning_rate :              5e-08
      loss :              1.7390734603637645
      loss_factor :              0.17390734603637645
      loss adapted learning_rate :              1.5121882502707989e-09
    epoch : 1 ; learning_rate : 1.5121882502707989e-09 ; fit : 0.09773333333333334
    batch_size :          30000
    best_batch_loss : 1.73978291031594
    Epoch 1, Loss: 1.7411267700785014, fit: 0.09743333333333333
      batch rate adapted learning_rate :              5e-08
      loss :              1.73978291031594
      loss_factor :              0.17397829103159398
      loss adapted learning_rate :              1.5134222875137006e-09
    epoch : 2 ; learning_rate : 1.5134222875137006e-09 ; fit : 0.09743333333333333
    batch_size :          30000
    best_batch_loss : 1.7389083676355765
    Epoch 2, Loss: 1.7411267676521325, fit: 0.09533333333333334
      batch rate adapted learning_rate :              5e-08
      loss :              1.7433451676686882
      loss_factor :              0.1743345167668688
      loss adapted learning_rate :              1.5196261868168831e-09
    epoch : 3 ; learning_rate : 1.5196261868168831e-09 ; fit : 0.09533333333333334
    batch_size :          30000
    best_batch_loss : 1.7367502348008523
    Epoch 3, Loss: 1.7411267653078233, fit: 0.09463333333333333
      batch rate adapted learning_rate :              5e-08
      loss :              1.745503295814794
      loss_factor :              0.1745503295814794
      loss adapted learning_rate :              1.523390877850154e-09
    epoch : 4 ; learning_rate : 1.523390877850154e-09 ; fit : 0.09463333333333333
    batch_size :          30000
    best_batch_loss : 1.7398785840856872
    Epoch 4, Loss: 1.7411267627925064, fit: 0.09646666666666667
      batch rate adapted learning_rate :              5e-08
      loss :              1.742374941499326
      loss_factor :              0.1742374941499326
      loss adapted learning_rate :              1.5179352183823894e-09
    epoch : 5 ; learning_rate : 1.5179352183823894e-09 ; fit : 0.09646666666666667
    batch_size :          30000
    best_batch_loss : 1.732422896299794
    Epoch 5, Loss: 1.741126760323247, fit: 0.0925
      batch rate adapted learning_rate :              5e-08
      loss :              1.7498306243466997
      loss_factor :              0.17498306243466996
      loss adapted learning_rate :              1.5309536069507802e-09
    epoch : 6 ; learning_rate : 1.5309536069507802e-09 ; fit : 0.0925
    batch_size :          30000
    best_batch_loss : 1.7410279331074179
    Epoch 6, Loss: 1.7411267578879412, fit: 0.09686666666666667
      batch rate adapted learning_rate :              5e-08
      loss :              1.7410279331074179
      loss_factor :              0.1741027933107418
      loss adapted learning_rate :              1.5155891319301438e-09
    epoch : 7 ; learning_rate : 1.5155891319301438e-09 ; fit : 0.09686666666666667
    batch_size :          30000
    best_batch_loss : 1.739844121960598
    Epoch 7, Loss: 1.7411267554711838, fit: 0.09646666666666667
      batch rate adapted learning_rate :              5e-08
      loss :              1.7424093889817693
      loss_factor :              0.17424093889817693
      loss adapted learning_rate :              1.5179952394059113e-09
    epoch : 8 ; learning_rate : 1.5179952394059113e-09 ; fit : 0.09646666666666667
    batch_size :          30000
    best_batch_loss : 1.7387797258787219
    Epoch 8, Loss: 1.7411267529578454, fit: 0.09816666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7387797258787219_fit_0.09816666666666667_2024-01-03_165001
  self.fit : 0.09816666666666667
  self.loss : 1.7387797258787219
  current_accuracy : 0.1034
   Accuracy mean: 0.1034
   Accuracy mean: 0.1034
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-05
    epoch : 0 ; learning_rate : 5e-05 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.766904493784552
    Epoch 0, Loss: 1.7682928423299193, fit: 0.08443333333333333
    Epoch 0, Loss: 1.7682928423299193
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.766904493784552_fit_0.08443333333333333_2024-01-03_165003
  self.fit : 0.08443333333333333
  self.loss : 1.766904493784552
  current_accuracy : 0.0813
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-05
      loss :              1.766904493784552
      loss_factor :              0.17669044937845518
      loss adapted learning_rate :              1.5609757450780219e-06
    epoch : 0 ; learning_rate : 1.5609757450780219e-06 ; fit : 0.08443333333333333
    batch_size :          30000
    best_batch_loss : 1.7658491558881462
    Epoch 0, Loss: 1.7682281083183407, fit: 0.0824
    Epoch 0, Loss: 1.7682281083183407
      batch rate adapted learning_rate :              5e-05
      loss :              1.7706070607485351
      loss_factor :              0.17706070607485352
      loss adapted learning_rate :              1.5675246817862838e-06
    epoch : 1 ; learning_rate : 1.5675246817862838e-06 ; fit : 0.0824
    batch_size :          30000
    best_batch_loss : 1.7681186604156682
    Epoch 1, Loss: 1.7682255771587858, fit: 0.08346666666666666
      batch rate adapted learning_rate :              5e-05
      loss :              1.7683324939019036
      loss_factor :              0.17683324939019035
      loss adapted learning_rate :              1.5634999044946628e-06
    epoch : 2 ; learning_rate : 1.5634999044946628e-06 ; fit : 0.08346666666666666
    batch_size :          30000
    best_batch_loss : 1.7662055674732826
    Epoch 2, Loss: 1.7682231230878207, fit: 0.08263333333333334
      batch rate adapted learning_rate :              5e-05
      loss :              1.7702406787023588
      loss_factor :              0.1770240678702359
      loss adapted learning_rate :              1.566876030266294e-06
    epoch : 3 ; learning_rate : 1.566876030266294e-06 ; fit : 0.08263333333333334
    batch_size :          30000
    best_batch_loss : 1.766942117089532
    Epoch 3, Loss: 1.768220530024923, fit: 0.08406666666666666
      batch rate adapted learning_rate :              5e-05
      loss :              1.766942117089532
      loss_factor :              0.17669421170895322
      loss adapted learning_rate :              1.561042222572419e-06
    epoch : 4 ; learning_rate : 1.561042222572419e-06 ; fit : 0.08406666666666666
    batch_size :          30000
    best_batch_loss : 1.7647591606091508
    Epoch 4, Loss: 1.7682178236970512, fit: 0.08176666666666667
      batch rate adapted learning_rate :              5e-05
      loss :              1.7716764867849515
      loss_factor :              0.17716764867849516
      loss adapted learning_rate :              1.5694187869133344e-06
    epoch : 5 ; learning_rate : 1.5694187869133344e-06 ; fit : 0.08176666666666667
    batch_size :          30000
    best_batch_loss : 1.7680436822877892
    Epoch 5, Loss: 1.768215295124735, fit: 0.0834
      batch rate adapted learning_rate :              5e-05
      loss :              1.7680436822877892
      loss_factor :              0.17680436822877893
      loss adapted learning_rate :              1.5629892312388827e-06
    epoch : 6 ; learning_rate : 1.5629892312388827e-06 ; fit : 0.0834
    batch_size :          30000
    best_batch_loss : 1.7637807420294067
    Epoch 6, Loss: 1.7682127677769572, fit: 0.08143333333333333
      batch rate adapted learning_rate :              5e-05
      loss :              1.772644793524508
      loss_factor :              0.1772644793524508
      loss adapted learning_rate :              1.5711347820047729e-06
    epoch : 7 ; learning_rate : 1.5711347820047729e-06 ; fit : 0.08143333333333333
    batch_size :          30000
    best_batch_loss : 1.7666051282443727
    Epoch 7, Loss: 1.7682101761846694, fit: 0.08433333333333333
      batch rate adapted learning_rate :              5e-05
      loss :              1.7666051282443727
      loss_factor :              0.17666051282443726
      loss adapted learning_rate :              1.5604468395696581e-06
    epoch : 8 ; learning_rate : 1.5604468395696581e-06 ; fit : 0.08433333333333333
    batch_size :          30000
    best_batch_loss : 1.7661576009700655
    Epoch 8, Loss: 1.7682077546502692, fit: 0.0844
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7661576009700655_fit_0.0844_2024-01-03_165024
  self.fit : 0.0844
  self.loss : 1.7661576009700655
  current_accuracy : 0.0813
   Accuracy mean: 0.0813
   Accuracy mean: 0.0813
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.0005
    epoch : 0 ; learning_rate : 0.0005 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7893429300876333
    Epoch 0, Loss: 1.7894708901941025, fit: 0.073
    Epoch 0, Loss: 1.7894708901941025
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7893429300876333_fit_0.073_2024-01-03_165027
  self.fit : 0.073
  self.loss : 1.7893429300876333
  current_accuracy : 0.0722
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.0005
      loss :              1.7893429300876333
      loss_factor :              0.17893429300876335
      loss adapted learning_rate :              1.6008740607272984e-05
    epoch : 0 ; learning_rate : 1.6008740607272984e-05 ; fit : 0.073
    batch_size :          30000
    best_batch_loss : 1.7883594106134106
    Epoch 0, Loss: 1.7886918226026602, fit: 0.0727
    Epoch 0, Loss: 1.7886918226026602
      batch rate adapted learning_rate :              0.0005
      loss :              1.78902423459191
      loss_factor :              0.178902423459191
      loss adapted learning_rate :              1.6003038559785846e-05
    epoch : 1 ; learning_rate : 1.6003038559785846e-05 ; fit : 0.0727
    batch_size :          30000
    best_batch_loss : 1.7866997238523092
    Epoch 1, Loss: 1.7886580296543229, fit: 0.0742
      batch rate adapted learning_rate :              0.0005
      loss :              1.7866997238523092
      loss_factor :              0.1786699723852309
      loss adapted learning_rate :              1.5961479516069587e-05
    epoch : 2 ; learning_rate : 1.5961479516069587e-05 ; fit : 0.0742
    batch_size :          30000
    best_batch_loss : 1.7855117262798905
    Epoch 2, Loss: 1.788625196367979, fit: 0.0717
      batch rate adapted learning_rate :              0.0005
      loss :              1.7917386664560673
      loss_factor :              0.17917386664560672
      loss adapted learning_rate :              1.605163724436883e-05
    epoch : 3 ; learning_rate : 1.605163724436883e-05 ; fit : 0.0717
    batch_size :          30000
    best_batch_loss : 1.7868186387363059
    Epoch 3, Loss: 1.7885914634058078, fit: 0.07446666666666667
      batch rate adapted learning_rate :              0.0005
      loss :              1.7868186387363059
      loss_factor :              0.17868186387363058
      loss adapted learning_rate :              1.5963604238677325e-05
    epoch : 4 ; learning_rate : 1.5963604238677325e-05 ; fit : 0.07446666666666667
    batch_size :          30000
    best_batch_loss : 1.7839448853190696
    Epoch 4, Loss: 1.7885581207792458, fit: 0.07093333333333333
      batch rate adapted learning_rate :              0.0005
      loss :              1.7931713562394218
      loss_factor :              0.1793171356239422
      loss adapted learning_rate :              1.607731756418764e-05
    epoch : 5 ; learning_rate : 1.607731756418764e-05 ; fit : 0.07093333333333333
    batch_size :          30000
    best_batch_loss : 1.7850534742923725
    Epoch 5, Loss: 1.788524725175048, fit: 0.07206666666666667
      batch rate adapted learning_rate :              0.0005
      loss :              1.7919959760577235
      loss_factor :              0.17919959760577236
      loss adapted learning_rate :              1.605624789103537e-05
    epoch : 6 ; learning_rate : 1.605624789103537e-05 ; fit : 0.07206666666666667
    batch_size :          30000
    best_batch_loss : 1.7884642502675718
    Epoch 6, Loss: 1.7884906533949843, fit: 0.07383333333333333
      batch rate adapted learning_rate :              0.0005
      loss :              1.7884642502675718
      loss_factor :              0.17884642502675718
      loss adapted learning_rate :              1.599302187242574e-05
    epoch : 7 ; learning_rate : 1.599302187242574e-05 ; fit : 0.07383333333333333
    batch_size :          30000
    best_batch_loss : 1.7870061873986443
    Epoch 7, Loss: 1.7884578108829494, fit: 0.07426666666666666
      batch rate adapted learning_rate :              0.0005
      loss :              1.7870061873986443
      loss_factor :              0.17870061873986443
      loss adapted learning_rate :              1.5966955569005192e-05
    epoch : 8 ; learning_rate : 1.5966955569005192e-05 ; fit : 0.07426666666666666
    batch_size :          30000
    best_batch_loss : 1.7870203475792037
    Epoch 8, Loss: 1.7884247017975865, fit: 0.0728
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7898290560159693_fit_0.0728_2024-01-03_165048
  self.fit : 0.0728
  self.loss : 1.7898290560159693
  current_accuracy : 0.0725
   Accuracy mean: 0.0722
   Accuracy mean: 0.0725
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.01
    epoch : 0 ; learning_rate : 0.01 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7471103406703856
    Epoch 0, Loss: 1.7514043180381345, fit: 0.09376666666666666
    Epoch 0, Loss: 1.7514043180381345
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7471103406703856_fit_0.09376666666666666_2024-01-03_165050
  self.fit : 0.09376666666666666
  self.loss : 1.7471103406703856
  current_accuracy : 0.1079
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.01
      loss :              1.7471103406703856
      loss_factor :              0.17471103406703856
      loss adapted learning_rate :              0.0003052394542477391
    epoch : 0 ; learning_rate : 0.0003052394542477391 ; fit : 0.09376666666666666
    batch_size :          30000
    best_batch_loss : 1.7307702633128288
    Epoch 0, Loss: 1.7349188625349283, fit: 0.1008
    Epoch 0, Loss: 1.7349188625349283
      batch rate adapted learning_rate :              0.01
      loss :              1.7307702633128288
      loss_factor :              0.17307702633128288
      loss adapted learning_rate :              0.00029955657043679587
    epoch : 1 ; learning_rate : 0.00029955657043679587 ; fit : 0.1008
    batch_size :          30000
    best_batch_loss : 1.733807893134121
    Epoch 1, Loss: 1.7342519269355736, fit: 0.09933333333333333
      batch rate adapted learning_rate :              0.01
      loss :              1.7346959607370263
      loss_factor :              0.17346959607370263
      loss adapted learning_rate :              0.00030091700761973546
    epoch : 2 ; learning_rate : 0.00030091700761973546 ; fit : 0.09933333333333333
    batch_size :          30000
    best_batch_loss : 1.7331060673470946
    Epoch 2, Loss: 1.7335936026368837, fit: 0.09926666666666667
      batch rate adapted learning_rate :              0.01
      loss :              1.7331060673470946
      loss_factor :              0.17331060673470947
      loss adapted learning_rate :              0.00030036566406753124
    epoch : 3 ; learning_rate : 0.00030036566406753124 ; fit : 0.09926666666666667
    batch_size :          30000
    best_batch_loss : 1.7306530133545168
    Epoch 3, Loss: 1.7329491043732075, fit: 0.09876666666666667
      batch rate adapted learning_rate :              0.01
      loss :              1.7352451953918984
      loss_factor :              0.17352451953918985
      loss adapted learning_rate :              0.0003011075888130668
    epoch : 4 ; learning_rate : 0.0003011075888130668 ; fit : 0.09876666666666667
    batch_size :          30000
    best_batch_loss : 1.7312597059967414
    Epoch 4, Loss: 1.7322975688710427, fit: 0.1009
      batch rate adapted learning_rate :              0.01
      loss :              1.7312597059967414
      loss_factor :              0.17312597059967413
      loss adapted learning_rate :              0.00029972601696079236
    epoch : 5 ; learning_rate : 0.00029972601696079236 ; fit : 0.1009
    batch_size :          30000
    best_batch_loss : 1.730439626460261
    Epoch 5, Loss: 1.7316325428566328, fit: 0.10103333333333334
      batch rate adapted learning_rate :              0.01
      loss :              1.730439626460261
      loss_factor :              0.17304396264602612
      loss adapted learning_rate :              0.0002994421300823928
    epoch : 6 ; learning_rate : 0.0002994421300823928 ; fit : 0.10103333333333334
    batch_size :          30000
    best_batch_loss : 1.7305218188297875
    Epoch 6, Loss: 1.7309516511616543, fit: 0.10013333333333334
      batch rate adapted learning_rate :              0.01
      loss :              1.731381483493521
      loss_factor :              0.1731381483493521
      loss adapted learning_rate :              0.00029976818413842256
    epoch : 7 ; learning_rate : 0.00029976818413842256 ; fit : 0.10013333333333334
    batch_size :          30000
    best_batch_loss : 1.7272612041574182
    Epoch 7, Loss: 1.7302642363564156, fit: 0.0992
      batch rate adapted learning_rate :              0.01
      loss :              1.733267268555413
      loss_factor :              0.1733267268555413
      loss adapted learning_rate :              0.0003004215424245542
    epoch : 8 ; learning_rate : 0.0003004215424245542 ; fit : 0.0992
    batch_size :          30000
    best_batch_loss : 1.7284286255493524
    Epoch 8, Loss: 1.7295938448082842, fit: 0.10096666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7307590640672161_fit_0.10096666666666666_2024-01-03_165111
  self.fit : 0.10096666666666666
  self.loss : 1.7307590640672161
  current_accuracy : 0.1099
   Accuracy mean: 0.1079
   Accuracy mean: 0.1099
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.0125
    epoch : 0 ; learning_rate : 0.0125 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.692396006528327
    Epoch 0, Loss: 1.7020678385914785, fit: 0.12056666666666667
    Epoch 0, Loss: 1.7020678385914785
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.692396006528327_fit_0.12056666666666667_2024-01-03_165113
  self.fit : 0.12056666666666667
  self.loss : 1.692396006528327
  current_accuracy : 0.1201
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.0125
      loss :              1.692396006528327
      loss_factor :              0.1692396006528327
      loss adapted learning_rate :              0.00035802553036412863
    epoch : 0 ; learning_rate : 0.00035802553036412863 ; fit : 0.12056666666666667
    batch_size :          30000
    best_batch_loss : 1.6737788705783796
    Epoch 0, Loss: 1.678952625290738, fit: 0.12913333333333332
    Epoch 0, Loss: 1.678952625290738
      batch rate adapted learning_rate :              0.0125
      loss :              1.6737788705783796
      loss_factor :              0.16737788705783796
      loss adapted learning_rate :              0.00035019196344932956
    epoch : 1 ; learning_rate : 0.00035019196344932956 ; fit : 0.12913333333333332
    batch_size :          30000
    best_batch_loss : 1.6743299709780435
    Epoch 1, Loss: 1.678304970923989, fit: 0.1288
      batch rate adapted learning_rate :              0.0125
      loss :              1.6743299709780435
      loss_factor :              0.16743299709780435
      loss adapted learning_rate :              0.000350422606464417
    epoch : 2 ; learning_rate : 0.000350422606464417 ; fit : 0.1288
    batch_size :          30000
    best_batch_loss : 1.6774601921569103
    Epoch 2, Loss: 1.6776772391689354, fit: 0.12746666666666667
      batch rate adapted learning_rate :              0.0125
      loss :              1.6778942861809607
      loss_factor :              0.16778942861809606
      loss adapted learning_rate :              0.0003519161544498395
    epoch : 3 ; learning_rate : 0.0003519161544498395 ; fit : 0.12746666666666667
    batch_size :          30000
    best_batch_loss : 1.6746755204985588
    Epoch 3, Loss: 1.6770553050582184, fit: 0.1263
      batch rate adapted learning_rate :              0.0125
      loss :              1.679435089617878
      loss_factor :              0.1679435089617878
      loss adapted learning_rate :              0.0003525627775299763
    epoch : 4 ; learning_rate : 0.0003525627775299763 ; fit : 0.1263
    batch_size :          30000
    best_batch_loss : 1.674087985822314
    Epoch 4, Loss: 1.6764356017779058, fit: 0.12923333333333334
      batch rate adapted learning_rate :              0.0125
      loss :              1.674087985822314
      loss_factor :              0.1674087985822314
      loss adapted learning_rate :              0.0003503213230343265
    epoch : 5 ; learning_rate : 0.0003503213230343265 ; fit : 0.12923333333333334
    batch_size :          30000
    best_batch_loss : 1.6721486324852726
    Epoch 5, Loss: 1.6757989616322007, fit: 0.13
      batch rate adapted learning_rate :              0.0125
      loss :              1.6721486324852726
      loss_factor :              0.16721486324852725
      loss adapted learning_rate :              0.00034951013114029587
    epoch : 6 ; learning_rate : 0.00034951013114029587 ; fit : 0.13
    batch_size :          30000
    best_batch_loss : 1.673425454871974
    Epoch 6, Loss: 1.6751652548356981, fit: 0.12993333333333335
      batch rate adapted learning_rate :              0.0125
      loss :              1.673425454871974
      loss_factor :              0.1673425454871974
      loss adapted learning_rate :              0.0003500440941266842
    epoch : 7 ; learning_rate : 0.0003500440941266842 ; fit : 0.12993333333333335
    batch_size :          30000
    best_batch_loss : 1.6701293070429553
    Epoch 7, Loss: 1.6745400566259327, fit: 0.13156666666666667
      batch rate adapted learning_rate :              0.0125
      loss :              1.6701293070429553
      loss_factor :              0.16701293070429551
      loss adapted learning_rate :              0.0003486664877804727
    epoch : 8 ; learning_rate : 0.0003486664877804727 ; fit : 0.13156666666666667
    batch_size :          30000
    best_batch_loss : 1.671131574181124
    Epoch 8, Loss: 1.6739172566484435, fit: 0.1284
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.676702939115763_fit_0.1284_2024-01-03_165134
  self.fit : 0.1284
  self.loss : 1.676702939115763
  current_accuracy : 0.1238
   Accuracy mean: 0.1201
   Accuracy mean: 0.1238
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.015
    epoch : 0 ; learning_rate : 0.015 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.696861075519845
    Epoch 0, Loss: 1.7035228007112493, fit: 0.12123333333333333
    Epoch 0, Loss: 1.7035228007112493
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.696861075519845_fit_0.12123333333333333_2024-01-03_165137
  self.fit : 0.12123333333333333
  self.loss : 1.696861075519845
  current_accuracy : 0.127
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.015
      loss :              1.696861075519845
      loss_factor :              0.1696861075519845
      loss adapted learning_rate :              0.0004319006264421547
    epoch : 0 ; learning_rate : 0.0004319006264421547 ; fit : 0.12123333333333333
    batch_size :          30000
    best_batch_loss : 1.6787679995147522
    Epoch 0, Loss: 1.686048582858211, fit: 0.12776666666666667
    Epoch 0, Loss: 1.686048582858211
      batch rate adapted learning_rate :              0.015
      loss :              1.6787679995147522
      loss_factor :              0.16787679995147523
      loss adapted learning_rate :              0.00042273929942921443
    epoch : 1 ; learning_rate : 0.00042273929942921443 ; fit : 0.12776666666666667
    batch_size :          30000
    best_batch_loss : 1.6811425939994915
    Epoch 1, Loss: 1.6854482645461182, fit: 0.12256666666666667
      batch rate adapted learning_rate :              0.015
      loss :              1.6897539350927449
      loss_factor :              0.16897539350927449
      loss adapted learning_rate :              0.0004282902541742124
    epoch : 2 ; learning_rate : 0.0004282902541742124 ; fit : 0.12256666666666667
    batch_size :          30000
    best_batch_loss : 1.6785545233980537
    Epoch 2, Loss: 1.6848766089816276, fit: 0.12146666666666667
      batch rate adapted learning_rate :              0.015
      loss :              1.6911986945652016
      loss_factor :              0.16911986945652016
      loss adapted learning_rate :              0.0004290229536748563
    epoch : 3 ; learning_rate : 0.0004290229536748563 ; fit : 0.12146666666666667
    batch_size :          30000
    best_batch_loss : 1.6772007562821232
    Epoch 3, Loss: 1.6843095536453978, fit: 0.12813333333333332
      batch rate adapted learning_rate :              0.015
      loss :              1.6772007562821232
      loss_factor :              0.1677200756282123
      loss adapted learning_rate :              0.0004219503565309989
    epoch : 4 ; learning_rate : 0.0004219503565309989 ; fit : 0.12813333333333332
    batch_size :          30000
    best_batch_loss : 1.6813771660278614
    Epoch 4, Loss: 1.6837271810158114, fit: 0.1261
      batch rate adapted learning_rate :              0.015
      loss :              1.6813771660278614
      loss_factor :              0.16813771660278615
      loss adapted learning_rate :              0.0004240543761659824
    epoch : 5 ; learning_rate : 0.0004240543761659824 ; fit : 0.1261
    batch_size :          30000
    best_batch_loss : 1.6808025465564038
    Epoch 5, Loss: 1.683170149546871, fit: 0.12426666666666666
      batch rate adapted learning_rate :              0.015
      loss :              1.685537752537338
      loss_factor :              0.1685537752537338
      loss adapted learning_rate :              0.0004261556272842931
    epoch : 6 ; learning_rate : 0.0004261556272842931 ; fit : 0.12426666666666666
    batch_size :          30000
    best_batch_loss : 1.6814647835473233
    Epoch 6, Loss: 1.6826084843382758, fit: 0.12506666666666666
      batch rate adapted learning_rate :              0.015
      loss :              1.6837521851292285
      loss_factor :              0.16837521851292286
      loss adapted learning_rate :              0.00042525321313911785
    epoch : 7 ; learning_rate : 0.00042525321313911785 ; fit : 0.12506666666666666
    batch_size :          30000
    best_batch_loss : 1.6801640800322428
    Epoch 7, Loss: 1.682041159749458, fit: 0.12743333333333334
      batch rate adapted learning_rate :              0.015
      loss :              1.6801640800322428
      loss_factor :              0.1680164080032243
      loss adapted learning_rate :              0.00042344270037458895
    epoch : 8 ; learning_rate : 0.00042344270037458895 ; fit : 0.12743333333333334
    batch_size :          30000
    best_batch_loss : 1.6802900691301663
    Epoch 8, Loss: 1.6815123117272548, fit: 0.12716666666666668
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6802900691301663_fit_0.12716666666666668_2024-01-03_165157
  self.fit : 0.12716666666666668
  self.loss : 1.6802900691301663
  current_accuracy : 0.129
   Accuracy mean: 0.127
   Accuracy mean: 0.129
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.05
    epoch : 0 ; learning_rate : 0.05 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.6962115706472405
    Epoch 0, Loss: 1.7150154354812988, fit: 0.11656666666666667
    Epoch 0, Loss: 1.7150154354812988
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6962115706472405_fit_0.11656666666666667_2024-01-03_165200
  self.fit : 0.11656666666666667
  self.loss : 1.6962115706472405
  current_accuracy : 0.1535
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.05
      loss :              1.6962115706472405
      loss_factor :              0.16962115706472405
      loss adapted learning_rate :              0.0014385668461987895
    epoch : 0 ; learning_rate : 0.0014385668461987895 ; fit : 0.11656666666666667
    batch_size :          30000
    best_batch_loss : 1.6288609513740278
    Epoch 0, Loss: 1.6317747983483946, fit: 0.14823333333333333
    Epoch 0, Loss: 1.6317747983483946
      batch rate adapted learning_rate :              0.05
      loss :              1.6288609513740278
      loss_factor :              0.16288609513740276
      loss adapted learning_rate :              0.0013265939994555513
    epoch : 1 ; learning_rate : 0.0013265939994555513 ; fit : 0.14823333333333333
    batch_size :          30000
    best_batch_loss : 1.6248761991107248
    Epoch 1, Loss: 1.628520220670272, fit: 0.1455
      batch rate adapted learning_rate :              0.05
      loss :              1.6321642422298193
      loss_factor :              0.16321642422298194
      loss adapted learning_rate :              0.0013319800568068205
    epoch : 2 ; learning_rate : 0.0013319800568068205 ; fit : 0.1455
    batch_size :          30000
    best_batch_loss : 1.6229200286684928
    Epoch 2, Loss: 1.6254137619904843, fit: 0.15046666666666667
      batch rate adapted learning_rate :              0.05
      loss :              1.6229200286684928
      loss_factor :              0.16229200286684928
      loss adapted learning_rate :              0.0013169347097266707
    epoch : 3 ; learning_rate : 0.0013169347097266707 ; fit : 0.15046666666666667
    batch_size :          30000
    best_batch_loss : 1.6178205703183577
    Epoch 3, Loss: 1.6222723534501653, fit: 0.15373333333333333
      batch rate adapted learning_rate :              0.05
      loss :              1.6178205703183577
      loss_factor :              0.16178205703183576
      loss adapted learning_rate :              0.0013086716988726082
    epoch : 4 ; learning_rate : 0.0013086716988726082 ; fit : 0.15373333333333333
    batch_size :          30000
    best_batch_loss : 1.6187119669011225
    Epoch 4, Loss: 1.6192674559003235, fit: 0.15336666666666668
      batch rate adapted learning_rate :              0.05
      loss :              1.6198229448995247
      loss_factor :              0.16198229448995247
      loss adapted learning_rate :              0.0013119131864114844
    epoch : 5 ; learning_rate : 0.0013119131864114844 ; fit : 0.15336666666666668
    batch_size :          30000
    best_batch_loss : 1.6158322324688676
    Epoch 5, Loss: 1.616224291127633, fit: 0.15446666666666667
      batch rate adapted learning_rate :              0.05
      loss :              1.6166163497863983
      loss_factor :              0.16166163497863983
      loss adapted learning_rate :              0.0013067242111983494
    epoch : 6 ; learning_rate : 0.0013067242111983494 ; fit : 0.15446666666666667
    batch_size :          30000
    best_batch_loss : 1.6098816824485984
    Epoch 6, Loss: 1.6132112998036918, fit: 0.15713333333333335
      batch rate adapted learning_rate :              0.05
      loss :              1.6098816824485984
      loss_factor :              0.16098816824485984
      loss adapted learning_rate :              0.0012958595157417652
    epoch : 7 ; learning_rate : 0.0012958595157417652 ; fit : 0.15713333333333335
    batch_size :          30000
    best_batch_loss : 1.6102398259401802
    Epoch 7, Loss: 1.610242477924694, fit: 0.15766666666666668
      batch rate adapted learning_rate :              0.05
      loss :              1.6102398259401802
      loss_factor :              0.161023982594018
      loss adapted learning_rate :              0.0012964361485219309
    epoch : 8 ; learning_rate : 0.0012964361485219309 ; fit : 0.15766666666666668
    batch_size :          30000
    best_batch_loss : 1.605639458445204
    Epoch 8, Loss: 1.6072151891304978, fit: 0.15776666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6087909198157915_fit_0.15776666666666667_2024-01-03_165221
  self.fit : 0.15776666666666667
  self.loss : 1.6087909198157915
  current_accuracy : 0.1668
   Accuracy mean: 0.1535
   Accuracy mean: 0.1668
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.45
    epoch : 0 ; learning_rate : 0.45 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.4615739793479632
    Epoch 0, Loss: 1.6289945093616645, fit: 0.245
    Epoch 0, Loss: 1.6289945093616645
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.4615739793479632_fit_0.245_2024-01-03_165224
  self.fit : 0.245
  self.loss : 1.4615739793479632
  current_accuracy : 0.434
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.45
      loss :              1.4615739793479632
      loss_factor :              0.14615739793479632
      loss adapted learning_rate :              0.009612893236981682
    epoch : 0 ; learning_rate : 0.009612893236981682 ; fit : 0.245
    batch_size :          30000
    best_batch_loss : 1.1126875762986221
    Epoch 0, Loss: 1.1152249744417855, fit: 0.4187666666666667
    Epoch 0, Loss: 1.1152249744417855
      batch rate adapted learning_rate :              0.45
      loss :              1.1126875762986221
      loss_factor :              0.11126875762986221
      loss adapted learning_rate :              0.0055713313910218595
    epoch : 1 ; learning_rate : 0.0055713313910218595 ; fit : 0.4187666666666667
    batch_size :          30000
    best_batch_loss : 1.1037026970085917
    Epoch 1, Loss: 1.1068838456431358, fit: 0.4206
      batch rate adapted learning_rate :              0.45
      loss :              1.11006499427768
      loss_factor :              0.111006499427768
      loss adapted learning_rate :              0.005545099311843176
    epoch : 2 ; learning_rate : 0.005545099311843176 ; fit : 0.4206
    batch_size :          30000
    best_batch_loss : 1.0964721325875773
    Epoch 2, Loss: 1.1015034132743882, fit: 0.4263
      batch rate adapted learning_rate :              0.45
      loss :              1.0964721325875773
      loss_factor :              0.10964721325875773
      loss adapted learning_rate :              0.005410130118935173
    epoch : 3 ; learning_rate : 0.005410130118935173 ; fit : 0.4263
    batch_size :          30000
    best_batch_loss : 1.093642246099601
    Epoch 3, Loss: 1.0967706502919619, fit: 0.4248
      batch rate adapted learning_rate :              0.45
      loss :              1.099899054484323
      loss_factor :              0.1099899054484323
      loss adapted learning_rate :              0.005444000685249786
    epoch : 4 ; learning_rate : 0.005444000685249786 ; fit : 0.4248
    batch_size :          30000
    best_batch_loss : 1.0900156705068205
    Epoch 4, Loss: 1.0922081557309324, fit: 0.42956666666666665
      batch rate adapted learning_rate :              0.45
      loss :              1.0900156705068205
      loss_factor :              0.10900156705068205
      loss adapted learning_rate :              0.005346603728776951
    epoch : 5 ; learning_rate : 0.005346603728776951 ; fit : 0.42956666666666665
    batch_size :          30000
    best_batch_loss : 1.08812979137558
    Epoch 5, Loss: 1.0882566704787293, fit: 0.43083333333333335
      batch rate adapted learning_rate :              0.45
      loss :              1.0883835495818786
      loss_factor :              0.10883835495818786
      loss adapted learning_rate :              0.005330604379502023
    epoch : 6 ; learning_rate : 0.005330604379502023 ; fit : 0.43083333333333335
    batch_size :          30000
    best_batch_loss : 1.0833853635856736
    Epoch 6, Loss: 1.0847120919803994, fit: 0.4316333333333333
      batch rate adapted learning_rate :              0.45
      loss :              1.0860388203751254
      loss_factor :              0.10860388203751255
      loss adapted learning_rate :              0.005307661437128073
    epoch : 7 ; learning_rate : 0.005307661437128073 ; fit : 0.4316333333333333
    batch_size :          30000
    best_batch_loss : 1.0740611834748675
    Epoch 7, Loss: 1.0812204942868435, fit: 0.43046666666666666
      batch rate adapted learning_rate :              0.45
      loss :              1.0883798050988196
      loss_factor :              0.10883798050988196
      loss adapted learning_rate :              0.005330567700661251
    epoch : 8 ; learning_rate : 0.005330567700661251 ; fit : 0.43046666666666666
    batch_size :          30000
    best_batch_loss : 1.0756859774903662
    Epoch 8, Loss: 1.0776496423310755, fit: 0.4368666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.0756859774903662_fit_0.4368666666666667_2024-01-03_165245
  self.fit : 0.4368666666666667
  self.loss : 1.0756859774903662
  current_accuracy : 0.4526
   Accuracy mean: 0.434
   Accuracy mean: 0.4526
  Error saving file: doc/out/test_combinations_results/20240103165245.
  normalized_accuracies :      [6.30914826e-02 6.30914826e-02 7.78128286e-02 7.78128286e-02
   8.20189274e-02 8.20189274e-02 2.39221872e-02 2.39221872e-02
   0.00000000e+00 7.88643533e-04 9.38485804e-02 9.91062040e-02
   1.25920084e-01 1.35646688e-01 1.44058885e-01 1.49316509e-01
   2.13722397e-01 2.48685594e-01 9.51104101e-01 1.00000000e+00]
batch_rate :  0.2
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.2
      batch rate adapted learning_rate :              2e-18
    epoch : 0 ; learning_rate : 2e-18 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.782893353645001
    Epoch 0, Loss: 1.7886306959021439, fit: 0.073
    Epoch 0, Loss: 1.7886306959021439
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7958866896947316_fit_0.073_2024-01-03_165248
  self.fit : 0.073
  self.loss : 1.7958866896947316
  current_accuracy : 0.0768
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          0.2
      batch rate adapted learning_rate :              2e-18
      loss :              1.7958866896947316
      loss_factor :              0.17958866896947318
      loss adapted learning_rate :              6.450418004445404e-20
    epoch : 0 ; learning_rate : 6.450418004445404e-20 ; fit : 0.073
    batch_size :          12000
    best_batch_loss : 1.7826643093723213
    Epoch 0, Loss: 1.7886306959021434, fit: 0.07408333333333333
    Epoch 0, Loss: 1.7886306959021434
      batch rate adapted learning_rate :              2e-18
      loss :              1.7905570892447487
      loss_factor :              0.17905570892447487
      loss adapted learning_rate :              6.412189379689254e-20
    epoch : 1 ; learning_rate : 6.412189379689254e-20 ; fit : 0.07408333333333333
    batch_size :          12000
    best_batch_loss : 1.78127046225365
    Epoch 1, Loss: 1.7886306959021436, fit: 0.07683333333333334
      batch rate adapted learning_rate :              2e-18
      loss :              1.78127046225365
      loss_factor :              0.178127046225365
      loss adapted learning_rate :              6.345848919394664e-20
    epoch : 2 ; learning_rate : 6.345848919394664e-20 ; fit : 0.07683333333333334
    batch_size :          12000
    best_batch_loss : 1.77928678951704
    Epoch 2, Loss: 1.7886306959021439, fit: 0.07416666666666667
      batch rate adapted learning_rate :              2e-18
      loss :              1.7916944283247862
      loss_factor :              0.17916944283247863
      loss adapted learning_rate :              6.420337848980165e-20
    epoch : 3 ; learning_rate : 6.420337848980165e-20 ; fit : 0.07416666666666667
    batch_size :          12000
    best_batch_loss : 1.7845690489259705
    Epoch 3, Loss: 1.7886306959021439, fit: 0.07258333333333333
      batch rate adapted learning_rate :              2e-18
      loss :              1.7920225611701437
      loss_factor :              0.17920225611701437
      loss adapted learning_rate :              6.422689719485603e-20
    epoch : 4 ; learning_rate : 6.422689719485603e-20 ; fit : 0.07258333333333333
    batch_size :          12000
    best_batch_loss : 1.7847692051449755
    Epoch 4, Loss: 1.7886306959021439, fit: 0.07633333333333334
      batch rate adapted learning_rate :              2e-18
      loss :              1.7847692051449755
      loss_factor :              0.17847692051449754
      loss adapted learning_rate :              6.370802231267654e-20
    epoch : 5 ; learning_rate : 6.370802231267654e-20 ; fit : 0.07633333333333334
    batch_size :          12000
    best_batch_loss : 1.7851374883995694
    Epoch 5, Loss: 1.7886306959021436, fit: 0.07608333333333334
      batch rate adapted learning_rate :              2e-18
      loss :              1.7851374883995694
      loss_factor :              0.17851374883995694
      loss adapted learning_rate :              6.373431704979046e-20
    epoch : 6 ; learning_rate : 6.373431704979046e-20 ; fit : 0.07608333333333334
    batch_size :          12000
    best_batch_loss : 1.7791889345747482
    Epoch 6, Loss: 1.7886306959021436, fit: 0.07108333333333333
      batch rate adapted learning_rate :              2e-18
      loss :              1.7988490633492735
      loss_factor :              0.17988490633492735
      loss adapted learning_rate :              6.471715905425117e-20
    epoch : 7 ; learning_rate : 6.471715905425117e-20 ; fit : 0.07108333333333333
    batch_size :          12000
    best_batch_loss : 1.7861504611573955
    Epoch 7, Loss: 1.7886306959021434, fit: 0.07616666666666666
      batch rate adapted learning_rate :              2e-18
      loss :              1.7872827317479838
      loss_factor :              0.1787282731747984
      loss adapted learning_rate :              6.388759126409071e-20
    epoch : 8 ; learning_rate : 6.388759126409071e-20 ; fit : 0.07616666666666666
    batch_size :          12000
    best_batch_loss : 1.781760206751471
    Epoch 8, Loss: 1.7886306959021436, fit: 0.07366666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7921403580779574_fit_0.07366666666666667_2024-01-03_165308
  self.fit : 0.07366666666666667
  self.loss : 1.7921403580779574
  current_accuracy : 0.0768
   Accuracy mean: 0.0768
   Accuracy mean: 0.0768
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.2
      batch rate adapted learning_rate :              2.0000000000000002e-11
    epoch : 0 ; learning_rate : 2.0000000000000002e-11 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.69432496312564
    Epoch 0, Loss: 1.6992397009750493, fit: 0.1195
    Epoch 0, Loss: 1.6992397009750493
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7028159300181567_fit_0.1195_2024-01-03_165311
  self.fit : 0.1195
  self.loss : 1.7028159300181567
  current_accuracy : 0.1329
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.2
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.7028159300181567
      loss_factor :              0.17028159300181567
      loss adapted learning_rate :              5.7991641830472e-13
    epoch : 0 ; learning_rate : 5.7991641830472e-13 ; fit : 0.1195
    batch_size :          12000
    best_batch_loss : 1.6950973592034737
    Epoch 0, Loss: 1.699239700863101, fit: 0.12408333333333334
    Epoch 0, Loss: 1.699239700863101
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.6950973592034737
      loss_factor :              0.16950973592034738
      loss adapted learning_rate :              5.746710114357182e-13
    epoch : 1 ; learning_rate : 5.746710114357182e-13 ; fit : 0.12408333333333334
    batch_size :          12000
    best_batch_loss : 1.6925026409930355
    Epoch 1, Loss: 1.6992397008580962, fit: 0.12116666666666667
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.6996748022507
      loss_factor :              0.16996748022507
      loss adapted learning_rate :              5.777788866811913e-13
    epoch : 2 ; learning_rate : 5.777788866811913e-13 ; fit : 0.12116666666666667
    batch_size :          12000
    best_batch_loss : 1.6865473389566779
    Epoch 2, Loss: 1.6992397008529037, fit: 0.12791666666666668
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.6865473389566779
      loss_factor :              0.1686547338956678
      loss adapted learning_rate :              5.688883853083704e-13
    epoch : 3 ; learning_rate : 5.688883853083704e-13 ; fit : 0.12791666666666668
    batch_size :          12000
    best_batch_loss : 1.6954285796108486
    Epoch 3, Loss: 1.6992397008479094, fit: 0.12283333333333334
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.699047331015708
      loss_factor :              0.16990473310157078
      loss adapted learning_rate :              5.773523666063201e-13
    epoch : 4 ; learning_rate : 5.773523666063201e-13 ; fit : 0.12283333333333334
    batch_size :          12000
    best_batch_loss : 1.6968626647736962
    Epoch 4, Loss: 1.6992397008426954, fit: 0.12308333333333334
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.6978160633633894
      loss_factor :              0.16978160633633893
      loss adapted learning_rate :              5.765158770029513e-13
    epoch : 5 ; learning_rate : 5.765158770029513e-13 ; fit : 0.12308333333333334
    batch_size :          12000
    best_batch_loss : 1.6933971121261324
    Epoch 5, Loss: 1.699239700837601, fit: 0.12308333333333334
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.6985890442143567
      loss_factor :              0.16985890442143567
      loss adapted learning_rate :              5.770409482250084e-13
    epoch : 6 ; learning_rate : 5.770409482250084e-13 ; fit : 0.12308333333333334
    batch_size :          12000
    best_batch_loss : 1.6929717102073532
    Epoch 6, Loss: 1.6992397008326332, fit: 0.11583333333333333
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.7106023907888863
      loss_factor :              0.17106023907888862
      loss adapted learning_rate :              5.852321078745307e-13
    epoch : 7 ; learning_rate : 5.852321078745307e-13 ; fit : 0.11583333333333333
    batch_size :          12000
    best_batch_loss : 1.6907289062191904
    Epoch 7, Loss: 1.6992397008274365, fit: 0.12183333333333334
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss :              1.7003901759723288
      loss_factor :              0.1700390175972329
      loss adapted learning_rate :              5.782653501086416e-13
    epoch : 8 ; learning_rate : 5.782653501086416e-13 ; fit : 0.12183333333333334
    batch_size :          12000
    best_batch_loss : 1.696130428007894
    Epoch 8, Loss: 1.6992397008221234, fit: 0.12208333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.70016898207871_fit_0.12208333333333334_2024-01-03_165331
  self.fit : 0.12208333333333334
  self.loss : 1.70016898207871
  current_accuracy : 0.1329
   Accuracy mean: 0.1329
   Accuracy mean: 0.1329
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.2
      batch rate adapted learning_rate :              2e-08
    epoch : 0 ; learning_rate : 2e-08 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.816780189430115
    Epoch 0, Loss: 1.8230270226446215, fit: 0.05525
    Epoch 0, Loss: 1.8230270226446215
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.8300424455579347_fit_0.05525_2024-01-03_165334
  self.fit : 0.05525
  self.loss : 1.8300424455579347
  current_accuracy : 0.0553
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.2
      batch rate adapted learning_rate :              2e-08
      loss :              1.8300424455579347
      loss_factor :              0.18300424455579348
      loss adapted learning_rate :              6.698110705087333e-10
    epoch : 0 ; learning_rate : 6.698110705087333e-10 ; fit : 0.05525
    batch_size :          12000
    best_batch_loss : 1.8178745877612186
    Epoch 0, Loss: 1.8230269665781393, fit: 0.057666666666666665
    Epoch 0, Loss: 1.8230269665781393
      batch rate adapted learning_rate :              2e-08
      loss :              1.826098667573404
      loss_factor :              0.18260986675734042
      loss adapted learning_rate :              6.669272687426725e-10
    epoch : 1 ; learning_rate : 6.669272687426725e-10 ; fit : 0.057666666666666665
    batch_size :          12000
    best_batch_loss : 1.8192282733662575
    Epoch 1, Loss: 1.8230269637509449, fit: 0.061
      batch rate adapted learning_rate :              2e-08
      loss :              1.8229993300253033
      loss_factor :              0.18229993300253033
      loss adapted learning_rate :              6.64665311454541e-10
    epoch : 2 ; learning_rate : 6.64665311454541e-10 ; fit : 0.061
    batch_size :          12000
    best_batch_loss : 1.820997663067528
    Epoch 2, Loss: 1.8230269608728724, fit: 0.06016666666666667
      batch rate adapted learning_rate :              2e-08
      loss :              1.820997663067528
      loss_factor :              0.18209976630675279
      loss adapted learning_rate :              6.632064977794795e-10
    epoch : 3 ; learning_rate : 6.632064977794795e-10 ; fit : 0.06016666666666667
    batch_size :          12000
    best_batch_loss : 1.8145458252775313
    Epoch 3, Loss: 1.8230269578647498, fit: 0.057666666666666665
      batch rate adapted learning_rate :              2e-08
      loss :              1.8255873808908702
      loss_factor :              0.182558738089087
      loss adapted learning_rate :              6.665538570535974e-10
    epoch : 4 ; learning_rate : 6.665538570535974e-10 ; fit : 0.057666666666666665
    batch_size :          12000
    best_batch_loss : 1.821222870889973
    Epoch 4, Loss: 1.8230269549217861, fit: 0.059333333333333335
      batch rate adapted learning_rate :              2e-08
      loss :              1.8219277102312692
      loss_factor :              0.18219277102312692
      loss adapted learning_rate :              6.638841162617111e-10
    epoch : 5 ; learning_rate : 6.638841162617111e-10 ; fit : 0.059333333333333335
    batch_size :          12000
    best_batch_loss : 1.8190301175856427
    Epoch 5, Loss: 1.8230269517917037, fit: 0.062
      batch rate adapted learning_rate :              2e-08
      loss :              1.8190301175856427
      loss_factor :              0.18190301175856427
      loss adapted learning_rate :              6.617741137367274e-10
    epoch : 6 ; learning_rate : 6.617741137367274e-10 ; fit : 0.062
    batch_size :          12000
    best_batch_loss : 1.8163455442768475
    Epoch 6, Loss: 1.8230269486974076, fit: 0.058
      batch rate adapted learning_rate :              2e-08
      loss :              1.8244733730099505
      loss_factor :              0.18244733730099505
      loss adapted learning_rate :              6.657406177644613e-10
    epoch : 7 ; learning_rate : 6.657406177644613e-10 ; fit : 0.058
    batch_size :          12000
    best_batch_loss : 1.819043010412945
    Epoch 7, Loss: 1.8230269457664394, fit: 0.059333333333333335
      batch rate adapted learning_rate :              2e-08
      loss :              1.82349591580381
      loss_factor :              0.182349591580381
      loss adapted learning_rate :              6.65027470990635e-10
    epoch : 8 ; learning_rate : 6.65027470990635e-10 ; fit : 0.059333333333333335
    batch_size :          12000
    best_batch_loss : 1.8210046345647457
    Epoch 8, Loss: 1.8230269427911892, fit: 0.060583333333333336
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.821084943049485_fit_0.060583333333333336_2024-01-03_165355
  self.fit : 0.060583333333333336
  self.loss : 1.821084943049485
  current_accuracy : 0.0553
   Accuracy mean: 0.0553
   Accuracy mean: 0.0553
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.2
      batch rate adapted learning_rate :              2e-05
    epoch : 0 ; learning_rate : 2e-05 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.6705136968324756
    Epoch 0, Loss: 1.6797089288813385, fit: 0.13083333333333333
    Epoch 0, Loss: 1.6797089288813385
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6780118865641667_fit_0.13083333333333333_2024-01-03_165358
  self.fit : 0.13083333333333333
  self.loss : 1.6780118865641667
  current_accuracy : 0.127
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.2
      batch rate adapted learning_rate :              2e-05
      loss :              1.6780118865641667
      loss_factor :              0.16780118865641666
      loss adapted learning_rate :              5.631447782901268e-07
    epoch : 0 ; learning_rate : 5.631447782901268e-07 ; fit : 0.13083333333333333
    batch_size :          12000
    best_batch_loss : 1.6695014287246095
    Epoch 0, Loss: 1.6796310902613287, fit: 0.12333333333333334
    Epoch 0, Loss: 1.6796310902613287
      batch rate adapted learning_rate :              2e-05
      loss :              1.691318558598981
      loss_factor :              0.1691318558598981
      loss adapted learning_rate :              5.721116933322669e-07
    epoch : 1 ; learning_rate : 5.721116933322669e-07 ; fit : 0.12333333333333334
    batch_size :          12000
    best_batch_loss : 1.673989921560443
    Epoch 1, Loss: 1.6796273085448017, fit: 0.13233333333333333
      batch rate adapted learning_rate :              2e-05
      loss :              1.674846938801215
      loss_factor :              0.1674846938801215
      loss adapted learning_rate :              5.610224536823601e-07
    epoch : 2 ; learning_rate : 5.610224536823601e-07 ; fit : 0.13233333333333333
    batch_size :          12000
    best_batch_loss : 1.6742937915956402
    Epoch 2, Loss: 1.6796236246134422, fit: 0.12775
      batch rate adapted learning_rate :              2e-05
      loss :              1.6833799231842452
      loss_factor :              0.16833799231842453
      loss adapted learning_rate :              5.667535931559591e-07
    epoch : 3 ; learning_rate : 5.667535931559591e-07 ; fit : 0.12775
    batch_size :          12000
    best_batch_loss : 1.6709055178058485
    Epoch 3, Loss: 1.679619992836438, fit: 0.13083333333333333
      batch rate adapted learning_rate :              2e-05
      loss :              1.6783146244660538
      loss_factor :              0.16783146244660538
      loss adapted learning_rate :              5.633479957393263e-07
    epoch : 4 ; learning_rate : 5.633479957393263e-07 ; fit : 0.13083333333333333
    batch_size :          12000
    best_batch_loss : 1.6742134821618961
    Epoch 4, Loss: 1.6796156629896744, fit: 0.13116666666666665
      batch rate adapted learning_rate :              2e-05
      loss :              1.676065840580488
      loss_factor :              0.1676065840580488
      loss adapted learning_rate :              5.618393403921556e-07
    epoch : 5 ; learning_rate : 5.618393403921556e-07 ; fit : 0.13116666666666665
    batch_size :          12000
    best_batch_loss : 1.6707441672947858
    Epoch 5, Loss: 1.679611966556522, fit: 0.12508333333333332
      batch rate adapted learning_rate :              2e-05
      loss :              1.6890257602960879
      loss_factor :              0.1689025760296088
      loss adapted learning_rate :              5.705616037887555e-07
    epoch : 6 ; learning_rate : 5.705616037887555e-07 ; fit : 0.12508333333333332
    batch_size :          12000
    best_batch_loss : 1.6730371111913482
    Epoch 6, Loss: 1.6796081937285463, fit: 0.13041666666666665
      batch rate adapted learning_rate :              2e-05
      loss :              1.676894523130902
      loss_factor :              0.1676894523130902
      loss adapted learning_rate :              5.623950483412832e-07
    epoch : 7 ; learning_rate : 5.623950483412832e-07 ; fit : 0.13041666666666665
    batch_size :          12000
    best_batch_loss : 1.670601008449137
    Epoch 7, Loss: 1.6796044278889677, fit: 0.13408333333333333
      batch rate adapted learning_rate :              2e-05
      loss :              1.670601008449137
      loss_factor :              0.16706010084491368
      loss adapted learning_rate :              5.581815458862547e-07
    epoch : 8 ; learning_rate : 5.581815458862547e-07 ; fit : 0.13408333333333333
    batch_size :          12000
    best_batch_loss : 1.6742644390909602
    Epoch 8, Loss: 1.6796006214337202, fit: 0.13066666666666665
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6781980125327973_fit_0.13066666666666665_2024-01-03_165418
  self.fit : 0.13066666666666665
  self.loss : 1.6781980125327973
  current_accuracy : 0.127
   Accuracy mean: 0.127
   Accuracy mean: 0.127
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.0002
    epoch : 0 ; learning_rate : 0.0002 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7466529243580395
    Epoch 0, Loss: 1.7508069190182962, fit: 0.09016666666666667
    Epoch 0, Loss: 1.7508069190182962
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7530096938445217_fit_0.09016666666666667_2024-01-03_165421
  self.fit : 0.09016666666666667
  self.loss : 1.7530096938445217
  current_accuracy : 0.1012
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.0002
      loss :              1.7530096938445217
      loss_factor :              0.17530096938445217
      loss adapted learning_rate :              6.146085973425727e-06
    epoch : 0 ; learning_rate : 6.146085973425727e-06 ; fit : 0.09016666666666667
    batch_size :          12000
    best_batch_loss : 1.7401819273702526
    Epoch 0, Loss: 1.7504156789291674, fit: 0.09683333333333333
    Epoch 0, Loss: 1.7504156789291674
      batch rate adapted learning_rate :              0.0002
      loss :              1.7401819273702526
      loss_factor :              0.17401819273702526
      loss adapted learning_rate :              6.0564662806920946e-06
    epoch : 1 ; learning_rate : 6.0564662806920946e-06 ; fit : 0.09683333333333333
    batch_size :          12000
    best_batch_loss : 1.746029114068837
    Epoch 1, Loss: 1.750392643990801, fit: 0.09266666666666666
      batch rate adapted learning_rate :              0.0002
      loss :              1.74763460543521
      loss_factor :              0.17476346054352102
      loss adapted learning_rate :              6.108453428229366e-06
    epoch : 2 ; learning_rate : 6.108453428229366e-06 ; fit : 0.09266666666666666
    batch_size :          12000
    best_batch_loss : 1.7468783253622648
    Epoch 2, Loss: 1.750371778143699, fit: 0.09225
      batch rate adapted learning_rate :              0.0002
      loss :              1.749682220280969
      loss_factor :              0.1749682220280969
      loss adapted learning_rate :              6.122775743934683e-06
    epoch : 3 ; learning_rate : 6.122775743934683e-06 ; fit : 0.09225
    batch_size :          12000
    best_batch_loss : 1.7446159591869694
    Epoch 3, Loss: 1.7503496147545814, fit: 0.092
      batch rate adapted learning_rate :              0.0002
      loss :              1.749289350784784
      loss_factor :              0.1749289350784784
      loss adapted learning_rate :              6.120026465538102e-06
    epoch : 4 ; learning_rate : 6.120026465538102e-06 ; fit : 0.092
    batch_size :          12000
    best_batch_loss : 1.7429661824837173
    Epoch 4, Loss: 1.7503275833234646, fit: 0.09166666666666666
      batch rate adapted learning_rate :              0.0002
      loss :              1.753978547760862
      loss_factor :              0.17539785477608622
      loss adapted learning_rate :              6.152881492010607e-06
    epoch : 5 ; learning_rate : 6.152881492010607e-06 ; fit : 0.09166666666666666
    batch_size :          12000
    best_batch_loss : 1.744179139524262
    Epoch 5, Loss: 1.750305221002306, fit: 0.0965
      batch rate adapted learning_rate :              0.0002
      loss :              1.744179139524262
      loss_factor :              0.1744179139524262
      loss adapted learning_rate :              6.08432174150319e-06
    epoch : 6 ; learning_rate : 6.08432174150319e-06 ; fit : 0.0965
    batch_size :          12000
    best_batch_loss : 1.738510864115838
    Epoch 6, Loss: 1.7502829398363853, fit: 0.09191666666666666
      batch rate adapted learning_rate :              0.0002
      loss :              1.7502034850668267
      loss_factor :              0.17502034850668266
      loss adapted learning_rate :              6.126424478280131e-06
    epoch : 7 ; learning_rate : 6.126424478280131e-06 ; fit : 0.09191666666666666
    batch_size :          12000
    best_batch_loss : 1.7448989974569562
    Epoch 7, Loss: 1.750262633724076, fit: 0.09258333333333334
      batch rate adapted learning_rate :              0.0002
      loss :              1.7489209957222114
      loss_factor :              0.17489209957222113
      loss adapted learning_rate :              6.1174492985559416e-06
    epoch : 8 ; learning_rate : 6.1174492985559416e-06 ; fit : 0.09258333333333334
    batch_size :          12000
    best_batch_loss : 1.73935901412993
    Epoch 8, Loss: 1.750241509433702, fit: 0.09233333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7486314748620888_fit_0.09233333333333334_2024-01-03_165441
  self.fit : 0.09233333333333334
  self.loss : 1.7486314748620888
  current_accuracy : 0.1013
   Accuracy mean: 0.1012
   Accuracy mean: 0.1013
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.004
    epoch : 0 ; learning_rate : 0.004 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7043528495886018
    Epoch 0, Loss: 1.7183121610624172, fit: 0.11341666666666667
    Epoch 0, Loss: 1.7183121610624172
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7114239366915585_fit_0.11341666666666667_2024-01-03_165444
  self.fit : 0.11341666666666667
  self.loss : 1.7114239366915585
  current_accuracy : 0.1205
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.004
      loss :              1.7114239366915585
      loss_factor :              0.17114239366915585
      loss adapted learning_rate :              0.00011715887564323326
    epoch : 0 ; learning_rate : 0.00011715887564323326 ; fit : 0.11341666666666667
    batch_size :          12000
    best_batch_loss : 1.6989155523272745
    Epoch 0, Loss: 1.7048655423597814, fit: 0.11825
    Epoch 0, Loss: 1.7048655423597814
      batch rate adapted learning_rate :              0.004
      loss :              1.7020169107986434
      loss_factor :              0.17020169107986433
      loss adapted learning_rate :              0.00011587446258578227
    epoch : 1 ; learning_rate : 0.00011587446258578227 ; fit : 0.11825
    batch_size :          12000
    best_batch_loss : 1.6993465296279737
    Epoch 1, Loss: 1.7042214086642749, fit: 0.1195
      batch rate adapted learning_rate :              0.004
      loss :              1.6993465296279737
      loss_factor :              0.16993465296279736
      loss adapted learning_rate :              0.00011551114511034549
    epoch : 2 ; learning_rate : 0.00011551114511034549 ; fit : 0.1195
    batch_size :          12000
    best_batch_loss : 1.6961336952568342
    Epoch 2, Loss: 1.7035003061714318, fit: 0.11708333333333333
      batch rate adapted learning_rate :              0.004
      loss :              1.703190694057637
      loss_factor :              0.1703190694057637
      loss adapted learning_rate :              0.0001160343416129814
    epoch : 3 ; learning_rate : 0.0001160343416129814 ; fit : 0.11708333333333333
    batch_size :          12000
    best_batch_loss : 1.6994817966650833
    Epoch 3, Loss: 1.702855515919441, fit: 0.11425
      batch rate adapted learning_rate :              0.004
      loss :              1.7077323680472263
      loss_factor :              0.17077323680472262
      loss adapted learning_rate :              0.00011665399363504747
    epoch : 4 ; learning_rate : 0.00011665399363504747 ; fit : 0.11425
    batch_size :          12000
    best_batch_loss : 1.6915300079522473
    Epoch 4, Loss: 1.7021806342712278, fit: 0.11808333333333333
      batch rate adapted learning_rate :              0.004
      loss :              1.6989994398939074
      loss_factor :              0.16989994398939073
      loss adapted learning_rate :              0.00011546396387039243
    epoch : 5 ; learning_rate : 0.00011546396387039243 ; fit : 0.11808333333333333
    batch_size :          12000
    best_batch_loss : 1.6959377490524643
    Epoch 5, Loss: 1.7015392054715819, fit: 0.11933333333333333
      batch rate adapted learning_rate :              0.004
      loss :              1.6987308941599764
      loss_factor :              0.16987308941599763
      loss adapted learning_rate :              0.0001154274660309421
    epoch : 6 ; learning_rate : 0.0001154274660309421 ; fit : 0.11933333333333333
    batch_size :          12000
    best_batch_loss : 1.6881263838651106
    Epoch 6, Loss: 1.700904261756979, fit: 0.11783333333333333
      batch rate adapted learning_rate :              0.004
      loss :              1.7019409293032992
      loss_factor :              0.17019409293032992
      loss adapted learning_rate :              0.00011586411707351111
    epoch : 7 ; learning_rate : 0.00011586411707351111 ; fit : 0.11783333333333333
    batch_size :          12000
    best_batch_loss : 1.69452962703152
    Epoch 7, Loss: 1.7002825426241264, fit: 0.1125
      batch rate adapted learning_rate :              0.004
      loss :              1.7089349926976425
      loss_factor :              0.17089349926976424
      loss adapted learning_rate :              0.00011681835237065965
    epoch : 8 ; learning_rate : 0.00011681835237065965 ; fit : 0.1125
    batch_size :          12000
    best_batch_loss : 1.690983332247992
    Epoch 8, Loss: 1.699652865845371, fit: 0.1175
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.700388780209418_fit_0.1175_2024-01-03_165504
  self.fit : 0.1175
  self.loss : 1.700388780209418
  current_accuracy : 0.1234
   Accuracy mean: 0.1205
   Accuracy mean: 0.1234
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.005000000000000001
    epoch : 0 ; learning_rate : 0.005000000000000001 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7469875545334856
    Epoch 0, Loss: 1.7594231690525977, fit: 0.09608333333333334
    Epoch 0, Loss: 1.7594231690525977
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7469875545334856_fit_0.09608333333333334_2024-01-03_165507
  self.fit : 0.09608333333333334
  self.loss : 1.7469875545334856
  current_accuracy : 0.0909
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.7469875545334856
      loss_factor :              0.17469875545334856
      loss adapted learning_rate :              0.00015259827578474445
    epoch : 0 ; learning_rate : 0.00015259827578474445 ; fit : 0.09608333333333334
    batch_size :          12000
    best_batch_loss : 1.739333496389967
    Epoch 0, Loss: 1.7445261529548515, fit: 0.096
    Epoch 0, Loss: 1.7445261529548515
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.7439261434192832
      loss_factor :              0.17439261434192832
      loss adapted learning_rate :              0.00015206391968506275
    epoch : 1 ; learning_rate : 0.00015206391968506275 ; fit : 0.096
    batch_size :          12000
    best_batch_loss : 1.7412622966878286
    Epoch 1, Loss: 1.743907252009056, fit: 0.09633333333333334
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.7439778448217684
      loss_factor :              0.17439778448217685
      loss adapted learning_rate :              0.00015207293616145907
    epoch : 2 ; learning_rate : 0.00015207293616145907 ; fit : 0.09633333333333334
    batch_size :          12000
    best_batch_loss : 1.7346759737953574
    Epoch 2, Loss: 1.7432039619034299, fit: 0.09666666666666666
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.742193503605385
      loss_factor :              0.1742193503605385
      loss adapted learning_rate :              0.00015176191020024039
    epoch : 3 ; learning_rate : 0.00015176191020024039 ; fit : 0.09666666666666666
    batch_size :          12000
    best_batch_loss : 1.7379167748099993
    Epoch 3, Loss: 1.7424385024157933, fit: 0.10033333333333333
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.7379167748099993
      loss_factor :              0.17379167748099994
      loss adapted learning_rate :              0.00015101773580829952
    epoch : 4 ; learning_rate : 0.00015101773580829952 ; fit : 0.10033333333333333
    batch_size :          12000
    best_batch_loss : 1.737491520114426
    Epoch 4, Loss: 1.741589451890612, fit: 0.09866666666666667
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.737491520114426
      loss_factor :              0.17374915201144261
      loss adapted learning_rate :              0.000150943839123477
    epoch : 5 ; learning_rate : 0.000150943839123477 ; fit : 0.09866666666666667
    batch_size :          12000
    best_batch_loss : 1.73307753996259
    Epoch 5, Loss: 1.7407961651942638, fit: 0.1005
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.73307753996259
      loss_factor :              0.173307753996259
      loss adapted learning_rate :              0.00015017788797613916
    epoch : 6 ; learning_rate : 0.00015017788797613916 ; fit : 0.1005
    batch_size :          12000
    best_batch_loss : 1.7288413524214525
    Epoch 6, Loss: 1.7400195504921991, fit: 0.10291666666666667
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.7288413524214525
      loss_factor :              0.17288413524214524
      loss adapted learning_rate :              0.00014944462109212187
    epoch : 7 ; learning_rate : 0.00014944462109212187 ; fit : 0.10291666666666667
    batch_size :          12000
    best_batch_loss : 1.7306025263076714
    Epoch 7, Loss: 1.7391846742186123, fit: 0.09633333333333334
      batch rate adapted learning_rate :              0.005000000000000001
      loss :              1.7410536100928533
      loss_factor :              0.17410536100928534
      loss adapted learning_rate :              0.0001515633836608679
    epoch : 8 ; learning_rate : 0.0001515633836608679 ; fit : 0.09633333333333334
    batch_size :          12000
    best_batch_loss : 1.7342552797160498
    Epoch 8, Loss: 1.738399625926092, fit: 0.09675
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7387730526076821_fit_0.09675_2024-01-03_165527
  self.fit : 0.09675
  self.loss : 1.7387730526076821
  current_accuracy : 0.0936
   Accuracy mean: 0.0909
   Accuracy mean: 0.0936
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.006
    epoch : 0 ; learning_rate : 0.006 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7747410008803683
    Epoch 0, Loss: 1.791592300700907, fit: 0.08108333333333333
    Epoch 0, Loss: 1.791592300700907
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7747410008803683_fit_0.08108333333333333_2024-01-03_165529
  self.fit : 0.08108333333333333
  self.loss : 1.7747410008803683
  current_accuracy : 0.0784
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.006
      loss :              1.7747410008803683
      loss_factor :              0.17747410008803682
      loss adapted learning_rate :              0.00018898233721235106
    epoch : 0 ; learning_rate : 0.00018898233721235106 ; fit : 0.08108333333333333
    batch_size :          12000
    best_batch_loss : 1.7606037953267162
    Epoch 0, Loss: 1.762331650240216, fit: 0.08708333333333333
    Epoch 0, Loss: 1.762331650240216
      batch rate adapted learning_rate :              0.006
      loss :              1.760664211815596
      loss_factor :              0.1760664211815596
      loss adapted learning_rate :              0.00018599630800609403
    epoch : 1 ; learning_rate : 0.00018599630800609403 ; fit : 0.08708333333333333
    batch_size :          12000
    best_batch_loss : 1.7503335101168647
    Epoch 1, Loss: 1.7610891135541582, fit: 0.09033333333333333
      batch rate adapted learning_rate :              0.006
      loss :              1.7503335101168647
      loss_factor :              0.17503335101168646
      loss adapted learning_rate :              0.00018382004379828147
    epoch : 2 ; learning_rate : 0.00018382004379828147 ; fit : 0.09033333333333333
    batch_size :          12000
    best_batch_loss : 1.753530985376877
    Epoch 2, Loss: 1.759807194379161, fit: 0.0845
      batch rate adapted learning_rate :              0.006
      loss :              1.7661735012264643
      loss_factor :              0.17661735012264643
      loss adapted learning_rate :              0.00018716213018607285
    epoch : 3 ; learning_rate : 0.00018716213018607285 ; fit : 0.0845
    batch_size :          12000
    best_batch_loss : 1.7497706532310215
    Epoch 3, Loss: 1.7585051319312917, fit: 0.08566666666666667
      batch rate adapted learning_rate :              0.006
      loss :              1.7624240550946257
      loss_factor :              0.17624240550946257
      loss adapted learning_rate :              0.00018636831299857106
    epoch : 4 ; learning_rate : 0.00018636831299857106 ; fit : 0.08566666666666667
    batch_size :          12000
    best_batch_loss : 1.75342283941938
    Epoch 4, Loss: 1.7572167059139359, fit: 0.08866666666666667
      batch rate adapted learning_rate :              0.006
      loss :              1.7575995920219867
      loss_factor :              0.17575995920219867
      loss adapted learning_rate :              0.00018534937955255124
    epoch : 5 ; learning_rate : 0.00018534937955255124 ; fit : 0.08866666666666667
    batch_size :          12000
    best_batch_loss : 1.746800859065289
    Epoch 5, Loss: 1.7558673314734983, fit: 0.09433333333333334
      batch rate adapted learning_rate :              0.006
      loss :              1.746800859065289
      loss_factor :              0.1746800859065289
      loss adapted learning_rate :              0.00018307879447387388
    epoch : 6 ; learning_rate : 0.00018307879447387388 ; fit : 0.09433333333333334
    batch_size :          12000
    best_batch_loss : 1.749712822527581
    Epoch 6, Loss: 1.7545873840355342, fit: 0.08983333333333333
      batch rate adapted learning_rate :              0.006
      loss :              1.7526209094191911
      loss_factor :              0.1752620909419191
      loss adapted learning_rate :              0.00018430080312800115
    epoch : 7 ; learning_rate : 0.00018430080312800115 ; fit : 0.08983333333333333
    batch_size :          12000
    best_batch_loss : 1.7493439539216316
    Epoch 7, Loss: 1.7533516196400067, fit: 0.09
      batch rate adapted learning_rate :              0.006
      loss :              1.7529858123441366
      loss_factor :              0.17529858123441366
      loss adapted learning_rate :              0.00018437755549678998
    epoch : 8 ; learning_rate : 0.00018437755549678998 ; fit : 0.09
    batch_size :          12000
    best_batch_loss : 1.7472206405310413
    Epoch 8, Loss: 1.7520558266372537, fit: 0.09225
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7472206405310413_fit_0.09225_2024-01-03_165551
  self.fit : 0.09225
  self.loss : 1.7472206405310413
  current_accuracy : 0.0832
   Accuracy mean: 0.0784
   Accuracy mean: 0.0832
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.020000000000000004
    epoch : 0 ; learning_rate : 0.020000000000000004 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.6841716719957982
    Epoch 0, Loss: 1.7197719893080494, fit: 0.12475
    Epoch 0, Loss: 1.7197719893080494
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6841716719957982_fit_0.12475_2024-01-03_165553
  self.fit : 0.12475
  self.loss : 1.6841716719957982
  current_accuracy : 0.1244
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.6841716719957982
      loss_factor :              0.16841716719957983
      loss adapted learning_rate :              0.0005672868441506247
    epoch : 0 ; learning_rate : 0.0005672868441506247 ; fit : 0.12475
    batch_size :          12000
    best_batch_loss : 1.6564391845031
    Epoch 0, Loss: 1.6619865708430988, fit: 0.13133333333333333
    Epoch 0, Loss: 1.6619865708430988
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.670473921248821
      loss_factor :              0.16704739212488212
      loss adapted learning_rate :              0.0005580966243144827
    epoch : 1 ; learning_rate : 0.0005580966243144827 ; fit : 0.13133333333333333
    batch_size :          12000
    best_batch_loss : 1.6513054081844618
    Epoch 1, Loss: 1.6595018421517047, fit: 0.135
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.6605820942205054
      loss_factor :              0.16605820942205055
      loss adapted learning_rate :              0.000551506578329152
    epoch : 2 ; learning_rate : 0.000551506578329152 ; fit : 0.135
    batch_size :          12000
    best_batch_loss : 1.6502665093490152
    Epoch 2, Loss: 1.6570033187504905, fit: 0.13683333333333333
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.6570423834566355
      loss_factor :              0.16570423834566356
      loss adapted learning_rate :              0.0005491578921143297
    epoch : 3 ; learning_rate : 0.0005491578921143297 ; fit : 0.13683333333333333
    batch_size :          12000
    best_batch_loss : 1.650461278476944
    Epoch 3, Loss: 1.654430945266091, fit: 0.137
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.6581809548127782
      loss_factor :              0.16581809548127782
      loss adapted learning_rate :              0.0005499128157807635
    epoch : 4 ; learning_rate : 0.0005499128157807635 ; fit : 0.137
    batch_size :          12000
    best_batch_loss : 1.645621114317421
    Epoch 4, Loss: 1.6519773524983292, fit: 0.1395
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.6498472631661987
      loss_factor :              0.16498472631661987
      loss adapted learning_rate :              0.0005443991983553993
    epoch : 5 ; learning_rate : 0.0005443991983553993 ; fit : 0.1395
    batch_size :          12000
    best_batch_loss : 1.6417232943259066
    Epoch 5, Loss: 1.6495777963787865, fit: 0.14383333333333334
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.6417232943259066
      loss_factor :              0.16417232943259066
      loss adapted learning_rate :              0.0005390510750264616
    epoch : 6 ; learning_rate : 0.0005390510750264616 ; fit : 0.14383333333333334
    batch_size :          12000
    best_batch_loss : 1.6374446789111408
    Epoch 6, Loss: 1.647092579359583, fit: 0.14533333333333334
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.6374446789111408
      loss_factor :              0.16374446789111408
      loss adapted learning_rate :              0.0005362450152988819
    epoch : 7 ; learning_rate : 0.0005362450152988819 ; fit : 0.14533333333333334
    batch_size :          12000
    best_batch_loss : 1.6353481306323925
    Epoch 7, Loss: 1.644726525064029, fit: 0.14333333333333334
      batch rate adapted learning_rate :              0.020000000000000004
      loss :              1.6430676766965204
      loss_factor :              0.16430676766965205
      loss adapted learning_rate :              0.0005399342780409805
    epoch : 8 ; learning_rate : 0.0005399342780409805 ; fit : 0.14333333333333334
    batch_size :          12000
    best_batch_loss : 1.6403137705172854
    Epoch 8, Loss: 1.6423328603398737, fit: 0.14216666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6425685299810415_fit_0.14216666666666666_2024-01-03_165614
  self.fit : 0.14216666666666666
  self.loss : 1.6425685299810415
  current_accuracy : 0.1339
   Accuracy mean: 0.1244
   Accuracy mean: 0.1339
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.18000000000000002
    epoch : 0 ; learning_rate : 0.18000000000000002 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.0887900901292933
    Epoch 0, Loss: 1.3889708299441077, fit: 0.4295
    Epoch 0, Loss: 1.3889708299441077
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.0887900901292933_fit_0.4295_2024-01-03_165617
  self.fit : 0.4295
  self.loss : 1.0887900901292933
  current_accuracy : 0.438
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              1.0887900901292933
      loss_factor :              0.10887900901292932
      loss adapted learning_rate :              0.0021338349486547583
    epoch : 0 ; learning_rate : 0.0021338349486547583 ; fit : 0.4295
    batch_size :          12000
    best_batch_loss : 1.0523355370048373
    Epoch 0, Loss: 1.0663790742278452, fit: 0.4514166666666667
    Epoch 0, Loss: 1.0663790742278452
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              1.0523355370048373
      loss_factor :              0.10523355370048373
      loss adapted learning_rate :              0.0019933381483978672
    epoch : 1 ; learning_rate : 0.0019933381483978672 ; fit : 0.4514166666666667
    batch_size :          12000
    best_batch_loss : 1.0174080571510777
    Epoch 1, Loss: 1.0286772132133455, fit: 0.4671666666666667
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              1.0214120313709083
      loss_factor :              0.10214120313709082
      loss adapted learning_rate :              0.0018779085680926417
    epoch : 2 ; learning_rate : 0.0018779085680926417 ; fit : 0.4671666666666667
    batch_size :          12000
    best_batch_loss : 0.9826475577612451
    Epoch 2, Loss: 0.9988703181341612, fit: 0.4846666666666667
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              0.9826475577612451
      loss_factor :              0.09826475577612451
      loss adapted learning_rate :              0.0017380732009934512
    epoch : 3 ; learning_rate : 0.0017380732009934512 ; fit : 0.4846666666666667
    batch_size :          12000
    best_batch_loss : 0.964084727043818
    Epoch 3, Loss: 0.9757822947372345, fit: 0.495
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              0.964084727043818
      loss_factor :              0.0964084727043818
      loss adapted learning_rate :              0.0016730268496544757
    epoch : 4 ; learning_rate : 0.0016730268496544757 ; fit : 0.495
    batch_size :          12000
    best_batch_loss : 0.9511289352517299
    Epoch 4, Loss: 0.9574942871773926, fit: 0.5021666666666667
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              0.9511289352517299
      loss_factor :              0.09511289352517299
      loss adapted learning_rate :              0.0016283632526515612
    epoch : 5 ; learning_rate : 0.0016283632526515612 ; fit : 0.5021666666666667
    batch_size :          12000
    best_batch_loss : 0.9334297638524822
    Epoch 5, Loss: 0.9417264653791022, fit: 0.5103333333333333
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              0.9334297638524822
      loss_factor :              0.09334297638524822
      loss adapted learning_rate :              0.0015683240232822614
    epoch : 6 ; learning_rate : 0.0015683240232822614 ; fit : 0.5103333333333333
    batch_size :          12000
    best_batch_loss : 0.9200949653769168
    Epoch 6, Loss: 0.9275389519072224, fit: 0.512
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              0.9288322336146819
      loss_factor :              0.09288322336146819
      loss adapted learning_rate :              0.0015529127727629502
    epoch : 7 ; learning_rate : 0.0015529127727629502 ; fit : 0.512
    batch_size :          12000
    best_batch_loss : 0.9024833645073222
    Epoch 7, Loss: 0.9147223625763966, fit: 0.5253333333333333
      batch rate adapted learning_rate :              0.18000000000000002
      loss :              0.9024833645073222
      loss_factor :              0.09024833645073223
      loss adapted learning_rate :              0.0014660572017824215
    epoch : 8 ; learning_rate : 0.0014660572017824215 ; fit : 0.5253333333333333
    batch_size :          12000
    best_batch_loss : 0.8885903440254868
    Epoch 8, Loss: 0.9035778379160707, fit: 0.5318333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.8885903440254868_fit_0.5318333333333334_2024-01-03_165637
  self.fit : 0.5318333333333334
  self.loss : 0.8885903440254868
  current_accuracy : 0.5342
   Accuracy mean: 0.438
   Accuracy mean: 0.5342
  Error saving file: doc/out/test_combinations_results/20240103165638.
  normalized_accuracies :      [0.04489455 0.04489455 0.162038   0.162038   0.         0.
   0.1497181  0.1497181  0.09584464 0.09605346 0.13614533 0.14220088
   0.07433702 0.07997494 0.04823554 0.05825851 0.144289   0.16412612
   0.79912299 1.        ]
batch_rate :  0.1
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.1
      batch rate adapted learning_rate :              1e-18
    epoch : 0 ; learning_rate : 1e-18 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7557348246695517
    Epoch 0, Loss: 1.762208979704512, fit: 0.0845
    Epoch 0, Loss: 1.762208979704512
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7669551335774158_fit_0.0845_2024-01-03_165641
  self.fit : 0.0845
  self.loss : 1.7669551335774158
  current_accuracy : 0.0883
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          0.1
      batch rate adapted learning_rate :              1e-18
      loss :              1.7669551335774158
      loss_factor :              0.17669551335774158
      loss adapted learning_rate :              3.122130444075584e-20
    epoch : 0 ; learning_rate : 3.122130444075584e-20 ; fit : 0.0845
    batch_size :          6000
    best_batch_loss : 1.7457525967126766
    Epoch 0, Loss: 1.7622089797045117, fit: 0.08516666666666667
    Epoch 0, Loss: 1.7622089797045117
      batch rate adapted learning_rate :              1e-18
      loss :              1.7675669432208179
      loss_factor :              0.1767566943220818
      loss adapted learning_rate :              3.1242928987669864e-20
    epoch : 1 ; learning_rate : 3.1242928987669864e-20 ; fit : 0.08516666666666667
    batch_size :          6000
    best_batch_loss : 1.7527431609352522
    Epoch 1, Loss: 1.7622089797045124, fit: 0.08383333333333333
      batch rate adapted learning_rate :              1e-18
      loss :              1.7679244565947887
      loss_factor :              0.17679244565947888
      loss adapted learning_rate :              3.12555688422598e-20
    epoch : 2 ; learning_rate : 3.12555688422598e-20 ; fit : 0.08383333333333333
    batch_size :          6000
    best_batch_loss : 1.7490562994197405
    Epoch 2, Loss: 1.7622089797045117, fit: 0.0945
      batch rate adapted learning_rate :              1e-18
      loss :              1.7501485849968363
      loss_factor :              0.17501485849968362
      loss adapted learning_rate :              3.063020069566428e-20
    epoch : 3 ; learning_rate : 3.063020069566428e-20 ; fit : 0.0945
    batch_size :          6000
    best_batch_loss : 1.7513671518316711
    Epoch 3, Loss: 1.762208979704512, fit: 0.08733333333333333
      batch rate adapted learning_rate :              1e-18
      loss :              1.7624595533774654
      loss_factor :              0.17624595533774653
      loss adapted learning_rate :              3.1062636772914946e-20
    epoch : 4 ; learning_rate : 3.1062636772914946e-20 ; fit : 0.08733333333333333
    batch_size :          6000
    best_batch_loss : 1.7468290252718652
    Epoch 4, Loss: 1.7622089797045117, fit: 0.0835
      batch rate adapted learning_rate :              1e-18
      loss :              1.7716476823659186
      loss_factor :              0.17716476823659186
      loss adapted learning_rate :              3.1387355104325306e-20
    epoch : 5 ; learning_rate : 3.1387355104325306e-20 ; fit : 0.0835
    batch_size :          6000
    best_batch_loss : 1.7489545010464047
    Epoch 5, Loss: 1.762208979704512, fit: 0.08683333333333333
      batch rate adapted learning_rate :              1e-18
      loss :              1.7647151096487863
      loss_factor :              0.17647151096487862
      loss adapted learning_rate :              3.114219418222728e-20
    epoch : 6 ; learning_rate : 3.114219418222728e-20 ; fit : 0.08683333333333333
    batch_size :          6000
    best_batch_loss : 1.7488220769763665
    Epoch 6, Loss: 1.762208979704512, fit: 0.084
      batch rate adapted learning_rate :              1e-18
      loss :              1.7680404592558459
      loss_factor :              0.17680404592558457
      loss adapted learning_rate :              3.125967065565622e-20
    epoch : 7 ; learning_rate : 3.125967065565622e-20 ; fit : 0.084
    batch_size :          6000
    best_batch_loss : 1.7497307163846114
    Epoch 7, Loss: 1.7622089797045117, fit: 0.09566666666666666
      batch rate adapted learning_rate :              1e-18
      loss :              1.7497307163846114
      loss_factor :              0.17497307163846115
      loss adapted learning_rate :              3.061557579859806e-20
    epoch : 8 ; learning_rate : 3.061557579859806e-20 ; fit : 0.09566666666666666
    batch_size :          6000
    best_batch_loss : 1.7446970731247948
    Epoch 8, Loss: 1.7622089797045117, fit: 0.094
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7483425471178076_fit_0.094_2024-01-03_165702
  self.fit : 0.094
  self.loss : 1.7483425471178076
  current_accuracy : 0.0883
   Accuracy mean: 0.0883
   Accuracy mean: 0.0883
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.1
      batch rate adapted learning_rate :              1.0000000000000001e-11
    epoch : 0 ; learning_rate : 1.0000000000000001e-11 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7795414676317565
    Epoch 0, Loss: 1.7875746023219874, fit: 0.081
    Epoch 0, Loss: 1.7875746023219874
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.789867750100176_fit_0.081_2024-01-03_165704
  self.fit : 0.081
  self.loss : 1.789867750100176
  current_accuracy : 0.0799
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.1
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.789867750100176
      loss_factor :              0.1789867750100176
      loss adapted learning_rate :              3.203626562848667e-13
    epoch : 0 ; learning_rate : 3.203626562848667e-13 ; fit : 0.081
    batch_size :          6000
    best_batch_loss : 1.7799752925987418
    Epoch 0, Loss: 1.7875746022859378, fit: 0.0835
    Epoch 0, Loss: 1.7875746022859378
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.7879900857425175
      loss_factor :              0.17879900857425174
      loss adapted learning_rate :              3.196908546713535e-13
    epoch : 1 ; learning_rate : 3.196908546713535e-13 ; fit : 0.0835
    batch_size :          6000
    best_batch_loss : 1.7770788892323144
    Epoch 1, Loss: 1.7875746022839603, fit: 0.087
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.7771941724353613
      loss_factor :              0.17771941724353613
      loss adapted learning_rate :              3.158419126538209e-13
    epoch : 2 ; learning_rate : 3.158419126538209e-13 ; fit : 0.087
    batch_size :          6000
    best_batch_loss : 1.7812824295931575
    Epoch 2, Loss: 1.7875746022814372, fit: 0.08766666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.7812824295931575
      loss_factor :              0.17812824295931576
      loss adapted learning_rate :              3.172967093977303e-13
    epoch : 3 ; learning_rate : 3.172967093977303e-13 ; fit : 0.08766666666666667
    batch_size :          6000
    best_batch_loss : 1.7759905400588414
    Epoch 3, Loss: 1.7875746022792145, fit: 0.08716666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.7795287637298498
      loss_factor :              0.17795287637298499
      loss adapted learning_rate :              3.166722620941888e-13
    epoch : 4 ; learning_rate : 3.166722620941888e-13 ; fit : 0.08716666666666667
    batch_size :          6000
    best_batch_loss : 1.7747310799477454
    Epoch 4, Loss: 1.7875746022770005, fit: 0.07866666666666666
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.7970678549419683
      loss_factor :              0.17970678549419683
      loss adapted learning_rate :              3.229452875265728e-13
    epoch : 5 ; learning_rate : 3.229452875265728e-13 ; fit : 0.07866666666666666
    batch_size :          6000
    best_batch_loss : 1.7731491624487992
    Epoch 5, Loss: 1.7875746022745547, fit: 0.08166666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.7882010365179006
      loss_factor :              0.17882010365179007
      loss adapted learning_rate :              3.197662947003695e-13
    epoch : 6 ; learning_rate : 3.197662947003695e-13 ; fit : 0.08166666666666667
    batch_size :          6000
    best_batch_loss : 1.7736924100198768
    Epoch 6, Loss: 1.7875746022730152, fit: 0.08016666666666666
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.7937491242700103
      loss_factor :              0.17937491242700104
      loss adapted learning_rate :              3.21753592081943e-13
    epoch : 7 ; learning_rate : 3.21753592081943e-13 ; fit : 0.08016666666666666
    batch_size :          6000
    best_batch_loss : 1.7795745593830317
    Epoch 7, Loss: 1.7875746022705943, fit: 0.081
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss :              1.7917165329734162
      loss_factor :              0.1791716532973416
      loss adapted learning_rate :              3.2102481345302786e-13
    epoch : 8 ; learning_rate : 3.2102481345302786e-13 ; fit : 0.081
    batch_size :          6000
    best_batch_loss : 1.7811237039633452
    Epoch 8, Loss: 1.7875746022685288, fit: 0.08566666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7834276962532079_fit_0.08566666666666667_2024-01-03_165725
  self.fit : 0.08566666666666667
  self.loss : 1.7834276962532079
  current_accuracy : 0.0799
   Accuracy mean: 0.0799
   Accuracy mean: 0.0799
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.1
      batch rate adapted learning_rate :              1e-08
    epoch : 0 ; learning_rate : 1e-08 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7617170326106903
    Epoch 0, Loss: 1.7680926833718988, fit: 0.089
    Epoch 0, Loss: 1.7680926833718988
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7723658251590777_fit_0.089_2024-01-03_165727
  self.fit : 0.089
  self.loss : 1.7723658251590777
  current_accuracy : 0.0916
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.1
      batch rate adapted learning_rate :              1e-08
      loss :              1.7723658251590777
      loss_factor :              0.17723658251590776
      loss adapted learning_rate :              3.1412806181918184e-10
    epoch : 0 ; learning_rate : 3.1412806181918184e-10 ; fit : 0.089
    batch_size :          6000
    best_batch_loss : 1.7591367627786483
    Epoch 0, Loss: 1.7680926340074365, fit: 0.0895
    Epoch 0, Loss: 1.7680926340074365
      batch rate adapted learning_rate :              1e-08
      loss :              1.7671607067081783
      loss_factor :              0.17671607067081782
      loss adapted learning_rate :              3.1228569633333477e-10
    epoch : 1 ; learning_rate : 3.1228569633333477e-10 ; fit : 0.0895
    batch_size :          6000
    best_batch_loss : 1.7548617846690187
    Epoch 1, Loss: 1.768092631081052, fit: 0.09133333333333334
      batch rate adapted learning_rate :              1e-08
      loss :              1.7693092716843026
      loss_factor :              0.17693092716843026
      loss adapted learning_rate :              3.1304552988680376e-10
    epoch : 2 ; learning_rate : 3.1304552988680376e-10 ; fit : 0.09133333333333334
    batch_size :          6000
    best_batch_loss : 1.7559309430473784
    Epoch 2, Loss: 1.7680926280649205, fit: 0.08883333333333333
      batch rate adapted learning_rate :              1e-08
      loss :              1.7703944589062524
      loss_factor :              0.17703944589062523
      loss adapted learning_rate :              3.134296540125962e-10
    epoch : 3 ; learning_rate : 3.134296540125962e-10 ; fit : 0.08883333333333333
    batch_size :          6000
    best_batch_loss : 1.7554146173053866
    Epoch 3, Loss: 1.7680926256098788, fit: 0.091
      batch rate adapted learning_rate :              1e-08
      loss :              1.766668853681888
      loss_factor :              0.1766668853681888
      loss adapted learning_rate :              3.1211188385696765e-10
    epoch : 4 ; learning_rate : 3.1211188385696765e-10 ; fit : 0.091
    batch_size :          6000
    best_batch_loss : 1.759770811466168
    Epoch 4, Loss: 1.7680926225446942, fit: 0.08583333333333333
      batch rate adapted learning_rate :              1e-08
      loss :              1.7750480300775398
      loss_factor :              0.17750480300775398
      loss adapted learning_rate :              3.150795509082155e-10
    epoch : 5 ; learning_rate : 3.150795509082155e-10 ; fit : 0.08583333333333333
    batch_size :          6000
    best_batch_loss : 1.7605966177141714
    Epoch 5, Loss: 1.7680926194918591, fit: 0.091
      batch rate adapted learning_rate :              1e-08
      loss :              1.7680999029730828
      loss_factor :              0.1768099902973083
      loss adapted learning_rate :              3.126177266893425e-10
    epoch : 6 ; learning_rate : 3.126177266893425e-10 ; fit : 0.091
    batch_size :          6000
    best_batch_loss : 1.7538213425919578
    Epoch 6, Loss: 1.7680926167285025, fit: 0.09266666666666666
      batch rate adapted learning_rate :              1e-08
      loss :              1.7617176642026713
      loss_factor :              0.17617176642026713
      loss adapted learning_rate :              3.103649128363716e-10
    epoch : 7 ; learning_rate : 3.103649128363716e-10 ; fit : 0.09266666666666666
    batch_size :          6000
    best_batch_loss : 1.7570580515059038
    Epoch 7, Loss: 1.7680926141070923, fit: 0.0855
      batch rate adapted learning_rate :              1e-08
      loss :              1.7777203396388628
      loss_factor :              0.17777203396388627
      loss adapted learning_rate :              3.160289605965713e-10
    epoch : 8 ; learning_rate : 3.160289605965713e-10 ; fit : 0.0855
    batch_size :          6000
    best_batch_loss : 1.7546890310281151
    Epoch 8, Loss: 1.7680926113175834, fit: 0.08333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.784043024653902_fit_0.08333333333333333_2024-01-03_165748
  self.fit : 0.08333333333333333
  self.loss : 1.784043024653902
  current_accuracy : 0.0916
   Accuracy mean: 0.0916
   Accuracy mean: 0.0916
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.1
      batch rate adapted learning_rate :              1e-05
    epoch : 0 ; learning_rate : 1e-05 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7225699569559625
    Epoch 0, Loss: 1.7390472493517306, fit: 0.10533333333333333
    Epoch 0, Loss: 1.7390472493517306
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7405967866943068_fit_0.10533333333333333_2024-01-03_165750
  self.fit : 0.10533333333333333
  self.loss : 1.7405967866943068
  current_accuracy : 0.1092
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.1
      batch rate adapted learning_rate :              1e-05
      loss :              1.7405967866943068
      loss_factor :              0.17405967866943067
      loss adapted learning_rate :              3.029677173850546e-07
    epoch : 0 ; learning_rate : 3.029677173850546e-07 ; fit : 0.10533333333333333
    batch_size :          6000
    best_batch_loss : 1.7270605116426687
    Epoch 0, Loss: 1.7389354776266643, fit: 0.10966666666666666
    Epoch 0, Loss: 1.7389354776266643
      batch rate adapted learning_rate :              1e-05
      loss :              1.7393042018623726
      loss_factor :              0.17393042018623725
      loss adapted learning_rate :              3.025179106616105e-07
    epoch : 1 ; learning_rate : 3.025179106616105e-07 ; fit : 0.10966666666666666
    batch_size :          6000
    best_batch_loss : 1.7225035520808138
    Epoch 1, Loss: 1.7389308743321286, fit: 0.10366666666666667
      batch rate adapted learning_rate :              1e-05
      loss :              1.7470059234126938
      loss_factor :              0.1747005923412694
      loss adapted learning_rate :              3.0520296964390394e-07
    epoch : 2 ; learning_rate : 3.0520296964390394e-07 ; fit : 0.10366666666666667
    batch_size :          6000
    best_batch_loss : 1.7302103714738328
    Epoch 2, Loss: 1.7389250812415964, fit: 0.11
      batch rate adapted learning_rate :              1e-05
      loss :              1.733417019272542
      loss_factor :              0.1733417019272542
      loss adapted learning_rate :              3.0047345627037046e-07
    epoch : 3 ; learning_rate : 3.0047345627037046e-07 ; fit : 0.11
    batch_size :          6000
    best_batch_loss : 1.7325207621028607
    Epoch 3, Loss: 1.7389194999545885, fit: 0.1025
      batch rate adapted learning_rate :              1e-05
      loss :              1.7467359031747423
      loss_factor :              0.17467359031747423
      loss adapted learning_rate :              3.051086315439683e-07
    epoch : 4 ; learning_rate : 3.051086315439683e-07 ; fit : 0.1025
    batch_size :          6000
    best_batch_loss : 1.728266583842366
    Epoch 4, Loss: 1.7389140110573316, fit: 0.10866666666666666
      batch rate adapted learning_rate :              1e-05
      loss :              1.7366913741449403
      loss_factor :              0.17366913741449402
      loss adapted learning_rate :              3.016096929029441e-07
    epoch : 5 ; learning_rate : 3.016096929029441e-07 ; fit : 0.10866666666666666
    batch_size :          6000
    best_batch_loss : 1.7314836788200318
    Epoch 5, Loss: 1.7389088827805756, fit: 0.11183333333333334
      batch rate adapted learning_rate :              1e-05
      loss :              1.7314836788200318
      loss_factor :              0.1731483678820032
      loss adapted learning_rate :              2.9980357300201516e-07
    epoch : 6 ; learning_rate : 2.9980357300201516e-07 ; fit : 0.11183333333333334
    batch_size :          6000
    best_batch_loss : 1.7282050962655178
    Epoch 6, Loss: 1.7389030671030246, fit: 0.10333333333333333
      batch rate adapted learning_rate :              1e-05
      loss :              1.7441125593089668
      loss_factor :              0.17441125593089668
      loss adapted learning_rate :              3.041928619539275e-07
    epoch : 7 ; learning_rate : 3.041928619539275e-07 ; fit : 0.10333333333333333
    batch_size :          6000
    best_batch_loss : 1.7205797900724638
    Epoch 7, Loss: 1.738897687972484, fit: 0.10816666666666666
      batch rate adapted learning_rate :              1e-05
      loss :              1.7366404585949147
      loss_factor :              0.17366404585949147
      loss adapted learning_rate :              3.015920082428756e-07
    epoch : 8 ; learning_rate : 3.015920082428756e-07 ; fit : 0.10816666666666666
    batch_size :          6000
    best_batch_loss : 1.7265940888939275
    Epoch 8, Loss: 1.7388920031977735, fit: 0.108
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.740865800001097_fit_0.108_2024-01-03_165811
  self.fit : 0.108
  self.loss : 1.740865800001097
  current_accuracy : 0.1092
   Accuracy mean: 0.1092
   Accuracy mean: 0.1092
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.761211055078879
    Epoch 0, Loss: 1.7697372157158033, fit: 0.08683333333333333
    Epoch 0, Loss: 1.7697372157158033
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.761211055078879_fit_0.08683333333333333_2024-01-03_165814
  self.fit : 0.08683333333333333
  self.loss : 1.761211055078879
  current_accuracy : 0.0819
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.0001
      loss :              1.761211055078879
      loss_factor :              0.1761211055078879
      loss adapted learning_rate :              3.101864380532058e-06
    epoch : 0 ; learning_rate : 3.101864380532058e-06 ; fit : 0.08683333333333333
    batch_size :          6000
    best_batch_loss : 1.7621056390807883
    Epoch 0, Loss: 1.7696206023970065, fit: 0.08233333333333333
    Epoch 0, Loss: 1.7696206023970065
      batch rate adapted learning_rate :              0.0001
      loss :              1.7708637842633828
      loss_factor :              0.1770863784263383
      loss adapted learning_rate :              3.135958542415629e-06
    epoch : 1 ; learning_rate : 3.135958542415629e-06 ; fit : 0.08233333333333333
    batch_size :          6000
    best_batch_loss : 1.7571175205032905
    Epoch 1, Loss: 1.7696134963307555, fit: 0.08583333333333333
      batch rate adapted learning_rate :              0.0001
      loss :              1.765748450558698
      loss_factor :              0.1765748450558698
      loss adapted learning_rate :              3.1178675906504423e-06
    epoch : 2 ; learning_rate : 3.1178675906504423e-06 ; fit : 0.08583333333333333
    batch_size :          6000
    best_batch_loss : 1.7579536641947098
    Epoch 2, Loss: 1.7696082117460108, fit: 0.07933333333333334
      batch rate adapted learning_rate :              0.0001
      loss :              1.7755370231105325
      loss_factor :              0.17755370231105325
      loss adapted learning_rate :              3.1525317204362116e-06
    epoch : 3 ; learning_rate : 3.1525317204362116e-06 ; fit : 0.07933333333333334
    batch_size :          6000
    best_batch_loss : 1.7582891430880423
    Epoch 3, Loss: 1.7696020259023453, fit: 0.07966666666666666
      batch rate adapted learning_rate :              0.0001
      loss :              1.7776372147838542
      loss_factor :              0.17776372147838543
      loss adapted learning_rate :              3.1599940673844987e-06
    epoch : 4 ; learning_rate : 3.1599940673844987e-06 ; fit : 0.07966666666666666
    batch_size :          6000
    best_batch_loss : 1.755918790465744
    Epoch 4, Loss: 1.7695954346900884, fit: 0.08316666666666667
      batch rate adapted learning_rate :              0.0001
      loss :              1.768704807521078
      loss_factor :              0.17687048075210782
      loss adapted learning_rate :              3.128316696148174e-06
    epoch : 5 ; learning_rate : 3.128316696148174e-06 ; fit : 0.08316666666666667
    batch_size :          6000
    best_batch_loss : 1.7599175628040415
    Epoch 5, Loss: 1.7695898004080026, fit: 0.08516666666666667
      batch rate adapted learning_rate :              0.0001
      loss :              1.764728013418313
      loss_factor :              0.1764728013418313
      loss adapted learning_rate :              3.1142649613433455e-06
    epoch : 6 ; learning_rate : 3.1142649613433455e-06 ; fit : 0.08516666666666667
    batch_size :          6000
    best_batch_loss : 1.7635372355077428
    Epoch 6, Loss: 1.7695838654717797, fit: 0.0825
      batch rate adapted learning_rate :              0.0001
      loss :              1.7693157259428445
      loss_factor :              0.17693157259428444
      loss adapted learning_rate :              3.130478138068654e-06
    epoch : 7 ; learning_rate : 3.130478138068654e-06 ; fit : 0.0825
    batch_size :          6000
    best_batch_loss : 1.7544414741833743
    Epoch 7, Loss: 1.7695766824247647, fit: 0.08383333333333333
      batch rate adapted learning_rate :              0.0001
      loss :              1.7643352404043362
      loss_factor :              0.1764335240404336
      loss adapted learning_rate :              3.1128788405326264e-06
    epoch : 8 ; learning_rate : 3.1128788405326264e-06 ; fit : 0.08383333333333333
    batch_size :          6000
    best_batch_loss : 1.7488098803470822
    Epoch 8, Loss: 1.769571062381725, fit: 0.08383333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7667066630239538_fit_0.08383333333333333_2024-01-03_165834
  self.fit : 0.08383333333333333
  self.loss : 1.7667066630239538
  current_accuracy : 0.082
   Accuracy mean: 0.0819
   Accuracy mean: 0.082
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.002
    epoch : 0 ; learning_rate : 0.002 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.634553020731378
    Epoch 0, Loss: 1.6635668808169666, fit: 0.146
    Epoch 0, Loss: 1.6635668808169666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.634553020731378_fit_0.146_2024-01-03_165837
  self.fit : 0.146
  self.loss : 1.634553020731378
  current_accuracy : 0.1299
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.002
      loss :              1.634553020731378
      loss_factor :              0.1634553020731378
      loss adapted learning_rate :              5.3435271551641456e-05
    epoch : 0 ; learning_rate : 5.3435271551641456e-05 ; fit : 0.146
    batch_size :          6000
    best_batch_loss : 1.6379141436850853
    Epoch 0, Loss: 1.645262198586397, fit: 0.13983333333333334
    Epoch 0, Loss: 1.645262198586397
      batch rate adapted learning_rate :              0.002
      loss :              1.6541592053505891
      loss_factor :              0.1654159205350589
      loss adapted learning_rate :              5.472485353292185e-05
    epoch : 1 ; learning_rate : 5.472485353292185e-05 ; fit : 0.13983333333333334
    batch_size :          6000
    best_batch_loss : 1.6339652788472723
    Epoch 1, Loss: 1.6443450437831846, fit: 0.14366666666666666
      batch rate adapted learning_rate :              0.002
      loss :              1.6434005726123821
      loss_factor :              0.1643400572612382
      loss adapted learning_rate :              5.401530884125411e-05
    epoch : 2 ; learning_rate : 5.401530884125411e-05 ; fit : 0.14366666666666666
    batch_size :          6000
    best_batch_loss : 1.6239956241664792
    Epoch 2, Loss: 1.6434046602817631, fit: 0.14616666666666667
      batch rate adapted learning_rate :              0.002
      loss :              1.6425316459571373
      loss_factor :              0.16425316459571374
      loss adapted learning_rate :              5.395820415941326e-05
    epoch : 3 ; learning_rate : 5.395820415941326e-05 ; fit : 0.14616666666666667
    batch_size :          6000
    best_batch_loss : 1.6227668296599411
    Epoch 3, Loss: 1.6425232529197777, fit: 0.15016666666666667
      batch rate adapted learning_rate :              0.002
      loss :              1.6327108385885092
      loss_factor :              0.16327108385885092
      loss adapted learning_rate :              5.331489364888786e-05
    epoch : 4 ; learning_rate : 5.331489364888786e-05 ; fit : 0.15016666666666667
    batch_size :          6000
    best_batch_loss : 1.6197626934842608
    Epoch 4, Loss: 1.6416244011635965, fit: 0.1475
      batch rate adapted learning_rate :              0.002
      loss :              1.640635449702148
      loss_factor :              0.1640635449702148
      loss adapted learning_rate :              5.3833693576387394e-05
    epoch : 5 ; learning_rate : 5.3833693576387394e-05 ; fit : 0.1475
    batch_size :          6000
    best_batch_loss : 1.6305638572347174
    Epoch 5, Loss: 1.6407545343685932, fit: 0.14183333333333334
      batch rate adapted learning_rate :              0.002
      loss :              1.6511944771128577
      loss_factor :              0.16511944771128578
      loss adapted learning_rate :              5.452886402496008e-05
    epoch : 6 ; learning_rate : 5.452886402496008e-05 ; fit : 0.14183333333333334
    batch_size :          6000
    best_batch_loss : 1.6221866261551523
    Epoch 6, Loss: 1.63991764659563, fit: 0.14616666666666667
      batch rate adapted learning_rate :              0.002
      loss :              1.6404972827995288
      loss_factor :              0.16404972827995287
      loss adapted learning_rate :              5.3824626697452733e-05
    epoch : 7 ; learning_rate : 5.3824626697452733e-05 ; fit : 0.14616666666666667
    batch_size :          6000
    best_batch_loss : 1.6229597395504465
    Epoch 7, Loss: 1.6390495322287482, fit: 0.14716666666666667
      batch rate adapted learning_rate :              0.002
      loss :              1.6371775118262744
      loss_factor :              0.16371775118262744
      loss adapted learning_rate :              5.360700410459342e-05
    epoch : 8 ; learning_rate : 5.360700410459342e-05 ; fit : 0.14716666666666667
    batch_size :          6000
    best_batch_loss : 1.6190279969700598
    Epoch 8, Loss: 1.638230497316382, fit: 0.14183333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.6494260847275306_fit_0.14183333333333334_2024-01-03_165858
  self.fit : 0.14183333333333334
  self.loss : 1.6494260847275306
  current_accuracy : 0.1337
   Accuracy mean: 0.1299
   Accuracy mean: 0.1337
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.0025000000000000005
    epoch : 0 ; learning_rate : 0.0025000000000000005 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7364891824323638
    Epoch 0, Loss: 1.7517494599166592, fit: 0.10416666666666667
    Epoch 0, Loss: 1.7517494599166592
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7364891824323638_fit_0.10416666666666667_2024-01-03_165900
  self.fit : 0.10416666666666667
  self.loss : 1.7364891824323638
  current_accuracy : 0.0983
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.7364891824323638
      loss_factor :              0.1736489182432364
      loss adapted learning_rate :              7.538486701761551e-05
    epoch : 0 ; learning_rate : 7.538486701761551e-05 ; fit : 0.10416666666666667
    batch_size :          6000
    best_batch_loss : 1.716198067676005
    Epoch 0, Loss: 1.7339879410053283, fit: 0.104
    Epoch 0, Loss: 1.7339879410053283
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.736054803659694
      loss_factor :              0.1736054803659694
      loss adapted learning_rate :              7.534715703274748e-05
    epoch : 1 ; learning_rate : 7.534715703274748e-05 ; fit : 0.104
    batch_size :          6000
    best_batch_loss : 1.7267747038371182
    Epoch 1, Loss: 1.732967942047942, fit: 0.106
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.7319855207711
      loss_factor :              0.17319855207711
      loss adapted learning_rate :              7.499434610401847e-05
    epoch : 2 ; learning_rate : 7.499434610401847e-05 ; fit : 0.106
    batch_size :          6000
    best_batch_loss : 1.7235525050734786
    Epoch 2, Loss: 1.7319903505260805, fit: 0.09966666666666667
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.7389122302861497
      loss_factor :              0.17389122302861498
      loss adapted learning_rate :              7.559539361596881e-05
    epoch : 3 ; learning_rate : 7.559539361596881e-05 ; fit : 0.09966666666666667
    batch_size :          6000
    best_batch_loss : 1.7096221804665201
    Epoch 3, Loss: 1.7310621717911991, fit: 0.10033333333333333
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.7414710730386402
      loss_factor :              0.174147107303864
      loss adapted learning_rate :              7.581803745575883e-05
    epoch : 4 ; learning_rate : 7.581803745575883e-05 ; fit : 0.10033333333333333
    batch_size :          6000
    best_batch_loss : 1.7151732149706025
    Epoch 4, Loss: 1.730001305251089, fit: 0.09933333333333333
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.7437909509383107
      loss_factor :              0.17437909509383107
      loss adapted learning_rate :              7.602017201435847e-05
    epoch : 5 ; learning_rate : 7.602017201435847e-05 ; fit : 0.09933333333333333
    batch_size :          6000
    best_batch_loss : 1.7207231814949173
    Epoch 5, Loss: 1.7290730380749375, fit: 0.10466666666666667
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.7303863649764493
      loss_factor :              0.17303863649764492
      loss adapted learning_rate :              7.485592430241025e-05
    epoch : 6 ; learning_rate : 7.485592430241025e-05 ; fit : 0.10466666666666667
    batch_size :          6000
    best_batch_loss : 1.7215374868838709
    Epoch 6, Loss: 1.7280447143521371, fit: 0.1035
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.7357131975712201
      loss_factor :              0.173571319757122
      loss adapted learning_rate :              7.531750760557274e-05
    epoch : 7 ; learning_rate : 7.531750760557274e-05 ; fit : 0.1035
    batch_size :          6000
    best_batch_loss : 1.720381470374037
    Epoch 7, Loss: 1.7270884458351448, fit: 0.11116666666666666
      batch rate adapted learning_rate :              0.0025000000000000005
      loss :              1.7205971179031603
      loss_factor :              0.17205971179031604
      loss adapted learning_rate :              7.401136105341655e-05
    epoch : 8 ; learning_rate : 7.401136105341655e-05 ; fit : 0.11116666666666666
    batch_size :          6000
    best_batch_loss : 1.715652289729783
    Epoch 8, Loss: 1.7261500834526506, fit: 0.10833333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7199283195462516_fit_0.10833333333333334_2024-01-03_165921
  self.fit : 0.10833333333333334
  self.loss : 1.7199283195462516
  current_accuracy : 0.1044
   Accuracy mean: 0.0983
   Accuracy mean: 0.1044
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.003
    epoch : 0 ; learning_rate : 0.003 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.5838653584982336
    Epoch 0, Loss: 1.5997434875467038, fit: 0.172
    Epoch 0, Loss: 1.5997434875467038
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.5943124034130427_fit_0.172_2024-01-03_165923
  self.fit : 0.172
  self.loss : 1.5943124034130427
  current_accuracy : 0.1851
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.003
      loss :              1.5943124034130427
      loss_factor :              0.15943124034130426
      loss adapted learning_rate :              7.625496119030018e-05
    epoch : 0 ; learning_rate : 7.625496119030018e-05 ; fit : 0.172
    batch_size :          6000
    best_batch_loss : 1.5687792372227056
    Epoch 0, Loss: 1.5866078335342326, fit: 0.1775
    Epoch 0, Loss: 1.5866078335342326
      batch rate adapted learning_rate :              0.003
      loss :              1.5789999457404746
      loss_factor :              0.15789999457404746
      loss adapted learning_rate :              7.479722485945265e-05
    epoch : 1 ; learning_rate : 7.479722485945265e-05 ; fit : 0.1775
    batch_size :          6000
    best_batch_loss : 1.5749584785070085
    Epoch 1, Loss: 1.5861132482482259, fit: 0.17366666666666666
      batch rate adapted learning_rate :              0.003
      loss :              1.589244510301467
      loss_factor :              0.15892445103014669
      loss adapted learning_rate :              7.577094340570048e-05
    epoch : 2 ; learning_rate : 7.577094340570048e-05 ; fit : 0.17366666666666666
    batch_size :          6000
    best_batch_loss : 1.57428264737103
    Epoch 2, Loss: 1.5856067843124444, fit: 0.18083333333333335
      batch rate adapted learning_rate :              0.003
      loss :              1.57428264737103
      loss_factor :              0.157428264737103
      loss adapted learning_rate :              7.435097561440616e-05
    epoch : 3 ; learning_rate : 7.435097561440616e-05 ; fit : 0.18083333333333335
    batch_size :          6000
    best_batch_loss : 1.5718142991204538
    Epoch 3, Loss: 1.5851167340292918, fit: 0.17866666666666667
      batch rate adapted learning_rate :              0.003
      loss :              1.579436836009291
      loss_factor :              0.15794368360092909
      loss adapted learning_rate :              7.483862156829119e-05
    epoch : 4 ; learning_rate : 7.483862156829119e-05 ; fit : 0.17866666666666667
    batch_size :          6000
    best_batch_loss : 1.567224697403748
    Epoch 4, Loss: 1.584613348812615, fit: 0.167
      batch rate adapted learning_rate :              0.003
      loss :              1.6000003627312902
      loss_factor :              0.16000003627312903
      loss adapted learning_rate :              7.680003482220782e-05
    epoch : 5 ; learning_rate : 7.680003482220782e-05 ; fit : 0.167
    batch_size :          6000
    best_batch_loss : 1.5647150404482002
    Epoch 5, Loss: 1.5841653984809243, fit: 0.18566666666666667
      batch rate adapted learning_rate :              0.003
      loss :              1.5647150404482002
      loss_factor :              0.15647150404482002
      loss adapted learning_rate :              7.344999473414439e-05
    epoch : 6 ; learning_rate : 7.344999473414439e-05 ; fit : 0.18566666666666667
    batch_size :          6000
    best_batch_loss : 1.5727419711795747
    Epoch 6, Loss: 1.5837155673843315, fit: 0.17283333333333334
      batch rate adapted learning_rate :              0.003
      loss :              1.5888069347261529
      loss_factor :              0.1588806934726153
      loss adapted learning_rate :              7.572922427501743e-05
    epoch : 7 ; learning_rate : 7.572922427501743e-05 ; fit : 0.17283333333333334
    batch_size :          6000
    best_batch_loss : 1.5709995199588644
    Epoch 7, Loss: 1.5832337749831038, fit: 0.18
      batch rate adapted learning_rate :              0.003
      loss :              1.5755552388973775
      loss_factor :              0.15755552388973776
      loss adapted learning_rate :              7.447122932450918e-05
    epoch : 8 ; learning_rate : 7.447122932450918e-05 ; fit : 0.18
    batch_size :          6000
    best_batch_loss : 1.5694145796039547
    Epoch 8, Loss: 1.582736496237848, fit: 0.17783333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.5817813563736918_fit_0.17783333333333334_2024-01-03_165945
  self.fit : 0.17783333333333334
  self.loss : 1.5817813563736918
  current_accuracy : 0.1885
   Accuracy mean: 0.1851
   Accuracy mean: 0.1885
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.010000000000000002
    epoch : 0 ; learning_rate : 0.010000000000000002 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.706328350665693
    Epoch 0, Loss: 1.7507571289783976, fit: 0.10883333333333334
    Epoch 0, Loss: 1.7507571289783976
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.706328350665693_fit_0.10883333333333334_2024-01-03_165947
  self.fit : 0.10883333333333334
  self.loss : 1.706328350665693
  current_accuracy : 0.1235
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.706328350665693
      loss_factor :              0.1706328350665693
      loss adapted learning_rate :              0.0002911556440285504
    epoch : 0 ; learning_rate : 0.0002911556440285504 ; fit : 0.10883333333333334
    batch_size :          6000
    best_batch_loss : 1.681547012196436
    Epoch 0, Loss: 1.6893637247343083, fit: 0.11833333333333333
    Epoch 0, Loss: 1.6893637247343083
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.6934467493415137
      loss_factor :              0.16934467493415137
      loss adapted learning_rate :              0.000286776189285534
    epoch : 1 ; learning_rate : 0.000286776189285534 ; fit : 0.11833333333333333
    batch_size :          6000
    best_batch_loss : 1.677482840716158
    Epoch 1, Loss: 1.686226173070668, fit: 0.11283333333333333
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.7027089800116215
      loss_factor :              0.17027089800116216
      loss adapted learning_rate :              0.00028992178706122176
    epoch : 2 ; learning_rate : 0.00028992178706122176 ; fit : 0.11283333333333333
    batch_size :          6000
    best_batch_loss : 1.6649293454756213
    Epoch 2, Loss: 1.6830379348112217, fit: 0.1205
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.681070926484732
      loss_factor :              0.1681070926484732
      loss adapted learning_rate :              0.0002825999459872236
    epoch : 3 ; learning_rate : 0.0002825999459872236 ; fit : 0.1205
    batch_size :          6000
    best_batch_loss : 1.6584666423685173
    Epoch 3, Loss: 1.6798323628523129, fit: 0.13133333333333333
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.6584666423685173
      loss_factor :              0.16584666423685174
      loss adapted learning_rate :              0.00027505116038491044
    epoch : 4 ; learning_rate : 0.00027505116038491044 ; fit : 0.13133333333333333
    batch_size :          6000
    best_batch_loss : 1.662367296324259
    Epoch 4, Loss: 1.6767133463826602, fit: 0.12716666666666668
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.6699058182036464
      loss_factor :              0.16699058182036464
      loss adapted learning_rate :              0.000278858544167039
    epoch : 5 ; learning_rate : 0.000278858544167039 ; fit : 0.12716666666666668
    batch_size :          6000
    best_batch_loss : 1.6649337409824894
    Epoch 5, Loss: 1.6736594594269163, fit: 0.1185
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.6905379372890421
      loss_factor :              0.16905379372890422
      loss adapted learning_rate :              0.00028579185174134903
    epoch : 6 ; learning_rate : 0.00028579185174134903 ; fit : 0.1185
    batch_size :          6000
    best_batch_loss : 1.65588907324909
    Epoch 6, Loss: 1.6705295119885217, fit: 0.12816666666666668
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.670223572786543
      loss_factor :              0.1670223572786543
      loss adapted learning_rate :              0.0002789646783091845
    epoch : 7 ; learning_rate : 0.0002789646783091845 ; fit : 0.12816666666666668
    batch_size :          6000
    best_batch_loss : 1.6564655486941204
    Epoch 7, Loss: 1.6673863107967555, fit: 0.13466666666666666
      batch rate adapted learning_rate :              0.010000000000000002
      loss :              1.6564655486941204
      loss_factor :              0.16564655486941204
      loss adapted learning_rate :              0.0002743878114010514
    epoch : 8 ; learning_rate : 0.0002743878114010514 ; fit : 0.13466666666666666
    batch_size :          6000
    best_batch_loss : 1.6474971037480797
    Epoch 8, Loss: 1.6644059473505943, fit: 0.132
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.667357915991083_fit_0.132_2024-01-03_170009
  self.fit : 0.132
  self.loss : 1.667357915991083
  current_accuracy : 0.1389
   Accuracy mean: 0.1235
   Accuracy mean: 0.1389
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.09000000000000001
    epoch : 0 ; learning_rate : 0.09000000000000001 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.036256516831562
    Epoch 0, Loss: 1.382722606509835, fit: 0.45116666666666666
    Epoch 0, Loss: 1.382722606509835
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.036256516831562_fit_0.45116666666666666_2024-01-03_170011
  self.fit : 0.45116666666666666
  self.loss : 1.036256516831562
  current_accuracy : 0.4898
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              1.036256516831562
      loss_factor :              0.10362565168315621
      loss adapted learning_rate :              0.0009664448118082934
    epoch : 0 ; learning_rate : 0.0009664448118082934 ; fit : 0.45116666666666666
    batch_size :          6000
    best_batch_loss : 0.9656059295281686
    Epoch 0, Loss: 0.9827436867478937, fit: 0.485
    Epoch 0, Loss: 0.9827436867478937
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              0.9751505010733482
      loss_factor :              0.09751505010733483
      loss adapted learning_rate :              0.0008558266497692421
    epoch : 1 ; learning_rate : 0.0008558266497692421 ; fit : 0.485
    batch_size :          6000
    best_batch_loss : 0.9487315325884904
    Epoch 1, Loss: 0.979243880809723, fit: 0.47933333333333333
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              0.9858434297231369
      loss_factor :              0.0985843429723137
      loss adapted learning_rate :              0.00087469854113545
    epoch : 2 ; learning_rate : 0.00087469854113545 ; fit : 0.47933333333333333
    batch_size :          6000
    best_batch_loss : 0.9615021353002785
    Epoch 2, Loss: 0.9758219918259738, fit: 0.48983333333333334
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              0.9615021353002785
      loss_factor :              0.09615021353002785
      loss adapted learning_rate :              0.0008320377205682957
    epoch : 3 ; learning_rate : 0.0008320377205682957 ; fit : 0.48983333333333334
    batch_size :          6000
    best_batch_loss : 0.9456402467549238
    Epoch 3, Loss: 0.9726305702360314, fit: 0.479
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              0.9842062183176282
      loss_factor :              0.09842062183176283
      loss adapted learning_rate :              0.0008717956921575784
    epoch : 4 ; learning_rate : 0.0008717956921575784 ; fit : 0.479
    batch_size :          6000
    best_batch_loss : 0.9534113106717815
    Epoch 4, Loss: 0.9693489555089567, fit: 0.48783333333333334
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              0.9696442330964865
      loss_factor :              0.09696442330964865
      loss adapted learning_rate :              0.0008461889448995462
    epoch : 5 ; learning_rate : 0.0008461889448995462 ; fit : 0.48783333333333334
    batch_size :          6000
    best_batch_loss : 0.9400477949047865
    Epoch 5, Loss: 0.966215299535163, fit: 0.4925
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              0.9583183854681278
      loss_factor :              0.09583183854681278
      loss adapted learning_rate :              0.0008265367151336153
    epoch : 6 ; learning_rate : 0.0008265367151336153 ; fit : 0.4925
    batch_size :          6000
    best_batch_loss : 0.9449474168595439
    Epoch 6, Loss: 0.9630496242940008, fit: 0.4945
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              0.9560869732114952
      loss_factor :              0.09560869732114952
      loss adapted learning_rate :              0.0008226920703102466
    epoch : 7 ; learning_rate : 0.0008226920703102466 ; fit : 0.4945
    batch_size :          6000
    best_batch_loss : 0.9315367776097945
    Epoch 7, Loss: 0.960072170170022, fit: 0.49133333333333334
      batch rate adapted learning_rate :              0.09000000000000001
      loss :              0.9657987376104212
      loss_factor :              0.09657987376104213
      loss adapted learning_rate :              0.0008394904814128951
    epoch : 8 ; learning_rate : 0.0008394904814128951 ; fit : 0.49133333333333334
    batch_size :          6000
    best_batch_loss : 0.9412857298341945
    Epoch 8, Loss: 0.957050400901897, fit: 0.4975
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.9547912787408824_fit_0.4975_2024-01-03_170032
  self.fit : 0.4975
  self.loss : 0.9547912787408824
  current_accuracy : 0.5063
   Accuracy mean: 0.4898
   Accuracy mean: 0.5063
  Error saving file: doc/out/test_combinations_results/20240103170032.
  normalized_accuracies :      [0.01969981 0.01969981 0.         0.         0.02743902 0.02743902
   0.06871482 0.06871482 0.00469043 0.00492495 0.11726079 0.12617261
   0.04315197 0.05745779 0.2467167  0.25469043 0.10225141 0.13836773
   0.96130394 1.        ]
batch_rate :  0.01
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.01
      batch rate adapted learning_rate :              1.0000000000000001e-19
    epoch : 0 ; learning_rate : 1.0000000000000001e-19 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.7388943098555054
    Epoch 0, Loss: 1.791628650507144, fit: 0.08166666666666667
    Epoch 0, Loss: 1.791628650507144
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7845277513144515_fit_0.08166666666666667_2024-01-03_170036
  self.fit : 0.08166666666666667
  self.loss : 1.7845277513144515
  current_accuracy : 0.0715
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          0.01
      batch rate adapted learning_rate :              1.0000000000000001e-19
      loss :              1.7845277513144515
      loss_factor :              0.17845277513144514
      loss adapted learning_rate :              3.1845392952114126e-21
    epoch : 0 ; learning_rate : 3.1845392952114126e-21 ; fit : 0.08166666666666667
    batch_size :          600
    best_batch_loss : 1.7172485216051583
    Epoch 0, Loss: 1.791628650507144, fit: 0.08333333333333333
    Epoch 0, Loss: 1.791628650507144
      batch rate adapted learning_rate :              1.0000000000000001e-19
      loss :              1.7772670029518156
      loss_factor :              0.17772670029518156
      loss adapted learning_rate :              3.1586779997813295e-21
    epoch : 1 ; learning_rate : 3.1586779997813295e-21 ; fit : 0.08333333333333333
    batch_size :          600
    best_batch_loss : 1.7278240052790095
    Epoch 1, Loss: 1.791628650507143, fit: 0.08833333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-19
      loss :              1.7616477102888644
      loss_factor :              0.17616477102888645
      loss adapted learning_rate :              3.1034026551659992e-21
    epoch : 2 ; learning_rate : 3.1034026551659992e-21 ; fit : 0.08833333333333333
    batch_size :          600
    best_batch_loss : 1.7244992620536663
    Epoch 2, Loss: 1.7916286505071435, fit: 0.09333333333333334
      batch rate adapted learning_rate :              1.0000000000000001e-19
      loss :              1.750586026355849
      loss_factor :              0.1750586026355849
      loss adapted learning_rate :              3.0645514356723614e-21
    epoch : 3 ; learning_rate : 3.0645514356723614e-21 ; fit : 0.09333333333333334
    batch_size :          600
    best_batch_loss : 1.7315173447842058
    Epoch 3, Loss: 1.7916286505071437, fit: 0.04833333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-19
      loss :              1.8431928219229796
      loss_factor :              0.18431928219229796
      loss adapted learning_rate :              3.397359778788397e-21
    epoch : 4 ; learning_rate : 3.397359778788397e-21 ; fit : 0.04833333333333333
    batch_size :          600
    best_batch_loss : 1.74196649193471
    Epoch 4, Loss: 1.7916286505071435, fit: 0.06166666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-19
      loss :              1.8152858197157096
      loss_factor :              0.18152858197157096
      loss adapted learning_rate :              3.2952626072609358e-21
    epoch : 5 ; learning_rate : 3.2952626072609358e-21 ; fit : 0.06166666666666667
    batch_size :          600
    best_batch_loss : 1.740890383824193
    Epoch 5, Loss: 1.7916286505071433, fit: 0.07166666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-19
      loss :              1.794231753981232
      loss_factor :              0.1794231753981232
      loss adapted learning_rate :              3.2192675869945684e-21
    epoch : 6 ; learning_rate : 3.2192675869945684e-21 ; fit : 0.07166666666666667
    batch_size :          600
    best_batch_loss : 1.7438916532104505
    Epoch 6, Loss: 1.791628650507144, fit: 0.08
      batch rate adapted learning_rate :              1.0000000000000001e-19
      loss :              1.792047881240688
      loss_factor :              0.1792047881240688
      loss adapted learning_rate :              3.2114356086592385e-21
    epoch : 7 ; learning_rate : 3.2114356086592385e-21 ; fit : 0.08
    batch_size :          600
    best_batch_loss : 1.7364729852779124
    Epoch 7, Loss: 1.791628650507143, fit: 0.065
      batch rate adapted learning_rate :              1.0000000000000001e-19
      loss :              1.8159898890274278
      loss_factor :              0.1815989889027428
      loss adapted learning_rate :              3.2978192770498498e-21
    epoch : 8 ; learning_rate : 3.2978192770498498e-21 ; fit : 0.065
    batch_size :          600
    best_batch_loss : 1.7446019420753556
    Epoch 8, Loss: 1.7916286505071444, fit: 0.07333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7973210013589531_fit_0.07333333333333333_2024-01-03_170101
  self.fit : 0.07333333333333333
  self.loss : 1.7973210013589531
  current_accuracy : 0.0715
   Accuracy mean: 0.0715
   Accuracy mean: 0.0715
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-12
    epoch : 0 ; learning_rate : 1e-12 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.687417834881424
    Epoch 0, Loss: 1.749772370004346, fit: 0.10666666666666667
    Epoch 0, Loss: 1.749772370004346
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.729248498509296_fit_0.10666666666666667_2024-01-03_170104
  self.fit : 0.10666666666666667
  self.loss : 1.729248498509296
  current_accuracy : 0.092
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-12
      loss :              1.729248498509296
      loss_factor :              0.1729248498509296
      loss adapted learning_rate :              2.9903003695966544e-14
    epoch : 0 ; learning_rate : 2.9903003695966544e-14 ; fit : 0.10666666666666667
    batch_size :          600
    best_batch_loss : 1.6578999678188817
    Epoch 0, Loss: 1.7497723698767191, fit: 0.105
    Epoch 0, Loss: 1.7497723698767191
      batch rate adapted learning_rate :              1e-12
      loss :              1.7277913193958958
      loss_factor :              0.17277913193958958
      loss adapted learning_rate :              2.98526284337981e-14
    epoch : 1 ; learning_rate : 2.98526284337981e-14 ; fit : 0.105
    batch_size :          600
    best_batch_loss : 1.6908234013354622
    Epoch 1, Loss: 1.7497723698699936, fit: 0.06333333333333334
      batch rate adapted learning_rate :              1e-12
      loss :              1.819652090459254
      loss_factor :              0.1819652090459254
      loss adapted learning_rate :              3.311133730312733e-14
    epoch : 2 ; learning_rate : 3.311133730312733e-14 ; fit : 0.06333333333333334
    batch_size :          600
    best_batch_loss : 1.688232498070105
    Epoch 2, Loss: 1.7497723698625132, fit: 0.095
      batch rate adapted learning_rate :              1e-12
      loss :              1.7598834223732513
      loss_factor :              0.17598834223732512
      loss adapted learning_rate :              3.0971896603441875e-14
    epoch : 3 ; learning_rate : 3.0971896603441875e-14 ; fit : 0.095
    batch_size :          600
    best_batch_loss : 1.6781921763377659
    Epoch 3, Loss: 1.7497723698551377, fit: 0.10166666666666667
      batch rate adapted learning_rate :              1e-12
      loss :              1.7440495985397497
      loss_factor :              0.17440495985397497
      loss adapted learning_rate :              3.041709002166662e-14
    epoch : 4 ; learning_rate : 3.041709002166662e-14 ; fit : 0.10166666666666667
    batch_size :          600
    best_batch_loss : 1.7008840059380725
    Epoch 4, Loss: 1.749772369848296, fit: 0.09833333333333333
      batch rate adapted learning_rate :              1e-12
      loss :              1.7498787134407614
      loss_factor :              0.17498787134407615
      loss adapted learning_rate :              3.0620755117530944e-14
    epoch : 5 ; learning_rate : 3.0620755117530944e-14 ; fit : 0.09833333333333333
    batch_size :          600
    best_batch_loss : 1.7039271314761064
    Epoch 5, Loss: 1.749772369841115, fit: 0.07833333333333334
      batch rate adapted learning_rate :              1e-12
      loss :              1.786441358292794
      loss_factor :              0.1786441358292794
      loss adapted learning_rate :              3.191372726619003e-14
    epoch : 6 ; learning_rate : 3.191372726619003e-14 ; fit : 0.07833333333333334
    batch_size :          600
    best_batch_loss : 1.695538130926681
    Epoch 6, Loss: 1.7497723698340075, fit: 0.105
      batch rate adapted learning_rate :              1e-12
      loss :              1.7429887104972173
      loss_factor :              0.17429887104972172
      loss adapted learning_rate :              3.038009644920752e-14
    epoch : 7 ; learning_rate : 3.038009644920752e-14 ; fit : 0.105
    batch_size :          600
    best_batch_loss : 1.6914505981888524
    Epoch 7, Loss: 1.7497723698268353, fit: 0.08
      batch rate adapted learning_rate :              1e-12
      loss :              1.781306901573056
      loss_factor :              0.1781306901573056
      loss adapted learning_rate :              3.173054277591801e-14
    epoch : 8 ; learning_rate : 3.173054277591801e-14 ; fit : 0.08
    batch_size :          600
    best_batch_loss : 1.6595759797508476
    Epoch 8, Loss: 1.7497723698195784, fit: 0.10666666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7288924761249576_fit_0.10666666666666667_2024-01-03_170129
  self.fit : 0.10666666666666667
  self.loss : 1.7288924761249576
  current_accuracy : 0.092
   Accuracy mean: 0.092
   Accuracy mean: 0.092
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-09
    epoch : 0 ; learning_rate : 1e-09 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6709074411663771
    Epoch 0, Loss: 1.7298599588198582, fit: 0.115
    Epoch 0, Loss: 1.7298599588198582
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7045764222744355_fit_0.115_2024-01-03_170131
  self.fit : 0.115
  self.loss : 1.7045764222744355
  current_accuracy : 0.1003
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-09
      loss :              1.7045764222744355
      loss_factor :              0.17045764222744356
      loss adapted learning_rate :              2.9055807793739152e-11
    epoch : 0 ; learning_rate : 2.9055807793739152e-11 ; fit : 0.115
    batch_size :          600
    best_batch_loss : 1.660491960986287
    Epoch 0, Loss: 1.729859886514221, fit: 0.08833333333333333
    Epoch 0, Loss: 1.729859886514221
      batch rate adapted learning_rate :              1e-09
      loss :              1.7589894621627158
      loss_factor :              0.17589894621627158
      loss adapted learning_rate :              3.094043927999481e-11
    epoch : 1 ; learning_rate : 3.094043927999481e-11 ; fit : 0.08833333333333333
    batch_size :          600
    best_batch_loss : 1.666857470692867
    Epoch 1, Loss: 1.7298598824399531, fit: 0.11
      batch rate adapted learning_rate :              1e-09
      loss :              1.7117308900670174
      loss_factor :              0.17117308900670175
      loss adapted learning_rate :              2.9300226400096244e-11
    epoch : 2 ; learning_rate : 2.9300226400096244e-11 ; fit : 0.11
    batch_size :          600
    best_batch_loss : 1.6761338464199562
    Epoch 2, Loss: 1.7298598783666497, fit: 0.09666666666666666
      batch rate adapted learning_rate :              1e-09
      loss :              1.7416483864156211
      loss_factor :              0.17416483864156213
      loss adapted learning_rate :              3.0333391019041374e-11
    epoch : 3 ; learning_rate : 3.0333391019041374e-11 ; fit : 0.09666666666666666
    batch_size :          600
    best_batch_loss : 1.6772550535378639
    Epoch 3, Loss: 1.729859874127315, fit: 0.11833333333333333
      batch rate adapted learning_rate :              1e-09
      loss :              1.7037624350947205
      loss_factor :              0.17037624350947206
      loss adapted learning_rate :              2.902806435239892e-11
    epoch : 4 ; learning_rate : 2.902806435239892e-11 ; fit : 0.11833333333333333
    batch_size :          600
    best_batch_loss : 1.6785638171358288
    Epoch 4, Loss: 1.7298598700952017, fit: 0.105
      batch rate adapted learning_rate :              1e-09
      loss :              1.7244581110903383
      loss_factor :              0.17244581110903384
      loss adapted learning_rate :              2.973755776905258e-11
    epoch : 5 ; learning_rate : 2.973755776905258e-11 ; fit : 0.105
    batch_size :          600
    best_batch_loss : 1.6398396382867868
    Epoch 5, Loss: 1.7298598658806539, fit: 0.13
      batch rate adapted learning_rate :              1e-09
      loss :              1.6895496061736908
      loss_factor :              0.1689549606173691
      loss adapted learning_rate :              2.854577871721674e-11
    epoch : 6 ; learning_rate : 2.854577871721674e-11 ; fit : 0.13
    batch_size :          600
    best_batch_loss : 1.6880832302244946
    Epoch 6, Loss: 1.7298598617962717, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-09
      loss :              1.7009957167104544
      loss_factor :              0.17009957167104545
      loss adapted learning_rate :              2.893386428267313e-11
    epoch : 7 ; learning_rate : 2.893386428267313e-11 ; fit : 0.11666666666666667
    batch_size :          600
    best_batch_loss : 1.6610966928991273
    Epoch 7, Loss: 1.7298598578792557, fit: 0.11
      batch rate adapted learning_rate :              1e-09
      loss :              1.7201747434321741
      loss_factor :              0.1720174743432174
      loss adapted learning_rate :              2.959001147941946e-11
    epoch : 8 ; learning_rate : 2.959001147941946e-11 ; fit : 0.11
    batch_size :          600
    best_batch_loss : 1.6547739433899145
    Epoch 8, Loss: 1.7298598538457175, fit: 0.10333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7350156429823176_fit_0.10333333333333333_2024-01-03_170155
  self.fit : 0.10333333333333333
  self.loss : 1.7350156429823176
  current_accuracy : 0.1003
   Accuracy mean: 0.1003
   Accuracy mean: 0.1003
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.01
      batch rate adapted learning_rate :              1.0000000000000002e-06
    epoch : 0 ; learning_rate : 1.0000000000000002e-06 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6734696231745991
    Epoch 0, Loss: 1.7531955826223653, fit: 0.085
    Epoch 0, Loss: 1.7531955826223653
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7683622647852515_fit_0.085_2024-01-03_170158
  self.fit : 0.085
  self.loss : 1.7683622647852515
  current_accuracy : 0.0876
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.01
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.7683622647852515
      loss_factor :              0.17683622647852515
      loss adapted learning_rate :              3.1271050995164245e-08
    epoch : 0 ; learning_rate : 3.1271050995164245e-08 ; fit : 0.085
    batch_size :          600
    best_batch_loss : 1.6923826279485858
    Epoch 0, Loss: 1.7531619234277673, fit: 0.08166666666666667
    Epoch 0, Loss: 1.7531619234277673
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.7789173857841927
      loss_factor :              0.17789173857841928
      loss adapted learning_rate :              3.1645470654452675e-08
    epoch : 1 ; learning_rate : 3.1645470654452675e-08 ; fit : 0.08166666666666667
    batch_size :          600
    best_batch_loss : 1.695021262188286
    Epoch 1, Loss: 1.7531595637476804, fit: 0.08833333333333333
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.7430749304046245
      loss_factor :              0.17430749304046245
      loss adapted learning_rate :              3.038310213005087e-08
    epoch : 2 ; learning_rate : 3.038310213005087e-08 ; fit : 0.08833333333333333
    batch_size :          600
    best_batch_loss : 1.6892592518760425
    Epoch 2, Loss: 1.7531576807698435, fit: 0.09666666666666666
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.7471231476552869
      loss_factor :              0.17471231476552868
      loss adapted learning_rate :              3.0524392930729175e-08
    epoch : 3 ; learning_rate : 3.0524392930729175e-08 ; fit : 0.09666666666666666
    batch_size :          600
    best_batch_loss : 1.698896313980589
    Epoch 3, Loss: 1.753155432494607, fit: 0.09666666666666666
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.7320104061635986
      loss_factor :              0.17320104061635985
      loss adapted learning_rate :              2.999860047058994e-08
    epoch : 4 ; learning_rate : 2.999860047058994e-08 ; fit : 0.09666666666666666
    batch_size :          600
    best_batch_loss : 1.693439349621456
    Epoch 4, Loss: 1.753153281962176, fit: 0.08833333333333333
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.7569540561072017
      loss_factor :              0.17569540561072017
      loss adapted learning_rate :              3.086887555271549e-08
    epoch : 5 ; learning_rate : 3.086887555271549e-08 ; fit : 0.08833333333333333
    batch_size :          600
    best_batch_loss : 1.6801719979074239
    Epoch 5, Loss: 1.7531513819013425, fit: 0.08833333333333333
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.7581579661335687
      loss_factor :              0.17581579661335686
      loss adapted learning_rate :              3.091119433878927e-08
    epoch : 6 ; learning_rate : 3.091119433878927e-08 ; fit : 0.08833333333333333
    batch_size :          600
    best_batch_loss : 1.6912159835453546
    Epoch 6, Loss: 1.7531494048108665, fit: 0.105
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.7221540040704322
      loss_factor :              0.17221540040704322
      loss adapted learning_rate :              2.9658144137358227e-08
    epoch : 7 ; learning_rate : 2.9658144137358227e-08 ; fit : 0.105
    batch_size :          600
    best_batch_loss : 1.6852950924661778
    Epoch 7, Loss: 1.7531470649371004, fit: 0.10166666666666667
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss :              1.7260998795653362
      loss_factor :              0.17260998795653362
      loss adapted learning_rate :              2.979420794235469e-08
    epoch : 8 ; learning_rate : 2.979420794235469e-08 ; fit : 0.10166666666666667
    batch_size :          600
    best_batch_loss : 1.6879278008235719
    Epoch 8, Loss: 1.7531452342055487, fit: 0.09666666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7375138559512315_fit_0.09666666666666666_2024-01-03_170224
  self.fit : 0.09666666666666666
  self.loss : 1.7375138559512315
  current_accuracy : 0.0876
   Accuracy mean: 0.0876
   Accuracy mean: 0.0876
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-05
    epoch : 0 ; learning_rate : 1e-05 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6975811534906404
    Epoch 0, Loss: 1.7773086171924377, fit: 0.06333333333333334
    Epoch 0, Loss: 1.7773086171924377
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.818882263724795_fit_0.06333333333333334_2024-01-03_170227
  self.fit : 0.06333333333333334
  self.loss : 1.818882263724795
  current_accuracy : 0.0804
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-05
      loss :              1.818882263724795
      loss_factor :              0.1818882263724795
      loss adapted learning_rate :              3.308332689292635e-07
    epoch : 0 ; learning_rate : 3.308332689292635e-07 ; fit : 0.06333333333333334
    batch_size :          600
    best_batch_loss : 1.7287189359426558
    Epoch 0, Loss: 1.7768512202020355, fit: 0.095
    Epoch 0, Loss: 1.7768512202020355
      batch rate adapted learning_rate :              1e-05
      loss :              1.7442142948844055
      loss_factor :              0.17442142948844055
      loss adapted learning_rate :              3.042283506479104e-07
    epoch : 1 ; learning_rate : 3.042283506479104e-07 ; fit : 0.095
    batch_size :          600
    best_batch_loss : 1.726924116438554
    Epoch 1, Loss: 1.7768208935782268, fit: 0.06
      batch rate adapted learning_rate :              1e-05
      loss :              1.8254403457719008
      loss_factor :              0.1825440345771901
      loss adapted learning_rate :              3.332232455971838e-07
    epoch : 2 ; learning_rate : 3.332232455971838e-07 ; fit : 0.06
    batch_size :          600
    best_batch_loss : 1.6931261968583722
    Epoch 2, Loss: 1.7767902742076942, fit: 0.07833333333333334
      batch rate adapted learning_rate :              1e-05
      loss :              1.765352386740652
      loss_factor :              0.1765352386740652
      loss adapted learning_rate :              3.1164690493709167e-07
    epoch : 3 ; learning_rate : 3.1164690493709167e-07 ; fit : 0.07833333333333334
    batch_size :          600
    best_batch_loss : 1.7138792301211947
    Epoch 3, Loss: 1.7767609614441713, fit: 0.07
      batch rate adapted learning_rate :              1e-05
      loss :              1.7981928157024571
      loss_factor :              0.1798192815702457
      loss adapted learning_rate :              3.233497402443931e-07
    epoch : 4 ; learning_rate : 3.233497402443931e-07 ; fit : 0.07
    batch_size :          600
    best_batch_loss : 1.6897624784008625
    Epoch 4, Loss: 1.7767290566335092, fit: 0.08
      batch rate adapted learning_rate :              1e-05
      loss :              1.7796486221227246
      loss_factor :              0.17796486221227245
      loss adapted learning_rate :              3.1671492182233124e-07
    epoch : 5 ; learning_rate : 3.1671492182233124e-07 ; fit : 0.08
    batch_size :          600
    best_batch_loss : 1.7174151512601006
    Epoch 5, Loss: 1.7766987625059298, fit: 0.09166666666666666
      batch rate adapted learning_rate :              1e-05
      loss :              1.7498743421450247
      loss_factor :              0.17498743421450247
      loss adapted learning_rate :              3.0620602132974834e-07
    epoch : 6 ; learning_rate : 3.0620602132974834e-07 ; fit : 0.09166666666666666
    batch_size :          600
    best_batch_loss : 1.715103813937598
    Epoch 6, Loss: 1.7766669909912665, fit: 0.085
      batch rate adapted learning_rate :              1e-05
      loss :              1.7670129380691377
      loss_factor :              0.17670129380691377
      loss adapted learning_rate :              3.1223347233037263e-07
    epoch : 7 ; learning_rate : 3.1223347233037263e-07 ; fit : 0.085
    batch_size :          600
    best_batch_loss : 1.7241753570198177
    Epoch 7, Loss: 1.7766392218771323, fit: 0.06
      batch rate adapted learning_rate :              1e-05
      loss :              1.8108331742435397
      loss_factor :              0.18108331742435396
      loss adapted learning_rate :              3.279116784940934e-07
    epoch : 8 ; learning_rate : 3.279116784940934e-07 ; fit : 0.06
    batch_size :          600
    best_batch_loss : 1.720799971944613
    Epoch 8, Loss: 1.776606034794386, fit: 0.08
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7776561139700835_fit_0.08_2024-01-03_170252
  self.fit : 0.08
  self.loss : 1.7776561139700835
  current_accuracy : 0.0805
   Accuracy mean: 0.0804
   Accuracy mean: 0.0805
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.0002
    epoch : 0 ; learning_rate : 0.0002 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6371350983177417
    Epoch 0, Loss: 1.70215521675967, fit: 0.145
    Epoch 0, Loss: 1.70215521675967
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.6539692572029316_fit_0.145_2024-01-03_170254
  self.fit : 0.145
  self.loss : 1.6539692572029316
  current_accuracy : 0.1272
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.0002
      loss :              1.6539692572029316
      loss_factor :              0.16539692572029316
      loss adapted learning_rate :              5.471228607544835e-06
    epoch : 0 ; learning_rate : 5.471228607544835e-06 ; fit : 0.145
    batch_size :          600
    best_batch_loss : 1.635337020525467
    Epoch 0, Loss: 1.6958489382803164, fit: 0.14
    Epoch 0, Loss: 1.6958489382803164
      batch rate adapted learning_rate :              0.0002
      loss :              1.635337020525467
      loss_factor :              0.1635337020525467
      loss adapted learning_rate :              5.348654341402223e-06
    epoch : 1 ; learning_rate : 5.348654341402223e-06 ; fit : 0.14
    batch_size :          600
    best_batch_loss : 1.6276534028502643
    Epoch 1, Loss: 1.695540576079966, fit: 0.10333333333333333
      batch rate adapted learning_rate :              0.0002
      loss :              1.7241579759891321
      loss_factor :              0.1724157975989132
      loss adapted learning_rate :              5.945441452333881e-06
    epoch : 2 ; learning_rate : 5.945441452333881e-06 ; fit : 0.10333333333333333
    batch_size :          600
    best_batch_loss : 1.642261858258061
    Epoch 2, Loss: 1.6952587221743938, fit: 0.10666666666666667
      batch rate adapted learning_rate :              0.0002
      loss :              1.7147952102644217
      loss_factor :              0.17147952102644218
      loss adapted learning_rate :              5.881045226291605e-06
    epoch : 3 ; learning_rate : 5.881045226291605e-06 ; fit : 0.10666666666666667
    batch_size :          600
    best_batch_loss : 1.6216340910208293
    Epoch 3, Loss: 1.6949223920340444, fit: 0.115
      batch rate adapted learning_rate :              0.0002
      loss :              1.6920117216657706
      loss_factor :              0.16920117216657707
      loss adapted learning_rate :              5.725807332508732e-06
    epoch : 4 ; learning_rate : 5.725807332508732e-06 ; fit : 0.115
    batch_size :          600
    best_batch_loss : 1.6168871468534276
    Epoch 4, Loss: 1.6945909182860168, fit: 0.13166666666666665
      batch rate adapted learning_rate :              0.0002
      loss :              1.664597472199002
      loss_factor :              0.1664597472199002
      loss adapted learning_rate :              5.541769488902615e-06
    epoch : 5 ; learning_rate : 5.541769488902615e-06 ; fit : 0.13166666666666665
    batch_size :          600
    best_batch_loss : 1.633750103499124
    Epoch 5, Loss: 1.694261185295979, fit: 0.13
      batch rate adapted learning_rate :              0.0002
      loss :              1.6744915715951412
      loss_factor :              0.1674491571595141
      loss adapted learning_rate :              5.607844046686331e-06
    epoch : 6 ; learning_rate : 5.607844046686331e-06 ; fit : 0.13
    batch_size :          600
    best_batch_loss : 1.6365805361401848
    Epoch 6, Loss: 1.6939319427332726, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.0002
      loss :              1.7023217277420257
      loss_factor :              0.17023217277420258
      loss adapted learning_rate :              5.795798529485192e-06
    epoch : 7 ; learning_rate : 5.795798529485192e-06 ; fit : 0.11666666666666667
    batch_size :          600
    best_batch_loss : 1.625523767275868
    Epoch 7, Loss: 1.6935683076950214, fit: 0.14166666666666666
      batch rate adapted learning_rate :              0.0002
      loss :              1.653358154447919
      loss_factor :              0.1653358154447919
      loss adapted learning_rate :              5.467186373758858e-06
    epoch : 8 ; learning_rate : 5.467186373758858e-06 ; fit : 0.14166666666666666
    batch_size :          600
    best_batch_loss : 1.6298390905369171
    Epoch 8, Loss: 1.6932627538405018, fit: 0.125
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.685769989261638_fit_0.125_2024-01-03_170317
  self.fit : 0.125
  self.loss : 1.685769989261638
  current_accuracy : 0.1295
   Accuracy mean: 0.1272
   Accuracy mean: 0.1295
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.00025
    epoch : 0 ; learning_rate : 0.00025 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.7027248590768935
    Epoch 0, Loss: 1.7466526047553999, fit: 0.095
    Epoch 0, Loss: 1.7466526047553999
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7564192533171232_fit_0.095_2024-01-03_170320
  self.fit : 0.095
  self.loss : 1.7564192533171232
  current_accuracy : 0.1041
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.00025
      loss :              1.7564192533171232
      loss_factor :              0.17564192533171233
      loss adapted learning_rate :              7.712521483557703e-06
    epoch : 0 ; learning_rate : 7.712521483557703e-06 ; fit : 0.095
    batch_size :          600
    best_batch_loss : 1.6525682209999715
    Epoch 0, Loss: 1.7326339379014981, fit: 0.09666666666666666
    Epoch 0, Loss: 1.7326339379014981
      batch rate adapted learning_rate :              0.00025
      loss :              1.7479204413990719
      loss_factor :              0.17479204413990718
      loss adapted learning_rate :              7.638064673651815e-06
    epoch : 1 ; learning_rate : 7.638064673651815e-06 ; fit : 0.09666666666666666
    batch_size :          600
    best_batch_loss : 1.6595036932570968
    Epoch 1, Loss: 1.731809081665814, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.00025
      loss :              1.7038275615536995
      loss_factor :              0.17038275615536996
      loss adapted learning_rate :              7.2575708987750655e-06
    epoch : 2 ; learning_rate : 7.2575708987750655e-06 ; fit : 0.11666666666666667
    batch_size :          600
    best_batch_loss : 1.6810821890796512
    Epoch 2, Loss: 1.7309975593415663, fit: 0.095
      batch rate adapted learning_rate :              0.00025
      loss :              1.7621760222004257
      loss_factor :              0.17621760222004257
      loss adapted learning_rate :              7.763160833045288e-06
    epoch : 3 ; learning_rate : 7.763160833045288e-06 ; fit : 0.095
    batch_size :          600
    best_batch_loss : 1.6714144373345754
    Epoch 3, Loss: 1.7302397052360587, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.00025
      loss :              1.6714144373345754
      loss_factor :              0.16714144373345755
      loss adapted learning_rate :              6.984065553326139e-06
    epoch : 4 ; learning_rate : 6.984065553326139e-06 ; fit : 0.13333333333333333
    batch_size :          600
    best_batch_loss : 1.6505131736603589
    Epoch 4, Loss: 1.729451173805424, fit: 0.09166666666666666
      batch rate adapted learning_rate :              0.00025
      loss :              1.7455735254455786
      loss_factor :              0.17455735254455787
      loss adapted learning_rate :              7.617567331841266e-06
    epoch : 5 ; learning_rate : 7.617567331841266e-06 ; fit : 0.09166666666666666
    batch_size :          600
    best_batch_loss : 1.656172313852081
    Epoch 5, Loss: 1.728720808080367, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.00025
      loss :              1.7163141148727936
      loss_factor :              0.17163141148727937
      loss adapted learning_rate :              7.364335352278953e-06
    epoch : 6 ; learning_rate : 7.364335352278953e-06 ; fit : 0.11666666666666667
    batch_size :          600
    best_batch_loss : 1.6758585791972271
    Epoch 6, Loss: 1.7279324734506716, fit: 0.1
      batch rate adapted learning_rate :              0.00025
      loss :              1.7329217104320915
      loss_factor :              0.17329217104320915
      loss adapted learning_rate :              7.5075441362172145e-06
    epoch : 7 ; learning_rate : 7.5075441362172145e-06 ; fit : 0.1
    batch_size :          600
    best_batch_loss : 1.675438354824014
    Epoch 7, Loss: 1.7271915949094876, fit: 0.10833333333333334
      batch rate adapted learning_rate :              0.00025
      loss :              1.7300842768011337
      loss_factor :              0.17300842768011337
      loss adapted learning_rate :              7.482979012086255e-06
    epoch : 8 ; learning_rate : 7.482979012086255e-06 ; fit : 0.10833333333333334
    batch_size :          600
    best_batch_loss : 1.662853503252352
    Epoch 8, Loss: 1.7264506908140993, fit: 0.10333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7297124015601866_fit_0.10333333333333333_2024-01-03_170343
  self.fit : 0.10333333333333333
  self.loss : 1.7297124015601866
  current_accuracy : 0.1078
   Accuracy mean: 0.1041
   Accuracy mean: 0.1078
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.0003
    epoch : 0 ; learning_rate : 0.0003 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6446215607691843
    Epoch 0, Loss: 1.7094588979471042, fit: 0.125
    Epoch 0, Loss: 1.7094588979471042
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6933278178321105_fit_0.125_2024-01-03_170346
  self.fit : 0.125
  self.loss : 1.6933278178321105
  current_accuracy : 0.1234
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.0003
      loss :              1.6933278178321105
      loss_factor :              0.16933278178321104
      loss adapted learning_rate :              8.602077295932169e-06
    epoch : 0 ; learning_rate : 8.602077295932169e-06 ; fit : 0.125
    batch_size :          600
    best_batch_loss : 1.646392616910965
    Epoch 0, Loss: 1.6920705915879364, fit: 0.13166666666666665
    Epoch 0, Loss: 1.6920705915879364
      batch rate adapted learning_rate :              0.0003
      loss :              1.663196306325785
      loss_factor :              0.1663196306325785
      loss adapted learning_rate :              8.298665860127204e-06
    epoch : 1 ; learning_rate : 8.298665860127204e-06 ; fit : 0.13166666666666665
    batch_size :          600
    best_batch_loss : 1.6098210745565167
    Epoch 1, Loss: 1.6910926479616717, fit: 0.15
      batch rate adapted learning_rate :              0.0003
      loss :              1.6333577664307333
      loss_factor :              0.16333577664307333
      loss adapted learning_rate :              8.00357277947878e-06
    epoch : 2 ; learning_rate : 8.00357277947878e-06 ; fit : 0.15
    batch_size :          600
    best_batch_loss : 1.6213731833433649
    Epoch 2, Loss: 1.690153492786183, fit: 0.13
      batch rate adapted learning_rate :              0.0003
      loss :              1.6750206303741262
      loss_factor :              0.1675020630374126
      loss adapted learning_rate :              8.417082336536803e-06
    epoch : 3 ; learning_rate : 8.417082336536803e-06 ; fit : 0.13
    batch_size :          600
    best_batch_loss : 1.6119516519399937
    Epoch 3, Loss: 1.6892436503920258, fit: 0.14
      batch rate adapted learning_rate :              0.0003
      loss :              1.654495092660495
      loss_factor :              0.1654495092660495
      loss adapted learning_rate :              8.21206203491298e-06
    epoch : 4 ; learning_rate : 8.21206203491298e-06 ; fit : 0.14
    batch_size :          600
    best_batch_loss : 1.6063469979962277
    Epoch 4, Loss: 1.6883047106850595, fit: 0.11166666666666666
      batch rate adapted learning_rate :              0.0003
      loss :              1.708894011488153
      loss_factor :              0.1708894011488153
      loss adapted learning_rate :              8.760956227500213e-06
    epoch : 5 ; learning_rate : 8.760956227500213e-06 ; fit : 0.11166666666666666
    batch_size :          600
    best_batch_loss : 1.6186772140674035
    Epoch 5, Loss: 1.687314162309889, fit: 0.11166666666666666
      batch rate adapted learning_rate :              0.0003
      loss :              1.7118121957552823
      loss_factor :              0.17118121957552823
      loss adapted learning_rate :              8.790902980609562e-06
    epoch : 6 ; learning_rate : 8.790902980609562e-06 ; fit : 0.11166666666666666
    batch_size :          600
    best_batch_loss : 1.6156026813287057
    Epoch 6, Loss: 1.6863713082934908, fit: 0.115
      batch rate adapted learning_rate :              0.0003
      loss :              1.708588809556622
      loss_factor :              0.1708588809556622
      loss adapted learning_rate :              8.757827160426344e-06
    epoch : 7 ; learning_rate : 8.757827160426344e-06 ; fit : 0.115
    batch_size :          600
    best_batch_loss : 1.6102626582449313
    Epoch 7, Loss: 1.6853609753546077, fit: 0.115
      batch rate adapted learning_rate :              0.0003
      loss :              1.6944324529505406
      loss_factor :              0.16944324529505406
      loss adapted learning_rate :              8.613304012835956e-06
    epoch : 8 ; learning_rate : 8.613304012835956e-06 ; fit : 0.115
    batch_size :          600
    best_batch_loss : 1.6171126836495182
    Epoch 8, Loss: 1.6844498499864498, fit: 0.14166666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.643648693599209_fit_0.14166666666666666_2024-01-03_170410
  self.fit : 0.14166666666666666
  self.loss : 1.643648693599209
  current_accuracy : 0.1287
   Accuracy mean: 0.1234
   Accuracy mean: 0.1287
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.001
    epoch : 0 ; learning_rate : 0.001 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.5862275878318914
    Epoch 0, Loss: 1.6916434682288635, fit: 0.15666666666666668
    Epoch 0, Loss: 1.6916434682288635
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6134313178758948_fit_0.15666666666666668_2024-01-03_170414
  self.fit : 0.15666666666666668
  self.loss : 1.6134313178758948
  current_accuracy : 0.1383
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.001
      loss :              1.6134313178758948
      loss_factor :              0.16134313178758947
      loss adapted learning_rate :              2.6031606175027462e-05
    epoch : 0 ; learning_rate : 2.6031606175027462e-05 ; fit : 0.15666666666666668
    batch_size :          600
    best_batch_loss : 1.5669551739420526
    Epoch 0, Loss: 1.6353526627295178, fit: 0.17833333333333334
    Epoch 0, Loss: 1.6353526627295178
      batch rate adapted learning_rate :              0.001
      loss :              1.5669551739420526
      loss_factor :              0.15669551739420526
      loss adapted learning_rate :              2.4553485171437683e-05
    epoch : 1 ; learning_rate : 2.4553485171437683e-05 ; fit : 0.17833333333333334
    batch_size :          600
    best_batch_loss : 1.5643459786827218
    Epoch 1, Loss: 1.6324211523736984, fit: 0.15666666666666668
      batch rate adapted learning_rate :              0.001
      loss :              1.6183567230986429
      loss_factor :              0.1618356723098643
      loss adapted learning_rate :              2.6190784831985776e-05
    epoch : 2 ; learning_rate : 2.6190784831985776e-05 ; fit : 0.15666666666666668
    batch_size :          600
    best_batch_loss : 1.5598436022855657
    Epoch 2, Loss: 1.629484873965194, fit: 0.16333333333333333
      batch rate adapted learning_rate :              0.001
      loss :              1.602727460039161
      loss_factor :              0.1602727460039161
      loss adapted learning_rate :              2.5687353111635807e-05
    epoch : 3 ; learning_rate : 2.5687353111635807e-05 ; fit : 0.16333333333333333
    batch_size :          600
    best_batch_loss : 1.5551523556152078
    Epoch 3, Loss: 1.6262881880982285, fit: 0.155
      batch rate adapted learning_rate :              0.001
      loss :              1.6124892244516813
      loss_factor :              0.16124892244516814
      loss adapted learning_rate :              2.600121498972785e-05
    epoch : 4 ; learning_rate : 2.600121498972785e-05 ; fit : 0.155
    batch_size :          600
    best_batch_loss : 1.5387505334903306
    Epoch 4, Loss: 1.623237355554694, fit: 0.15333333333333332
      batch rate adapted learning_rate :              0.001
      loss :              1.636476333249832
      loss_factor :              0.1636476333249832
      loss adapted learning_rate :              2.678054789286815e-05
    epoch : 5 ; learning_rate : 2.678054789286815e-05 ; fit : 0.15333333333333332
    batch_size :          600
    best_batch_loss : 1.5425571678306118
    Epoch 5, Loss: 1.620046677472976, fit: 0.165
      batch rate adapted learning_rate :              0.001
      loss :              1.6032986975940067
      loss_factor :              0.16032986975940067
      loss adapted learning_rate :              2.570566713706638e-05
    epoch : 6 ; learning_rate : 2.570566713706638e-05 ; fit : 0.165
    batch_size :          600
    best_batch_loss : 1.541297805800496
    Epoch 6, Loss: 1.6167452510261544, fit: 0.155
      batch rate adapted learning_rate :              0.001
      loss :              1.6170764960689332
      loss_factor :              0.16170764960689332
      loss adapted learning_rate :              2.6149363941385787e-05
    epoch : 7 ; learning_rate : 2.6149363941385787e-05 ; fit : 0.155
    batch_size :          600
    best_batch_loss : 1.527792586872235
    Epoch 7, Loss: 1.6135440972964752, fit: 0.175
      batch rate adapted learning_rate :              0.001
      loss :              1.579567641071002
      loss_factor :              0.1579567641071002
      loss adapted learning_rate :              2.4950339327186097e-05
    epoch : 8 ; learning_rate : 2.4950339327186097e-05 ; fit : 0.175
    batch_size :          600
    best_batch_loss : 1.519779227539339
    Epoch 8, Loss: 1.6103649364643957, fit: 0.145
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.639035531397589_fit_0.145_2024-01-03_170439
  self.fit : 0.145
  self.loss : 1.639035531397589
  current_accuracy : 0.152
   Accuracy mean: 0.1383
   Accuracy mean: 0.152
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.009000000000000001
    epoch : 0 ; learning_rate : 0.009000000000000001 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.0444502398766626
    Epoch 0, Loss: 1.3899511755640779, fit: 0.43666666666666665
    Epoch 0, Loss: 1.3899511755640779
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.0711833951646599_fit_0.43666666666666665_2024-01-03_170443
  self.fit : 0.43666666666666665
  self.loss : 1.0711833951646599
  current_accuracy : 0.4413
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              1.0711833951646599
      loss_factor :              0.10711833951646599
      loss adapted learning_rate :              0.00010326904794688392
    epoch : 0 ; learning_rate : 0.00010326904794688392 ; fit : 0.43666666666666665
    batch_size :          600
    best_batch_loss : 0.9317474905296812
    Epoch 0, Loss: 1.0550816179390166, fit: 0.465
    Epoch 0, Loss: 1.0550816179390166
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              1.0059033911582496
      loss_factor :              0.10059033911582496
      loss adapted learning_rate :              9.106574691092999e-05
    epoch : 1 ; learning_rate : 9.106574691092999e-05 ; fit : 0.465
    batch_size :          600
    best_batch_loss : 0.9526983098907091
    Epoch 1, Loss: 1.0495733234245737, fit: 0.4816666666666667
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              0.9747465215604693
      loss_factor :              0.09747465215604692
      loss adapted learning_rate :              8.55117703164811e-05
    epoch : 2 ; learning_rate : 8.55117703164811e-05 ; fit : 0.4816666666666667
    batch_size :          600
    best_batch_loss : 0.9402376129866912
    Epoch 2, Loss: 1.044682278552244, fit: 0.44333333333333336
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              1.0407002312003186
      loss_factor :              0.10407002312003186
      loss adapted learning_rate :              9.74751274098357e-05
    epoch : 3 ; learning_rate : 9.74751274098357e-05 ; fit : 0.44333333333333336
    batch_size :          600
    best_batch_loss : 0.9474559381469152
    Epoch 3, Loss: 1.039831346648494, fit: 0.455
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              1.0349399828494428
      loss_factor :              0.10349399828494428
      loss adapted learning_rate :              9.639906912903646e-05
    epoch : 4 ; learning_rate : 9.639906912903646e-05 ; fit : 0.455
    batch_size :          600
    best_batch_loss : 0.9545509787950408
    Epoch 4, Loss: 1.0347775735884266, fit: 0.47333333333333333
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              1.0013894126765335
      loss_factor :              0.10013894126765335
      loss adapted learning_rate :              9.025026802385876e-05
    epoch : 5 ; learning_rate : 9.025026802385876e-05 ; fit : 0.47333333333333333
    batch_size :          600
    best_batch_loss : 0.8991957281138294
    Epoch 5, Loss: 1.0300231611269846, fit: 0.45166666666666666
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              1.0263100680962962
      loss_factor :              0.10263100680962962
      loss adapted learning_rate :              9.47981120288242e-05
    epoch : 6 ; learning_rate : 9.47981120288242e-05 ; fit : 0.45166666666666666
    batch_size :          600
    best_batch_loss : 0.9307010393264128
    Epoch 6, Loss: 1.0254987206790362, fit: 0.48
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              1.007531807330187
      loss_factor :              0.10075318073301871
      loss adapted learning_rate :              9.136083085038302e-05
    epoch : 7 ; learning_rate : 9.136083085038302e-05 ; fit : 0.48
    batch_size :          600
    best_batch_loss : 0.9111427455823925
    Epoch 7, Loss: 1.0209550587975476, fit: 0.425
      batch rate adapted learning_rate :              0.009000000000000001
      loss :              1.0884348838295592
      loss_factor :              0.10884348838295592
      loss adapted learning_rate :              0.00010662214467033594
    epoch : 8 ; learning_rate : 0.00010662214467033594 ; fit : 0.425
    batch_size :          600
    best_batch_loss : 0.9144343870267568
    Epoch 8, Loss: 1.016257627064196, fit: 0.46166666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.0080853594441124_fit_0.46166666666666667_2024-01-03_170508
  self.fit : 0.46166666666666667
  self.loss : 1.0080853594441124
  current_accuracy : 0.4644
   Accuracy mean: 0.4413
   Accuracy mean: 0.4644
  Error saving file: doc/out/test_combinations_results/20240103170508.
  normalized_accuracies :      [0.         0.         0.05217613 0.05217613 0.07330109 0.07330109
   0.04097735 0.04097735 0.02265207 0.02290659 0.14176635 0.14762026
   0.08297277 0.09238992 0.13209468 0.14558412 0.17001782 0.20488674
   0.94120641 1.        ]
batch_rate :  0.001
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-20
    epoch : 0 ; learning_rate : 1.0000000000000001e-20 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5150841859681143
    Epoch 0, Loss: 1.7901187054801404, fit: 0.1
    Epoch 0, Loss: 1.7901187054801404
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.755671125800508_fit_0.1_2024-01-03_170517
  self.fit : 0.1
  self.loss : 1.755671125800508
  current_accuracy : 0.0702
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-20
      loss :              1.755671125800508
      loss_factor :              0.1755671125800508
      loss adapted learning_rate :              3.0823811019696236e-22
    epoch : 0 ; learning_rate : 3.0823811019696236e-22 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5762173611452903
    Epoch 0, Loss: 1.7901187054801375, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7901187054801375
      batch rate adapted learning_rate :              1.0000000000000001e-20
      loss :              1.7504015670821473
      loss_factor :              0.17504015670821474
      loss adapted learning_rate :              3.063905646043638e-22
    epoch : 1 ; learning_rate : 3.063905646043638e-22 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.56474687390171
    Epoch 1, Loss: 1.7901187054801395, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-20
      loss :              1.7108524988556875
      loss_factor :              0.17108524988556875
      loss adapted learning_rate :              2.9270162728407508e-22
    epoch : 2 ; learning_rate : 2.9270162728407508e-22 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5306660033874409
    Epoch 2, Loss: 1.7901187054801422, fit: 0.03333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-20
      loss :              1.897760923855959
      loss_factor :              0.1897760923855959
      loss adapted learning_rate :              3.6014965241146235e-22
    epoch : 3 ; learning_rate : 3.6014965241146235e-22 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.4921790449128347
    Epoch 3, Loss: 1.7901187054801402, fit: 0.05
      batch rate adapted learning_rate :              1.0000000000000001e-20
      loss :              1.8162436798341455
      loss_factor :              0.18162436798341455
      loss adapted learning_rate :              3.2987411045374783e-22
    epoch : 4 ; learning_rate : 3.2987411045374783e-22 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5300304395821864
    Epoch 4, Loss: 1.7901187054801397, fit: 0.1
      batch rate adapted learning_rate :              1.0000000000000001e-20
      loss :              1.7386692587629653
      loss_factor :              0.17386692587629654
      loss adapted learning_rate :              3.02297079136736e-22
    epoch : 5 ; learning_rate : 3.02297079136736e-22 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.552995298655982
    Epoch 5, Loss: 1.7901187054801375, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-20
      loss :              1.78894096179264
      loss_factor :              0.178894096179264
      loss adapted learning_rate :              3.200309764779576e-22
    epoch : 6 ; learning_rate : 3.200309764779576e-22 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5745215460387907
    Epoch 6, Loss: 1.7901187054801384, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-20
      loss :              1.821813220822681
      loss_factor :              0.1821813220822681
      loss adapted learning_rate :              3.31900341156431e-22
    epoch : 7 ; learning_rate : 3.31900341156431e-22 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.533791264117636
    Epoch 7, Loss: 1.7901187054801393, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-20
      loss :              1.703683274943087
      loss_factor :              0.1703683274943087
      loss adapted learning_rate :              2.902536701320802e-22
    epoch : 8 ; learning_rate : 2.902536701320802e-22 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5609350018248673
    Epoch 8, Loss: 1.7901187054801377, fit: 0.08333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7707392207463708_fit_0.08333333333333333_2024-01-03_170622
  self.fit : 0.08333333333333333
  self.loss : 1.7707392207463708
  current_accuracy : 0.0702
   Accuracy mean: 0.0702
   Accuracy mean: 0.0702
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-13
    epoch : 0 ; learning_rate : 1e-13 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4556004119944803
    Epoch 0, Loss: 1.7379609776177942, fit: 0.13333333333333333
    Epoch 0, Loss: 1.7379609776177942
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.649928926828691_fit_0.13333333333333333_2024-01-03_170630
  self.fit : 0.13333333333333333
  self.loss : 1.649928926828691
  current_accuracy : 0.1024
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-13
      loss :              1.649928926828691
      loss_factor :              0.1649928926828691
      loss adapted learning_rate :              2.722265463586076e-15
    epoch : 0 ; learning_rate : 2.722265463586076e-15 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4796742783728334
    Epoch 0, Loss: 1.7379609775715485, fit: 0.1
    Epoch 0, Loss: 1.7379609775715485
      batch rate adapted learning_rate :              1e-13
      loss :              1.7189120772539317
      loss_factor :              0.17189120772539318
      loss adapted learning_rate :              2.954658729329427e-15
    epoch : 1 ; learning_rate : 2.954658729329427e-15 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4886885451786946
    Epoch 1, Loss: 1.73796097756909, fit: 0.16666666666666666
      batch rate adapted learning_rate :              1e-13
      loss :              1.6316304834865714
      loss_factor :              0.16316304834865714
      loss adapted learning_rate :              2.6622180346426228e-15
    epoch : 2 ; learning_rate : 2.6622180346426228e-15 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.3954090217553512
    Epoch 2, Loss: 1.7379609775667184, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1e-13
      loss :              1.6858139879464649
      loss_factor :              0.16858139879464648
      loss adapted learning_rate :              2.8419688019559634e-15
    epoch : 3 ; learning_rate : 2.8419688019559634e-15 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4534001142846127
    Epoch 3, Loss: 1.7379609775644929, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-13
      loss :              1.7055388928684612
      loss_factor :              0.17055388928684612
      loss adapted learning_rate :              2.9088629150869762e-15
    epoch : 4 ; learning_rate : 2.9088629150869762e-15 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.374680031079719
    Epoch 4, Loss: 1.737960977562149, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1e-13
      loss :              1.7111912166253025
      loss_factor :              0.17111912166253024
      loss adapted learning_rate :              2.9281753798555827e-15
    epoch : 5 ; learning_rate : 2.9281753798555827e-15 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4892538101560266
    Epoch 5, Loss: 1.7379609775596965, fit: 0.15
      batch rate adapted learning_rate :              1e-13
      loss :              1.6501549461708782
      loss_factor :              0.1650154946170878
      loss adapted learning_rate :              2.723011346372214e-15
    epoch : 6 ; learning_rate : 2.723011346372214e-15 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.3916910997834828
    Epoch 6, Loss: 1.7379609775573561, fit: 0.2
      batch rate adapted learning_rate :              1e-13
      loss :              1.5482968112350741
      loss_factor :              0.15482968112350742
      loss adapted learning_rate :              2.397223015680699e-15
    epoch : 7 ; learning_rate : 2.397223015680699e-15 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.4411742035684072
    Epoch 7, Loss: 1.7379609775550493, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-13
      loss :              1.76638503889672
      loss_factor :              0.176638503889672
      loss adapted learning_rate :              3.1201161056381674e-15
    epoch : 8 ; learning_rate : 3.1201161056381674e-15 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.466624231776906
    Epoch 8, Loss: 1.737960977552684, fit: 0.1
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7348350943409758_fit_0.1_2024-01-03_170736
  self.fit : 0.1
  self.loss : 1.7348350943409758
  current_accuracy : 0.1024
   Accuracy mean: 0.1024
   Accuracy mean: 0.1024
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-10
    epoch : 0 ; learning_rate : 1e-10 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4833085786378324
    Epoch 0, Loss: 1.7884558742856924, fit: 0.05
    Epoch 0, Loss: 1.7884558742856924
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.8528957859291992_fit_0.05_2024-01-03_170743
  self.fit : 0.05
  self.loss : 1.8528957859291992
  current_accuracy : 0.0808
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-10
      loss :              1.8528957859291992
      loss_factor :              0.18528957859291992
      loss adapted learning_rate :              3.433222793514185e-12
    epoch : 0 ; learning_rate : 3.433222793514185e-12 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.33356691358479
    Epoch 0, Loss: 1.78845584354182, fit: 0.11666666666666667
    Epoch 0, Loss: 1.78845584354182
      batch rate adapted learning_rate :              1e-10
      loss :              1.7156605862775336
      loss_factor :              0.17156605862775337
      loss adapted learning_rate :              2.943491247306171e-12
    epoch : 1 ; learning_rate : 2.943491247306171e-12 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5184703255920415
    Epoch 1, Loss: 1.7884558417151426, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1e-10
      loss :              1.7326000351682869
      loss_factor :              0.1732600035168287
      loss adapted learning_rate :              3.001902881865149e-12
    epoch : 2 ; learning_rate : 3.001902881865149e-12 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5260500412951474
    Epoch 2, Loss: 1.7884558401956736, fit: 0.05
      batch rate adapted learning_rate :              1e-10
      loss :              1.791276467760841
      loss_factor :              0.17912764677608412
      loss adapted learning_rate :              3.2086713839537562e-12
    epoch : 3 ; learning_rate : 3.2086713839537562e-12 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4199024099027235
    Epoch 3, Loss: 1.7884558383904237, fit: 0.05
      batch rate adapted learning_rate :              1e-10
      loss :              1.8685761691773437
      loss_factor :              0.18685761691773436
      loss adapted learning_rate :              3.491576900017477e-12
    epoch : 4 ; learning_rate : 3.491576900017477e-12 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5115061737802367
    Epoch 4, Loss: 1.7884558370047905, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1e-10
      loss :              1.820170640711498
      loss_factor :              0.1820170640711498
      loss adapted learning_rate :              3.3130211613081044e-12
    epoch : 5 ; learning_rate : 3.3130211613081044e-12 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.540459467281223
    Epoch 5, Loss: 1.7884558348222743, fit: 0.08333333333333333
      batch rate adapted learning_rate :              1e-10
      loss :              1.8194195927104952
      loss_factor :              0.18194195927104953
      loss adapted learning_rate :              3.3102876543388247e-12
    epoch : 6 ; learning_rate : 3.3102876543388247e-12 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4996608949538854
    Epoch 6, Loss: 1.7884558330659128, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss :              1.7778417424492274
      loss_factor :              0.17778417424492274
      loss adapted learning_rate :              3.160721261194905e-12
    epoch : 7 ; learning_rate : 3.160721261194905e-12 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.511800097235677
    Epoch 7, Loss: 1.7884558312840155, fit: 0.1
      batch rate adapted learning_rate :              1e-10
      loss :              1.7640907701372834
      loss_factor :              0.17640907701372835
      loss adapted learning_rate :              3.112016245283554e-12
    epoch : 8 ; learning_rate : 3.112016245283554e-12 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5111320573216787
    Epoch 8, Loss: 1.7884558297300914, fit: 0.1
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.747262630090059_fit_0.1_2024-01-03_170849
  self.fit : 0.1
  self.loss : 1.747262630090059
  current_accuracy : 0.0808
   Accuracy mean: 0.0808
   Accuracy mean: 0.0808
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-07
    epoch : 0 ; learning_rate : 1.0000000000000001e-07 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4278469142659513
    Epoch 0, Loss: 1.7047258046050642, fit: 0.15
    Epoch 0, Loss: 1.7047258046050642
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6063093912754676_fit_0.15_2024-01-03_170856
  self.fit : 0.15
  self.loss : 1.6063093912754676
  current_accuracy : 0.1124
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.6063093912754676
      loss_factor :              0.16063093912754675
      loss adapted learning_rate :              2.580229860499763e-09
    epoch : 0 ; learning_rate : 2.580229860499763e-09 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4441006805720706
    Epoch 0, Loss: 1.704666965163328, fit: 0.11666666666666667
    Epoch 0, Loss: 1.704666965163328
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.7352209216417616
      loss_factor :              0.17352209216417616
      loss adapted learning_rate :              3.010991646903285e-09
    epoch : 1 ; learning_rate : 3.010991646903285e-09 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4517572728249784
    Epoch 1, Loss: 1.704663955887392, fit: 0.11666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.706749819322434
      loss_factor :              0.1706749819322434
      loss adapted learning_rate :              2.9129949457571613e-09
    epoch : 2 ; learning_rate : 2.9129949457571613e-09 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.398918781073494
    Epoch 2, Loss: 1.7046607431868943, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.6485896879067243
      loss_factor :              0.16485896879067244
      loss adapted learning_rate :              2.717847959072391e-09
    epoch : 3 ; learning_rate : 2.717847959072391e-09 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4085452157454124
    Epoch 3, Loss: 1.7046575197739209, fit: 0.15
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.6563024790345338
      loss_factor :              0.16563024790345338
      loss adapted learning_rate :              2.7433379020559424e-09
    epoch : 4 ; learning_rate : 2.7433379020559424e-09 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.392728028529842
    Epoch 4, Loss: 1.7046547159319814, fit: 0.18333333333333332
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.6000185166640553
      loss_factor :              0.16000185166640551
      loss adapted learning_rate :              2.5600592536678434e-09
    epoch : 5 ; learning_rate : 2.5600592536678434e-09 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.3381742100950993
    Epoch 5, Loss: 1.7046516854614422, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.7939629168119435
      loss_factor :              0.17939629168119436
      loss adapted learning_rate :              3.218302946896417e-09
    epoch : 6 ; learning_rate : 3.218302946896417e-09 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4352439461908733
    Epoch 6, Loss: 1.7046487455862553, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.82526037388172
      loss_factor :              0.182526037388172
      loss adapted learning_rate :              3.3315754324628363e-09
    epoch : 7 ; learning_rate : 3.3315754324628363e-09 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.3851200548166458
    Epoch 7, Loss: 1.7046451306542985, fit: 0.18333333333333332
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss :              1.5554835188835716
      loss_factor :              0.15554835188835717
      loss adapted learning_rate :              2.4195289775184187e-09
    epoch : 8 ; learning_rate : 2.4195289775184187e-09 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.4230460702322054
    Epoch 8, Loss: 1.704641921370856, fit: 0.08333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7401173871654314_fit_0.08333333333333333_2024-01-03_171000
  self.fit : 0.08333333333333333
  self.loss : 1.7401173871654314
  current_accuracy : 0.1124
   Accuracy mean: 0.1124
   Accuracy mean: 0.1124
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-06
    epoch : 0 ; learning_rate : 1e-06 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.595069621106086
    Epoch 0, Loss: 1.8108358922594305, fit: 0.05
    Epoch 0, Loss: 1.8108358922594305
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.8527670912232184_fit_0.05_2024-01-03_171007
  self.fit : 0.05
  self.loss : 1.8527670912232184
  current_accuracy : 0.0684
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-06
      loss :              1.8527670912232184
      loss_factor :              0.18527670912232183
      loss adapted learning_rate :              3.432745894319745e-08
    epoch : 0 ; learning_rate : 3.432745894319745e-08 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5697602590823507
    Epoch 0, Loss: 1.8101260007999032, fit: 0.13333333333333333
    Epoch 0, Loss: 1.8101260007999032
      batch rate adapted learning_rate :              1e-06
      loss :              1.6623077245468716
      loss_factor :              0.16623077245468715
      loss adapted learning_rate :              2.7632669710881977e-08
    epoch : 1 ; learning_rate : 2.7632669710881977e-08 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.5516964456066076
    Epoch 1, Loss: 1.8100828294839424, fit: 0.05
      batch rate adapted learning_rate :              1e-06
      loss :              1.8255292148837503
      loss_factor :              0.18255292148837504
      loss adapted learning_rate :              3.3325569143940824e-08
    epoch : 2 ; learning_rate : 3.3325569143940824e-08 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.505268959167295
    Epoch 2, Loss: 1.8100395844693475, fit: 0.1
      batch rate adapted learning_rate :              1e-06
      loss :              1.7236044224760185
      loss_factor :              0.17236044224760186
      loss adapted learning_rate :              2.9708122051788894e-08
    epoch : 3 ; learning_rate : 2.9708122051788894e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.534563370981428
    Epoch 3, Loss: 1.80999463005182, fit: 0.1
      batch rate adapted learning_rate :              1e-06
      loss :              1.7452192203619528
      loss_factor :              0.17452192203619527
      loss adapted learning_rate :              3.045790127120782e-08
    epoch : 4 ; learning_rate : 3.045790127120782e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5291631818312075
    Epoch 4, Loss: 1.8099526310508904, fit: 0.06666666666666667
      batch rate adapted learning_rate :              1e-06
      loss :              1.7913644481702433
      loss_factor :              0.17913644481702434
      loss adapted learning_rate :              3.208986586168281e-08
    epoch : 5 ; learning_rate : 3.208986586168281e-08 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5739266716546974
    Epoch 5, Loss: 1.8099088265574448, fit: 0.05
      batch rate adapted learning_rate :              1e-06
      loss :              1.840096062362876
      loss_factor :              0.1840096062362876
      loss adapted learning_rate :              3.38595351872336e-08
    epoch : 6 ; learning_rate : 3.38595351872336e-08 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5096243910766916
    Epoch 6, Loss: 1.8098622524831065, fit: 0.1
      batch rate adapted learning_rate :              1e-06
      loss :              1.7249502361646436
      loss_factor :              0.17249502361646435
      loss adapted learning_rate :              2.9754533172444593e-08
    epoch : 7 ; learning_rate : 2.9754533172444593e-08 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5804301654978905
    Epoch 7, Loss: 1.8098167224157673, fit: 0.13333333333333333
      batch rate adapted learning_rate :              1e-06
      loss :              1.6826900785518173
      loss_factor :              0.16826900785518173
      loss adapted learning_rate :              2.831445900456721e-08
    epoch : 8 ; learning_rate : 2.831445900456721e-08 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.5818052968010785
    Epoch 8, Loss: 1.8097754902554557, fit: 0.05
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.8110307641369423_fit_0.05_2024-01-03_171110
  self.fit : 0.05
  self.loss : 1.8110307641369423
  current_accuracy : 0.0684
   Accuracy mean: 0.0684
   Accuracy mean: 0.0684
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.001
      batch rate adapted learning_rate :              2e-05
    epoch : 0 ; learning_rate : 2e-05 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.3993385856522347
    Epoch 0, Loss: 1.7319631403824765, fit: 0.1
    Epoch 0, Loss: 1.7319631403824765
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7072459095484969_fit_0.1_2024-01-03_171117
  self.fit : 0.1
  self.loss : 1.7072459095484969
  current_accuracy : 0.1107
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.001
      batch rate adapted learning_rate :              2e-05
      loss :              1.7072459095484969
      loss_factor :              0.17072459095484968
      loss adapted learning_rate :              5.829377191340149e-07
    epoch : 0 ; learning_rate : 5.829377191340149e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.410753654835227
    Epoch 0, Loss: 1.7222546335238664, fit: 0.05
    Epoch 0, Loss: 1.7222546335238664
      batch rate adapted learning_rate :              2e-05
      loss :              1.80072039369398
      loss_factor :              0.180072039369398
      loss adapted learning_rate :              6.485187872530806e-07
    epoch : 1 ; learning_rate : 6.485187872530806e-07 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4520189756097472
    Epoch 1, Loss: 1.7217443220482127, fit: 0.15
      batch rate adapted learning_rate :              2e-05
      loss :              1.6527881123194386
      loss_factor :              0.16527881123194385
      loss adapted learning_rate :              5.463417088448905e-07
    epoch : 2 ; learning_rate : 5.463417088448905e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4402783407425843
    Epoch 2, Loss: 1.7211863013974598, fit: 0.06666666666666667
      batch rate adapted learning_rate :              2e-05
      loss :              1.8354465298903255
      loss_factor :              0.18354465298903255
      loss adapted learning_rate :              6.737727928172876e-07
    epoch : 3 ; learning_rate : 6.737727928172876e-07 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4269602167143398
    Epoch 3, Loss: 1.7207049167903912, fit: 0.1
      batch rate adapted learning_rate :              2e-05
      loss :              1.775293534563418
      loss_factor :              0.1775293534563418
      loss adapted learning_rate :              6.303334267725349e-07
    epoch : 4 ; learning_rate : 6.303334267725349e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4047476001393675
    Epoch 4, Loss: 1.720121501859527, fit: 0.11666666666666667
      batch rate adapted learning_rate :              2e-05
      loss :              1.7069383963322078
      loss_factor :              0.17069383963322077
      loss adapted learning_rate :              5.827277377746338e-07
    epoch : 5 ; learning_rate : 5.827277377746338e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.454811801441425
    Epoch 5, Loss: 1.719578708776933, fit: 0.13333333333333333
      batch rate adapted learning_rate :              2e-05
      loss :              1.6759772055796194
      loss_factor :              0.16759772055796193
      loss adapted learning_rate :              5.61779918724494e-07
    epoch : 6 ; learning_rate : 5.61779918724494e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4740539322433956
    Epoch 6, Loss: 1.7190692914963843, fit: 0.1
      batch rate adapted learning_rate :              2e-05
      loss :              1.7235332474463694
      loss_factor :              0.17235332474463694
      loss adapted learning_rate :              5.941133710106057e-07
    epoch : 7 ; learning_rate : 5.941133710106057e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4547614176742498
    Epoch 7, Loss: 1.7185378010799435, fit: 0.1
      batch rate adapted learning_rate :              2e-05
      loss :              1.7442268410290709
      loss_factor :              0.1744226841029071
      loss adapted learning_rate :              6.084654545932505e-07
    epoch : 8 ; learning_rate : 6.084654545932505e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4099684022590842
    Epoch 8, Loss: 1.718001051536136, fit: 0.11666666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.679123222772263_fit_0.11666666666666667_2024-01-03_171222
  self.fit : 0.11666666666666667
  self.loss : 1.679123222772263
  current_accuracy : 0.1127
   Accuracy mean: 0.1107
   Accuracy mean: 0.1127
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.001
      batch rate adapted learning_rate :              2.5e-05
    epoch : 0 ; learning_rate : 2.5e-05 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4030582771404458
    Epoch 0, Loss: 1.6773051921233981, fit: 0.1
    Epoch 0, Loss: 1.6773051921233981
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.775005675304618_fit_0.1_2024-01-03_171229
  self.fit : 0.1
  self.loss : 1.775005675304618
  current_accuracy : 0.1397
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.001
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.775005675304618
      loss_factor :              0.17750056753046178
      loss adapted learning_rate :              7.876612868409006e-07
    epoch : 0 ; learning_rate : 7.876612868409006e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.315106010444483
    Epoch 0, Loss: 1.6589603694415243, fit: 0.15
    Epoch 0, Loss: 1.6589603694415243
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.613432906052689
      loss_factor :              0.16134329060526892
      loss adapted learning_rate :              6.507914355834065e-07
    epoch : 1 ; learning_rate : 6.507914355834065e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.3310438934769322
    Epoch 1, Loss: 1.6579948442387866, fit: 0.15
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.6966438784167082
      loss_factor :              0.16966438784167082
      loss adapted learning_rate :              7.196501125422225e-07
    epoch : 2 ; learning_rate : 7.196501125422225e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.2469922796612383
    Epoch 2, Loss: 1.6570446045283, fit: 0.13333333333333333
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.6704167159814967
      loss_factor :              0.16704167159814967
      loss adapted learning_rate :              6.975730012576021e-07
    epoch : 3 ; learning_rate : 6.975730012576021e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4129256985556786
    Epoch 3, Loss: 1.656043025959437, fit: 0.13333333333333333
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.6925505894787385
      loss_factor :              0.16925505894787385
      loss adapted learning_rate :              7.161818744862064e-07
    epoch : 4 ; learning_rate : 7.161818744862064e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.3054411408218454
    Epoch 4, Loss: 1.6550429253495549, fit: 0.15
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.6485116850197334
      loss_factor :              0.16485116850197334
      loss adapted learning_rate :              6.793976939116502e-07
    epoch : 5 ; learning_rate : 6.793976939116502e-07 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.3913660826225311
    Epoch 5, Loss: 1.6540606932496305, fit: 0.08333333333333333
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.7875352839445637
      loss_factor :              0.17875352839445638
      loss adapted learning_rate :              7.988205978366932e-07
    epoch : 6 ; learning_rate : 7.988205978366932e-07 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.3190362823778226
    Epoch 6, Loss: 1.6530470802747983, fit: 0.1
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.723248338398336
      loss_factor :              0.1723248338398336
      loss adapted learning_rate :              7.423962089481566e-07
    epoch : 7 ; learning_rate : 7.423962089481566e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.3468670165647831
    Epoch 7, Loss: 1.6519681155011745, fit: 0.11666666666666667
      batch rate adapted learning_rate :              2.5e-05
      loss :              1.7232895488354332
      loss_factor :              0.17232895488354333
      loss adapted learning_rate :              7.424317172813578e-07
    epoch : 8 ; learning_rate : 7.424317172813578e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.2942109972032625
    Epoch 8, Loss: 1.6509683020514607, fit: 0.13333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.646427272669995_fit_0.13333333333333333_2024-01-03_171333
  self.fit : 0.13333333333333333
  self.loss : 1.646427272669995
  current_accuracy : 0.1444
   Accuracy mean: 0.1397
   Accuracy mean: 0.1444
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              3e-05
    epoch : 0 ; learning_rate : 3e-05 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4504035601627936
    Epoch 0, Loss: 1.7179841335929, fit: 0.1
    Epoch 0, Loss: 1.7179841335929
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7474160782566817_fit_0.1_2024-01-03_171341
  self.fit : 0.1
  self.loss : 1.7474160782566817
  current_accuracy : 0.1178
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              3e-05
      loss :              1.7474160782566817
      loss_factor :              0.17474160782566817
      loss adapted learning_rate :              9.160388851649885e-07
    epoch : 0 ; learning_rate : 9.160388851649885e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.3799221755861315
    Epoch 0, Loss: 1.705117311767711, fit: 0.1
    Epoch 0, Loss: 1.705117311767711
      batch rate adapted learning_rate :              3e-05
      loss :              1.7192492163268096
      loss_factor :              0.17192492163268097
      loss adapted learning_rate :              8.867453603521048e-07
    epoch : 1 ; learning_rate : 8.867453603521048e-07 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.3785603111295552
    Epoch 1, Loss: 1.7042854203927231, fit: 0.11666666666666667
      batch rate adapted learning_rate :              3e-05
      loss :              1.6773763978098593
      loss_factor :              0.16773763978098594
      loss adapted learning_rate :              8.44077473978874e-07
    epoch : 2 ; learning_rate : 8.44077473978874e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4186541548463196
    Epoch 2, Loss: 1.7034544543890826, fit: 0.11666666666666667
      batch rate adapted learning_rate :              3e-05
      loss :              1.722477157651496
      loss_factor :              0.17224771576514958
      loss adapted learning_rate :              8.900782675893527e-07
    epoch : 3 ; learning_rate : 8.900782675893527e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4312893063766567
    Epoch 3, Loss: 1.702699774647775, fit: 0.11666666666666667
      batch rate adapted learning_rate :              3e-05
      loss :              1.706165062933063
      loss_factor :              0.1706165062933063
      loss adapted learning_rate :              8.732997665920148e-07
    epoch : 4 ; learning_rate : 8.732997665920148e-07 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.3878373904258692
    Epoch 4, Loss: 1.7018688843489505, fit: 0.25
      batch rate adapted learning_rate :              3e-05
      loss :              1.4357904837695414
      loss_factor :              0.14357904837695415
      loss adapted learning_rate :              6.184482939849522e-07
    epoch : 5 ; learning_rate : 6.184482939849522e-07 ; fit : 0.25
    batch_size :          60
    best_batch_loss : 1.4370634007809906
    Epoch 5, Loss: 1.7011832256285822, fit: 0.18333333333333332
      batch rate adapted learning_rate :              3e-05
      loss :              1.5644623694179984
      loss_factor :              0.15644623694179985
      loss adapted learning_rate :              7.342627515974933e-07
    epoch : 6 ; learning_rate : 7.342627515974933e-07 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.4173970264733002
    Epoch 6, Loss: 1.700581112383671, fit: 0.13333333333333333
      batch rate adapted learning_rate :              3e-05
      loss :              1.6665245340457184
      loss_factor :              0.16665245340457185
      loss adapted learning_rate :              8.331912067728898e-07
    epoch : 7 ; learning_rate : 8.331912067728898e-07 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.427174882324483
    Epoch 7, Loss: 1.6998700784279053, fit: 0.18333333333333332
      batch rate adapted learning_rate :              3e-05
      loss :              1.5888649486125825
      loss_factor :              0.15888649486125825
      loss adapted learning_rate :              7.573475474788993e-07
    epoch : 8 ; learning_rate : 7.573475474788993e-07 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.4088964944972606
    Epoch 8, Loss: 1.6991370566583173, fit: 0.11666666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7369986879650696_fit_0.11666666666666667_2024-01-03_171447
  self.fit : 0.11666666666666667
  self.loss : 1.7369986879650696
  current_accuracy : 0.12
   Accuracy mean: 0.1178
   Accuracy mean: 0.12
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.50746153929026
    Epoch 0, Loss: 1.7608168434681832, fit: 0.1
    Epoch 0, Loss: 1.7608168434681832
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7051535788645358_fit_0.1_2024-01-03_171454
  self.fit : 0.1
  self.loss : 1.7051535788645358
  current_accuracy : 0.1147
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0001
      loss :              1.7051535788645358
      loss_factor :              0.17051535788645358
      loss adapted learning_rate :              2.9075487275145348e-06
    epoch : 0 ; learning_rate : 2.9075487275145348e-06 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.385344051111428
    Epoch 0, Loss: 1.6998954646494195, fit: 0.06666666666666667
    Epoch 0, Loss: 1.6998954646494195
      batch rate adapted learning_rate :              0.0001
      loss :              1.7684393650680932
      loss_factor :              0.17684393650680932
      loss adapted learning_rate :              3.1273777879224404e-06
    epoch : 1 ; learning_rate : 3.1273777879224404e-06 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4045138193041276
    Epoch 1, Loss: 1.6966404096875953, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.0001
      loss :              1.7798115644939898
      loss_factor :              0.177981156449399
      loss adapted learning_rate :              3.167729205106544e-06
    epoch : 2 ; learning_rate : 3.167729205106544e-06 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.3613528878401373
    Epoch 2, Loss: 1.6931830147239626, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0001
      loss :              1.5675016289183181
      loss_factor :              0.1567501628918318
      loss adapted learning_rate :              2.4570613566615808e-06
    epoch : 3 ; learning_rate : 2.4570613566615808e-06 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.351856710257508
    Epoch 3, Loss: 1.6902166298644834, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0001
      loss :              1.6300349137061838
      loss_factor :              0.16300349137061837
      loss adapted learning_rate :              2.657013819901126e-06
    epoch : 4 ; learning_rate : 2.657013819901126e-06 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.3957392113771894
    Epoch 4, Loss: 1.6874732835092376, fit: 0.15
      batch rate adapted learning_rate :              0.0001
      loss :              1.6374161378601726
      loss_factor :              0.16374161378601726
      loss adapted learning_rate :              2.6811316085249236e-06
    epoch : 5 ; learning_rate : 2.6811316085249236e-06 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.3972583293015797
    Epoch 5, Loss: 1.684692235063038, fit: 0.21666666666666667
      batch rate adapted learning_rate :              0.0001
      loss :              1.517233884696185
      loss_factor :              0.1517233884696185
      loss adapted learning_rate :              2.3019986608702768e-06
    epoch : 6 ; learning_rate : 2.3019986608702768e-06 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.3479129602057298
    Epoch 6, Loss: 1.682008194169423, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.0001
      loss :              1.5672158535173366
      loss_factor :              0.15672158535173367
      loss adapted learning_rate :              2.456165531516074e-06
    epoch : 7 ; learning_rate : 2.456165531516074e-06 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.4243705057480474
    Epoch 7, Loss: 1.6795045543919263, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.0001
      loss :              1.6531856184875693
      loss_factor :              0.16531856184875693
      loss adapted learning_rate :              2.733022689174127e-06
    epoch : 8 ; learning_rate : 2.733022689174127e-06 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.3626633141957858
    Epoch 8, Loss: 1.6767283265893944, fit: 0.13333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6820771495235063_fit_0.13333333333333333_2024-01-03_171600
  self.fit : 0.13333333333333333
  self.loss : 1.6820771495235063
  current_accuracy : 0.1255
   Accuracy mean: 0.1147
   Accuracy mean: 0.1255
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0009000000000000001
    epoch : 0 ; learning_rate : 0.0009000000000000001 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 0.7121358762564479
    Epoch 0, Loss: 1.3260379183477256, fit: 0.5
    Epoch 0, Loss: 1.3260379183477256
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.9680509044064715_fit_0.5_2024-01-03_171607
  self.fit : 0.5
  self.loss : 0.9680509044064715
  current_accuracy : 0.4735
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              0.9680509044064715
      loss_factor :              0.09680509044064714
      loss adapted learning_rate :              8.434102981699686e-06
    epoch : 0 ; learning_rate : 8.434102981699686e-06 ; fit : 0.5
    batch_size :          60
    best_batch_loss : 0.6610415108826255
    Epoch 0, Loss: 1.0131944152544998, fit: 0.5333333333333333
    Epoch 0, Loss: 1.0131944152544998
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              0.9210641180476365
      loss_factor :              0.09210641180476366
      loss adapted learning_rate :              7.635231985993836e-06
    epoch : 1 ; learning_rate : 7.635231985993836e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.6023451692932611
    Epoch 1, Loss: 1.009592365880178, fit: 0.4166666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              1.1403678850430623
      loss_factor :              0.11403678850430624
      loss adapted learning_rate :              1.1703950219138284e-05
    epoch : 2 ; learning_rate : 1.1703950219138284e-05 ; fit : 0.4166666666666667
    batch_size :          60
    best_batch_loss : 0.6572817291524131
    Epoch 2, Loss: 1.0052763811809626, fit: 0.5166666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              0.9230386021526514
      loss_factor :              0.09230386021526514
      loss adapted learning_rate :              7.668002349575287e-06
    epoch : 3 ; learning_rate : 7.668002349575287e-06 ; fit : 0.5166666666666667
    batch_size :          60
    best_batch_loss : 0.5683468662316439
    Epoch 3, Loss: 1.001134100417462, fit: 0.5666666666666667
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              0.7919409584200702
      loss_factor :              0.07919409584200701
      loss adapted learning_rate :              5.644534334609694e-06
    epoch : 4 ; learning_rate : 5.644534334609694e-06 ; fit : 0.5666666666666667
    batch_size :          60
    best_batch_loss : 0.6582220273566627
    Epoch 4, Loss: 0.9982772041009499, fit: 0.5333333333333333
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              0.8561590915659048
      loss_factor :              0.08561590915659048
      loss adapted learning_rate :              6.5970755106385995e-06
    epoch : 5 ; learning_rate : 6.5970755106385995e-06 ; fit : 0.5333333333333333
    batch_size :          60
    best_batch_loss : 0.5768786056043013
    Epoch 5, Loss: 0.9956637662965917, fit: 0.45
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              1.0646863703146816
      loss_factor :              0.10646863703146817
      loss adapted learning_rate :              1.0202013604204663e-05
    epoch : 6 ; learning_rate : 1.0202013604204663e-05 ; fit : 0.45
    batch_size :          60
    best_batch_loss : 0.5977886022126611
    Epoch 6, Loss: 0.9920907525151395, fit: 0.36666666666666664
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              1.1969121702252035
      loss_factor :              0.11969121702252035
      loss adapted learning_rate :              1.289338868909886e-05
    epoch : 7 ; learning_rate : 1.289338868909886e-05 ; fit : 0.36666666666666664
    batch_size :          60
    best_batch_loss : 0.5560776810682511
    Epoch 7, Loss: 0.9873394302281369, fit: 0.43333333333333335
      batch rate adapted learning_rate :              0.0009000000000000001
      loss :              1.0752756291172767
      loss_factor :              0.10752756291172767
      loss adapted learning_rate :              1.0405959107161997e-05
    epoch : 8 ; learning_rate : 1.0405959107161997e-05 ; fit : 0.43333333333333335
    batch_size :          60
    best_batch_loss : 0.5685656359435443
    Epoch 8, Loss: 0.9826249714017717, fit: 0.48333333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.971425773819601_fit_0.48333333333333334_2024-01-03_171712
  self.fit : 0.48333333333333334
  self.loss : 0.971425773819601
  current_accuracy : 0.4886
   Accuracy mean: 0.4735
   Accuracy mean: 0.4886
  Error saving file: doc/out/test_combinations_results/20240103171712.
  normalized_accuracies :      [0.00428367 0.00428367 0.08091385 0.08091385 0.02950976 0.02950976
   0.10471204 0.10471204 0.         0.         0.10066635 0.10542599
   0.1696811  0.18086625 0.11756307 0.12279867 0.11018563 0.13588767
   0.96406473 1.        ]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  1.0
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              1.2755102040816326e-67
      batch rate adapted learning_rate :              1.2755102040816326e-67
    epoch : 0 ; learning_rate : 1.2755102040816326e-67 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7565092878631003
    Epoch 0, Loss: 1.7565092878631003, fit: 0.08826666666666666
    Epoch 0, Loss: 1.7565092878631003
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7565092878631003_fit_0.08826666666666666_2024-01-03_171802
  self.fit : 0.08826666666666666
  self.loss : 1.7565092878631003
  current_accuracy : 0.0863
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-64
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              1.2755102040816326e-67
      batch rate adapted learning_rate :              1.2755102040816326e-67
      loss :              1.7565092878631003
      loss_factor :              0.17565092878631003
      loss adapted learning_rate :              3.566058330976524e-75
    epoch : 0 ; learning_rate : 3.566058330976524e-75 ; fit : 0.08826666666666666
    batch_size :          60000
    best_batch_loss : 1.7565092878631006
    Epoch 0, Loss: 1.7565092878631006, fit: 0.08826666666666666
    Epoch 0, Loss: 1.7565092878631006
      neuron quantity adapted learning_rate :              1.2755102040816326e-67
      batch rate adapted learning_rate :              1.2755102040816326e-67
      loss :              1.7565092878631006
      loss_factor :              0.17565092878631006
      loss adapted learning_rate :              3.5660583309765296e-75
    epoch : 1 ; learning_rate : 3.5660583309765296e-75 ; fit : 0.08826666666666666
    batch_size :          60000
    best_batch_loss : 1.7565092878631006
    Epoch 1, Loss: 1.7565092878631006, fit: 0.08826666666666666
      neuron quantity adapted learning_rate :              1.2755102040816326e-67
      batch rate adapted learning_rate :              1.2755102040816326e-67
      loss :              1.7565092878631006
      loss_factor :              0.17565092878631006
      loss adapted learning_rate :              3.5660583309765296e-75
    epoch : 2 ; learning_rate : 3.5660583309765296e-75 ; fit : 0.08826666666666666
    batch_size :          60000
    best_batch_loss : 1.7565092878631
    Epoch 2, Loss: 1.7565092878631, fit: 0.08826666666666666
      neuron quantity adapted learning_rate :              1.2755102040816326e-67
      batch rate adapted learning_rate :              1.2755102040816326e-67
      loss :              1.7565092878631
      loss_factor :              0.17565092878631
      loss adapted learning_rate :              3.5660583309765184e-75
    epoch : 3 ; learning_rate : 3.5660583309765184e-75 ; fit : 0.08826666666666666
    batch_size :          60000
    best_batch_loss : 1.7565092878631008
    Epoch 3, Loss: 1.7565092878631008, fit: 0.08826666666666666
      neuron quantity adapted learning_rate :              1.2755102040816326e-67
      batch rate adapted learning_rate :              1.2755102040816326e-67
      loss :              1.7565092878631008
      loss_factor :              0.17565092878631008
      loss adapted learning_rate :              3.566058330976535e-75
    epoch : 4 ; learning_rate : 3.566058330976535e-75 ; fit : 0.08826666666666666
    batch_size :          60000
    best_batch_loss : 1.7565092878631003
    Epoch 4, Loss: 1.7565092878631003, fit: 0.08826666666666666
      neuron quantity adapted learning_rate :              1.2755102040816326e-67
      batch rate adapted learning_rate :              1.2755102040816326e-67
      loss :              1.7565092878631003
      loss_factor :              0.17565092878631003
      loss adapted learning_rate :              3.566058330976524e-75
    epoch : 5 ; learning_rate : 3.566058330976524e-75 ; fit : 0.08826666666666666
    batch_size :          60000
    best_batch_loss : 1.756509287863101
    Epoch 5, Loss: 1.756509287863101, fit: 0.08826666666666666
      neuron quantity adapted learning_rate :              1.2755102040816326e-67
      batch rate adapted learning_rate :              1.2755102040816326e-67
      loss :              1.756509287863101
      loss_factor :              0.1756509287863101
      loss adapted learning_rate :              3.5660583309765404e-75
    epoch : 6 ; learning_rate : 3.5660583309765404e-75 ; fit : 0.08826666666666666
    batch_size :          60000
    best_batch_loss : 1.7565092878631003
    Epoch 6, Loss: 1.7565092878631003, fit: 0.08826666666666666
      neuron quantity adapted learning_rate :              1.2755102040816326e-67
      batch rate adapted learning_rate :              1.2755102040816326e-67
      loss :              1.7565092878631003
      loss_factor :              0.17565092878631003
      loss adapted learning_rate :              3.566058330976524e-75
    epoch : 7 ; learning_rate : 3.566058330976524e-75 ; fit : 0.08826666666666666
    batch_size :          60000
    best_batch_loss : 1.7565092878631006
    Epoch 7, Loss: 1.7565092878631006, fit: 0.08826666666666666
      neuron quantity adapted learning_rate :              1.2755102040816326e-67
      batch rate adapted learning_rate :              1.2755102040816326e-67
      loss :              1.7565092878631006
      loss_factor :              0.17565092878631006
      loss adapted learning_rate :              3.5660583309765296e-75
    epoch : 8 ; learning_rate : 3.5660583309765296e-75 ; fit : 0.08826666666666666
    batch_size :          60000
    best_batch_loss : 1.756509287863101
    Epoch 8, Loss: 1.756509287863101, fit: 0.08826666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.756509287863101_fit_0.08826666666666666_2024-01-03_171824
  self.fit : 0.08826666666666666
  self.loss : 1.756509287863101
  current_accuracy : 0.0863
   Accuracy mean: 0.0863
   Accuracy mean: 0.0863
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              1.2755102040816327e-36
      batch rate adapted learning_rate :              1.2755102040816327e-36
    epoch : 0 ; learning_rate : 1.2755102040816327e-36 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6827252882734778
    Epoch 0, Loss: 1.6827252882734778, fit: 0.13233333333333333
    Epoch 0, Loss: 1.6827252882734778
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.6827252882734778_fit_0.13233333333333333_2024-01-03_171826
  self.fit : 0.13233333333333333
  self.loss : 1.6827252882734778
  current_accuracy : 0.1299
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-33
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              1.2755102040816327e-36
      batch rate adapted learning_rate :              1.2755102040816327e-36
      loss :              1.6827252882734778
      loss_factor :              0.16827252882734778
      loss adapted learning_rate :              2.3217537281593253e-44
    epoch : 0 ; learning_rate : 2.3217537281593253e-44 ; fit : 0.13233333333333333
    batch_size :          60000
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  1.0
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              1.2755102040816326e-67
      batch rate adapted learning_rate :              1.2755102040816326e-67
    epoch : 0 ; learning_rate : 1.2755102040816326e-67 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.802428515472502
    Epoch 0, Loss: 1.802428515472502, fit: 0.0746
    Epoch 0, Loss: 1.802428515472502
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.802428515472502_fit_0.0746_2024-01-03_171837
  self.fit : 0.0746
  self.loss : 1.802428515472502
  current_accuracy : 0.0756
   Accuracy mean: 0.0756
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              1.2755102040816327e-36
      batch rate adapted learning_rate :              1.2755102040816327e-36
    epoch : 0 ; learning_rate : 1.2755102040816327e-36 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.752674407485241
    Epoch 0, Loss: 1.752674407485241, fit: 0.09165
    Epoch 0, Loss: 1.752674407485241
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.752674407485241_fit_0.09165_2024-01-03_171840
  self.fit : 0.09165
  self.loss : 1.752674407485241
  current_accuracy : 0.0849
   Accuracy mean: 0.0849
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              1.2755102040816328e-20
      batch rate adapted learning_rate :              1.2755102040816328e-20
    epoch : 0 ; learning_rate : 1.2755102040816328e-20 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7538433510514648
    Epoch 0, Loss: 1.7538433510514648, fit: 0.09556666666666666
    Epoch 0, Loss: 1.7538433510514648
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7538433510514648_fit_0.09556666666666666_2024-01-03_171843
  self.fit : 0.09556666666666666
  self.loss : 1.7538433510514648
  current_accuracy : 0.0947
   Accuracy mean: 0.0947
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              1.2755102040816326e-13
      batch rate adapted learning_rate :              1.2755102040816326e-13
    epoch : 0 ; learning_rate : 1.2755102040816326e-13 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6797647115159433
    Epoch 0, Loss: 1.6797647115159433, fit: 0.12943333333333334
    Epoch 0, Loss: 1.6797647115159433
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.6797647115159433_fit_0.12943333333333334_2024-01-03_171846
  self.fit : 0.12943333333333334
  self.loss : 1.6797647115159433
  current_accuracy : 0.1314
   Accuracy mean: 0.1314
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              1.2755102040816326e-10
      batch rate adapted learning_rate :              1.2755102040816326e-10
    epoch : 0 ; learning_rate : 1.2755102040816326e-10 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7659966731368895
    Epoch 0, Loss: 1.7659966731368895, fit: 0.08516666666666667
    Epoch 0, Loss: 1.7659966731368895
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7659966731368895_fit_0.08516666666666667_2024-01-03_171849
  self.fit : 0.08516666666666667
  self.loss : 1.7659966731368895
  current_accuracy : 0.085
   Accuracy mean: 0.085
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              1.2755102040816328e-07
      batch rate adapted learning_rate :              1.2755102040816328e-07
    epoch : 0 ; learning_rate : 1.2755102040816328e-07 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6859176141449022
    Epoch 0, Loss: 1.6859176141449022, fit: 0.13183333333333333
    Epoch 0, Loss: 1.6859176141449022
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6859176141449022_fit_0.13183333333333333_2024-01-03_171851
  self.fit : 0.13183333333333333
  self.loss : 1.6859176141449022
  current_accuracy : 0.1319
   Accuracy mean: 0.1319
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              1.2755102040816327e-06
      batch rate adapted learning_rate :              1.2755102040816327e-06
    epoch : 0 ; learning_rate : 1.2755102040816327e-06 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7722572616503658
    Epoch 0, Loss: 1.7722572616503658, fit: 0.08615
    Epoch 0, Loss: 1.7722572616503658
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7722572616503658_fit_0.08615_2024-01-03_171854
  self.fit : 0.08615
  self.loss : 1.7722572616503658
  current_accuracy : 0.0817
   Accuracy mean: 0.0817
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              2.5510204081632654e-05
      batch rate adapted learning_rate :              2.5510204081632654e-05
    epoch : 0 ; learning_rate : 2.5510204081632654e-05 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7283532175815821
    Epoch 0, Loss: 1.7283532175815821, fit: 0.10468333333333334
    Epoch 0, Loss: 1.7283532175815821
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7283532175815821_fit_0.10468333333333334_2024-01-03_171856
  self.fit : 0.10468333333333334
  self.loss : 1.7283532175815821
  current_accuracy : 0.1083
   Accuracy mean: 0.1083
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              3.188775510204082e-05
      batch rate adapted learning_rate :              3.188775510204082e-05
    epoch : 0 ; learning_rate : 3.188775510204082e-05 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.75179208069471
    Epoch 0, Loss: 1.75179208069471, fit: 0.09365
    Epoch 0, Loss: 1.75179208069471
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.75179208069471_fit_0.09365_2024-01-03_171859
  self.fit : 0.09365
  self.loss : 1.75179208069471
  current_accuracy : 0.0864
   Accuracy mean: 0.0864
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              3.826530612244898e-05
      batch rate adapted learning_rate :              3.826530612244898e-05
    epoch : 0 ; learning_rate : 3.826530612244898e-05 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6987330269862555
    Epoch 0, Loss: 1.6987330269862555, fit: 0.11941666666666667
    Epoch 0, Loss: 1.6987330269862555
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6987330269862555_fit_0.11941666666666667_2024-01-03_171901
  self.fit : 0.11941666666666667
  self.loss : 1.6987330269862555
  current_accuracy : 0.1132
   Accuracy mean: 0.1132
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              0.00012755102040816328
      batch rate adapted learning_rate :              0.00012755102040816328
    epoch : 0 ; learning_rate : 0.00012755102040816328 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7352276073978843
    Epoch 0, Loss: 1.7352276073978843, fit: 0.10333333333333333
    Epoch 0, Loss: 1.7352276073978843
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7352276073978843_fit_0.10333333333333333_2024-01-03_171904
  self.fit : 0.10333333333333333
  self.loss : 1.7352276073978843
  current_accuracy : 0.1094
   Accuracy mean: 0.1094
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              0.0011479591836734695
      batch rate adapted learning_rate :              0.0011479591836734695
    epoch : 0 ; learning_rate : 0.0011479591836734695 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7206604419604026
    Epoch 0, Loss: 1.7206604419604026, fit: 0.1093
    Epoch 0, Loss: 1.7206604419604026
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7206604419604026_fit_0.1093_2024-01-03_171907
  self.fit : 0.1093
  self.loss : 1.7206604419604026
  current_accuracy : 0.1103
   Accuracy mean: 0.1103
  Error saving file: doc/out/test_combinations_results/20240103171907.
  normalized_accuracies :      [0.         0.1651865  0.339254   0.99111901 0.1669627  1.
   0.10834813 0.58081705 0.19182948 0.6678508  0.60035524 0.61634103]
batch_rate :  0.5
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              1.2755102040816326e-67
      batch rate adapted learning_rate :              6.377551020408163e-68
    epoch : 0 ; learning_rate : 6.377551020408163e-68 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7100711359734488
    Epoch 0, Loss: 1.713256292859861, fit: 0.1074
    Epoch 0, Loss: 1.713256292859861
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.716441449746273_fit_0.1074_2024-01-03_171910
  self.fit : 0.1074
  self.loss : 1.716441449746273
  current_accuracy : 0.112
   Accuracy mean: 0.112
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              1.2755102040816327e-36
      batch rate adapted learning_rate :              6.3775510204081635e-37
    epoch : 0 ; learning_rate : 6.3775510204081635e-37 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.6704583039901748
    Epoch 0, Loss: 1.6723945860247773, fit: 0.1322
    Epoch 0, Loss: 1.6723945860247773
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.6743308680593796_fit_0.1322_2024-01-03_171913
  self.fit : 0.1322
  self.loss : 1.6743308680593796
  current_accuracy : 0.1394
   Accuracy mean: 0.1394
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              1.2755102040816328e-20
      batch rate adapted learning_rate :              6.377551020408164e-21
    epoch : 0 ; learning_rate : 6.377551020408164e-21 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.6966242526375852
    Epoch 0, Loss: 1.6997987254729376, fit: 0.12203333333333333
    Epoch 0, Loss: 1.6997987254729376
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7029731983082903_fit_0.12203333333333333_2024-01-03_171916
  self.fit : 0.12203333333333333
  self.loss : 1.7029731983082903
  current_accuracy : 0.1235
   Accuracy mean: 0.1235
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              1.2755102040816326e-13
      batch rate adapted learning_rate :              6.377551020408163e-14
    epoch : 0 ; learning_rate : 6.377551020408163e-14 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.840404867671333
    Epoch 0, Loss: 1.8414515933648543, fit: 0.04903333333333333
    Epoch 0, Loss: 1.8414515933648543
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.842498319058376_fit_0.04903333333333333_2024-01-03_171918
  self.fit : 0.04903333333333333
  self.loss : 1.842498319058376
  current_accuracy : 0.0475
   Accuracy mean: 0.0475
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              1.2755102040816326e-10
      batch rate adapted learning_rate :              6.377551020408163e-11
    epoch : 0 ; learning_rate : 6.377551020408163e-11 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.6901428786115442
    Epoch 0, Loss: 1.6918039042866495, fit: 0.12616666666666668
    Epoch 0, Loss: 1.6918039042866495
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.6934649299617548_fit_0.12616666666666668_2024-01-03_171921
  self.fit : 0.12616666666666668
  self.loss : 1.6934649299617548
  current_accuracy : 0.1258
   Accuracy mean: 0.1258
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              1.2755102040816328e-07
      batch rate adapted learning_rate :              6.377551020408164e-08
    epoch : 0 ; learning_rate : 6.377551020408164e-08 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.74942201334193
    Epoch 0, Loss: 1.7499964982960288, fit: 0.09703333333333333
    Epoch 0, Loss: 1.7499964982960288
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7505709832501277_fit_0.09703333333333333_2024-01-03_171923
  self.fit : 0.09703333333333333
  self.loss : 1.7505709832501277
  current_accuracy : 0.0969
   Accuracy mean: 0.0969
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              1.2755102040816327e-06
      batch rate adapted learning_rate :              6.377551020408163e-07
    epoch : 0 ; learning_rate : 6.377551020408163e-07 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7179206780652425
    Epoch 0, Loss: 1.7183784984476902, fit: 0.11226666666666667
    Epoch 0, Loss: 1.7183784984476902
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7179206780652425_fit_0.11226666666666667_2024-01-03_171925
  self.fit : 0.11226666666666667
  self.loss : 1.7179206780652425
  current_accuracy : 0.1166
   Accuracy mean: 0.1166
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              2.5510204081632654e-05
      batch rate adapted learning_rate :              1.2755102040816327e-05
    epoch : 0 ; learning_rate : 1.2755102040816327e-05 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7695932512241856
    Epoch 0, Loss: 1.769796458293514, fit: 0.083
    Epoch 0, Loss: 1.769796458293514
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7699996653628423_fit_0.083_2024-01-03_171928
  self.fit : 0.083
  self.loss : 1.7699996653628423
  current_accuracy : 0.0818
   Accuracy mean: 0.0818
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              3.188775510204082e-05
      batch rate adapted learning_rate :              1.594387755102041e-05
    epoch : 0 ; learning_rate : 1.594387755102041e-05 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7291856549298166
    Epoch 0, Loss: 1.7303848561140311, fit: 0.1086
    Epoch 0, Loss: 1.7303848561140311
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7291856549298166_fit_0.1086_2024-01-03_171930
  self.fit : 0.1086
  self.loss : 1.7291856549298166
  current_accuracy : 0.1091
   Accuracy mean: 0.1091
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              3.826530612244898e-05
      batch rate adapted learning_rate :              1.913265306122449e-05
    epoch : 0 ; learning_rate : 1.913265306122449e-05 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7662447396361989
    Epoch 0, Loss: 1.7710627429798524, fit: 0.087
    Epoch 0, Loss: 1.7710627429798524
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7662447396361989_fit_0.087_2024-01-03_171933
  self.fit : 0.087
  self.loss : 1.7662447396361989
  current_accuracy : 0.076
   Accuracy mean: 0.076
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              0.00012755102040816328
      batch rate adapted learning_rate :              6.377551020408164e-05
    epoch : 0 ; learning_rate : 6.377551020408164e-05 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7819432271394595
    Epoch 0, Loss: 1.7844111894921864, fit: 0.0774
    Epoch 0, Loss: 1.7844111894921864
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7868791518449134_fit_0.0774_2024-01-03_171935
  self.fit : 0.0774
  self.loss : 1.7868791518449134
  current_accuracy : 0.0752
   Accuracy mean: 0.0752
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              0.0011479591836734695
      batch rate adapted learning_rate :              0.0005739795918367347
    epoch : 0 ; learning_rate : 0.0005739795918367347 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7653750420005359
    Epoch 0, Loss: 1.766482118323336, fit: 0.08616666666666667
    Epoch 0, Loss: 1.766482118323336
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.767589194646136_fit_0.08616666666666667_2024-01-03_171938
  self.fit : 0.08616666666666667
  self.loss : 1.767589194646136
  current_accuracy : 0.0901
   Accuracy mean: 0.0901
  Error saving file: doc/out/test_combinations_results/20240103171938.
  normalized_accuracies :      [0.70184984 1.         0.82698585 0.         0.85201306 0.53754081
   0.75190424 0.37323177 0.6702938  0.3101197  0.30141458 0.46354733]
batch_rate :  0.2
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              1.2755102040816326e-67
      batch rate adapted learning_rate :              2.551020408163265e-68
    epoch : 0 ; learning_rate : 2.551020408163265e-68 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.6580087300898485
    Epoch 0, Loss: 1.6695729173287388, fit: 0.13608333333333333
    Epoch 0, Loss: 1.6695729173287388
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.6739533847309622_fit_0.13608333333333333_2024-01-03_171941
  self.fit : 0.13608333333333333
  self.loss : 1.6739533847309622
  current_accuracy : 0.1385
   Accuracy mean: 0.1385
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              1.2755102040816327e-36
      batch rate adapted learning_rate :              2.5510204081632657e-37
    epoch : 0 ; learning_rate : 2.5510204081632657e-37 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.666913933521045
    Epoch 0, Loss: 1.6796090711845908, fit: 0.138
    Epoch 0, Loss: 1.6796090711845908
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.6796579440213695_fit_0.138_2024-01-03_171944
  self.fit : 0.138
  self.loss : 1.6796579440213695
  current_accuracy : 0.1368
   Accuracy mean: 0.1368
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              1.2755102040816328e-20
      batch rate adapted learning_rate :              2.551020408163266e-21
    epoch : 0 ; learning_rate : 2.551020408163266e-21 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7820163100855348
    Epoch 0, Loss: 1.7896196065945853, fit: 0.07975
    Epoch 0, Loss: 1.7896196065945853
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7827963700769827_fit_0.07975_2024-01-03_171946
  self.fit : 0.07975
  self.loss : 1.7827963700769827
  current_accuracy : 0.0802
   Accuracy mean: 0.0802
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              1.2755102040816326e-13
      batch rate adapted learning_rate :              2.5510204081632653e-14
    epoch : 0 ; learning_rate : 2.5510204081632653e-14 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7304677121563363
    Epoch 0, Loss: 1.73321239909308, fit: 0.10683333333333334
    Epoch 0, Loss: 1.73321239909308
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7318513217258993_fit_0.10683333333333334_2024-01-03_171949
  self.fit : 0.10683333333333334
  self.loss : 1.7318513217258993
  current_accuracy : 0.1068
   Accuracy mean: 0.1068
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              1.2755102040816326e-10
      batch rate adapted learning_rate :              2.5510204081632654e-11
    epoch : 0 ; learning_rate : 2.5510204081632654e-11 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.671617848647225
    Epoch 0, Loss: 1.6777522449233748, fit: 0.12591666666666668
    Epoch 0, Loss: 1.6777522449233748
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.676428858802212_fit_0.12591666666666668_2024-01-03_171952
  self.fit : 0.12591666666666668
  self.loss : 1.676428858802212
  current_accuracy : 0.1253
   Accuracy mean: 0.1253
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              1.2755102040816328e-07
      batch rate adapted learning_rate :              2.5510204081632658e-08
    epoch : 0 ; learning_rate : 2.5510204081632658e-08 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7925081703218742
    Epoch 0, Loss: 1.7965723606144353, fit: 0.06875
    Epoch 0, Loss: 1.7965723606144353
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.8020205019589617_fit_0.06875_2024-01-03_171955
  self.fit : 0.06875
  self.loss : 1.8020205019589617
  current_accuracy : 0.0747
   Accuracy mean: 0.0747
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              1.2755102040816327e-06
      batch rate adapted learning_rate :              2.5510204081632656e-07
    epoch : 0 ; learning_rate : 2.5510204081632656e-07 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7212999819854204
    Epoch 0, Loss: 1.734373885814593, fit: 0.11175
    Epoch 0, Loss: 1.734373885814593
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.725157281967136_fit_0.11175_2024-01-03_171958
  self.fit : 0.11175
  self.loss : 1.725157281967136
  current_accuracy : 0.0956
   Accuracy mean: 0.0956
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              2.5510204081632654e-05
      batch rate adapted learning_rate :              5.1020408163265315e-06
    epoch : 0 ; learning_rate : 5.1020408163265315e-06 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7384486418790581
    Epoch 0, Loss: 1.7481648596344224, fit: 0.10316666666666667
    Epoch 0, Loss: 1.7481648596344224
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7384486418790581_fit_0.10316666666666667_2024-01-03_172000
  self.fit : 0.10316666666666667
  self.loss : 1.7384486418790581
  current_accuracy : 0.0964
   Accuracy mean: 0.0964
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              3.188775510204082e-05
      batch rate adapted learning_rate :              6.377551020408164e-06
    epoch : 0 ; learning_rate : 6.377551020408164e-06 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7661157709473658
    Epoch 0, Loss: 1.7680549580468312, fit: 0.08541666666666667
    Epoch 0, Loss: 1.7680549580468312
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7676144864408714_fit_0.08541666666666667_2024-01-03_172003
  self.fit : 0.08541666666666667
  self.loss : 1.7676144864408714
  current_accuracy : 0.0786
   Accuracy mean: 0.0786
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              3.826530612244898e-05
      batch rate adapted learning_rate :              7.653061224489796e-06
    epoch : 0 ; learning_rate : 7.653061224489796e-06 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7069038393920704
    Epoch 0, Loss: 1.7178704641647409, fit: 0.1065
    Epoch 0, Loss: 1.7178704641647409
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.723996646690331_fit_0.1065_2024-01-03_172006
  self.fit : 0.1065
  self.loss : 1.723996646690331
  current_accuracy : 0.1095
   Accuracy mean: 0.1095
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              0.00012755102040816328
      batch rate adapted learning_rate :              2.5510204081632657e-05
    epoch : 0 ; learning_rate : 2.5510204081632657e-05 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7390668918177028
    Epoch 0, Loss: 1.7456178739396124, fit: 0.09916666666666667
    Epoch 0, Loss: 1.7456178739396124
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7390668918177028_fit_0.09916666666666667_2024-01-03_172009
  self.fit : 0.09916666666666667
  self.loss : 1.7390668918177028
  current_accuracy : 0.1011
   Accuracy mean: 0.1011
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              0.0011479591836734695
      batch rate adapted learning_rate :              0.00022959183673469392
    epoch : 0 ; learning_rate : 0.00022959183673469392 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.6447935003664522
    Epoch 0, Loss: 1.653529830795887, fit: 0.1315
    Epoch 0, Loss: 1.653529830795887
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.667526071868926_fit_0.1315_2024-01-03_172011
  self.fit : 0.1315
  self.loss : 1.667526071868926
  current_accuracy : 0.1348
   Accuracy mean: 0.1348
  Error saving file: doc/out/test_combinations_results/20240103172012.
  normalized_accuracies :      [1.         0.97335423 0.0862069  0.5031348  0.79310345 0.
   0.32758621 0.34012539 0.06112853 0.54545455 0.4137931  0.94200627]
batch_rate :  0.1
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              1.2755102040816326e-67
      batch rate adapted learning_rate :              1.2755102040816326e-68
    epoch : 0 ; learning_rate : 1.2755102040816326e-68 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7458168562166199
    Epoch 0, Loss: 1.7579729006577538, fit: 0.09766666666666667
    Epoch 0, Loss: 1.7579729006577538
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.754467918910439_fit_0.09766666666666667_2024-01-03_172015
  self.fit : 0.09766666666666667
  self.loss : 1.754467918910439
  current_accuracy : 0.0958
   Accuracy mean: 0.0958
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              1.2755102040816327e-36
      batch rate adapted learning_rate :              1.2755102040816329e-37
    epoch : 0 ; learning_rate : 1.2755102040816329e-37 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.8016893406938377
    Epoch 0, Loss: 1.8085081581971212, fit: 0.064
    Epoch 0, Loss: 1.8085081581971212
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.8089665688334566_fit_0.064_2024-01-03_172018
  self.fit : 0.064
  self.loss : 1.8089665688334566
  current_accuracy : 0.067
   Accuracy mean: 0.067
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              1.2755102040816328e-20
      batch rate adapted learning_rate :              1.275510204081633e-21
    epoch : 0 ; learning_rate : 1.275510204081633e-21 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7479514370212277
    Epoch 0, Loss: 1.7622841305024748, fit: 0.08916666666666667
    Epoch 0, Loss: 1.7622841305024748
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7581447434737258_fit_0.08916666666666667_2024-01-03_172020
  self.fit : 0.08916666666666667
  self.loss : 1.7581447434737258
  current_accuracy : 0.0923
   Accuracy mean: 0.0923
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              1.2755102040816326e-13
      batch rate adapted learning_rate :              1.2755102040816326e-14
    epoch : 0 ; learning_rate : 1.2755102040816326e-14 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.8077126091982187
    Epoch 0, Loss: 1.8167679664738412, fit: 0.06683333333333333
    Epoch 0, Loss: 1.8167679664738412
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.8127773141718342_fit_0.06683333333333333_2024-01-03_172023
  self.fit : 0.06683333333333333
  self.loss : 1.8127773141718342
  current_accuracy : 0.0685
   Accuracy mean: 0.0685
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              1.2755102040816326e-10
      batch rate adapted learning_rate :              1.2755102040816327e-11
    epoch : 0 ; learning_rate : 1.2755102040816327e-11 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7327568090596221
    Epoch 0, Loss: 1.742121592457598, fit: 0.09233333333333334
    Epoch 0, Loss: 1.742121592457598
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.759804985521349_fit_0.09233333333333334_2024-01-03_172025
  self.fit : 0.09233333333333334
  self.loss : 1.759804985521349
  current_accuracy : 0.1031
   Accuracy mean: 0.1031
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              1.2755102040816328e-07
      batch rate adapted learning_rate :              1.2755102040816329e-08
    epoch : 0 ; learning_rate : 1.2755102040816329e-08 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7599696565741132
    Epoch 0, Loss: 1.765420030689928, fit: 0.09066666666666667
    Epoch 0, Loss: 1.765420030689928
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7599696565741132_fit_0.09066666666666667_2024-01-03_172028
  self.fit : 0.09066666666666667
  self.loss : 1.7599696565741132
  current_accuracy : 0.0835
   Accuracy mean: 0.0835
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              1.2755102040816327e-06
      batch rate adapted learning_rate :              1.2755102040816328e-07
    epoch : 0 ; learning_rate : 1.2755102040816328e-07 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.6961704299687477
    Epoch 0, Loss: 1.7119975218084136, fit: 0.12233333333333334
    Epoch 0, Loss: 1.7119975218084136
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7103482639957617_fit_0.12233333333333334_2024-01-03_172030
  self.fit : 0.12233333333333334
  self.loss : 1.7103482639957617
  current_accuracy : 0.1295
   Accuracy mean: 0.1295
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              2.5510204081632654e-05
      batch rate adapted learning_rate :              2.5510204081632657e-06
    epoch : 0 ; learning_rate : 2.5510204081632657e-06 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.6191421593881514
    Epoch 0, Loss: 1.6351433873868986, fit: 0.15183333333333332
    Epoch 0, Loss: 1.6351433873868986
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.6352834965730945_fit_0.15183333333333332_2024-01-03_172034
  self.fit : 0.15183333333333332
  self.loss : 1.6352834965730945
  current_accuracy : 0.1601
   Accuracy mean: 0.1601
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              3.188775510204082e-05
      batch rate adapted learning_rate :              3.188775510204082e-06
    epoch : 0 ; learning_rate : 3.188775510204082e-06 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.6464117993272818
    Epoch 0, Loss: 1.658225182832061, fit: 0.144
    Epoch 0, Loss: 1.658225182832061
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.6631006763220197_fit_0.144_2024-01-03_172036
  self.fit : 0.144
  self.loss : 1.6631006763220197
  current_accuracy : 0.1455
   Accuracy mean: 0.1455
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              3.826530612244898e-05
      batch rate adapted learning_rate :              3.826530612244898e-06
    epoch : 0 ; learning_rate : 3.826530612244898e-06 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.746967371836696
    Epoch 0, Loss: 1.7562587894040926, fit: 0.08966666666666667
    Epoch 0, Loss: 1.7562587894040926
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7626492063375325_fit_0.08966666666666667_2024-01-03_172039
  self.fit : 0.08966666666666667
  self.loss : 1.7626492063375325
  current_accuracy : 0.0983
   Accuracy mean: 0.0983
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              0.00012755102040816328
      batch rate adapted learning_rate :              1.2755102040816329e-05
    epoch : 0 ; learning_rate : 1.2755102040816329e-05 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7102153358122305
    Epoch 0, Loss: 1.7163946210610124, fit: 0.11583333333333333
    Epoch 0, Loss: 1.7163946210610124
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.721184712888371_fit_0.11583333333333333_2024-01-03_172041
  self.fit : 0.11583333333333333
  self.loss : 1.721184712888371
  current_accuracy : 0.1223
   Accuracy mean: 0.1223
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              0.0011479591836734695
      batch rate adapted learning_rate :              0.00011479591836734696
    epoch : 0 ; learning_rate : 0.00011479591836734696 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7551600431410608
    Epoch 0, Loss: 1.7667243408670463, fit: 0.09083333333333334
    Epoch 0, Loss: 1.7667243408670463
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7577887369143255_fit_0.09083333333333334_2024-01-03_172044
  self.fit : 0.09083333333333334
  self.loss : 1.7577887369143255
  current_accuracy : 0.089
   Accuracy mean: 0.089
  Error saving file: doc/out/test_combinations_results/20240103172044.
  normalized_accuracies :      [0.30934479 0.         0.27175081 0.01611171 0.3877551  0.17722879
   0.67132116 1.         0.84317938 0.33619764 0.59398496 0.23630505]
batch_rate :  0.01
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              1.2755102040816326e-67
      batch rate adapted learning_rate :              1.2755102040816327e-69
    epoch : 0 ; learning_rate : 1.2755102040816327e-69 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.662262348712718
    Epoch 0, Loss: 1.7401566655390281, fit: 0.11166666666666666
    Epoch 0, Loss: 1.7401566655390281
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7268156280542182_fit_0.11166666666666666_2024-01-03_172048
  self.fit : 0.11166666666666666
  self.loss : 1.7268156280542182
  current_accuracy : 0.1019
   Accuracy mean: 0.1019
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              1.2755102040816327e-36
      batch rate adapted learning_rate :              1.2755102040816327e-38
    epoch : 0 ; learning_rate : 1.2755102040816327e-38 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.7028102927558892
    Epoch 0, Loss: 1.757766950573805, fit: 0.09666666666666666
    Epoch 0, Loss: 1.757766950573805
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7392048823237534_fit_0.09666666666666666_2024-01-03_172051
  self.fit : 0.09666666666666666
  self.loss : 1.7392048823237534
  current_accuracy : 0.0955
   Accuracy mean: 0.0955
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              1.2755102040816328e-20
      batch rate adapted learning_rate :              1.2755102040816328e-22
    epoch : 0 ; learning_rate : 1.2755102040816328e-22 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.746512830080959
    Epoch 0, Loss: 1.7943288148542686, fit: 0.06166666666666667
    Epoch 0, Loss: 1.7943288148542686
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.8216416143018084_fit_0.06166666666666667_2024-01-03_172054
  self.fit : 0.06166666666666667
  self.loss : 1.8216416143018084
  current_accuracy : 0.076
   Accuracy mean: 0.076
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              1.2755102040816326e-13
      batch rate adapted learning_rate :              1.2755102040816326e-15
    epoch : 0 ; learning_rate : 1.2755102040816326e-15 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6948869166244158
    Epoch 0, Loss: 1.7717667540783613, fit: 0.058333333333333334
    Epoch 0, Loss: 1.7717667540783613
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.806931000949968_fit_0.058333333333333334_2024-01-03_172057
  self.fit : 0.058333333333333334
  self.loss : 1.806931000949968
  current_accuracy : 0.0845
   Accuracy mean: 0.0845
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              1.2755102040816326e-10
      batch rate adapted learning_rate :              1.2755102040816327e-12
    epoch : 0 ; learning_rate : 1.2755102040816327e-12 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.7042775320371513
    Epoch 0, Loss: 1.7511689858059416, fit: 0.09333333333333334
    Epoch 0, Loss: 1.7511689858059416
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7552567445957883_fit_0.09333333333333334_2024-01-03_172059
  self.fit : 0.09333333333333334
  self.loss : 1.7552567445957883
  current_accuracy : 0.0955
   Accuracy mean: 0.0955
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              1.2755102040816328e-07
      batch rate adapted learning_rate :              1.275510204081633e-09
    epoch : 0 ; learning_rate : 1.275510204081633e-09 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6888042482716514
    Epoch 0, Loss: 1.7611420831634288, fit: 0.095
    Epoch 0, Loss: 1.7611420831634288
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7636738509642995_fit_0.095_2024-01-03_172102
  self.fit : 0.095
  self.loss : 1.7636738509642995
  current_accuracy : 0.0919
   Accuracy mean: 0.0919
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              1.2755102040816327e-06
      batch rate adapted learning_rate :              1.2755102040816328e-08
    epoch : 0 ; learning_rate : 1.2755102040816328e-08 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6946739993213795
    Epoch 0, Loss: 1.762864815780605, fit: 0.08333333333333333
    Epoch 0, Loss: 1.762864815780605
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.774013608299169_fit_0.08333333333333333_2024-01-03_172105
  self.fit : 0.08333333333333333
  self.loss : 1.774013608299169
  current_accuracy : 0.0892
   Accuracy mean: 0.0892
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              2.5510204081632654e-05
      batch rate adapted learning_rate :              2.5510204081632656e-07
    epoch : 0 ; learning_rate : 2.5510204081632656e-07 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.63295724005567
    Epoch 0, Loss: 1.7030458990151598, fit: 0.15
    Epoch 0, Loss: 1.7030458990151598
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.63295724005567_fit_0.15_2024-01-03_172107
  self.fit : 0.15
  self.loss : 1.63295724005567
  current_accuracy : 0.1221
   Accuracy mean: 0.1221
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              3.188775510204082e-05
      batch rate adapted learning_rate :              3.188775510204082e-07
    epoch : 0 ; learning_rate : 3.188775510204082e-07 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.7373216146770922
    Epoch 0, Loss: 1.8027341737639404, fit: 0.08333333333333333
    Epoch 0, Loss: 1.8027341737639404
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.781487910349994_fit_0.08333333333333333_2024-01-03_172110
  self.fit : 0.08333333333333333
  self.loss : 1.781487910349994
  current_accuracy : 0.0725
   Accuracy mean: 0.0725
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              3.826530612244898e-05
      batch rate adapted learning_rate :              3.8265306122448977e-07
    epoch : 0 ; learning_rate : 3.8265306122448977e-07 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.702880040788037
    Epoch 0, Loss: 1.742805054269627, fit: 0.08833333333333333
    Epoch 0, Loss: 1.742805054269627
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7564883526499768_fit_0.08833333333333333_2024-01-03_172113
  self.fit : 0.08833333333333333
  self.loss : 1.7564883526499768
  current_accuracy : 0.0965
   Accuracy mean: 0.0965
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              0.00012755102040816328
      batch rate adapted learning_rate :              1.2755102040816329e-06
    epoch : 0 ; learning_rate : 1.2755102040816329e-06 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.691691450326509
    Epoch 0, Loss: 1.7399912001232762, fit: 0.12833333333333333
    Epoch 0, Loss: 1.7399912001232762
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.691691450326509_fit_0.12833333333333333_2024-01-03_172116
  self.fit : 0.12833333333333333
  self.loss : 1.691691450326509
  current_accuracy : 0.0962
   Accuracy mean: 0.0962
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              0.0011479591836734695
      batch rate adapted learning_rate :              1.1479591836734695e-05
    epoch : 0 ; learning_rate : 1.1479591836734695e-05 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.758557030242692
    Epoch 0, Loss: 1.8216090867050605, fit: 0.051666666666666666
    Epoch 0, Loss: 1.8216090867050605
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.8373833012857204_fit_0.051666666666666666_2024-01-03_172118
  self.fit : 0.051666666666666666
  self.loss : 1.8373833012857204
  current_accuracy : 0.0525
   Accuracy mean: 0.0525
  Error saving file: doc/out/test_combinations_results/20240103172119.
  normalized_accuracies :      [0.70977011 0.61781609 0.33764368 0.45977011 0.61781609 0.56609195
   0.52729885 1.         0.28735632 0.63218391 0.62787356 0.        ]
batch_rate :  0.001
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.001
      neuron quantity adapted learning_rate :              1.2755102040816326e-67
      batch rate adapted learning_rate :              1.2755102040816326e-70
    epoch : 0 ; learning_rate : 1.2755102040816326e-70 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4218009361127424
    Epoch 0, Loss: 1.726948543137531, fit: 0.05
    Epoch 0, Loss: 1.726948543137531
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.8403127510959967_fit_0.05_2024-01-03_172128
  self.fit : 0.05
  self.loss : 1.8403127510959967
  current_accuracy : 0.1106
   Accuracy mean: 0.1106
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.001
      neuron quantity adapted learning_rate :              1.2755102040816327e-36
      batch rate adapted learning_rate :              1.2755102040816327e-39
    epoch : 0 ; learning_rate : 1.2755102040816327e-39 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5249920090375544
    Epoch 0, Loss: 1.77178860278114, fit: 0.1
    Epoch 0, Loss: 1.77178860278114
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7502350352554812_fit_0.1_2024-01-03_172135
  self.fit : 0.1
  self.loss : 1.7502350352554812
  current_accuracy : 0.084
   Accuracy mean: 0.084
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.001
      neuron quantity adapted learning_rate :              1.2755102040816328e-20
      batch rate adapted learning_rate :              1.2755102040816329e-23
    epoch : 0 ; learning_rate : 1.2755102040816329e-23 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.3384337141385374
    Epoch 0, Loss: 1.6992908510692672, fit: 0.15
    Epoch 0, Loss: 1.6992908510692672
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.64840719475488_fit_0.15_2024-01-03_172143
  self.fit : 0.15
  self.loss : 1.64840719475488
  current_accuracy : 0.1222
   Accuracy mean: 0.1222
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.001
      neuron quantity adapted learning_rate :              1.2755102040816326e-13
      batch rate adapted learning_rate :              1.2755102040816327e-16
    epoch : 0 ; learning_rate : 1.2755102040816327e-16 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4241739853701239
    Epoch 0, Loss: 1.749104867968142, fit: 0.1
    Epoch 0, Loss: 1.749104867968142
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.719862679750038_fit_0.1_2024-01-03_172150
  self.fit : 0.1
  self.loss : 1.719862679750038
  current_accuracy : 0.0997
   Accuracy mean: 0.0997
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.001
      neuron quantity adapted learning_rate :              1.2755102040816326e-10
      batch rate adapted learning_rate :              1.2755102040816326e-13
    epoch : 0 ; learning_rate : 1.2755102040816326e-13 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5086586976699152
    Epoch 0, Loss: 1.805143119653123, fit: 0.03333333333333333
    Epoch 0, Loss: 1.805143119653123
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.8993079474958094_fit_0.03333333333333333_2024-01-03_172157
  self.fit : 0.03333333333333333
  self.loss : 1.8993079474958094
  current_accuracy : 0.0654
   Accuracy mean: 0.0654
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.001
      neuron quantity adapted learning_rate :              1.2755102040816328e-07
      batch rate adapted learning_rate :              1.2755102040816329e-10
    epoch : 0 ; learning_rate : 1.2755102040816329e-10 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4401070145569383
    Epoch 0, Loss: 1.7283901821857037, fit: 0.15
    Epoch 0, Loss: 1.7283901821857037
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6273598860974954_fit_0.15_2024-01-03_172205
  self.fit : 0.15
  self.loss : 1.6273598860974954
  current_accuracy : 0.1049
   Accuracy mean: 0.1049
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.001
      neuron quantity adapted learning_rate :              1.2755102040816327e-06
      batch rate adapted learning_rate :              1.2755102040816327e-09
    epoch : 0 ; learning_rate : 1.2755102040816327e-09 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5763797654892913
    Epoch 0, Loss: 1.8243085854641587, fit: 0.1
    Epoch 0, Loss: 1.8243085854641587
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7727233604226156_fit_0.1_2024-01-03_172212
  self.fit : 0.1
  self.loss : 1.7727233604226156
  current_accuracy : 0.0706
   Accuracy mean: 0.0706
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.001
      neuron quantity adapted learning_rate :              2.5510204081632654e-05
      batch rate adapted learning_rate :              2.5510204081632655e-08
    epoch : 0 ; learning_rate : 2.5510204081632655e-08 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4908978709957816
    Epoch 0, Loss: 1.766624432364225, fit: 0.11666666666666667
    Epoch 0, Loss: 1.766624432364225
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.6888291570614842_fit_0.11666666666666667_2024-01-03_172219
  self.fit : 0.11666666666666667
  self.loss : 1.6888291570614842
  current_accuracy : 0.0861
   Accuracy mean: 0.0861
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.001
      neuron quantity adapted learning_rate :              3.188775510204082e-05
      batch rate adapted learning_rate :              3.188775510204082e-08
    epoch : 0 ; learning_rate : 3.188775510204082e-08 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.542391572648584
    Epoch 0, Loss: 1.8038958956120166, fit: 0.05
    Epoch 0, Loss: 1.8038958956120166
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.8210270457028912_fit_0.05_2024-01-03_172227
  self.fit : 0.05
  self.loss : 1.8210270457028912
  current_accuracy : 0.0735
   Accuracy mean: 0.0735
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      neuron quantity adapted learning_rate :              3.826530612244898e-05
      batch rate adapted learning_rate :              3.826530612244898e-08
    epoch : 0 ; learning_rate : 3.826530612244898e-08 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.497320319303384
    Epoch 0, Loss: 1.7506121404339015, fit: 0.13333333333333333
    Epoch 0, Loss: 1.7506121404339015
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6606657103869376_fit_0.13333333333333333_2024-01-03_172234
  self.fit : 0.13333333333333333
  self.loss : 1.6606657103869376
  current_accuracy : 0.0901
   Accuracy mean: 0.0901
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.001
      neuron quantity adapted learning_rate :              0.00012755102040816328
      batch rate adapted learning_rate :              1.2755102040816328e-07
    epoch : 0 ; learning_rate : 1.2755102040816328e-07 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.474782978805725
    Epoch 0, Loss: 1.773527909742418, fit: 0.11666666666666667
    Epoch 0, Loss: 1.773527909742418
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7250615433137044_fit_0.11666666666666667_2024-01-03_172242
  self.fit : 0.11666666666666667
  self.loss : 1.7250615433137044
  current_accuracy : 0.0886
   Accuracy mean: 0.0886
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
      neuron quantity adapted learning_rate :              0.0011479591836734695
      batch rate adapted learning_rate :              1.1479591836734695e-06
    epoch : 0 ; learning_rate : 1.1479591836734695e-06 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4483089983834971
    Epoch 0, Loss: 1.7113659535854284, fit: 0.16666666666666666
    Epoch 0, Loss: 1.7113659535854284
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.6457733478596144_fit_0.16666666666666666_2024-01-03_172249
  self.fit : 0.16666666666666666
  self.loss : 1.6457733478596144
  current_accuracy : 0.1132
   Accuracy mean: 0.1132
  Error saving file: doc/out/test_combinations_results/20240103172249.
  normalized_accuracies :      [0.79577465 0.32746479 1.         0.60387324 0.         0.69542254
   0.0915493  0.36443662 0.14260563 0.43485915 0.4084507  0.8415493 ]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  1.0
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.84e-62
      batch rate adapted learning_rate :              7.84e-62
    epoch : 0 ; learning_rate : 7.84e-62 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.8294177441865924
    Epoch 0, Loss: 1.8294177441865924, fit: 0.05628333333333333
    Epoch 0, Loss: 1.8294177441865924
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.8294177441865924_fit_0.05628333333333333_2024-01-03_172951
  self.fit : 0.05628333333333333
  self.loss : 1.8294177441865924
  current_accuracy : 0.0536
   Accuracy mean: 0.0536
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.8400000000000005e-31
      batch rate adapted learning_rate :              7.8400000000000005e-31
    epoch : 0 ; learning_rate : 7.8400000000000005e-31 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7960208439038543
    Epoch 0, Loss: 1.7960208439038543, fit: 0.06876666666666667
    Epoch 0, Loss: 1.7960208439038543
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7960208439038543_fit_0.06876666666666667_2024-01-03_172954
  self.fit : 0.06876666666666667
  self.loss : 1.7960208439038543
  current_accuracy : 0.0654
   Accuracy mean: 0.0654
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.840000000000001e-15
      batch rate adapted learning_rate :              7.840000000000001e-15
    epoch : 0 ; learning_rate : 7.840000000000001e-15 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7786741908895625
    Epoch 0, Loss: 1.7786741908895625, fit: 0.08238333333333334
    Epoch 0, Loss: 1.7786741908895625
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7786741908895625_fit_0.08238333333333334_2024-01-03_172957
  self.fit : 0.08238333333333334
  self.loss : 1.7786741908895625
  current_accuracy : 0.0829
   Accuracy mean: 0.0829
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.84e-08
      batch rate adapted learning_rate :              7.84e-08
    epoch : 0 ; learning_rate : 7.84e-08 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7655620772070157
    Epoch 0, Loss: 1.7655620772070157, fit: 0.0869
    Epoch 0, Loss: 1.7655620772070157
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7655620772070157_fit_0.0869_2024-01-03_172959
  self.fit : 0.0869
  self.loss : 1.7655620772070157
  current_accuracy : 0.0866
   Accuracy mean: 0.0866
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.84e-05
      batch rate adapted learning_rate :              7.84e-05
    epoch : 0 ; learning_rate : 7.84e-05 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6882894981209509
    Epoch 0, Loss: 1.6882894981209509, fit: 0.13725
    Epoch 0, Loss: 1.6882894981209509
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.6882894981209509_fit_0.13725_2024-01-03_173002
  self.fit : 0.13725
  self.loss : 1.6882894981209509
  current_accuracy : 0.1419
   Accuracy mean: 0.1419
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              0.0784
      batch rate adapted learning_rate :              0.0784
    epoch : 0 ; learning_rate : 0.0784 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7420609493064159
    Epoch 0, Loss: 1.7420609493064159, fit: 0.09951666666666667
    Epoch 0, Loss: 1.7420609493064159
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7420609493064159_fit_0.09951666666666667_2024-01-03_173005
  self.fit : 0.09951666666666667
  self.loss : 1.7420609493064159
  current_accuracy : 0.1239
   Accuracy mean: 0.1239
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              0.784
      batch rate adapted learning_rate :              0.784
    epoch : 0 ; learning_rate : 0.784 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7257502827395454
    Epoch 0, Loss: 1.7257502827395454, fit: 0.10805
    Epoch 0, Loss: 1.7257502827395454
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7257502827395454_fit_0.10805_2024-01-03_173007
  self.fit : 0.10805
  self.loss : 1.7257502827395454
  current_accuracy : 0.1604
   Accuracy mean: 0.1604
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              15.68
      batch rate adapted learning_rate :              15.68
    epoch : 0 ; learning_rate : 15.68 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.779337638102694
    Epoch 0, Loss: 1.779337638102694, fit: 0.08806666666666667
    Epoch 0, Loss: 1.779337638102694
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.779337638102694_fit_0.08806666666666667_2024-01-03_173010
  self.fit : 0.08806666666666667
  self.loss : 1.779337638102694
  current_accuracy : 0.3558
   Accuracy mean: 0.3558
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              19.6
      batch rate adapted learning_rate :              19.6
    epoch : 0 ; learning_rate : 19.6 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6529546360937208
    Epoch 0, Loss: 1.6529546360937208, fit: 0.1461
    Epoch 0, Loss: 1.6529546360937208
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.6529546360937208_fit_0.1461_2024-01-03_173013
  self.fit : 0.1461
  self.loss : 1.6529546360937208
  current_accuracy : 0.3358
   Accuracy mean: 0.3358
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              23.52
      batch rate adapted learning_rate :              23.52
    epoch : 0 ; learning_rate : 23.52 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6948479318935201
    Epoch 0, Loss: 1.6948479318935201, fit: 0.11948333333333333
    Epoch 0, Loss: 1.6948479318935201
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6948479318935201_fit_0.11948333333333333_2024-01-03_173015
  self.fit : 0.11948333333333333
  self.loss : 1.6948479318935201
  current_accuracy : 0.4109
   Accuracy mean: 0.4109
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              78.4
      batch rate adapted learning_rate :              78.4
    epoch : 0 ; learning_rate : 78.4 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6940209613447264
    Epoch 0, Loss: 1.6940209613447264, fit: 0.12691666666666668
    Epoch 0, Loss: 1.6940209613447264
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6940209613447264_fit_0.12691666666666668_2024-01-03_173018
  self.fit : 0.12691666666666668
  self.loss : 1.6940209613447264
  current_accuracy : 0.2245
   Accuracy mean: 0.2245
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              705.6
      batch rate adapted learning_rate :              705.6
    epoch : 0 ; learning_rate : 705.6 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.8397272067833705
    Epoch 0, Loss: 1.8397272067833705, fit: 0.053733333333333334
    Epoch 0, Loss: 1.8397272067833705
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.8397272067833705_fit_0.053733333333333334_2024-01-03_173021
  self.fit : 0.053733333333333334
  self.loss : 1.8397272067833705
  current_accuracy : 0.123
   Accuracy mean: 0.123
  Error saving file: doc/out/test_combinations_results/20240103173021.
  normalized_accuracies :      [0.         0.03302547 0.08200392 0.09235936 0.24713126 0.19675343
   0.29890848 0.84578785 0.78981248 1.         0.47830954 0.19423454]
batch_rate :  0.5
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.84e-62
      batch rate adapted learning_rate :              3.92e-62
    epoch : 0 ; learning_rate : 3.92e-62 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.723732414112323
    Epoch 0, Loss: 1.7259479463503409, fit: 0.1057
    Epoch 0, Loss: 1.7259479463503409
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7281634785883588_fit_0.1057_2024-01-03_173025
  self.fit : 0.1057
  self.loss : 1.7281634785883588
  current_accuracy : 0.1073
   Accuracy mean: 0.1073
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.8400000000000005e-31
      batch rate adapted learning_rate :              3.9200000000000003e-31
    epoch : 0 ; learning_rate : 3.9200000000000003e-31 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7579656723464778
    Epoch 0, Loss: 1.7599170472738277, fit: 0.09286666666666667
    Epoch 0, Loss: 1.7599170472738277
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7618684222011778_fit_0.09286666666666667_2024-01-03_173027
  self.fit : 0.09286666666666667
  self.loss : 1.7618684222011778
  current_accuracy : 0.09
   Accuracy mean: 0.09
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.840000000000001e-15
      batch rate adapted learning_rate :              3.9200000000000006e-15
    epoch : 0 ; learning_rate : 3.9200000000000006e-15 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7112458559302375
    Epoch 0, Loss: 1.7133246836895277, fit: 0.11216666666666666
    Epoch 0, Loss: 1.7133246836895277
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.715403511448818_fit_0.11216666666666666_2024-01-03_173030
  self.fit : 0.11216666666666666
  self.loss : 1.715403511448818
  current_accuracy : 0.1157
   Accuracy mean: 0.1157
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.84e-08
      batch rate adapted learning_rate :              3.92e-08
    epoch : 0 ; learning_rate : 3.92e-08 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.792122690980966
    Epoch 0, Loss: 1.7969047908027609, fit: 0.07536666666666667
    Epoch 0, Loss: 1.7969047908027609
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.8016868906245558_fit_0.07536666666666667_2024-01-03_173032
  self.fit : 0.07536666666666667
  self.loss : 1.8016868906245558
  current_accuracy : 0.075
   Accuracy mean: 0.075
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.84e-05
      batch rate adapted learning_rate :              3.92e-05
    epoch : 0 ; learning_rate : 3.92e-05 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7511658483877193
    Epoch 0, Loss: 1.757068833273629, fit: 0.10006666666666666
    Epoch 0, Loss: 1.757068833273629
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7511658483877193_fit_0.10006666666666666_2024-01-03_173035
  self.fit : 0.10006666666666666
  self.loss : 1.7511658483877193
  current_accuracy : 0.0949
   Accuracy mean: 0.0949
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              0.0784
      batch rate adapted learning_rate :              0.0392
    epoch : 0 ; learning_rate : 0.0392 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.6857865942083292
    Epoch 0, Loss: 1.6976879406281966, fit: 0.12376666666666666
    Epoch 0, Loss: 1.6976879406281966
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6857865942083292_fit_0.12376666666666666_2024-01-03_173037
  self.fit : 0.12376666666666666
  self.loss : 1.6857865942083292
  current_accuracy : 0.1388
   Accuracy mean: 0.1388
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              0.784
      batch rate adapted learning_rate :              0.392
    epoch : 0 ; learning_rate : 0.392 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.3630470741887672
    Epoch 0, Loss: 1.586193330650742, fit: 0.2947666666666667
    Epoch 0, Loss: 1.586193330650742
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.3630470741887672_fit_0.2947666666666667_2024-01-03_173040
  self.fit : 0.2947666666666667
  self.loss : 1.3630470741887672
  current_accuracy : 0.322
   Accuracy mean: 0.322
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              15.68
      batch rate adapted learning_rate :              7.84
    epoch : 0 ; learning_rate : 7.84 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.3528043137803882
    Epoch 0, Loss: 1.5545265228848684, fit: 0.3163666666666667
    Epoch 0, Loss: 1.5545265228848684
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.3528043137803882_fit_0.3163666666666667_2024-01-03_173042
  self.fit : 0.3163666666666667
  self.loss : 1.3528043137803882
  current_accuracy : 0.098
   Accuracy mean: 0.098
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              19.6
      batch rate adapted learning_rate :              9.8
    epoch : 0 ; learning_rate : 9.8 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.5242684044636452
    Epoch 0, Loss: 1.6337153511309421, fit: 0.22453333333333333
    Epoch 0, Loss: 1.6337153511309421
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.5242684044636452_fit_0.22453333333333333_2024-01-03_173045
  self.fit : 0.22453333333333333
  self.loss : 1.5242684044636452
  current_accuracy : 0.1032
   Accuracy mean: 0.1032
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              23.52
      batch rate adapted learning_rate :              11.76
    epoch : 0 ; learning_rate : 11.76 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.4409470709733347
    Epoch 0, Loss: 1.6162663496881133, fit: 0.26603333333333334
    Epoch 0, Loss: 1.6162663496881133
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.4409470709733347_fit_0.26603333333333334_2024-01-03_173048
  self.fit : 0.26603333333333334
  self.loss : 1.4409470709733347
  current_accuracy : 0.101
   Accuracy mean: 0.101
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              78.4
      batch rate adapted learning_rate :              39.2
    epoch : 0 ; learning_rate : 39.2 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.3609116335143314
    Epoch 0, Loss: 1.5004030132159876, fit: 0.3007666666666667
    Epoch 0, Loss: 1.5004030132159876
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.3609116335143314_fit_0.3007666666666667_2024-01-03_173050
  self.fit : 0.3007666666666667
  self.loss : 1.3609116335143314
  current_accuracy : 0.101
   Accuracy mean: 0.101
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              705.6
      batch rate adapted learning_rate :              352.8
    epoch : 0 ; learning_rate : 352.8 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7046953118740809
    Epoch 0, Loss: 1.7359940028137582, fit: 0.11573333333333333
    Epoch 0, Loss: 1.7359940028137582
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7672926937534355_fit_0.11573333333333333_2024-01-03_173053
  self.fit : 0.11573333333333333
  self.loss : 1.7672926937534355
  current_accuracy : 0.1028
   Accuracy mean: 0.1028
  Error saving file: doc/out/test_combinations_results/20240103173053.
  normalized_accuracies :      [0.13076923 0.06072874 0.16477733 0.         0.0805668  0.2582996
   1.         0.09311741 0.11417004 0.10526316 0.10526316 0.11255061]
batch_rate :  0.2
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              7.84e-62
      batch rate adapted learning_rate :              1.568e-62
    epoch : 0 ; learning_rate : 1.568e-62 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.713776887074519
    Epoch 0, Loss: 1.7170629719006198, fit: 0.10775
    Epoch 0, Loss: 1.7170629719006198
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.724312367186696_fit_0.10775_2024-01-03_173056
  self.fit : 0.10775
  self.loss : 1.724312367186696
  current_accuracy : 0.1054
   Accuracy mean: 0.1054
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              7.8400000000000005e-31
      batch rate adapted learning_rate :              1.5680000000000002e-31
    epoch : 0 ; learning_rate : 1.5680000000000002e-31 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7310461311632943
    Epoch 0, Loss: 1.740202177122156, fit: 0.10608333333333334
    Epoch 0, Loss: 1.740202177122156
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7310461311632943_fit_0.10608333333333334_2024-01-03_173059
  self.fit : 0.10608333333333334
  self.loss : 1.7310461311632943
  current_accuracy : 0.101
   Accuracy mean: 0.101
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              7.840000000000001e-15
      batch rate adapted learning_rate :              1.5680000000000002e-15
    epoch : 0 ; learning_rate : 1.5680000000000002e-15 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.6366757382546506
    Epoch 0, Loss: 1.647981705420545, fit: 0.14316666666666666
    Epoch 0, Loss: 1.647981705420545
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.647689005253593_fit_0.14316666666666666_2024-01-03_173101
  self.fit : 0.14316666666666666
  self.loss : 1.647689005253593
  current_accuracy : 0.1346
   Accuracy mean: 0.1346
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              7.84e-08
      batch rate adapted learning_rate :              1.5680000000000002e-08
    epoch : 0 ; learning_rate : 1.5680000000000002e-08 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.725299865587301
    Epoch 0, Loss: 1.728951132131829, fit: 0.11225
    Epoch 0, Loss: 1.728951132131829
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.728896531360315_fit_0.11225_2024-01-03_173104
  self.fit : 0.11225
  self.loss : 1.728896531360315
  current_accuracy : 0.1104
   Accuracy mean: 0.1104
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              7.84e-05
      batch rate adapted learning_rate :              1.568e-05
    epoch : 0 ; learning_rate : 1.568e-05 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7520769084422145
    Epoch 0, Loss: 1.7547516324401107, fit: 0.09333333333333334
    Epoch 0, Loss: 1.7547516324401107
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7571723624962647_fit_0.09333333333333334_2024-01-03_173106
  self.fit : 0.09333333333333334
  self.loss : 1.7571723624962647
  current_accuracy : 0.0951
   Accuracy mean: 0.0951
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              0.0784
      batch rate adapted learning_rate :              0.01568
    epoch : 0 ; learning_rate : 0.01568 ; fit : 0.0
    batch_size :          12000
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  1.0
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.84e-62
      batch rate adapted learning_rate :              7.84e-62
    epoch : 0 ; learning_rate : 7.84e-62 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.832718070296947
    Epoch 0, Loss: 1.832718070296947, fit: 0.05205
    Epoch 0, Loss: 1.832718070296947
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.832718070296947_fit_0.05205_2024-01-03_173931
  self.fit : 0.05205
  self.loss : 1.832718070296947
  current_accuracy : 0.0525
   Accuracy mean: 0.0525
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.8400000000000005e-31
      batch rate adapted learning_rate :              7.8400000000000005e-31
    epoch : 0 ; learning_rate : 7.8400000000000005e-31 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7656719441991122
    Epoch 0, Loss: 1.7656719441991122, fit: 0.0872
    Epoch 0, Loss: 1.7656719441991122
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7656719441991122_fit_0.0872_2024-01-03_173934
  self.fit : 0.0872
  self.loss : 1.7656719441991122
  current_accuracy : 0.0927
   Accuracy mean: 0.0927
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.840000000000001e-15
      batch rate adapted learning_rate :              7.840000000000001e-15
    epoch : 0 ; learning_rate : 7.840000000000001e-15 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.676578591130677
    Epoch 0, Loss: 1.676578591130677, fit: 0.13253333333333334
    Epoch 0, Loss: 1.676578591130677
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.676578591130677_fit_0.13253333333333334_2024-01-03_173936
  self.fit : 0.13253333333333334
  self.loss : 1.676578591130677
  current_accuracy : 0.1293
   Accuracy mean: 0.1293
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.84e-08
      batch rate adapted learning_rate :              7.84e-08
    epoch : 0 ; learning_rate : 7.84e-08 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.787712697695942
    Epoch 0, Loss: 1.787712697695942, fit: 0.08026666666666667
    Epoch 0, Loss: 1.787712697695942
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.787712697695942_fit_0.08026666666666667_2024-01-03_173939
  self.fit : 0.08026666666666667
  self.loss : 1.787712697695942
  current_accuracy : 0.0829
   Accuracy mean: 0.0829
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.84e-05
      batch rate adapted learning_rate :              7.84e-05
    epoch : 0 ; learning_rate : 7.84e-05 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.737866774842999
    Epoch 0, Loss: 1.737866774842999, fit: 0.10233333333333333
    Epoch 0, Loss: 1.737866774842999
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.737866774842999_fit_0.10233333333333333_2024-01-03_173941
  self.fit : 0.10233333333333333
  self.loss : 1.737866774842999
  current_accuracy : 0.0984
   Accuracy mean: 0.0984
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              0.0784
      batch rate adapted learning_rate :              0.0784
    epoch : 0 ; learning_rate : 0.0784 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7630551235776957
    Epoch 0, Loss: 1.7630551235776957, fit: 0.09135
    Epoch 0, Loss: 1.7630551235776957
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7630551235776957_fit_0.09135_2024-01-03_173944
  self.fit : 0.09135
  self.loss : 1.7630551235776957
  current_accuracy : 0.0937
   Accuracy mean: 0.0937
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              0.784
      batch rate adapted learning_rate :              0.784
    epoch : 0 ; learning_rate : 0.784 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7373853770464462
    Epoch 0, Loss: 1.7373853770464462, fit: 0.10406666666666667
    Epoch 0, Loss: 1.7373853770464462
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7373853770464462_fit_0.10406666666666667_2024-01-03_173946
  self.fit : 0.10406666666666667
  self.loss : 1.7373853770464462
  current_accuracy : 0.2472
   Accuracy mean: 0.2472
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              15.68
      batch rate adapted learning_rate :              15.68
    epoch : 0 ; learning_rate : 15.68 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.814990851630107
    Epoch 0, Loss: 1.814990851630107, fit: 0.06285
    Epoch 0, Loss: 1.814990851630107
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.814990851630107_fit_0.06285_2024-01-03_173949
  self.fit : 0.06285
  self.loss : 1.814990851630107
  current_accuracy : 0.3141
   Accuracy mean: 0.3141
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              19.6
      batch rate adapted learning_rate :              19.6
    epoch : 0 ; learning_rate : 19.6 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.8030889175983655
    Epoch 0, Loss: 1.8030889175983655, fit: 0.07573333333333333
    Epoch 0, Loss: 1.8030889175983655
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.8030889175983655_fit_0.07573333333333333_2024-01-03_173951
  self.fit : 0.07573333333333333
  self.loss : 1.8030889175983655
  current_accuracy : 0.3627
   Accuracy mean: 0.3627
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              23.52
      batch rate adapted learning_rate :              23.52
    epoch : 0 ; learning_rate : 23.52 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6653583194540855
    Epoch 0, Loss: 1.6653583194540855, fit: 0.1371
    Epoch 0, Loss: 1.6653583194540855
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6653583194540855_fit_0.1371_2024-01-03_173954
  self.fit : 0.1371
  self.loss : 1.6653583194540855
  current_accuracy : 0.3558
   Accuracy mean: 0.3558
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              78.4
      batch rate adapted learning_rate :              78.4
    epoch : 0 ; learning_rate : 78.4 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7469960630808976
    Epoch 0, Loss: 1.7469960630808976, fit: 0.10365
    Epoch 0, Loss: 1.7469960630808976
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7469960630808976_fit_0.10365_2024-01-03_173957
  self.fit : 0.10365
  self.loss : 1.7469960630808976
  current_accuracy : 0.1136
   Accuracy mean: 0.1136
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              705.6
      batch rate adapted learning_rate :              705.6
    epoch : 0 ; learning_rate : 705.6 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7512323346712577
    Epoch 0, Loss: 1.7512323346712577, fit: 0.0904
    Epoch 0, Loss: 1.7512323346712577
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7512323346712577_fit_0.0904_2024-01-03_173959
  self.fit : 0.0904
  self.loss : 1.7512323346712577
  current_accuracy : 0.1895
   Accuracy mean: 0.1895
  Error saving file: doc/out/test_combinations_results/20240103174000.
  normalized_accuracies :      [0.         0.12959381 0.24758221 0.09800129 0.14796905 0.13281754
   0.62765957 0.84332689 1.         0.97775629 0.1969697  0.44165055]
batch_rate :  0.5
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.84e-62
      batch rate adapted learning_rate :              3.92e-62
    epoch : 0 ; learning_rate : 3.92e-62 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7854267528789178
    Epoch 0, Loss: 1.786180183579964, fit: 0.07803333333333333
    Epoch 0, Loss: 1.786180183579964
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.78693361428101_fit_0.07803333333333333_2024-01-03_174003
  self.fit : 0.07803333333333333
  self.loss : 1.78693361428101
  current_accuracy : 0.0818
   Accuracy mean: 0.0818
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.8400000000000005e-31
      batch rate adapted learning_rate :              3.9200000000000003e-31
    epoch : 0 ; learning_rate : 3.9200000000000003e-31 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.6988953045808661
    Epoch 0, Loss: 1.698938602421129, fit: 0.11983333333333333
    Epoch 0, Loss: 1.698938602421129
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.698981900261392_fit_0.11983333333333333_2024-01-03_174006
  self.fit : 0.11983333333333333
  self.loss : 1.698981900261392
  current_accuracy : 0.1145
   Accuracy mean: 0.1145
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.840000000000001e-15
      batch rate adapted learning_rate :              3.9200000000000006e-15
    epoch : 0 ; learning_rate : 3.9200000000000006e-15 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.6891225479229917
    Epoch 0, Loss: 1.6894323209318038, fit: 0.12456666666666667
    Epoch 0, Loss: 1.6894323209318038
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.6891225479229917_fit_0.12456666666666667_2024-01-03_174008
  self.fit : 0.12456666666666667
  self.loss : 1.6891225479229917
  current_accuracy : 0.1217
   Accuracy mean: 0.1217
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.84e-08
      batch rate adapted learning_rate :              3.92e-08
    epoch : 0 ; learning_rate : 3.92e-08 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.6793980685748444
    Epoch 0, Loss: 1.6814067073626358, fit: 0.12843333333333334
    Epoch 0, Loss: 1.6814067073626358
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.6793980685748444_fit_0.12843333333333334_2024-01-03_174011
  self.fit : 0.12843333333333334
  self.loss : 1.6793980685748444
  current_accuracy : 0.1226
   Accuracy mean: 0.1226
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.84e-05
      batch rate adapted learning_rate :              3.92e-05
    epoch : 0 ; learning_rate : 3.92e-05 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.6894168607666167
    Epoch 0, Loss: 1.693506882533222, fit: 0.1216
    Epoch 0, Loss: 1.693506882533222
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.6975969042998273_fit_0.1216_2024-01-03_174013
  self.fit : 0.1216
  self.loss : 1.6975969042998273
  current_accuracy : 0.122
   Accuracy mean: 0.122
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              0.0784
      batch rate adapted learning_rate :              0.0392
    epoch : 0 ; learning_rate : 0.0392 ; fit : 0.0
    batch_size :          30000
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  1.0
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.84e-62
      batch rate adapted learning_rate :              7.84e-62
    epoch : 0 ; learning_rate : 7.84e-62 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7829828954034679
    Epoch 0, Loss: 1.7829828954034679, fit: 0.08331666666666666
    Epoch 0, Loss: 1.7829828954034679
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7829828954034679_fit_0.08331666666666666_2024-01-03_174344
  self.fit : 0.08331666666666666
  self.loss : 1.7829828954034679
  current_accuracy : 0.0824
   Accuracy mean: 0.0824
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.8400000000000005e-31
      batch rate adapted learning_rate :              7.8400000000000005e-31
    epoch : 0 ; learning_rate : 7.8400000000000005e-31 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6892905327392134
    Epoch 0, Loss: 1.6892905327392134, fit: 0.12415
    Epoch 0, Loss: 1.6892905327392134
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.6892905327392134_fit_0.12415_2024-01-03_174346
  self.fit : 0.12415
  self.loss : 1.6892905327392134
  current_accuracy : 0.1295
   Accuracy mean: 0.1295
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.840000000000001e-15
      batch rate adapted learning_rate :              7.840000000000001e-15
    epoch : 0 ; learning_rate : 7.840000000000001e-15 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6509049540003957
    Epoch 0, Loss: 1.6509049540003957, fit: 0.14595
    Epoch 0, Loss: 1.6509049540003957
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.6509049540003957_fit_0.14595_2024-01-03_174349
  self.fit : 0.14595
  self.loss : 1.6509049540003957
  current_accuracy : 0.1483
   Accuracy mean: 0.1483
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.84e-08
      batch rate adapted learning_rate :              7.84e-08
    epoch : 0 ; learning_rate : 7.84e-08 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7037836181278316
    Epoch 0, Loss: 1.7037836181278316, fit: 0.119
    Epoch 0, Loss: 1.7037836181278316
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7037836181278316_fit_0.119_2024-01-03_174351
  self.fit : 0.119
  self.loss : 1.7037836181278316
  current_accuracy : 0.1156
   Accuracy mean: 0.1156
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.84e-05
      batch rate adapted learning_rate :              7.84e-05
    epoch : 0 ; learning_rate : 7.84e-05 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7452598350403155
    Epoch 0, Loss: 1.7452598350403155, fit: 0.09815
    Epoch 0, Loss: 1.7452598350403155
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7452598350403155_fit_0.09815_2024-01-03_174354
  self.fit : 0.09815
  self.loss : 1.7452598350403155
  current_accuracy : 0.1007
   Accuracy mean: 0.1007
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              0.0784
      batch rate adapted learning_rate :              0.0784
    epoch : 0 ; learning_rate : 0.0784 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7422571653762138
    Epoch 0, Loss: 1.7422571653762138, fit: 0.10576666666666666
    Epoch 0, Loss: 1.7422571653762138
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7422571653762138_fit_0.10576666666666666_2024-01-03_174356
  self.fit : 0.10576666666666666
  self.loss : 1.7422571653762138
  current_accuracy : 0.1491
   Accuracy mean: 0.1491
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              0.784
      batch rate adapted learning_rate :              0.784
    epoch : 0 ; learning_rate : 0.784 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.741985184133383
    Epoch 0, Loss: 1.741985184133383, fit: 0.10325
    Epoch 0, Loss: 1.741985184133383
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.741985184133383_fit_0.10325_2024-01-03_174359
  self.fit : 0.10325
  self.loss : 1.741985184133383
  current_accuracy : 0.2308
   Accuracy mean: 0.2308
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              15.68
      batch rate adapted learning_rate :              15.68
    epoch : 0 ; learning_rate : 15.68 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.762818628546191
    Epoch 0, Loss: 1.762818628546191, fit: 0.091
    Epoch 0, Loss: 1.762818628546191
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.762818628546191_fit_0.091_2024-01-03_174401
  self.fit : 0.091
  self.loss : 1.762818628546191
  current_accuracy : 0.1981
   Accuracy mean: 0.1981
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              19.6
      batch rate adapted learning_rate :              19.6
    epoch : 0 ; learning_rate : 19.6 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6932240801262917
    Epoch 0, Loss: 1.6932240801262917, fit: 0.12626666666666667
    Epoch 0, Loss: 1.6932240801262917
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.6932240801262917_fit_0.12626666666666667_2024-01-03_174404
  self.fit : 0.12626666666666667
  self.loss : 1.6932240801262917
  current_accuracy : 0.2569
   Accuracy mean: 0.2569
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              23.52
      batch rate adapted learning_rate :              23.52
    epoch : 0 ; learning_rate : 23.52 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6233674903254045
    Epoch 0, Loss: 1.6233674903254045, fit: 0.15985
    Epoch 0, Loss: 1.6233674903254045
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6233674903254045_fit_0.15985_2024-01-03_174407
  self.fit : 0.15985
  self.loss : 1.6233674903254045
  current_accuracy : 0.3106
   Accuracy mean: 0.3106
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              78.4
      batch rate adapted learning_rate :              78.4
    epoch : 0 ; learning_rate : 78.4 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7871288750534051
    Epoch 0, Loss: 1.7871288750534051, fit: 0.0831
    Epoch 0, Loss: 1.7871288750534051
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7871288750534051_fit_0.0831_2024-01-03_174409
  self.fit : 0.0831
  self.loss : 1.7871288750534051
  current_accuracy : 0.1251
   Accuracy mean: 0.1251
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              705.6
      batch rate adapted learning_rate :              705.6
    epoch : 0 ; learning_rate : 705.6 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7470881305641894
    Epoch 0, Loss: 1.7470881305641894, fit: 0.09431666666666666
    Epoch 0, Loss: 1.7470881305641894
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7470881305641894_fit_0.09431666666666666_2024-01-03_174412
  self.fit : 0.09431666666666666
  self.loss : 1.7470881305641894
  current_accuracy : 0.1712
   Accuracy mean: 0.1712
  Error saving file: doc/out/test_combinations_results/20240103174412.
  normalized_accuracies :      [0.         0.2063979  0.28878177 0.14548642 0.08019281 0.29228747
   0.65030675 0.50701139 0.76468011 1.         0.18711656 0.38913234]
batch_rate :  0.5
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.84e-62
      batch rate adapted learning_rate :              3.92e-62
    epoch : 0 ; learning_rate : 3.92e-62 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7402362746448532
    Epoch 0, Loss: 1.741661523254645, fit: 0.09493333333333333
    Epoch 0, Loss: 1.741661523254645
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7430867718644367_fit_0.09493333333333333_2024-01-03_174415
  self.fit : 0.09493333333333333
  self.loss : 1.7430867718644367
  current_accuracy : 0.0893
   Accuracy mean: 0.0893
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.8400000000000005e-31
      batch rate adapted learning_rate :              3.9200000000000003e-31
    epoch : 0 ; learning_rate : 3.9200000000000003e-31 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7319354140083039
    Epoch 0, Loss: 1.73448811340623, fit: 0.10693333333333334
    Epoch 0, Loss: 1.73448811340623
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7319354140083039_fit_0.10693333333333334_2024-01-03_174418
  self.fit : 0.10693333333333334
  self.loss : 1.7319354140083039
  current_accuracy : 0.1087
   Accuracy mean: 0.1087
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.840000000000001e-15
      batch rate adapted learning_rate :              3.9200000000000006e-15
    epoch : 0 ; learning_rate : 3.9200000000000006e-15 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7436325961361945
    Epoch 0, Loss: 1.7451469421137937, fit: 0.0995
    Epoch 0, Loss: 1.7451469421137937
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7466612880913932_fit_0.0995_2024-01-03_174421
  self.fit : 0.0995
  self.loss : 1.7466612880913932
  current_accuracy : 0.1044
   Accuracy mean: 0.1044
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  1.0
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.84e-62
      batch rate adapted learning_rate :              7.84e-62
      loss_factor :              1
      loss adapted learning_rate :              7.84e-62
    epoch : 0 ; learning_rate : 7.84e-62 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7915579512866993
    Epoch 0, Loss: 1.7915579512866993, fit: 0.07896666666666667
    Epoch 0, Loss: 1.7915579512866993
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7915579512866993_fit_0.07896666666666667_2024-01-03_180209
  self.fit : 0.07896666666666667
  self.loss : 1.7915579512866993
  current_accuracy : 0.0756
   Accuracy mean: 0.0756
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.8400000000000005e-31
      batch rate adapted learning_rate :              7.8400000000000005e-31
      loss_factor :              1
      loss adapted learning_rate :              7.8400000000000005e-31
    epoch : 0 ; learning_rate : 7.8400000000000005e-31 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7434718610414306
    Epoch 0, Loss: 1.7434718610414306, fit: 0.09793333333333333
    Epoch 0, Loss: 1.7434718610414306
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7434718610414306_fit_0.09793333333333333_2024-01-03_180212
  self.fit : 0.09793333333333333
  self.loss : 1.7434718610414306
  current_accuracy : 0.1016
   Accuracy mean: 0.1016
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.840000000000001e-15
      batch rate adapted learning_rate :              7.840000000000001e-15
      loss_factor :              1
      loss adapted learning_rate :              7.840000000000001e-15
    epoch : 0 ; learning_rate : 7.840000000000001e-15 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7185270942523572
    Epoch 0, Loss: 1.7185270942523572, fit: 0.10855
    Epoch 0, Loss: 1.7185270942523572
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7185270942523572_fit_0.10855_2024-01-03_180215
  self.fit : 0.10855
  self.loss : 1.7185270942523572
  current_accuracy : 0.1025
   Accuracy mean: 0.1025
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.84e-08
      batch rate adapted learning_rate :              7.84e-08
      loss_factor :              1
      loss adapted learning_rate :              7.84e-08
    epoch : 0 ; learning_rate : 7.84e-08 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.744023814940327
    Epoch 0, Loss: 1.744023814940327, fit: 0.09643333333333333
    Epoch 0, Loss: 1.744023814940327
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.744023814940327_fit_0.09643333333333333_2024-01-03_180217
  self.fit : 0.09643333333333333
  self.loss : 1.744023814940327
  current_accuracy : 0.092
   Accuracy mean: 0.092
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              7.84e-05
      batch rate adapted learning_rate :              7.84e-05
      loss_factor :              1
      loss adapted learning_rate :              7.84e-05
    epoch : 0 ; learning_rate : 7.84e-05 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.8140850129868342
    Epoch 0, Loss: 1.8140850129868342, fit: 0.0646
    Epoch 0, Loss: 1.8140850129868342
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.8140850129868342_fit_0.0646_2024-01-03_180220
  self.fit : 0.0646
  self.loss : 1.8140850129868342
  current_accuracy : 0.0677
   Accuracy mean: 0.0677
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              0.0784
      batch rate adapted learning_rate :              0.0784
      loss_factor :              1
      loss adapted learning_rate :              0.0784
    epoch : 0 ; learning_rate : 0.0784 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7691679964222584
    Epoch 0, Loss: 1.7691679964222584, fit: 0.08591666666666667
    Epoch 0, Loss: 1.7691679964222584
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7691679964222584_fit_0.08591666666666667_2024-01-03_180222
  self.fit : 0.08591666666666667
  self.loss : 1.7691679964222584
  current_accuracy : 0.115
   Accuracy mean: 0.115
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              0.784
      batch rate adapted learning_rate :              0.784
      loss_factor :              1
      loss adapted learning_rate :              0.784
    epoch : 0 ; learning_rate : 0.784 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.781548916388121
    Epoch 0, Loss: 1.781548916388121, fit: 0.08013333333333333
    Epoch 0, Loss: 1.781548916388121
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.781548916388121_fit_0.08013333333333333_2024-01-03_180225
  self.fit : 0.08013333333333333
  self.loss : 1.781548916388121
  current_accuracy : 0.3036
   Accuracy mean: 0.3036
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              15.68
      batch rate adapted learning_rate :              15.68
      loss_factor :              1
      loss adapted learning_rate :              15.68
    epoch : 0 ; learning_rate : 15.68 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7334183575658886
    Epoch 0, Loss: 1.7334183575658886, fit: 0.1072
    Epoch 0, Loss: 1.7334183575658886
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7334183575658886_fit_0.1072_2024-01-03_180227
  self.fit : 0.1072
  self.loss : 1.7334183575658886
  current_accuracy : 0.2277
   Accuracy mean: 0.2277
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              19.6
      batch rate adapted learning_rate :              19.6
      loss_factor :              1
      loss adapted learning_rate :              19.6
    epoch : 0 ; learning_rate : 19.6 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7359070658472102
    Epoch 0, Loss: 1.7359070658472102, fit: 0.1049
    Epoch 0, Loss: 1.7359070658472102
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7359070658472102_fit_0.1049_2024-01-03_180230
  self.fit : 0.1049
  self.loss : 1.7359070658472102
  current_accuracy : 0.3187
   Accuracy mean: 0.3187
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              23.52
      batch rate adapted learning_rate :              23.52
      loss_factor :              1
      loss adapted learning_rate :              23.52
    epoch : 0 ; learning_rate : 23.52 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7815177248622005
    Epoch 0, Loss: 1.7815177248622005, fit: 0.08138333333333334
    Epoch 0, Loss: 1.7815177248622005
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7815177248622005_fit_0.08138333333333334_2024-01-03_180232
  self.fit : 0.08138333333333334
  self.loss : 1.7815177248622005
  current_accuracy : 0.3892
   Accuracy mean: 0.3892
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              78.4
      batch rate adapted learning_rate :              78.4
      loss_factor :              1
      loss adapted learning_rate :              78.4
    epoch : 0 ; learning_rate : 78.4 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7275889304813332
    Epoch 0, Loss: 1.7275889304813332, fit: 0.10751666666666666
    Epoch 0, Loss: 1.7275889304813332
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7275889304813332_fit_0.10751666666666666_2024-01-03_180235
  self.fit : 0.10751666666666666
  self.loss : 1.7275889304813332
  current_accuracy : 0.1299
   Accuracy mean: 0.1299
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          1.0
      neuron quantity adapted learning_rate :              705.6
      batch rate adapted learning_rate :              705.6
      loss_factor :              1
      loss adapted learning_rate :              705.6
    epoch : 0 ; learning_rate : 705.6 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6736238485855741
    Epoch 0, Loss: 1.6736238485855741, fit: 0.13838333333333333
    Epoch 0, Loss: 1.6736238485855741
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.6736238485855741_fit_0.13838333333333333_2024-01-03_180238
  self.fit : 0.13838333333333333
  self.loss : 1.6736238485855741
  current_accuracy : 0.2001
   Accuracy mean: 0.2001
  Error saving file: doc/out/test_combinations_results/20240103180238.
  normalized_accuracies :      [0.02457232 0.10544323 0.10824261 0.0755832  0.         0.14712286
   0.73374806 0.49766719 0.7807154  1.         0.19346812 0.4118196 ]
batch_rate :  0.5
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.84e-62
      batch rate adapted learning_rate :              3.92e-62
      loss_factor :              1
      loss adapted learning_rate :              3.92e-62
    epoch : 0 ; learning_rate : 3.92e-62 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.675037933235557
    Epoch 0, Loss: 1.6762817581574918, fit: 0.135
    Epoch 0, Loss: 1.6762817581574918
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.675037933235557_fit_0.135_2024-01-03_180241
  self.fit : 0.135
  self.loss : 1.675037933235557
  current_accuracy : 0.1392
   Accuracy mean: 0.1392
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.8400000000000005e-31
      batch rate adapted learning_rate :              3.9200000000000003e-31
      loss_factor :              1
      loss adapted learning_rate :              3.9200000000000003e-31
    epoch : 0 ; learning_rate : 3.9200000000000003e-31 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7795428309328047
    Epoch 0, Loss: 1.7813289535702008, fit: 0.07486666666666666
    Epoch 0, Loss: 1.7813289535702008
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7831150762075971_fit_0.07486666666666666_2024-01-03_180244
  self.fit : 0.07486666666666666
  self.loss : 1.7831150762075971
  current_accuracy : 0.0779
   Accuracy mean: 0.0779
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.840000000000001e-15
      batch rate adapted learning_rate :              3.9200000000000006e-15
      loss_factor :              1
      loss adapted learning_rate :              3.9200000000000006e-15
    epoch : 0 ; learning_rate : 3.9200000000000006e-15 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.8151989181859918
    Epoch 0, Loss: 1.8159458044648016, fit: 0.0635
    Epoch 0, Loss: 1.8159458044648016
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.8151989181859918_fit_0.0635_2024-01-03_180246
  self.fit : 0.0635
  self.loss : 1.8151989181859918
  current_accuracy : 0.063
   Accuracy mean: 0.063
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.84e-08
      batch rate adapted learning_rate :              3.92e-08
      loss_factor :              1
      loss adapted learning_rate :              3.92e-08
    epoch : 0 ; learning_rate : 3.92e-08 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7338948190311667
    Epoch 0, Loss: 1.737207448074176, fit: 0.10616666666666667
    Epoch 0, Loss: 1.737207448074176
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7338948190311667_fit_0.10616666666666667_2024-01-03_180249
  self.fit : 0.10616666666666667
  self.loss : 1.7338948190311667
  current_accuracy : 0.1059
   Accuracy mean: 0.1059
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              7.84e-05
      batch rate adapted learning_rate :              3.92e-05
      loss_factor :              1
      loss adapted learning_rate :              3.92e-05
    epoch : 0 ; learning_rate : 3.92e-05 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.8380557125831583
    Epoch 0, Loss: 1.8390743346693554, fit: 0.05553333333333333
    Epoch 0, Loss: 1.8390743346693554
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.8400929567555528_fit_0.05553333333333333_2024-01-03_180251
  self.fit : 0.05553333333333333
  self.loss : 1.8400929567555528
  current_accuracy : 0.0525
   Accuracy mean: 0.0525
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              0.0784
      batch rate adapted learning_rate :              0.0392
      loss_factor :              1
      loss adapted learning_rate :              0.0392
    epoch : 0 ; learning_rate : 0.0392 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.762830324275237
    Epoch 0, Loss: 1.7812174946776844, fit: 0.08376666666666667
    Epoch 0, Loss: 1.7812174946776844
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.762830324275237_fit_0.08376666666666667_2024-01-03_180254
  self.fit : 0.08376666666666667
  self.loss : 1.762830324275237
  current_accuracy : 0.1057
   Accuracy mean: 0.1057
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              0.784
      batch rate adapted learning_rate :              0.392
      loss_factor :              1
      loss adapted learning_rate :              0.392
    epoch : 0 ; learning_rate : 0.392 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.5551334686704812
    Epoch 0, Loss: 1.6536709054706384, fit: 0.2037
    Epoch 0, Loss: 1.6536709054706384
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.5551334686704812_fit_0.2037_2024-01-03_180257
  self.fit : 0.2037
  self.loss : 1.5551334686704812
  current_accuracy : 0.47
   Accuracy mean: 0.47
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              15.68
      batch rate adapted learning_rate :              7.84
      loss_factor :              1
      loss adapted learning_rate :              7.84
    epoch : 0 ; learning_rate : 7.84 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.3571158742408862
    Epoch 0, Loss: 1.5611839740733253, fit: 0.3027666666666667
    Epoch 0, Loss: 1.5611839740733253
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.3571158742408862_fit_0.3027666666666667_2024-01-03_180259
  self.fit : 0.3027666666666667
  self.loss : 1.3571158742408862
  current_accuracy : 0.101
   Accuracy mean: 0.101
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              19.6
      batch rate adapted learning_rate :              9.8
      loss_factor :              1
      loss adapted learning_rate :              9.8
    epoch : 0 ; learning_rate : 9.8 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.4854520007653038
    Epoch 0, Loss: 1.5577761249172948, fit: 0.238
    Epoch 0, Loss: 1.5577761249172948
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.4854520007653038_fit_0.238_2024-01-03_180302
  self.fit : 0.238
  self.loss : 1.4854520007653038
  current_accuracy : 0.1135
   Accuracy mean: 0.1135
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              23.52
      batch rate adapted learning_rate :              11.76
      loss_factor :              1
      loss adapted learning_rate :              11.76
    epoch : 0 ; learning_rate : 11.76 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.248430980040542
    Epoch 0, Loss: 1.488396791417099, fit: 0.3597666666666667
    Epoch 0, Loss: 1.488396791417099
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.248430980040542_fit_0.3597666666666667_2024-01-03_180304
  self.fit : 0.3597666666666667
  self.loss : 1.248430980040542
  current_accuracy : 0.1032
   Accuracy mean: 0.1032
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              78.4
      batch rate adapted learning_rate :              39.2
      loss_factor :              1
      loss adapted learning_rate :              39.2
    epoch : 0 ; learning_rate : 39.2 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.6194201997796434
    Epoch 0, Loss: 1.7027753572883177, fit: 0.18013333333333334
    Epoch 0, Loss: 1.7027753572883177
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6194201997796434_fit_0.18013333333333334_2024-01-03_180307
  self.fit : 0.18013333333333334
  self.loss : 1.6194201997796434
  current_accuracy : 0.101
   Accuracy mean: 0.101
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.5
      neuron quantity adapted learning_rate :              705.6
      batch rate adapted learning_rate :              352.8
      loss_factor :              1
      loss adapted learning_rate :              352.8
    epoch : 0 ; learning_rate : 352.8 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.6749147718190847
    Epoch 0, Loss: 1.759900569309076, fit: 0.16113333333333332
    Epoch 0, Loss: 1.759900569309076
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.6749147718190847_fit_0.16113333333333332_2024-01-03_180309
  self.fit : 0.16113333333333332
  self.loss : 1.6749147718190847
  current_accuracy : 0.1032
   Accuracy mean: 0.1032
  Error saving file: doc/out/test_combinations_results/20240103180309.
  normalized_accuracies :      [0.20766467 0.06083832 0.0251497  0.12790419 0.         0.12742515
   1.         0.11616766 0.14610778 0.12143713 0.11616766 0.12143713]
batch_rate :  0.2
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              7.84e-62
      batch rate adapted learning_rate :              1.568e-62
      loss_factor :              1
      loss adapted learning_rate :              1.568e-62
    epoch : 0 ; learning_rate : 1.568e-62 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.8383874192890017
    Epoch 0, Loss: 1.8437109581252984, fit: 0.0475
    Epoch 0, Loss: 1.8437109581252984
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.8465793381197202_fit_0.0475_2024-01-03_180313
  self.fit : 0.0475
  self.loss : 1.8465793381197202
  current_accuracy : 0.0551
   Accuracy mean: 0.0551
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              7.8400000000000005e-31
      batch rate adapted learning_rate :              1.5680000000000002e-31
      loss_factor :              1
      loss adapted learning_rate :              1.5680000000000002e-31
    epoch : 0 ; learning_rate : 1.5680000000000002e-31 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7577168360384245
    Epoch 0, Loss: 1.7628070174547856, fit: 0.09458333333333334
    Epoch 0, Loss: 1.7628070174547856
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7577168360384245_fit_0.09458333333333334_2024-01-03_180315
  self.fit : 0.09458333333333334
  self.loss : 1.7577168360384245
  current_accuracy : 0.0826
   Accuracy mean: 0.0826
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              7.840000000000001e-15
      batch rate adapted learning_rate :              1.5680000000000002e-15
      loss_factor :              1
      loss adapted learning_rate :              1.5680000000000002e-15
    epoch : 0 ; learning_rate : 1.5680000000000002e-15 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.6736004785884888
    Epoch 0, Loss: 1.6769078033450922, fit: 0.13075
    Epoch 0, Loss: 1.6769078033450922
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.6761318804277912_fit_0.13075_2024-01-03_180318
  self.fit : 0.13075
  self.loss : 1.6761318804277912
  current_accuracy : 0.1306
   Accuracy mean: 0.1306
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              7.84e-08
      batch rate adapted learning_rate :              1.5680000000000002e-08
      loss_factor :              1
      loss adapted learning_rate :              1.5680000000000002e-08
    epoch : 0 ; learning_rate : 1.5680000000000002e-08 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.756880526608971
    Epoch 0, Loss: 1.7629305087308418, fit: 0.09925
    Epoch 0, Loss: 1.7629305087308418
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.756880526608971_fit_0.09925_2024-01-03_180321
  self.fit : 0.09925
  self.loss : 1.756880526608971
  current_accuracy : 0.0959
   Accuracy mean: 0.0959
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              7.84e-05
      batch rate adapted learning_rate :              1.568e-05
      loss_factor :              1
      loss adapted learning_rate :              1.568e-05
    epoch : 0 ; learning_rate : 1.568e-05 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7382966408328606
    Epoch 0, Loss: 1.7519780751789027, fit: 0.09441666666666666
    Epoch 0, Loss: 1.7519780751789027
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.756393894854702_fit_0.09441666666666666_2024-01-03_180323
  self.fit : 0.09441666666666666
  self.loss : 1.756393894854702
  current_accuracy : 0.0932
   Accuracy mean: 0.0932
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              0.0784
      batch rate adapted learning_rate :              0.01568
      loss_factor :              1
      loss adapted learning_rate :              0.01568
    epoch : 0 ; learning_rate : 0.01568 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.698906624235136
    Epoch 0, Loss: 1.7400859626868719, fit: 0.11441666666666667
    Epoch 0, Loss: 1.7400859626868719
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.698906624235136_fit_0.11441666666666667_2024-01-03_180325
  self.fit : 0.11441666666666667
  self.loss : 1.698906624235136
  current_accuracy : 0.1331
   Accuracy mean: 0.1331
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              0.784
      batch rate adapted learning_rate :              0.15680000000000002
      loss_factor :              1
      loss adapted learning_rate :              0.15680000000000002
    epoch : 0 ; learning_rate : 0.15680000000000002 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.104569903456187
    Epoch 0, Loss: 1.4067260108376054, fit: 0.4196666666666667
    Epoch 0, Loss: 1.4067260108376054
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.104569903456187_fit_0.4196666666666667_2024-01-03_180328
  self.fit : 0.4196666666666667
  self.loss : 1.104569903456187
  current_accuracy : 0.4594
   Accuracy mean: 0.4594
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              15.68
      batch rate adapted learning_rate :              3.136
      loss_factor :              1
      loss adapted learning_rate :              3.136
    epoch : 0 ; learning_rate : 3.136 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.5973290001691218
    Epoch 0, Loss: 1.7442914553539932, fit: 0.09808333333333333
    Epoch 0, Loss: 1.7442914553539932
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.8038333333333334_fit_0.09808333333333333_2024-01-03_180330
  self.fit : 0.09808333333333333
  self.loss : 1.8038333333333334
  current_accuracy : 0.0958
   Accuracy mean: 0.0958
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              19.6
      batch rate adapted learning_rate :              3.9200000000000004
      loss_factor :              1
      loss adapted learning_rate :              3.9200000000000004
    epoch : 0 ; learning_rate : 3.9200000000000004 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.5910235464862548
    Epoch 0, Loss: 1.7480502033242336, fit: 0.09008333333333333
    Epoch 0, Loss: 1.7480502033242336
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.8198333333333334_fit_0.09008333333333333_2024-01-03_180333
  self.fit : 0.09008333333333333
  self.loss : 1.8198333333333334
  current_accuracy : 0.0892
   Accuracy mean: 0.0892
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              23.52
      batch rate adapted learning_rate :              4.704
      loss_factor :              1
      loss adapted learning_rate :              4.704
    epoch : 0 ; learning_rate : 4.704 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.3231797310043507
    Epoch 0, Loss: 1.703002237048649, fit: 0.10241666666666667
    Epoch 0, Loss: 1.703002237048649
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7951666666666666_fit_0.10241666666666667_2024-01-03_180336
  self.fit : 0.10241666666666667
  self.loss : 1.7951666666666666
  current_accuracy : 0.1032
   Accuracy mean: 0.1032
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              78.4
      batch rate adapted learning_rate :              15.680000000000001
      loss_factor :              1
      loss adapted learning_rate :              15.680000000000001
    epoch : 0 ; learning_rate : 15.680000000000001 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.541302134839559
    Epoch 0, Loss: 1.7311803079865913, fit: 0.09825
    Epoch 0, Loss: 1.7311803079865913
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.8035_fit_0.09825_2024-01-03_180338
  self.fit : 0.09825
  self.loss : 1.8035
  current_accuracy : 0.0974
   Accuracy mean: 0.0974
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.2
      neuron quantity adapted learning_rate :              705.6
      batch rate adapted learning_rate :              141.12
      loss_factor :              1
      loss adapted learning_rate :              141.12
    epoch : 0 ; learning_rate : 141.12 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7048261684605899
    Epoch 0, Loss: 1.7864546877043566, fit: 0.0995
    Epoch 0, Loss: 1.7864546877043566
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.801_fit_0.0995_2024-01-03_180341
  self.fit : 0.0995
  self.loss : 1.801
  current_accuracy : 0.1009
   Accuracy mean: 0.1009
  Error saving file: doc/out/test_combinations_results/20240103180341.
  normalized_accuracies :      [0.         0.0680188  0.18674252 0.10091516 0.09423695 0.19292605
   1.         0.10066782 0.08434331 0.11897106 0.10462528 0.11328222]
batch_rate :  0.1
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              7.84e-62
      batch rate adapted learning_rate :              7.84e-63
      loss_factor :              1
      loss adapted learning_rate :              7.84e-63
    epoch : 0 ; learning_rate : 7.84e-63 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7430564401380366
    Epoch 0, Loss: 1.7576895767134615, fit: 0.097
    Epoch 0, Loss: 1.7576895767134615
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7430564401380366_fit_0.097_2024-01-03_180344
  self.fit : 0.097
  self.loss : 1.7430564401380366
  current_accuracy : 0.0891
   Accuracy mean: 0.0891
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              7.8400000000000005e-31
      batch rate adapted learning_rate :              7.840000000000001e-32
      loss_factor :              1
      loss adapted learning_rate :              7.840000000000001e-32
    epoch : 0 ; learning_rate : 7.840000000000001e-32 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.6230339685600086
    Epoch 0, Loss: 1.6329478132373587, fit: 0.14633333333333334
    Epoch 0, Loss: 1.6329478132373587
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.6441797584771247_fit_0.14633333333333334_2024-01-03_180347
  self.fit : 0.14633333333333334
  self.loss : 1.6441797584771247
  current_accuracy : 0.157
   Accuracy mean: 0.157
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              7.840000000000001e-15
      batch rate adapted learning_rate :              7.840000000000001e-16
      loss_factor :              1
      loss adapted learning_rate :              7.840000000000001e-16
    epoch : 0 ; learning_rate : 7.840000000000001e-16 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7075945595405038
    Epoch 0, Loss: 1.7320189045721057, fit: 0.1035
    Epoch 0, Loss: 1.7320189045721057
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7371803588237704_fit_0.1035_2024-01-03_180349
  self.fit : 0.1035
  self.loss : 1.7371803588237704
  current_accuracy : 0.1143
   Accuracy mean: 0.1143
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              7.84e-08
      batch rate adapted learning_rate :              7.840000000000001e-09
      loss_factor :              1
      loss adapted learning_rate :              7.840000000000001e-09
    epoch : 0 ; learning_rate : 7.840000000000001e-09 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7753634715745672
    Epoch 0, Loss: 1.7872377122423373, fit: 0.0725
    Epoch 0, Loss: 1.7872377122423373
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7919424866636857_fit_0.0725_2024-01-03_180352
  self.fit : 0.0725
  self.loss : 1.7919424866636857
  current_accuracy : 0.0726
   Accuracy mean: 0.0726
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              7.84e-05
      batch rate adapted learning_rate :              7.84e-06
      loss_factor :              1
      loss adapted learning_rate :              7.84e-06
    epoch : 0 ; learning_rate : 7.84e-06 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.703051561192854
    Epoch 0, Loss: 1.7168879918724835, fit: 0.113
    Epoch 0, Loss: 1.7168879918724835
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7133861009379918_fit_0.113_2024-01-03_180354
  self.fit : 0.113
  self.loss : 1.7133861009379918
  current_accuracy : 0.1103
   Accuracy mean: 0.1103
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              0.0784
      batch rate adapted learning_rate :              0.00784
      loss_factor :              1
      loss adapted learning_rate :              0.00784
    epoch : 0 ; learning_rate : 0.00784 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.6784685875375953
    Epoch 0, Loss: 1.7129522029964945, fit: 0.12633333333333333
    Epoch 0, Loss: 1.7129522029964945
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6784685875375953_fit_0.12633333333333333_2024-01-03_180356
  self.fit : 0.12633333333333333
  self.loss : 1.6784685875375953
  current_accuracy : 0.1458
   Accuracy mean: 0.1458
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              0.784
      batch rate adapted learning_rate :              0.07840000000000001
      loss_factor :              1
      loss adapted learning_rate :              0.07840000000000001
    epoch : 0 ; learning_rate : 0.07840000000000001 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.0360352277081442
    Epoch 0, Loss: 1.318473211014684, fit: 0.45166666666666666
    Epoch 0, Loss: 1.318473211014684
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.0360352277081442_fit_0.45166666666666666_2024-01-03_180359
  self.fit : 0.45166666666666666
  self.loss : 1.0360352277081442
  current_accuracy : 0.491
   Accuracy mean: 0.491
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              15.68
      batch rate adapted learning_rate :              1.568
      loss_factor :              1
      loss adapted learning_rate :              1.568
    epoch : 0 ; learning_rate : 1.568 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.3337574818010394
    Epoch 0, Loss: 1.7238118599101038, fit: 0.10133333333333333
    Epoch 0, Loss: 1.7238118599101038
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7973333333333332_fit_0.10133333333333333_2024-01-03_180401
  self.fit : 0.10133333333333333
  self.loss : 1.7973333333333332
  current_accuracy : 0.0974
   Accuracy mean: 0.0974
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              19.6
      batch rate adapted learning_rate :              1.9600000000000002
      loss_factor :              1
      loss adapted learning_rate :              1.9600000000000002
    epoch : 0 ; learning_rate : 1.9600000000000002 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.253755823318187
    Epoch 0, Loss: 1.6958407686346968, fit: 0.099
    Epoch 0, Loss: 1.6958407686346968
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.802_fit_0.099_2024-01-03_180404
  self.fit : 0.099
  self.loss : 1.802
  current_accuracy : 0.098
   Accuracy mean: 0.098
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              23.52
      batch rate adapted learning_rate :              2.352
      loss_factor :              1
      loss adapted learning_rate :              2.352
    epoch : 0 ; learning_rate : 2.352 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.5695041313311058
    Epoch 0, Loss: 1.7777157957428233, fit: 0.09616666666666666
    Epoch 0, Loss: 1.7777157957428233
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.8076666666666668_fit_0.09616666666666666_2024-01-03_180407
  self.fit : 0.09616666666666666
  self.loss : 1.8076666666666668
  current_accuracy : 0.1009
   Accuracy mean: 0.1009
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              78.4
      batch rate adapted learning_rate :              7.840000000000001
      loss_factor :              1
      loss adapted learning_rate :              7.840000000000001
    epoch : 0 ; learning_rate : 7.840000000000001 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.461159230597715
    Epoch 0, Loss: 1.7634682278032958, fit: 0.0915
    Epoch 0, Loss: 1.7634682278032958
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.817_fit_0.0915_2024-01-03_180410
  self.fit : 0.0915
  self.loss : 1.817
  current_accuracy : 0.098
   Accuracy mean: 0.098
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.1
      neuron quantity adapted learning_rate :              705.6
      batch rate adapted learning_rate :              70.56
      loss_factor :              1
      loss adapted learning_rate :              70.56
    epoch : 0 ; learning_rate : 70.56 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.4492995377181794
    Epoch 0, Loss: 1.7848465819315553, fit: 0.09166666666666666
    Epoch 0, Loss: 1.7848465819315553
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.8166666666666667_fit_0.09166666666666666_2024-01-03_180412
  self.fit : 0.09166666666666666
  self.loss : 1.8166666666666667
  current_accuracy : 0.0892
   Accuracy mean: 0.0892
  Error saving file: doc/out/test_combinations_results/20240103180412.
  normalized_accuracies :      [0.03943595 0.20172084 0.09966539 0.         0.09010516 0.1749522
   1.         0.05927342 0.06070746 0.06763862 0.06070746 0.03967495]
batch_rate :  0.01
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              7.84e-62
      batch rate adapted learning_rate :              7.840000000000001e-64
      loss_factor :              1
      loss adapted learning_rate :              7.840000000000001e-64
    epoch : 0 ; learning_rate : 7.840000000000001e-64 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.7029147727941047
    Epoch 0, Loss: 1.7580966383189067, fit: 0.09333333333333334
    Epoch 0, Loss: 1.7580966383189067
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7487005293337756_fit_0.09333333333333334_2024-01-03_180417
  self.fit : 0.09333333333333334
  self.loss : 1.7487005293337756
  current_accuracy : 0.0952
   Accuracy mean: 0.0952
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              7.8400000000000005e-31
      batch rate adapted learning_rate :              7.840000000000001e-33
      loss_factor :              1
      loss adapted learning_rate :              7.840000000000001e-33
    epoch : 0 ; learning_rate : 7.840000000000001e-33 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.7129316760418223
    Epoch 0, Loss: 1.7702585164556077, fit: 0.07833333333333334
    Epoch 0, Loss: 1.7702585164556077
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7852062850171426_fit_0.07833333333333334_2024-01-03_180419
  self.fit : 0.07833333333333334
  self.loss : 1.7852062850171426
  current_accuracy : 0.0869
   Accuracy mean: 0.0869
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              7.840000000000001e-15
      batch rate adapted learning_rate :              7.840000000000001e-17
      loss_factor :              1
      loss adapted learning_rate :              7.840000000000001e-17
    epoch : 0 ; learning_rate : 7.840000000000001e-17 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6956729283773426
    Epoch 0, Loss: 1.7605052377854684, fit: 0.09166666666666666
    Epoch 0, Loss: 1.7605052377854684
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7597143265334925_fit_0.09166666666666666_2024-01-03_180422
  self.fit : 0.09166666666666666
  self.loss : 1.7597143265334925
  current_accuracy : 0.0932
   Accuracy mean: 0.0932
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              7.84e-08
      batch rate adapted learning_rate :              7.84e-10
      loss_factor :              1
      loss adapted learning_rate :              7.84e-10
    epoch : 0 ; learning_rate : 7.84e-10 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6832263300416856
    Epoch 0, Loss: 1.7757061473333635, fit: 0.095
    Epoch 0, Loss: 1.7757061473333635
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7625333682395072_fit_0.095_2024-01-03_180425
  self.fit : 0.095
  self.loss : 1.7625333682395072
  current_accuracy : 0.084
   Accuracy mean: 0.084
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              7.84e-05
      batch rate adapted learning_rate :              7.839999999999999e-07
      loss_factor :              1
      loss adapted learning_rate :              7.839999999999999e-07
    epoch : 0 ; learning_rate : 7.839999999999999e-07 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6166381881548044
    Epoch 0, Loss: 1.6854334589100433, fit: 0.14
    Epoch 0, Loss: 1.6854334589100433
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.6753342138566838_fit_0.14_2024-01-03_180428
  self.fit : 0.14
  self.loss : 1.6753342138566838
  current_accuracy : 0.1309
   Accuracy mean: 0.1309
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              0.0784
      batch rate adapted learning_rate :              0.000784
      loss_factor :              1
      loss adapted learning_rate :              0.000784
    epoch : 0 ; learning_rate : 0.000784 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.67579107348351
    Epoch 0, Loss: 1.751955518426057, fit: 0.1
    Epoch 0, Loss: 1.751955518426057
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.729803377784909_fit_0.1_2024-01-03_180431
  self.fit : 0.1
  self.loss : 1.729803377784909
  current_accuracy : 0.1016
   Accuracy mean: 0.1016
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              0.784
      batch rate adapted learning_rate :              0.00784
      loss_factor :              1
      loss adapted learning_rate :              0.00784
    epoch : 0 ; learning_rate : 0.00784 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.0772360332616013
    Epoch 0, Loss: 1.415859216989376, fit: 0.395
    Epoch 0, Loss: 1.415859216989376
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.1397673410092872_fit_0.395_2024-01-03_180434
  self.fit : 0.395
  self.loss : 1.1397673410092872
  current_accuracy : 0.4236
   Accuracy mean: 0.4236
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              15.68
      batch rate adapted learning_rate :              0.1568
      loss_factor :              1
      loss adapted learning_rate :              0.1568
    epoch : 0 ; learning_rate : 0.1568 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 0.2928921421562021
    Epoch 0, Loss: 0.5095771313351606, fit: 0.8466666666666667
    Epoch 0, Loss: 0.5095771313351606
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_0.3015152339394972_fit_0.8466666666666667_2024-01-03_180438
  self.fit : 0.8466666666666667
  self.loss : 0.3015152339394972
  current_accuracy : 0.8433
   Accuracy mean: 0.8433
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              19.6
      batch rate adapted learning_rate :              0.196
      loss_factor :              1
      loss adapted learning_rate :              0.196
    epoch : 0 ; learning_rate : 0.196 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 0.23151386655630424
    Epoch 0, Loss: 0.46351268188103123, fit: 0.8283333333333334
    Epoch 0, Loss: 0.46351268188103123
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_0.32876100611158565_fit_0.8283333333333334_2024-01-03_180441
  self.fit : 0.8283333333333334
  self.loss : 0.32876100611158565
  current_accuracy : 0.8532
   Accuracy mean: 0.8532
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              23.52
      batch rate adapted learning_rate :              0.2352
      loss_factor :              1
      loss adapted learning_rate :              0.2352
    epoch : 0 ; learning_rate : 0.2352 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 0.24175512930146908
    Epoch 0, Loss: 0.4405445431687516, fit: 0.865
    Epoch 0, Loss: 0.4405445431687516
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_0.2533209331788105_fit_0.865_2024-01-03_180443
  self.fit : 0.865
  self.loss : 0.2533209331788105
  current_accuracy : 0.8568
   Accuracy mean: 0.8568
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.01
      neuron quantity adapted learning_rate :              78.4
      batch rate adapted learning_rate :              0.784
      loss_factor :              1
      loss adapted learning_rate :              0.784
    epoch : 0 ; learning_rate : 0.784 ; fit : 0.0
    batch_size :          600
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  1.0
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-64
      loss_factor :              1
      loss adapted learning_rate :              1e-64
    epoch : 0 ; learning_rate : 1e-64 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.719621423744226
    Epoch 0, Loss: 1.719621423744226, fit: 0.10715
    Epoch 0, Loss: 1.719621423744226
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.719621423744226_fit_0.10715_2024-01-03_180454
  self.fit : 0.10715
  self.loss : 1.719621423744226
  current_accuracy : 0.1038
   Accuracy mean: 0.1038
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-33
      loss_factor :              1
      loss adapted learning_rate :              1e-33
    epoch : 0 ; learning_rate : 1e-33 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7887465720972193
    Epoch 0, Loss: 1.7887465720972193, fit: 0.08135
    Epoch 0, Loss: 1.7887465720972193
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7887465720972193_fit_0.08135_2024-01-03_180457
  self.fit : 0.08135
  self.loss : 1.7887465720972193
  current_accuracy : 0.0766
   Accuracy mean: 0.0766
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-17
      loss_factor :              1
      loss adapted learning_rate :              1e-17
    epoch : 0 ; learning_rate : 1e-17 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6973578950731016
    Epoch 0, Loss: 1.6973578950731016, fit: 0.11881666666666667
    Epoch 0, Loss: 1.6973578950731016
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.6973578950731016_fit_0.11881666666666667_2024-01-03_180459
  self.fit : 0.11881666666666667
  self.loss : 1.6973578950731016
  current_accuracy : 0.1222
   Accuracy mean: 0.1222
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-10
      loss_factor :              1
      loss adapted learning_rate :              1e-10
    epoch : 0 ; learning_rate : 1e-10 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6252758317119367
    Epoch 0, Loss: 1.6252758317119367, fit: 0.16025
    Epoch 0, Loss: 1.6252758317119367
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.6252758317119367_fit_0.16025_2024-01-03_180502
  self.fit : 0.16025
  self.loss : 1.6252758317119367
  current_accuracy : 0.1612
   Accuracy mean: 0.1612
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-07
      loss_factor :              1
      loss adapted learning_rate :              1e-07
    epoch : 0 ; learning_rate : 1e-07 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7385233085606464
    Epoch 0, Loss: 1.7385233085606464, fit: 0.10081666666666667
    Epoch 0, Loss: 1.7385233085606464
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7385233085606464_fit_0.10081666666666667_2024-01-03_180504
  self.fit : 0.10081666666666667
  self.loss : 1.7385233085606464
  current_accuracy : 0.0945
   Accuracy mean: 0.0945
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.0001
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7901577368347819
    Epoch 0, Loss: 1.7901577368347819, fit: 0.07665
    Epoch 0, Loss: 1.7901577368347819
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7901577368347819_fit_0.07665_2024-01-03_180507
  self.fit : 0.07665
  self.loss : 1.7901577368347819
  current_accuracy : 0.08
   Accuracy mean: 0.08
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.001
      loss_factor :              1
      loss adapted learning_rate :              0.001
    epoch : 0 ; learning_rate : 0.001 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7143227658571707
    Epoch 0, Loss: 1.7143227658571707, fit: 0.11265
    Epoch 0, Loss: 1.7143227658571707
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7143227658571707_fit_0.11265_2024-01-03_180510
  self.fit : 0.11265
  self.loss : 1.7143227658571707
  current_accuracy : 0.107
   Accuracy mean: 0.107
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.02
      loss_factor :              1
      loss adapted learning_rate :              0.02
    epoch : 0 ; learning_rate : 0.02 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7703051107635117
    Epoch 0, Loss: 1.7703051107635117, fit: 0.08466666666666667
    Epoch 0, Loss: 1.7703051107635117
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7703051107635117_fit_0.08466666666666667_2024-01-03_180512
  self.fit : 0.08466666666666667
  self.loss : 1.7703051107635117
  current_accuracy : 0.1034
   Accuracy mean: 0.1034
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.025
      loss_factor :              1
      loss adapted learning_rate :              0.025
    epoch : 0 ; learning_rate : 0.025 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7367966465612599
    Epoch 0, Loss: 1.7367966465612599, fit: 0.10246666666666666
    Epoch 0, Loss: 1.7367966465612599
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7367966465612599_fit_0.10246666666666666_2024-01-03_180515
  self.fit : 0.10246666666666666
  self.loss : 1.7367966465612599
  current_accuracy : 0.117
   Accuracy mean: 0.117
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.03
      loss_factor :              1
      loss adapted learning_rate :              0.03
    epoch : 0 ; learning_rate : 0.03 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7331512551592838
    Epoch 0, Loss: 1.7331512551592838, fit: 0.10266666666666667
    Epoch 0, Loss: 1.7331512551592838
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7331512551592838_fit_0.10266666666666667_2024-01-03_180517
  self.fit : 0.10266666666666667
  self.loss : 1.7331512551592838
  current_accuracy : 0.1171
   Accuracy mean: 0.1171
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.1
      loss_factor :              1
      loss adapted learning_rate :              0.1
    epoch : 0 ; learning_rate : 0.1 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.69360865066075
    Epoch 0, Loss: 1.69360865066075, fit: 0.12363333333333333
    Epoch 0, Loss: 1.69360865066075
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.69360865066075_fit_0.12363333333333333_2024-01-03_180520
  self.fit : 0.12363333333333333
  self.loss : 1.69360865066075
  current_accuracy : 0.207
   Accuracy mean: 0.207
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.9
      loss_factor :              1
      loss adapted learning_rate :              0.9
    epoch : 0 ; learning_rate : 0.9 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.757115262356433
    Epoch 0, Loss: 1.757115262356433, fit: 0.09703333333333333
    Epoch 0, Loss: 1.757115262356433
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.757115262356433_fit_0.09703333333333333_2024-01-03_180522
  self.fit : 0.09703333333333333
  self.loss : 1.757115262356433
  current_accuracy : 0.3136
   Accuracy mean: 0.3136
  Error saving file: doc/out/test_combinations_results/20240103180523.
  normalized_accuracies :      [0.11476793 0.         0.19240506 0.35696203 0.07552743 0.01434599
   0.12827004 0.11308017 0.17046414 0.17088608 0.55021097 1.        ]
batch_rate :  0.5
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-65
      loss_factor :              1
      loss adapted learning_rate :              5e-65
    epoch : 0 ; learning_rate : 5e-65 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7885084684736552
    Epoch 0, Loss: 1.7900793977086384, fit: 0.07543333333333334
    Epoch 0, Loss: 1.7900793977086384
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7885084684736552_fit_0.07543333333333334_2024-01-03_180526
  self.fit : 0.07543333333333334
  self.loss : 1.7885084684736552
  current_accuracy : 0.0734
   Accuracy mean: 0.0734
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-34
      loss_factor :              1
      loss adapted learning_rate :              5e-34
    epoch : 0 ; learning_rate : 5e-34 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.773081702266924
    Epoch 0, Loss: 1.7736687367308006, fit: 0.08113333333333334
    Epoch 0, Loss: 1.7736687367308006
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7742557711946771_fit_0.08113333333333334_2024-01-03_180528
  self.fit : 0.08113333333333334
  self.loss : 1.7742557711946771
  current_accuracy : 0.0795
   Accuracy mean: 0.0795
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-18
      loss_factor :              1
      loss adapted learning_rate :              5e-18
    epoch : 0 ; learning_rate : 5e-18 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7428987324744112
    Epoch 0, Loss: 1.7442512872332596, fit: 0.09606666666666666
    Epoch 0, Loss: 1.7442512872332596
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.745603841992108_fit_0.09606666666666666_2024-01-03_180531
  self.fit : 0.09606666666666666
  self.loss : 1.745603841992108
  current_accuracy : 0.0966
   Accuracy mean: 0.0966
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-11
      loss_factor :              1
      loss adapted learning_rate :              5e-11
    epoch : 0 ; learning_rate : 5e-11 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7493520782375367
    Epoch 0, Loss: 1.7516535970585814, fit: 0.0889
    Epoch 0, Loss: 1.7516535970585814
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7539551158796263_fit_0.0889_2024-01-03_180533
  self.fit : 0.0889
  self.loss : 1.7539551158796263
  current_accuracy : 0.0965
   Accuracy mean: 0.0965
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-08
      loss_factor :              1
      loss adapted learning_rate :              5e-08
    epoch : 0 ; learning_rate : 5e-08 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.6660453862638547
    Epoch 0, Loss: 1.6704684157696992, fit: 0.1384
    Epoch 0, Loss: 1.6704684157696992
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.6660453862638547_fit_0.1384_2024-01-03_180536
  self.fit : 0.1384
  self.loss : 1.6660453862638547
  current_accuracy : 0.1434
   Accuracy mean: 0.1434
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.5
      batch rate adapted learning_rate :              5e-05
      loss_factor :              1
      loss adapted learning_rate :              5e-05
    epoch : 0 ; learning_rate : 5e-05 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.633358395444638
    Epoch 0, Loss: 1.6339362484642495, fit: 0.15413333333333334
    Epoch 0, Loss: 1.6339362484642495
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.633358395444638_fit_0.15413333333333334_2024-01-03_180538
  self.fit : 0.15413333333333334
  self.loss : 1.633358395444638
  current_accuracy : 0.1612
   Accuracy mean: 0.1612
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.0005
      loss_factor :              1
      loss adapted learning_rate :              0.0005
    epoch : 0 ; learning_rate : 0.0005 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7358923706969627
    Epoch 0, Loss: 1.737453004988836, fit: 0.10246666666666666
    Epoch 0, Loss: 1.737453004988836
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7358923706969627_fit_0.10246666666666666_2024-01-03_180541
  self.fit : 0.10246666666666666
  self.loss : 1.7358923706969627
  current_accuracy : 0.1037
   Accuracy mean: 0.1037
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.01
      loss_factor :              1
      loss adapted learning_rate :              0.01
    epoch : 0 ; learning_rate : 0.01 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7577219115957543
    Epoch 0, Loss: 1.7596346525217692, fit: 0.092
    Epoch 0, Loss: 1.7596346525217692
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7577219115957543_fit_0.092_2024-01-03_180543
  self.fit : 0.092
  self.loss : 1.7577219115957543
  current_accuracy : 0.0972
   Accuracy mean: 0.0972
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.0125
      loss_factor :              1
      loss adapted learning_rate :              0.0125
    epoch : 0 ; learning_rate : 0.0125 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7643407084376164
    Epoch 0, Loss: 1.7712905949199258, fit: 0.0854
    Epoch 0, Loss: 1.7712905949199258
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7643407084376164_fit_0.0854_2024-01-03_180546
  self.fit : 0.0854
  self.loss : 1.7643407084376164
  current_accuracy : 0.0877
   Accuracy mean: 0.0877
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.015
      loss_factor :              1
      loss adapted learning_rate :              0.015
    epoch : 0 ; learning_rate : 0.015 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7444234273262769
    Epoch 0, Loss: 1.757838030046511, fit: 0.09763333333333334
    Epoch 0, Loss: 1.757838030046511
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7444234273262769_fit_0.09763333333333334_2024-01-03_180548
  self.fit : 0.09763333333333334
  self.loss : 1.7444234273262769
  current_accuracy : 0.112
   Accuracy mean: 0.112
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.05
      loss_factor :              1
      loss adapted learning_rate :              0.05
    epoch : 0 ; learning_rate : 0.05 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.660651915923669
    Epoch 0, Loss: 1.7028457596090028, fit: 0.1356
    Epoch 0, Loss: 1.7028457596090028
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.660651915923669_fit_0.1356_2024-01-03_180551
  self.fit : 0.1356
  self.loss : 1.660651915923669
  current_accuracy : 0.1491
   Accuracy mean: 0.1491
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.45
      loss_factor :              1
      loss adapted learning_rate :              0.45
    epoch : 0 ; learning_rate : 0.45 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.5212663601593195
    Epoch 0, Loss: 1.6516368107498856, fit: 0.2187
    Epoch 0, Loss: 1.6516368107498856
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.5212663601593195_fit_0.2187_2024-01-03_180554
  self.fit : 0.2187
  self.loss : 1.5212663601593195
  current_accuracy : 0.3928
   Accuracy mean: 0.3928
  Error saving file: doc/out/test_combinations_results/20240103180554.
  normalized_accuracies :      [0.         0.01909831 0.07263619 0.07232311 0.21916093 0.27489042
   0.09486537 0.07451472 0.04477145 0.1208516  0.23700689 1.        ]
batch_rate :  0.2
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.2
      batch rate adapted learning_rate :              2e-65
      loss_factor :              1
      loss adapted learning_rate :              2e-65
    epoch : 0 ; learning_rate : 2e-65 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.68629137566732
    Epoch 0, Loss: 1.6978201775629247, fit: 0.12491666666666666
    Epoch 0, Loss: 1.6978201775629247
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.6948607553924413_fit_0.12491666666666666_2024-01-03_180557
  self.fit : 0.12491666666666666
  self.loss : 1.6948607553924413
  current_accuracy : 0.1359
   Accuracy mean: 0.1359
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.2
      batch rate adapted learning_rate :              2.0000000000000003e-34
      loss_factor :              1
      loss adapted learning_rate :              2.0000000000000003e-34
    epoch : 0 ; learning_rate : 2.0000000000000003e-34 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.736223627888665
    Epoch 0, Loss: 1.7410932398584409, fit: 0.09325
    Epoch 0, Loss: 1.7410932398584409
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7491694443277992_fit_0.09325_2024-01-03_180600
  self.fit : 0.09325
  self.loss : 1.7491694443277992
  current_accuracy : 0.0921
   Accuracy mean: 0.0921
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.2
      batch rate adapted learning_rate :              2e-18
      loss_factor :              1
      loss adapted learning_rate :              2e-18
    epoch : 0 ; learning_rate : 2e-18 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7403087191531375
    Epoch 0, Loss: 1.7458340467226214, fit: 0.09916666666666667
    Epoch 0, Loss: 1.7458340467226214
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7490700194710938_fit_0.09916666666666667_2024-01-03_180603
  self.fit : 0.09916666666666667
  self.loss : 1.7490700194710938
  current_accuracy : 0.1021
   Accuracy mean: 0.1021
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.2
      batch rate adapted learning_rate :              2.0000000000000002e-11
      loss_factor :              1
      loss adapted learning_rate :              2.0000000000000002e-11
    epoch : 0 ; learning_rate : 2.0000000000000002e-11 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.6599449639314967
    Epoch 0, Loss: 1.669739994918088, fit: 0.12975
    Epoch 0, Loss: 1.669739994918088
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.6775917072612787_fit_0.12975_2024-01-03_180606
  self.fit : 0.12975
  self.loss : 1.6775917072612787
  current_accuracy : 0.1308
   Accuracy mean: 0.1308
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.2
      batch rate adapted learning_rate :              2e-08
      loss_factor :              1
      loss adapted learning_rate :              2e-08
    epoch : 0 ; learning_rate : 2e-08 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7144220135489519
    Epoch 0, Loss: 1.721728546465371, fit: 0.10316666666666667
    Epoch 0, Loss: 1.721728546465371
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7304675265579283_fit_0.10316666666666667_2024-01-03_180608
  self.fit : 0.10316666666666667
  self.loss : 1.7304675265579283
  current_accuracy : 0.1077
   Accuracy mean: 0.1077
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.2
      batch rate adapted learning_rate :              2e-05
      loss_factor :              1
      loss adapted learning_rate :              2e-05
    epoch : 0 ; learning_rate : 2e-05 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.6627519663585872
    Epoch 0, Loss: 1.6691162153504289, fit: 0.14341666666666666
    Epoch 0, Loss: 1.6691162153504289
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6667890266231256_fit_0.14341666666666666_2024-01-03_180611
  self.fit : 0.14341666666666666
  self.loss : 1.6667890266231256
  current_accuracy : 0.147
   Accuracy mean: 0.147
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.0002
      loss_factor :              1
      loss adapted learning_rate :              0.0002
    epoch : 0 ; learning_rate : 0.0002 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.735641934166164
    Epoch 0, Loss: 1.7414181916623526, fit: 0.09958333333333333
    Epoch 0, Loss: 1.7414181916623526
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7373701672486892_fit_0.09958333333333333_2024-01-03_180613
  self.fit : 0.09958333333333333
  self.loss : 1.7373701672486892
  current_accuracy : 0.0982
   Accuracy mean: 0.0982
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.004
      loss_factor :              1
      loss adapted learning_rate :              0.004
    epoch : 0 ; learning_rate : 0.004 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.777098579206108
    Epoch 0, Loss: 1.7823745144722714, fit: 0.07283333333333333
    Epoch 0, Loss: 1.7823745144722714
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7842866240320205_fit_0.07283333333333333_2024-01-03_180616
  self.fit : 0.07283333333333333
  self.loss : 1.7842866240320205
  current_accuracy : 0.0826
   Accuracy mean: 0.0826
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.005000000000000001
      loss_factor :              1
      loss adapted learning_rate :              0.005000000000000001
    epoch : 0 ; learning_rate : 0.005000000000000001 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.798107061454253
    Epoch 0, Loss: 1.8116535626498946, fit: 0.07008333333333333
    Epoch 0, Loss: 1.8116535626498946
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.798107061454253_fit_0.07008333333333333_2024-01-03_180618
  self.fit : 0.07008333333333333
  self.loss : 1.798107061454253
  current_accuracy : 0.065
   Accuracy mean: 0.065
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.006
      loss_factor :              1
      loss adapted learning_rate :              0.006
    epoch : 0 ; learning_rate : 0.006 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7114767731510774
    Epoch 0, Loss: 1.718592458419752, fit: 0.11066666666666666
    Epoch 0, Loss: 1.718592458419752
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7188956318976456_fit_0.11066666666666666_2024-01-03_180621
  self.fit : 0.11066666666666666
  self.loss : 1.7188956318976456
  current_accuracy : 0.121
   Accuracy mean: 0.121
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.020000000000000004
      loss_factor :              1
      loss adapted learning_rate :              0.020000000000000004
    epoch : 0 ; learning_rate : 0.020000000000000004 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.6500213066174905
    Epoch 0, Loss: 1.7060009198617212, fit: 0.13983333333333334
    Epoch 0, Loss: 1.7060009198617212
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6500213066174905_fit_0.13983333333333334_2024-01-03_180624
  self.fit : 0.13983333333333334
  self.loss : 1.6500213066174905
  current_accuracy : 0.1576
   Accuracy mean: 0.1576
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.18000000000000002
      loss_factor :              1
      loss adapted learning_rate :              0.18000000000000002
    epoch : 0 ; learning_rate : 0.18000000000000002 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.1110197824089603
    Epoch 0, Loss: 1.40153572914142, fit: 0.421
    Epoch 0, Loss: 1.40153572914142
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.1110197824089603_fit_0.421_2024-01-03_180626
  self.fit : 0.421
  self.loss : 1.1110197824089603
  current_accuracy : 0.5011
   Accuracy mean: 0.5011
  Error saving file: doc/out/test_combinations_results/20240103180626.
  normalized_accuracies :      [0.16257739 0.06214171 0.08507223 0.15088283 0.09791332 0.18803027
   0.07612933 0.04035772 0.         0.12841091 0.21233662 1.        ]
batch_rate :  0.1
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.1
      batch rate adapted learning_rate :              1e-65
      loss_factor :              1
      loss adapted learning_rate :              1e-65
    epoch : 0 ; learning_rate : 1e-65 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.6787301820427232
    Epoch 0, Loss: 1.6917728274954085, fit: 0.127
    Epoch 0, Loss: 1.6917728274954085
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.6882171418499026_fit_0.127_2024-01-03_180630
  self.fit : 0.127
  self.loss : 1.6882171418499026
  current_accuracy : 0.1244
   Accuracy mean: 0.1244
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.1
      batch rate adapted learning_rate :              1.0000000000000001e-34
      loss_factor :              1
      loss adapted learning_rate :              1.0000000000000001e-34
    epoch : 0 ; learning_rate : 1.0000000000000001e-34 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7843267413119699
    Epoch 0, Loss: 1.8017660283415884, fit: 0.07166666666666667
    Epoch 0, Loss: 1.8017660283415884
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.8118952916705782_fit_0.07166666666666667_2024-01-03_180632
  self.fit : 0.07166666666666667
  self.loss : 1.8118952916705782
  current_accuracy : 0.0725
   Accuracy mean: 0.0725
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.1
      batch rate adapted learning_rate :              1e-18
      loss_factor :              1
      loss adapted learning_rate :              1e-18
    epoch : 0 ; learning_rate : 1e-18 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7176455771853776
    Epoch 0, Loss: 1.729441574223617, fit: 0.10933333333333334
    Epoch 0, Loss: 1.729441574223617
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7281081155843427_fit_0.10933333333333334_2024-01-03_180635
  self.fit : 0.10933333333333334
  self.loss : 1.7281081155843427
  current_accuracy : 0.109
   Accuracy mean: 0.109
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.1
      batch rate adapted learning_rate :              1.0000000000000001e-11
      loss_factor :              1
      loss adapted learning_rate :              1.0000000000000001e-11
    epoch : 0 ; learning_rate : 1.0000000000000001e-11 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7624088271489458
    Epoch 0, Loss: 1.772620824621669, fit: 0.08533333333333333
    Epoch 0, Loss: 1.772620824621669
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.769877189199081_fit_0.08533333333333333_2024-01-03_180638
  self.fit : 0.08533333333333333
  self.loss : 1.769877189199081
  current_accuracy : 0.0831
   Accuracy mean: 0.0831
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.1
      batch rate adapted learning_rate :              1e-08
      loss_factor :              1
      loss adapted learning_rate :              1e-08
    epoch : 0 ; learning_rate : 1e-08 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7978489689314865
    Epoch 0, Loss: 1.8068959682712316, fit: 0.068
    Epoch 0, Loss: 1.8068959682712316
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7978489689314865_fit_0.068_2024-01-03_180640
  self.fit : 0.068
  self.loss : 1.7978489689314865
  current_accuracy : 0.0731
   Accuracy mean: 0.0731
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.1
      batch rate adapted learning_rate :              1e-05
      loss_factor :              1
      loss adapted learning_rate :              1e-05
    epoch : 0 ; learning_rate : 1e-05 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7532172691659182
    Epoch 0, Loss: 1.7592418257978002, fit: 0.098
    Epoch 0, Loss: 1.7592418257978002
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.760604216268746_fit_0.098_2024-01-03_180643
  self.fit : 0.098
  self.loss : 1.760604216268746
  current_accuracy : 0.0918
   Accuracy mean: 0.0918
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.0001
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.779377772079051
    Epoch 0, Loss: 1.7918133311128455, fit: 0.06866666666666667
    Epoch 0, Loss: 1.7918133311128455
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.803093500879823_fit_0.06866666666666667_2024-01-03_180645
  self.fit : 0.06866666666666667
  self.loss : 1.803093500879823
  current_accuracy : 0.068
   Accuracy mean: 0.068
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.002
      loss_factor :              1
      loss adapted learning_rate :              0.002
    epoch : 0 ; learning_rate : 0.002 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.6332182040751102
    Epoch 0, Loss: 1.6508610394780872, fit: 0.13916666666666666
    Epoch 0, Loss: 1.6508610394780872
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.65517440355977_fit_0.13916666666666666_2024-01-03_180648
  self.fit : 0.13916666666666666
  self.loss : 1.65517440355977
  current_accuracy : 0.1531
   Accuracy mean: 0.1531
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.0025000000000000005
      loss_factor :              1
      loss adapted learning_rate :              0.0025000000000000005
    epoch : 0 ; learning_rate : 0.0025000000000000005 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7124046218468079
    Epoch 0, Loss: 1.7233167251840822, fit: 0.10983333333333334
    Epoch 0, Loss: 1.7233167251840822
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7152706378591014_fit_0.10983333333333334_2024-01-03_180650
  self.fit : 0.10983333333333334
  self.loss : 1.7152706378591014
  current_accuracy : 0.1097
   Accuracy mean: 0.1097
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.003
      loss_factor :              1
      loss adapted learning_rate :              0.003
    epoch : 0 ; learning_rate : 0.003 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7652775794561877
    Epoch 0, Loss: 1.7890464385614484, fit: 0.08416666666666667
    Epoch 0, Loss: 1.7890464385614484
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.7663380480955913_fit_0.08416666666666667_2024-01-03_180653
  self.fit : 0.08416666666666667
  self.loss : 1.7663380480955913
  current_accuracy : 0.0935
   Accuracy mean: 0.0935
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.010000000000000002
      loss_factor :              1
      loss adapted learning_rate :              0.010000000000000002
    epoch : 0 ; learning_rate : 0.010000000000000002 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.5888053569393839
    Epoch 0, Loss: 1.6356739817336239, fit: 0.1695
    Epoch 0, Loss: 1.6356739817336239
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.5888053569393839_fit_0.1695_2024-01-03_180655
  self.fit : 0.1695
  self.loss : 1.5888053569393839
  current_accuracy : 0.1616
   Accuracy mean: 0.1616
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.09000000000000001
      loss_factor :              1
      loss adapted learning_rate :              0.09000000000000001
    epoch : 0 ; learning_rate : 0.09000000000000001 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 0.9888211014107107
    Epoch 0, Loss: 1.3057610817571752, fit: 0.4786666666666667
    Epoch 0, Loss: 1.3057610817571752
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.9888211014107107_fit_0.4786666666666667_2024-01-03_180658
  self.fit : 0.4786666666666667
  self.loss : 0.9888211014107107
  current_accuracy : 0.5065
   Accuracy mean: 0.5065
  Error saving file: doc/out/test_combinations_results/20240103180658.
  normalized_accuracies :      [0.1286203  0.01026226 0.09350057 0.03443558 0.01163056 0.05427594
   0.         0.1940707  0.09509692 0.05815279 0.21345496 1.        ]
batch_rate :  0.01
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-66
      loss_factor :              1
      loss adapted learning_rate :              1e-66
    epoch : 0 ; learning_rate : 1e-66 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6877948966990497
    Epoch 0, Loss: 1.7468788922676997, fit: 0.10666666666666667
    Epoch 0, Loss: 1.7468788922676997
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7341364204738037_fit_0.10666666666666667_2024-01-03_180702
  self.fit : 0.10666666666666667
  self.loss : 1.7341364204738037
  current_accuracy : 0.0907
   Accuracy mean: 0.0907
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.01
      batch rate adapted learning_rate :              1.0000000000000001e-35
      loss_factor :              1
      loss adapted learning_rate :              1.0000000000000001e-35
    epoch : 0 ; learning_rate : 1.0000000000000001e-35 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6454483542448903
    Epoch 0, Loss: 1.7335072919247785, fit: 0.1
    Epoch 0, Loss: 1.7335072919247785
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7374257251022807_fit_0.1_2024-01-03_180705
  self.fit : 0.1
  self.loss : 1.7374257251022807
  current_accuracy : 0.1129
   Accuracy mean: 0.1129
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.01
      batch rate adapted learning_rate :              1.0000000000000001e-19
      loss_factor :              1
      loss adapted learning_rate :              1.0000000000000001e-19
    epoch : 0 ; learning_rate : 1.0000000000000001e-19 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.7419683209495291
    Epoch 0, Loss: 1.7940654214714729, fit: 0.056666666666666664
    Epoch 0, Loss: 1.7940654214714729
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.8289787703106102_fit_0.056666666666666664_2024-01-03_180708
  self.fit : 0.056666666666666664
  self.loss : 1.8289787703106102
  current_accuracy : 0.073
   Accuracy mean: 0.073
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-12
      loss_factor :              1
      loss adapted learning_rate :              1e-12
    epoch : 0 ; learning_rate : 1e-12 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.7309276498155228
    Epoch 0, Loss: 1.7880410396273985, fit: 0.08666666666666667
    Epoch 0, Loss: 1.7880410396273985
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7599491287957236_fit_0.08666666666666667_2024-01-03_180710
  self.fit : 0.08666666666666667
  self.loss : 1.7599491287957236
  current_accuracy : 0.0692
   Accuracy mean: 0.0692
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-09
      loss_factor :              1
      loss adapted learning_rate :              1e-09
    epoch : 0 ; learning_rate : 1e-09 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.7388538885674718
    Epoch 0, Loss: 1.7906642390307428, fit: 0.07666666666666666
    Epoch 0, Loss: 1.7906642390307428
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.792117040090135_fit_0.07666666666666666_2024-01-03_180713
  self.fit : 0.07666666666666666
  self.loss : 1.792117040090135
  current_accuracy : 0.0767
   Accuracy mean: 0.0767
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.01
      batch rate adapted learning_rate :              1.0000000000000002e-06
      loss_factor :              1
      loss adapted learning_rate :              1.0000000000000002e-06
    epoch : 0 ; learning_rate : 1.0000000000000002e-06 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.7105870325714372
    Epoch 0, Loss: 1.7765969099962775, fit: 0.06833333333333333
    Epoch 0, Loss: 1.7765969099962775
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.794298901631033_fit_0.06833333333333333_2024-01-03_180716
  self.fit : 0.06833333333333333
  self.loss : 1.794298901631033
  current_accuracy : 0.0865
   Accuracy mean: 0.0865
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.01
      batch rate adapted learning_rate :              1e-05
      loss_factor :              1
      loss adapted learning_rate :              1e-05
    epoch : 0 ; learning_rate : 1e-05 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6832251479203906
    Epoch 0, Loss: 1.7575131528742778, fit: 0.07666666666666666
    Epoch 0, Loss: 1.7575131528742778
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7824152836369354_fit_0.07666666666666666_2024-01-03_180719
  self.fit : 0.07666666666666666
  self.loss : 1.7824152836369354
  current_accuracy : 0.0954
   Accuracy mean: 0.0954
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.0002
      loss_factor :              1
      loss adapted learning_rate :              0.0002
    epoch : 0 ; learning_rate : 0.0002 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6354298657515276
    Epoch 0, Loss: 1.7047772769655491, fit: 0.12333333333333334
    Epoch 0, Loss: 1.7047772769655491
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.6860030230621268_fit_0.12333333333333334_2024-01-03_180721
  self.fit : 0.12333333333333334
  self.loss : 1.6860030230621268
  current_accuracy : 0.1224
   Accuracy mean: 0.1224
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.00025
      loss_factor :              1
      loss adapted learning_rate :              0.00025
    epoch : 0 ; learning_rate : 0.00025 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6238600031824826
    Epoch 0, Loss: 1.6897394806844488, fit: 0.11166666666666666
    Epoch 0, Loss: 1.6897394806844488
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.702268464477987_fit_0.11166666666666666_2024-01-03_180724
  self.fit : 0.11166666666666666
  self.loss : 1.702268464477987
  current_accuracy : 0.1425
   Accuracy mean: 0.1425
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.0003
      loss_factor :              1
      loss adapted learning_rate :              0.0003
    epoch : 0 ; learning_rate : 0.0003 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.617095630281784
    Epoch 0, Loss: 1.6770420527229302, fit: 0.15
    Epoch 0, Loss: 1.6770420527229302
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6393414521480594_fit_0.15_2024-01-03_180727
  self.fit : 0.15
  self.loss : 1.6393414521480594
  current_accuracy : 0.143
   Accuracy mean: 0.143
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.001
      loss_factor :              1
      loss adapted learning_rate :              0.001
    epoch : 0 ; learning_rate : 0.001 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.6405329623712719
    Epoch 0, Loss: 1.7366275432914635, fit: 0.14333333333333334
    Epoch 0, Loss: 1.7366275432914635
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6405329623712719_fit_0.14333333333333334_2024-01-03_180729
  self.fit : 0.14333333333333334
  self.loss : 1.6405329623712719
  current_accuracy : 0.1224
   Accuracy mean: 0.1224
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.009000000000000001
      loss_factor :              1
      loss adapted learning_rate :              0.009000000000000001
    epoch : 0 ; learning_rate : 0.009000000000000001 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 0.9181385561451775
    Epoch 0, Loss: 1.311511112875933, fit: 0.5133333333333333
    Epoch 0, Loss: 1.311511112875933
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.9181385561451775_fit_0.5133333333333333_2024-01-03_180732
  self.fit : 0.5133333333333333
  self.loss : 0.9181385561451775
  current_accuracy : 0.4893
   Accuracy mean: 0.4893
  Error saving file: doc/out/test_combinations_results/20240103180732.
  normalized_accuracies :      [0.05117829 0.10402285 0.00904547 0.         0.01785289 0.04118067
   0.0623661  0.12663652 0.17448227 0.17567246 0.12663652 1.        ]
batch_rate :  0.001
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-67
      loss_factor :              1
      loss adapted learning_rate :              1e-67
    epoch : 0 ; learning_rate : 1e-67 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.569747242566676
    Epoch 0, Loss: 1.7984717274202318, fit: 0.05
    Epoch 0, Loss: 1.7984717274202318
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.8577239480758527_fit_0.05_2024-01-03_180741
  self.fit : 0.05
  self.loss : 1.8577239480758527
  current_accuracy : 0.0686
   Accuracy mean: 0.0686
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-36
      loss_factor :              1
      loss adapted learning_rate :              1.0000000000000001e-36
    epoch : 0 ; learning_rate : 1.0000000000000001e-36 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.405038408014542
    Epoch 0, Loss: 1.6823171765441332, fit: 0.06666666666666667
    Epoch 0, Loss: 1.6823171765441332
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.8085214546827937_fit_0.06666666666666667_2024-01-03_180748
  self.fit : 0.06666666666666667
  self.loss : 1.8085214546827937
  current_accuracy : 0.1327
   Accuracy mean: 0.1327
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-20
      loss_factor :              1
      loss adapted learning_rate :              1.0000000000000001e-20
    epoch : 0 ; learning_rate : 1.0000000000000001e-20 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.614116411061641
    Epoch 0, Loss: 1.818576703187912, fit: 0.016666666666666666
    Epoch 0, Loss: 1.818576703187912
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.8908694001062316_fit_0.016666666666666666_2024-01-03_180755
  self.fit : 0.016666666666666666
  self.loss : 1.8908694001062316
  current_accuracy : 0.0603
   Accuracy mean: 0.0603
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-13
      loss_factor :              1
      loss adapted learning_rate :              1e-13
    epoch : 0 ; learning_rate : 1e-13 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4766578982968552
    Epoch 0, Loss: 1.7712851102490614, fit: 0.15
    Epoch 0, Loss: 1.7712851102490614
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.6519911257005053_fit_0.15_2024-01-03_180802
  self.fit : 0.15
  self.loss : 1.6519911257005053
  current_accuracy : 0.0883
   Accuracy mean: 0.0883
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-10
      loss_factor :              1
      loss adapted learning_rate :              1e-10
    epoch : 0 ; learning_rate : 1e-10 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.6106691840867442
    Epoch 0, Loss: 1.808910507083634, fit: 0.11666666666666667
    Epoch 0, Loss: 1.808910507083634
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7337720576056852_fit_0.11666666666666667_2024-01-03_180809
  self.fit : 0.11666666666666667
  self.loss : 1.7337720576056852
  current_accuracy : 0.072
   Accuracy mean: 0.072
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1.0000000000000001e-07
      loss_factor :              1
      loss adapted learning_rate :              1.0000000000000001e-07
    epoch : 0 ; learning_rate : 1.0000000000000001e-07 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4403753413572062
    Epoch 0, Loss: 1.730894139529922, fit: 0.13333333333333333
    Epoch 0, Loss: 1.730894139529922
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6698254722785881_fit_0.13333333333333333_2024-01-03_180816
  self.fit : 0.13333333333333333
  self.loss : 1.6698254722785881
  current_accuracy : 0.1099
   Accuracy mean: 0.1099
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.001
      batch rate adapted learning_rate :              1e-06
      loss_factor :              1
      loss adapted learning_rate :              1e-06
    epoch : 0 ; learning_rate : 1e-06 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5044986842172192
    Epoch 0, Loss: 1.7731459735059338, fit: 0.06666666666666667
    Epoch 0, Loss: 1.7731459735059338
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7914032483965212_fit_0.06666666666666667_2024-01-03_180824
  self.fit : 0.06666666666666667
  self.loss : 1.7914032483965212
  current_accuracy : 0.0794
   Accuracy mean: 0.0794
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.001
      batch rate adapted learning_rate :              2e-05
      loss_factor :              1
      loss adapted learning_rate :              2e-05
    epoch : 0 ; learning_rate : 2e-05 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4168488129774977
    Epoch 0, Loss: 1.6585685184988628, fit: 0.13333333333333333
    Epoch 0, Loss: 1.6585685184988628
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.6746481185718385_fit_0.13333333333333333_2024-01-03_180831
  self.fit : 0.13333333333333333
  self.loss : 1.6746481185718385
  current_accuracy : 0.148
   Accuracy mean: 0.148
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.001
      batch rate adapted learning_rate :              2.5e-05
      loss_factor :              1
      loss adapted learning_rate :              2.5e-05
    epoch : 0 ; learning_rate : 2.5e-05 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4163299710744361
    Epoch 0, Loss: 1.7076604931049972, fit: 0.1
    Epoch 0, Loss: 1.7076604931049972
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7325880158765743_fit_0.1_2024-01-03_180838
  self.fit : 0.1
  self.loss : 1.7325880158765743
  current_accuracy : 0.1237
   Accuracy mean: 0.1237
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              3e-05
      loss_factor :              1
      loss adapted learning_rate :              3e-05
    epoch : 0 ; learning_rate : 3e-05 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.508381843275861
    Epoch 0, Loss: 1.7805716663466842, fit: 0.13333333333333333
    Epoch 0, Loss: 1.7805716663466842
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6462714682921293_fit_0.13333333333333333_2024-01-03_180845
  self.fit : 0.13333333333333333
  self.loss : 1.6462714682921293
  current_accuracy : 0.0932
   Accuracy mean: 0.0932
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0001
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.376633207462256
    Epoch 0, Loss: 1.6986953530442233, fit: 0.2
    Epoch 0, Loss: 1.6986953530442233
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.5450010882256089_fit_0.2_2024-01-03_180852
  self.fit : 0.2
  self.loss : 1.5450010882256089
  current_accuracy : 0.1555
   Accuracy mean: 0.1555
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.0009000000000000001
      loss_factor :              1
      loss adapted learning_rate :              0.0009000000000000001
    epoch : 0 ; learning_rate : 0.0009000000000000001 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 0.6969080586013447
    Epoch 0, Loss: 1.2599788725317123, fit: 0.4166666666666667
    Epoch 0, Loss: 1.2599788725317123
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.122237178455326_fit_0.4166666666666667_2024-01-03_180900
  self.fit : 0.4166666666666667
  self.loss : 1.122237178455326
  current_accuracy : 0.5016
   Accuracy mean: 0.5016
  Error saving file: doc/out/test_combinations_results/20240103180900.
  normalized_accuracies :      [0.01880807 0.16406073 0.         0.0634489  0.02651258 0.1123952
   0.04328121 0.19873102 0.14366644 0.07455246 0.21572626 1.        ]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  1.0
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-64
      loss_factor :              1
      loss adapted learning_rate :              1e-64
    epoch : 0 ; learning_rate : 1e-64 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6918120155563021
    Epoch 0, Loss: 1.6918120155563021, fit: 0.11901666666666667
    Epoch 0, Loss: 1.6918120155563021
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.6918120155563021_fit_0.11901666666666667_2024-01-03_182055
  self.fit : 0.11901666666666667
  self.loss : 1.6918120155563021
  current_accuracy : 0.1193
   Accuracy mean: 0.1193
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-33
      loss_factor :              1
      loss adapted learning_rate :              1e-33
    epoch : 0 ; learning_rate : 1e-33 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7509453557523427
    Epoch 0, Loss: 1.7509453557523427, fit: 0.09413333333333333
    Epoch 0, Loss: 1.7509453557523427
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7509453557523427_fit_0.09413333333333333_2024-01-03_182058
  self.fit : 0.09413333333333333
  self.loss : 1.7509453557523427
  current_accuracy : 0.0966
   Accuracy mean: 0.0966
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-17
      loss_factor :              1
      loss adapted learning_rate :              1e-17
    epoch : 0 ; learning_rate : 1e-17 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.739649970848924
    Epoch 0, Loss: 1.739649970848924, fit: 0.102
    Epoch 0, Loss: 1.739649970848924
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.739649970848924_fit_0.102_2024-01-03_182101
  self.fit : 0.102
  self.loss : 1.739649970848924
  current_accuracy : 0.1058
   Accuracy mean: 0.1058
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-10
      loss_factor :              1
      loss adapted learning_rate :              1e-10
    epoch : 0 ; learning_rate : 1e-10 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7246353996735246
    Epoch 0, Loss: 1.7246353996735246, fit: 0.10911666666666667
    Epoch 0, Loss: 1.7246353996735246
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7246353996735246_fit_0.10911666666666667_2024-01-03_182103
  self.fit : 0.10911666666666667
  self.loss : 1.7246353996735246
  current_accuracy : 0.1033
   Accuracy mean: 0.1033
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-07
      loss_factor :              1
      loss adapted learning_rate :              1e-07
    epoch : 0 ; learning_rate : 1e-07 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.713928657118198
    Epoch 0, Loss: 1.713928657118198, fit: 0.11113333333333333
    Epoch 0, Loss: 1.713928657118198
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.713928657118198_fit_0.11113333333333333_2024-01-03_182106
  self.fit : 0.11113333333333333
  self.loss : 1.713928657118198
  current_accuracy : 0.1137
   Accuracy mean: 0.1137
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.0001
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7240736128954421
    Epoch 0, Loss: 1.7240736128954421, fit: 0.11111666666666667
    Epoch 0, Loss: 1.7240736128954421
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7240736128954421_fit_0.11111666666666667_2024-01-03_182108
  self.fit : 0.11111666666666667
  self.loss : 1.7240736128954421
  current_accuracy : 0.1178
   Accuracy mean: 0.1178
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.001
      loss_factor :              1
      loss adapted learning_rate :              0.001
    epoch : 0 ; learning_rate : 0.001 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6747414046489428
    Epoch 0, Loss: 1.6747414046489428, fit: 0.1302
    Epoch 0, Loss: 1.6747414046489428
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.6747414046489428_fit_0.1302_2024-01-03_182111
  self.fit : 0.1302
  self.loss : 1.6747414046489428
  current_accuracy : 0.1364
   Accuracy mean: 0.1364
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.02
      loss_factor :              1
      loss adapted learning_rate :              0.02
    epoch : 0 ; learning_rate : 0.02 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.801993976543431
    Epoch 0, Loss: 1.801993976543431, fit: 0.07036666666666666
    Epoch 0, Loss: 1.801993976543431
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.801993976543431_fit_0.07036666666666666_2024-01-03_182113
  self.fit : 0.07036666666666666
  self.loss : 1.801993976543431
  current_accuracy : 0.0784
   Accuracy mean: 0.0784
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.025
      loss_factor :              1
      loss adapted learning_rate :              0.025
    epoch : 0 ; learning_rate : 0.025 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7542940316990352
    Epoch 0, Loss: 1.7542940316990352, fit: 0.09423333333333334
    Epoch 0, Loss: 1.7542940316990352
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7542940316990352_fit_0.09423333333333334_2024-01-03_182116
  self.fit : 0.09423333333333334
  self.loss : 1.7542940316990352
  current_accuracy : 0.1116
   Accuracy mean: 0.1116
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.03
      loss_factor :              1
      loss adapted learning_rate :              0.03
    epoch : 0 ; learning_rate : 0.03 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.748723806959427
    Epoch 0, Loss: 1.748723806959427, fit: 0.10338333333333333
    Epoch 0, Loss: 1.748723806959427
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.748723806959427_fit_0.10338333333333333_2024-01-03_182118
  self.fit : 0.10338333333333333
  self.loss : 1.748723806959427
  current_accuracy : 0.1028
   Accuracy mean: 0.1028
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.1
      loss_factor :              1
      loss adapted learning_rate :              0.1
    epoch : 0 ; learning_rate : 0.1 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7661640133830632
    Epoch 0, Loss: 1.7661640133830632, fit: 0.08538333333333334
    Epoch 0, Loss: 1.7661640133830632
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7661640133830632_fit_0.08538333333333334_2024-01-03_182121
  self.fit : 0.08538333333333334
  self.loss : 1.7661640133830632
  current_accuracy : 0.0972
   Accuracy mean: 0.0972
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.9
      loss_factor :              1
      loss adapted learning_rate :              0.9
    epoch : 0 ; learning_rate : 0.9 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.66914589706756
    Epoch 0, Loss: 1.66914589706756, fit: 0.13701666666666668
    Epoch 0, Loss: 1.66914589706756
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.66914589706756_fit_0.13701666666666668_2024-01-03_182123
  self.fit : 0.13701666666666668
  self.loss : 1.66914589706756
  current_accuracy : 0.3552
   Accuracy mean: 0.3552
  Error saving file: doc/out/test_combinations_results/20240103182124.
  normalized_accuracies :      [0.14776012 0.06575145 0.09898844 0.08995665 0.1275289  0.14234104
   0.20953757 0.         0.1199422  0.08815029 0.06791908 1.        ]
batch_rate :  0.5
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.5
      batch rate adapted learning_rate :              9.999999999999999e-33
      loss_factor :              1
      loss adapted learning_rate :              9.999999999999999e-33
    epoch : 0 ; learning_rate : 9.999999999999999e-33 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7883845005512522
    Epoch 0, Loss: 1.7885445072589579, fit: 0.08093333333333333
    Epoch 0, Loss: 1.7885445072589579
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7883845005512522_fit_0.08093333333333333_2024-01-03_182127
  self.fit : 0.08093333333333333
  self.loss : 1.7883845005512522
  current_accuracy : 0.0864
   Accuracy mean: 0.0864
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.5
      batch rate adapted learning_rate :              3.1622776601683796e-17
      loss_factor :              1
      loss adapted learning_rate :              3.1622776601683796e-17
    epoch : 0 ; learning_rate : 3.1622776601683796e-17 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7684498731212341
    Epoch 0, Loss: 1.771519412458487, fit: 0.08643333333333333
    Epoch 0, Loss: 1.771519412458487
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7745889517957396_fit_0.08643333333333333_2024-01-03_182129
  self.fit : 0.08643333333333333
  self.loss : 1.7745889517957396
  current_accuracy : 0.0885
   Accuracy mean: 0.0885
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.5
      batch rate adapted learning_rate :              3.1622776601683795e-09
      loss_factor :              1
      loss adapted learning_rate :              3.1622776601683795e-09
    epoch : 0 ; learning_rate : 3.1622776601683795e-09 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7395920949344739
    Epoch 0, Loss: 1.7401877970596789, fit: 0.0994
    Epoch 0, Loss: 1.7401877970596789
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.740783499184884_fit_0.0994_2024-01-03_182132
  self.fit : 0.0994
  self.loss : 1.740783499184884
  current_accuracy : 0.0982
   Accuracy mean: 0.0982
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.5
      batch rate adapted learning_rate :              1e-05
      loss_factor :              1
      loss adapted learning_rate :              1e-05
    epoch : 0 ; learning_rate : 1e-05 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.722068311115939
    Epoch 0, Loss: 1.7223426667122266, fit: 0.11206666666666666
    Epoch 0, Loss: 1.7223426667122266
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7226170223085142_fit_0.11206666666666666_2024-01-03_182134
  self.fit : 0.11206666666666666
  self.loss : 1.7226170223085142
  current_accuracy : 0.112
   Accuracy mean: 0.112
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.00031622776601683794
      loss_factor :              1
      loss adapted learning_rate :              0.00031622776601683794
    epoch : 0 ; learning_rate : 0.00031622776601683794 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.772425167035269
    Epoch 0, Loss: 1.7728659526620136, fit: 0.08866666666666667
    Epoch 0, Loss: 1.7728659526620136
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.772425167035269_fit_0.08866666666666667_2024-01-03_182137
  self.fit : 0.08866666666666667
  self.loss : 1.772425167035269
  current_accuracy : 0.0908
   Accuracy mean: 0.0908
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.01
      loss_factor :              1
      loss adapted learning_rate :              0.01
    epoch : 0 ; learning_rate : 0.01 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7385308993165705
    Epoch 0, Loss: 1.7449150588978193, fit: 0.09866666666666667
    Epoch 0, Loss: 1.7449150588978193
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7385308993165705_fit_0.09866666666666667_2024-01-03_182139
  self.fit : 0.09866666666666667
  self.loss : 1.7385308993165705
  current_accuracy : 0.1021
   Accuracy mean: 0.1021
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.03162277660168379
      loss_factor :              1
      loss adapted learning_rate :              0.03162277660168379
    epoch : 0 ; learning_rate : 0.03162277660168379 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.702596431557232
    Epoch 0, Loss: 1.7105158153948876, fit: 0.11616666666666667
    Epoch 0, Loss: 1.7105158153948876
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.702596431557232_fit_0.11616666666666667_2024-01-03_182142
  self.fit : 0.11616666666666667
  self.loss : 1.702596431557232
  current_accuracy : 0.1282
   Accuracy mean: 0.1282
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.1414213562373095
      loss_factor :              1
      loss adapted learning_rate :              0.1414213562373095
    epoch : 0 ; learning_rate : 0.1414213562373095 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.6104876977816538
    Epoch 0, Loss: 1.6812431904424476, fit: 0.16186666666666666
    Epoch 0, Loss: 1.6812431904424476
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.6104876977816538_fit_0.16186666666666666_2024-01-03_182144
  self.fit : 0.16186666666666666
  self.loss : 1.6104876977816538
  current_accuracy : 0.2498
   Accuracy mean: 0.2498
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.15811388300841897
      loss_factor :              1
      loss adapted learning_rate :              0.15811388300841897
    epoch : 0 ; learning_rate : 0.15811388300841897 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.5874295536885803
    Epoch 0, Loss: 1.656414115533778, fit: 0.18006666666666668
    Epoch 0, Loss: 1.656414115533778
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.5874295536885803_fit_0.18006666666666668_2024-01-03_182146
  self.fit : 0.18006666666666668
  self.loss : 1.5874295536885803
  current_accuracy : 0.2907
   Accuracy mean: 0.2907
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.17320508075688773
      loss_factor :              1
      loss adapted learning_rate :              0.17320508075688773
    epoch : 0 ; learning_rate : 0.17320508075688773 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.598482721504383
    Epoch 0, Loss: 1.653704920269463, fit: 0.17163333333333333
    Epoch 0, Loss: 1.653704920269463
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.598482721504383_fit_0.17163333333333333_2024-01-03_182149
  self.fit : 0.17163333333333333
  self.loss : 1.598482721504383
  current_accuracy : 0.3203
   Accuracy mean: 0.3203
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.31622776601683794
      loss_factor :              1
      loss adapted learning_rate :              0.31622776601683794
    epoch : 0 ; learning_rate : 0.31622776601683794 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.436958067236945
    Epoch 0, Loss: 1.6012867611189103, fit: 0.25426666666666664
    Epoch 0, Loss: 1.6012867611189103
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.436958067236945_fit_0.25426666666666664_2024-01-03_182151
  self.fit : 0.25426666666666664
  self.loss : 1.436958067236945
  current_accuracy : 0.3157
   Accuracy mean: 0.3157
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.9486832980505138
      loss_factor :              1
      loss adapted learning_rate :              0.9486832980505138
    epoch : 0 ; learning_rate : 0.9486832980505138 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.1127618005981936
    Epoch 0, Loss: 1.4212222186359844, fit: 0.423
    Epoch 0, Loss: 1.4212222186359844
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.1127618005981936_fit_0.423_2024-01-03_182154
  self.fit : 0.423
  self.loss : 1.1127618005981936
  current_accuracy : 0.4106
   Accuracy mean: 0.4106
  Error saving file: doc/out/test_combinations_results/20240103182154.
  normalized_accuracies :      [0.         0.00647748 0.03639729 0.0789636  0.01357187 0.0484269
   0.12893276 0.50400987 0.63016656 0.72146823 0.70727946 1.        ]
batch_rate :  0.2
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.2
      batch rate adapted learning_rate :              1.584893192461111e-13
      loss_factor :              1
      loss adapted learning_rate :              1.584893192461111e-13
    epoch : 0 ; learning_rate : 1.584893192461111e-13 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7784051941802166
    Epoch 0, Loss: 1.7842288208474466, fit: 0.08233333333333333
    Epoch 0, Loss: 1.7842288208474466
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7871113027220442_fit_0.08233333333333333_2024-01-03_182157
  self.fit : 0.08233333333333333
  self.loss : 1.7871113027220442
  current_accuracy : 0.0809
   Accuracy mean: 0.0809
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.2
      batch rate adapted learning_rate :              2.511886431509578e-07
      loss_factor :              1
      loss adapted learning_rate :              2.511886431509578e-07
    epoch : 0 ; learning_rate : 2.511886431509578e-07 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7374135439557101
    Epoch 0, Loss: 1.7412013566952378, fit: 0.09775
    Epoch 0, Loss: 1.7412013566952378
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.747093870875781_fit_0.09775_2024-01-03_182200
  self.fit : 0.09775
  self.loss : 1.747093870875781
  current_accuracy : 0.1083
   Accuracy mean: 0.1083
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.0003981071705534971
      loss_factor :              1
      loss adapted learning_rate :              0.0003981071705534971
    epoch : 0 ; learning_rate : 0.0003981071705534971 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7936039241693007
    Epoch 0, Loss: 1.7966212657433083, fit: 0.06825
    Epoch 0, Loss: 1.7966212657433083
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7942290174901385_fit_0.06825_2024-01-03_182203
  self.fit : 0.06825
  self.loss : 1.7942290174901385
  current_accuracy : 0.0637
   Accuracy mean: 0.0637
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.009999999999999997
      loss_factor :              1
      loss adapted learning_rate :              0.009999999999999997
    epoch : 0 ; learning_rate : 0.009999999999999997 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.6940116474036935
    Epoch 0, Loss: 1.7055676468875405, fit: 0.11983333333333333
    Epoch 0, Loss: 1.7055676468875405
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.6950198618681331_fit_0.11983333333333333_2024-01-03_182205
  self.fit : 0.11983333333333333
  self.loss : 1.6950198618681331
  current_accuracy : 0.1345
   Accuracy mean: 0.1345
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.03981071705534972
      loss_factor :              1
      loss adapted learning_rate :              0.03981071705534972
    epoch : 0 ; learning_rate : 0.03981071705534972 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.603824458770325
    Epoch 0, Loss: 1.7074159496378365, fit: 0.16166666666666665
    Epoch 0, Loss: 1.7074159496378365
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.603824458770325_fit_0.16166666666666665_2024-01-03_182208
  self.fit : 0.16166666666666665
  self.loss : 1.603824458770325
  current_accuracy : 0.1817
   Accuracy mean: 0.1817
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.15848931924611134
      loss_factor :              1
      loss adapted learning_rate :              0.15848931924611134
    epoch : 0 ; learning_rate : 0.15848931924611134 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.1074908009164384
    Epoch 0, Loss: 1.4510425442499182, fit: 0.42083333333333334
    Epoch 0, Loss: 1.4510425442499182
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.1074908009164384_fit_0.42083333333333334_2024-01-03_182210
  self.fit : 0.42083333333333334
  self.loss : 1.1074908009164384
  current_accuracy : 0.442
   Accuracy mean: 0.442
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.251188643150958
      loss_factor :              1
      loss adapted learning_rate :              0.251188643150958
    epoch : 0 ; learning_rate : 0.251188643150958 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.09002320984037
    Epoch 0, Loss: 1.3944606253199794, fit: 0.43816666666666665
    Epoch 0, Loss: 1.3944606253199794
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.09002320984037_fit_0.43816666666666665_2024-01-03_182213
  self.fit : 0.43816666666666665
  self.loss : 1.09002320984037
  current_accuracy : 0.5207
   Accuracy mean: 0.5207
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.45730505192732634
      loss_factor :              1
      loss adapted learning_rate :              0.45730505192732634
    epoch : 0 ; learning_rate : 0.45730505192732634 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 0.9950182214411565
    Epoch 0, Loss: 1.3558315612250438, fit: 0.48741666666666666
    Epoch 0, Loss: 1.3558315612250438
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_0.9950182214411565_fit_0.48741666666666666_2024-01-03_182215
  self.fit : 0.48741666666666666
  self.loss : 0.9950182214411565
  current_accuracy : 0.5947
   Accuracy mean: 0.5947
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.4781762498950185
      loss_factor :              1
      loss adapted learning_rate :              0.4781762498950185
    epoch : 0 ; learning_rate : 0.4781762498950185 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 0.986342227198969
    Epoch 0, Loss: 1.3341544765557831, fit: 0.48941666666666667
    Epoch 0, Loss: 1.3341544765557831
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_0.986342227198969_fit_0.48941666666666667_2024-01-03_182218
  self.fit : 0.48941666666666667
  self.loss : 0.986342227198969
  current_accuracy : 0.5984
   Accuracy mean: 0.5984
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.4959344196412831
      loss_factor :              1
      loss adapted learning_rate :              0.4959344196412831
    epoch : 0 ; learning_rate : 0.4959344196412831 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 0.8404099792155697
    Epoch 0, Loss: 1.2741883932129119, fit: 0.5660833333333334
    Epoch 0, Loss: 1.2741883932129119
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_0.8404099792155697_fit_0.5660833333333334_2024-01-03_182220
  self.fit : 0.5660833333333334
  self.loss : 0.8404099792155697
  current_accuracy : 0.5796
   Accuracy mean: 0.5796
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.6309573444801932
      loss_factor :              1
      loss adapted learning_rate :              0.6309573444801932
    epoch : 0 ; learning_rate : 0.6309573444801932 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.0040735812080375
    Epoch 0, Loss: 1.3959080131353203, fit: 0.4725
    Epoch 0, Loss: 1.3959080131353203
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.0040735812080375_fit_0.4725_2024-01-03_182223
  self.fit : 0.4725
  self.loss : 1.0040735812080375
  current_accuracy : 0.4942
   Accuracy mean: 0.4942
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.9791483623609768
      loss_factor :              1
      loss adapted learning_rate :              0.9791483623609768
    epoch : 0 ; learning_rate : 0.9791483623609768 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.0874607051108873
    Epoch 0, Loss: 1.3369886739504395, fit: 0.41533333333333333
    Epoch 0, Loss: 1.3369886739504395
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.1321103157383168_fit_0.41533333333333333_2024-01-03_182225
  self.fit : 0.41533333333333333
  self.loss : 1.1321103157383168
  current_accuracy : 0.2838
   Accuracy mean: 0.2838
  Error saving file: doc/out/test_combinations_results/20240103182225.
  normalized_accuracies :      [0.03216757 0.08341126 0.         0.1324107  0.2206845  0.70749953
   0.85468487 0.99308023 1.         0.9648401  0.80512437 0.41163269]
batch_rate :  0.1
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.1
      batch rate adapted learning_rate :              3.981071705534969e-07
      loss_factor :              1
      loss adapted learning_rate :              3.981071705534969e-07
    epoch : 0 ; learning_rate : 3.981071705534969e-07 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7260573814314641
    Epoch 0, Loss: 1.7430487809467756, fit: 0.1005
    Epoch 0, Loss: 1.7430487809467756
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7429394812355283_fit_0.1005_2024-01-03_182229
  self.fit : 0.1005
  self.loss : 1.7429394812355283
  current_accuracy : 0.0955
   Accuracy mean: 0.0955
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.0005011872336272721
      loss_factor :              1
      loss adapted learning_rate :              0.0005011872336272721
    epoch : 0 ; learning_rate : 0.0005011872336272721 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7296414839301757
    Epoch 0, Loss: 1.7419871843882648, fit: 0.0955
    Epoch 0, Loss: 1.7419871843882648
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7490569253716588_fit_0.0955_2024-01-03_182231
  self.fit : 0.0955
  self.loss : 1.7490569253716588
  current_accuracy : 0.1048
   Accuracy mean: 0.1048
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.019952623149688792
      loss_factor :              1
      loss adapted learning_rate :              0.019952623149688792
    epoch : 0 ; learning_rate : 0.019952623149688792 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.4984800223181343
    Epoch 0, Loss: 1.5852656870470143, fit: 0.2145
    Epoch 0, Loss: 1.5852656870470143
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.4984800223181343_fit_0.2145_2024-01-03_182233
  self.fit : 0.2145
  self.loss : 1.4984800223181343
  current_accuracy : 0.2195
   Accuracy mean: 0.2195
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.09999999999999999
      loss_factor :              1
      loss adapted learning_rate :              0.09999999999999999
    epoch : 0 ; learning_rate : 0.09999999999999999 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 0.9380016842191893
    Epoch 0, Loss: 1.2561139015691642, fit: 0.5066666666666667
    Epoch 0, Loss: 1.2561139015691642
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_0.9380016842191893_fit_0.5066666666666667_2024-01-03_182235
  self.fit : 0.5066666666666667
  self.loss : 0.9380016842191893
  current_accuracy : 0.5305
   Accuracy mean: 0.5305
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.19952623149688795
      loss_factor :              1
      loss adapted learning_rate :              0.19952623149688795
    epoch : 0 ; learning_rate : 0.19952623149688795 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 0.6398154654398934
    Epoch 0, Loss: 1.0716567484835298, fit: 0.6603333333333333
    Epoch 0, Loss: 1.0716567484835298
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_0.6398154654398934_fit_0.6603333333333333_2024-01-03_182238
  self.fit : 0.6603333333333333
  self.loss : 0.6398154654398934
  current_accuracy : 0.6819
   Accuracy mean: 0.6819
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.39810717055349726
      loss_factor :              1
      loss adapted learning_rate :              0.39810717055349726
    epoch : 0 ; learning_rate : 0.39810717055349726 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 0.4498597245929078
    Epoch 0, Loss: 0.9377538967404778, fit: 0.7633333333333333
    Epoch 0, Loss: 0.9377538967404778
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_0.4498597245929078_fit_0.7633333333333333_2024-01-03_182240
  self.fit : 0.7633333333333333
  self.loss : 0.4498597245929078
  current_accuracy : 0.7881
   Accuracy mean: 0.7881
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.5011872336272722
      loss_factor :              1
      loss adapted learning_rate :              0.5011872336272722
    epoch : 0 ; learning_rate : 0.5011872336272722 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 0.5323955303605497
    Epoch 0, Loss: 0.9548737053895205, fit: 0.7198333333333333
    Epoch 0, Loss: 0.9548737053895205
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_0.5323955303605497_fit_0.7198333333333333_2024-01-03_182242
  self.fit : 0.7198333333333333
  self.loss : 0.5323955303605497
  current_accuracy : 0.762
   Accuracy mean: 0.762
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.6762433378062414
      loss_factor :              1
      loss adapted learning_rate :              0.6762433378062414
    epoch : 0 ; learning_rate : 0.6762433378062414 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 0.751098414140006
    Epoch 0, Loss: 1.2696707911111846, fit: 0.09533333333333334
    Epoch 0, Loss: 1.2696707911111846
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.8092516599343926_fit_0.09533333333333334_2024-01-03_182245
  self.fit : 0.09533333333333334
  self.loss : 1.8092516599343926
  current_accuracy : 0.0982
   Accuracy mean: 0.0982
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.6915028921812392
      loss_factor :              1
      loss adapted learning_rate :              0.6915028921812392
    epoch : 0 ; learning_rate : 0.6915028921812392 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 0.5585847402951215
    Epoch 0, Loss: 0.9632861026764367, fit: 0.6266666666666667
    Epoch 0, Loss: 0.9632861026764367
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_0.7110154660277652_fit_0.6266666666666667_2024-01-03_182247
  self.fit : 0.6266666666666667
  self.loss : 0.7110154660277652
  current_accuracy : 0.5974
   Accuracy mean: 0.5974
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.7042261140012369
      loss_factor :              1
      loss adapted learning_rate :              0.7042261140012369
    epoch : 0 ; learning_rate : 0.7042261140012369 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 0.6158559825938398
    Epoch 0, Loss: 0.9931005792795402, fit: 0.5283333333333333
    Epoch 0, Loss: 0.9931005792795402
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_0.8949480710740148_fit_0.5283333333333333_2024-01-03_182249
  self.fit : 0.5283333333333333
  self.loss : 0.8949480710740148
  current_accuracy : 0.4439
   Accuracy mean: 0.4439
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.7943282347242815
      loss_factor :              1
      loss adapted learning_rate :              0.7943282347242815
    epoch : 0 ; learning_rate : 0.7943282347242815 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.0554599163017195
    Epoch 0, Loss: 1.4949835736603663, fit: 0.09666666666666666
    Epoch 0, Loss: 1.4949835736603663
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.8066666666666666_fit_0.09666666666666666_2024-01-03_182252
  self.fit : 0.09666666666666666
  self.loss : 1.8066666666666666
  current_accuracy : 0.1032
   Accuracy mean: 0.1032
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.9895192582062144
      loss_factor :              1
      loss adapted learning_rate :              0.9895192582062144
    epoch : 0 ; learning_rate : 0.9895192582062144 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 0.8005793915805205
    Epoch 0, Loss: 1.3103340955801668, fit: 0.0945
    Epoch 0, Loss: 1.3103340955801668
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.811_fit_0.0945_2024-01-03_182254
  self.fit : 0.0945
  self.loss : 1.811
  current_accuracy : 0.0974
   Accuracy mean: 0.0974
  Error saving file: doc/out/test_combinations_results/20240103182254.
  normalized_accuracies :      [0.         0.01342766 0.17903552 0.62806815 0.84666474 1.
   0.96231591 0.00389835 0.7246607  0.50303205 0.01111753 0.00274329]
batch_rate :  0.01
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.2290867652767773
      loss_factor :              1
      loss adapted learning_rate :              0.2290867652767773
    epoch : 0 ; learning_rate : 0.2290867652767773 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 0.25079769956786035
    Epoch 0, Loss: 0.44588623679019623, fit: 0.8366666666666667
    Epoch 0, Loss: 0.44588623679019623
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_0.3086023932134799_fit_0.8366666666666667_2024-01-03_182258
  self.fit : 0.8366666666666667
  self.loss : 0.3086023932134799
  current_accuracy : 0.8606
   Accuracy mean: 0.8606
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.4677351412871982
      loss_factor :              1
      loss adapted learning_rate :              0.4677351412871982
    epoch : 0 ; learning_rate : 0.4677351412871982 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 0.540789850068062
    Epoch 0, Loss: 0.8945482226130069, fit: 0.335
    Epoch 0, Loss: 0.8945482226130069
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.3040730085002652_fit_0.335_2024-01-03_182301
  self.fit : 0.335
  self.loss : 1.3040730085002652
  current_accuracy : 0.3496
   Accuracy mean: 0.3496
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.6760829753919818
      loss_factor :              1
      loss adapted learning_rate :              0.6760829753919818
    epoch : 0 ; learning_rate : 0.6760829753919818 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 0.6533707467328956
    Epoch 0, Loss: 1.7257094685059422, fit: 0.08666666666666667
    Epoch 0, Loss: 1.7257094685059422
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.8266666666666667_fit_0.08666666666666667_2024-01-03_182304
  self.fit : 0.08666666666666667
  self.loss : 1.8266666666666667
  current_accuracy : 0.0974
   Accuracy mean: 0.0974
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.7943282347242815
      loss_factor :              1
      loss adapted learning_rate :              0.7943282347242815
    epoch : 0 ; learning_rate : 0.7943282347242815 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 0.8973898880560224
    Epoch 0, Loss: 1.7493629907124968, fit: 0.12166666666666667
    Epoch 0, Loss: 1.7493629907124968
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7566666666666666_fit_0.12166666666666667_2024-01-03_182306
  self.fit : 0.12166666666666667
  self.loss : 1.7566666666666666
  current_accuracy : 0.0974
   Accuracy mean: 0.0974
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.8511380382023764
      loss_factor :              1
      loss adapted learning_rate :              0.8511380382023764
    epoch : 0 ; learning_rate : 0.8511380382023764 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.2003172041946735
    Epoch 0, Loss: 1.7577998444310665, fit: 0.10333333333333333
    Epoch 0, Loss: 1.7577998444310665
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7933333333333332_fit_0.10333333333333333_2024-01-03_182309
  self.fit : 0.10333333333333333
  self.loss : 1.7933333333333332
  current_accuracy : 0.1135
   Accuracy mean: 0.1135
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9120108393559098
      loss_factor :              1
      loss adapted learning_rate :              0.9120108393559098
    epoch : 0 ; learning_rate : 0.9120108393559098 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.3262563089899824
    Epoch 0, Loss: 1.7765744636968264, fit: 0.09166666666666666
    Epoch 0, Loss: 1.7765744636968264
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.8166666666666667_fit_0.09166666666666666_2024-01-03_182312
  self.fit : 0.09166666666666666
  self.loss : 1.8166666666666667
  current_accuracy : 0.1028
   Accuracy mean: 0.1028
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.933254300796991
      loss_factor :              1
      loss adapted learning_rate :              0.933254300796991
    epoch : 0 ; learning_rate : 0.933254300796991 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.0458811372620864
    Epoch 0, Loss: 1.7819011548262507, fit: 0.10833333333333334
    Epoch 0, Loss: 1.7819011548262507
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7833333333333334_fit_0.10833333333333334_2024-01-03_182315
  self.fit : 0.10833333333333334
  self.loss : 1.7833333333333334
  current_accuracy : 0.0974
   Accuracy mean: 0.0974
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9616350847573034
      loss_factor :              1
      loss adapted learning_rate :              0.9616350847573034
    epoch : 0 ; learning_rate : 0.9616350847573034 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.0784289152377649
    Epoch 0, Loss: 1.7709775061087505, fit: 0.09333333333333334
    Epoch 0, Loss: 1.7709775061087505
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.8133333333333332_fit_0.09333333333333334_2024-01-03_182317
  self.fit : 0.09333333333333334
  self.loss : 1.8133333333333332
  current_accuracy : 0.0982
   Accuracy mean: 0.0982
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9637833073548235
      loss_factor :              1
      loss adapted learning_rate :              0.9637833073548235
    epoch : 0 ; learning_rate : 0.9637833073548235 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 0.7707926950607726
    Epoch 0, Loss: 1.7585832918546087, fit: 0.10333333333333333
    Epoch 0, Loss: 1.7585832918546087
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7933333333333332_fit_0.10333333333333333_2024-01-03_182320
  self.fit : 0.10333333333333333
  self.loss : 1.7933333333333332
  current_accuracy : 0.0982
   Accuracy mean: 0.0982
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9655420949221489
      loss_factor :              1
      loss adapted learning_rate :              0.9655420949221489
    epoch : 0 ; learning_rate : 0.9655420949221489 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.0303529480336864
    Epoch 0, Loss: 1.7693404316663328, fit: 0.09333333333333334
    Epoch 0, Loss: 1.7693404316663328
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.8133333333333332_fit_0.09333333333333334_2024-01-03_182323
  self.fit : 0.09333333333333334
  self.loss : 1.8133333333333332
  current_accuracy : 0.101
   Accuracy mean: 0.101
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9772372209558107
      loss_factor :              1
      loss adapted learning_rate :              0.9772372209558107
    epoch : 0 ; learning_rate : 0.9772372209558107 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 0.5897698754324094
    Epoch 0, Loss: 1.7495412959216747, fit: 0.11
    Epoch 0, Loss: 1.7495412959216747
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.78_fit_0.11_2024-01-03_182326
  self.fit : 0.11
  self.loss : 1.78
  current_accuracy : 0.1009
   Accuracy mean: 0.1009
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9989469496904544
      loss_factor :              1
      loss adapted learning_rate :              0.9989469496904544
    epoch : 0 ; learning_rate : 0.9989469496904544 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 0.9998465218884989
    Epoch 0, Loss: 1.7624217069630521, fit: 0.115
    Epoch 0, Loss: 1.7624217069630521
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.77_fit_0.115_2024-01-03_182329
  self.fit : 0.115
  self.loss : 1.77
  current_accuracy : 0.101
   Accuracy mean: 0.101
  Error saving file: doc/out/test_combinations_results/20240103182329.
  normalized_accuracies :      [1.         0.33045073 0.         0.         0.02109539 0.00707547
   0.         0.00104822 0.00104822 0.00471698 0.00458595 0.00471698]
batch_rate :  0.001
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.8629785477669702
      loss_factor :              1
      loss adapted learning_rate :              0.8629785477669702
    epoch : 0 ; learning_rate : 0.8629785477669702 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.3706786076623922
    Epoch 0, Loss: 1.801224927368769, fit: 0.016666666666666666
    Epoch 0, Loss: 1.801224927368769
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.9666666666666666_fit_0.016666666666666666_2024-01-03_182338
  self.fit : 0.016666666666666666
  self.loss : 1.9666666666666666
  current_accuracy : 0.098
   Accuracy mean: 0.098
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9268298233793493
      loss_factor :              1
      loss adapted learning_rate :              0.9268298233793493
    epoch : 0 ; learning_rate : 0.9268298233793493 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.2436254114353447
    Epoch 0, Loss: 1.8009125474284986, fit: 0.13333333333333333
    Epoch 0, Loss: 1.8009125474284986
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7333333333333334_fit_0.13333333333333333_2024-01-03_182345
  self.fit : 0.13333333333333333
  self.loss : 1.7333333333333334
  current_accuracy : 0.0958
   Accuracy mean: 0.0958
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9616122783836647
      loss_factor :              1
      loss adapted learning_rate :              0.9616122783836647
    epoch : 0 ; learning_rate : 0.9616122783836647 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4880514622491074
    Epoch 0, Loss: 1.801337497792533, fit: 0.08333333333333333
    Epoch 0, Loss: 1.801337497792533
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.8333333333333333_fit_0.08333333333333333_2024-01-03_182353
  self.fit : 0.08333333333333333
  self.loss : 1.8333333333333333
  current_accuracy : 0.1009
   Accuracy mean: 0.1009
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9772372209558107
      loss_factor :              1
      loss adapted learning_rate :              0.9772372209558107
    epoch : 0 ; learning_rate : 0.9772372209558107 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 0, Loss: 1.8046005239296579, fit: 0.05
    Epoch 0, Loss: 1.8046005239296579
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.9_fit_0.05_2024-01-03_182401
  self.fit : 0.05
  self.loss : 1.9
  current_accuracy : 0.0974
   Accuracy mean: 0.0974
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9840111057611337
      loss_factor :              1
      loss adapted learning_rate :              0.9840111057611337
    epoch : 0 ; learning_rate : 0.9840111057611337 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.1301695955106932
    Epoch 0, Loss: 1.8033730997014479, fit: 0.16666666666666666
    Epoch 0, Loss: 1.8033730997014479
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.6666666666666667_fit_0.16666666666666666_2024-01-03_182408
  self.fit : 0.16666666666666666
  self.loss : 1.6666666666666667
  current_accuracy : 0.0982
   Accuracy mean: 0.0982
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9908319448927676
      loss_factor :              1
      loss adapted learning_rate :              0.9908319448927676
    epoch : 0 ; learning_rate : 0.9908319448927676 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.3471515188675491
    Epoch 0, Loss: 1.8178723052255168, fit: 0.08333333333333333
    Epoch 0, Loss: 1.8178723052255168
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.8333333333333333_fit_0.08333333333333333_2024-01-03_182416
  self.fit : 0.08333333333333333
  self.loss : 1.8333333333333333
  current_accuracy : 0.0892
   Accuracy mean: 0.0892
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9931160484209338
      loss_factor :              1
      loss adapted learning_rate :              0.9931160484209338
    epoch : 0 ; learning_rate : 0.9931160484209338 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4647038793898328
    Epoch 0, Loss: 1.800798141802389, fit: 0.05
    Epoch 0, Loss: 1.800798141802389
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.9_fit_0.05_2024-01-03_182423
  self.fit : 0.05
  self.loss : 1.9
  current_accuracy : 0.1032
   Accuracy mean: 0.1032
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9960956189881034
      loss_factor :              1
      loss adapted learning_rate :              0.9960956189881034
    epoch : 0 ; learning_rate : 0.9960956189881034 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.061218052113208
    Epoch 0, Loss: 1.789901988642027, fit: 0.08333333333333333
    Epoch 0, Loss: 1.789901988642027
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.8333333333333333_fit_0.08333333333333333_2024-01-03_182431
  self.fit : 0.08333333333333333
  self.loss : 1.8333333333333333
  current_accuracy : 0.1028
   Accuracy mean: 0.1028
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9963179161031344
      loss_factor :              1
      loss adapted learning_rate :              0.9963179161031344
    epoch : 0 ; learning_rate : 0.9963179161031344 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.2283332893532852
    Epoch 0, Loss: 1.803733450105335, fit: 0.05
    Epoch 0, Loss: 1.803733450105335
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.9_fit_0.05_2024-01-03_182438
  self.fit : 0.05
  self.loss : 1.9
  current_accuracy : 0.0982
   Accuracy mean: 0.0982
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9964995828970431
      loss_factor :              1
      loss adapted learning_rate :              0.9964995828970431
    epoch : 0 ; learning_rate : 0.9964995828970431 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.436988874497358
    Epoch 0, Loss: 1.8001338704297571, fit: 0.1
    Epoch 0, Loss: 1.8001338704297571
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.8_fit_0.1_2024-01-03_182445
  self.fit : 0.1
  self.loss : 1.8
  current_accuracy : 0.1032
   Accuracy mean: 0.1032
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9977000638225533
      loss_factor :              1
      loss adapted learning_rate :              0.9977000638225533
    epoch : 0 ; learning_rate : 0.9977000638225533 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 0.9592539585436202
    Epoch 0, Loss: 1.8017847563143938, fit: 0.08333333333333333
    Epoch 0, Loss: 1.8017847563143938
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.8333333333333333_fit_0.08333333333333333_2024-01-03_182452
  self.fit : 0.08333333333333333
  self.loss : 1.8333333333333333
  current_accuracy : 0.0974
   Accuracy mean: 0.0974
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9998946450345664
      loss_factor :              1
      loss adapted learning_rate :              0.9998946450345664
    epoch : 0 ; learning_rate : 0.9998946450345664 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4346274138259834
    Epoch 0, Loss: 1.8017017471257857, fit: 0.1
    Epoch 0, Loss: 1.8017017471257857
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.8_fit_0.1_2024-01-03_182459
  self.fit : 0.1
  self.loss : 1.8
  current_accuracy : 0.098
   Accuracy mean: 0.098
  Error saving file: doc/out/test_combinations_results/20240103182500.
  normalized_accuracies :      [0.62857143 0.47142857 0.83571429 0.58571429 0.64285714 0.
   1.         0.97142857 0.64285714 1.         0.58571429 0.62857143]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  1.0
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-64
      loss_factor :              1
      loss adapted learning_rate :              1e-64
    epoch : 0 ; learning_rate : 1e-64 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7898691150050638
    Epoch 0, Loss: 1.7898691150050638, fit: 0.07691666666666666
    Epoch 0, Loss: 1.7898691150050638
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7898691150050638_fit_0.07691666666666666_2024-01-03_182650
  self.fit : 0.07691666666666666
  self.loss : 1.7898691150050638
  current_accuracy : 0.0754
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-64
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-64
      loss_factor :              1
      loss adapted learning_rate :              1e-64
    epoch : 0 ; learning_rate : 1e-64 ; fit : 0.07691666666666666
    batch_size :          60000
    best_batch_loss : 1.7898691150050638
    Epoch 0, Loss: 1.7898691150050638, fit: 0.07691666666666666
    Epoch 0, Loss: 1.7898691150050638
      batch rate adapted learning_rate :              1e-64
      loss :              1.7898691150050638
      loss_factor :              0.17898691150050638
      loss adapted learning_rate :              1.9148176232266105e-139
    epoch : 1 ; learning_rate : 1.9148176232266105e-139 ; fit : 0.07691666666666666
    batch_size :          60000
    best_batch_loss : 1.7898691150050638
    Epoch 1, Loss: 1.7898691150050638, fit: 0.07691666666666666
      batch rate adapted learning_rate :              1e-64
      loss :              1.7898691150050638
      loss_factor :              0.17898691150050638
      loss adapted learning_rate :              1.9148176232266105e-139
    epoch : 2 ; learning_rate : 1.9148176232266105e-139 ; fit : 0.07691666666666666
    batch_size :          60000
    best_batch_loss : 1.7898691150050645
    Epoch 2, Loss: 1.7898691150050645, fit: 0.07691666666666666
      batch rate adapted learning_rate :              1e-64
      loss :              1.7898691150050645
      loss_factor :              0.17898691150050644
      loss adapted learning_rate :              1.9148176232266695e-139
    epoch : 3 ; learning_rate : 1.9148176232266695e-139 ; fit : 0.07691666666666666
    batch_size :          60000
    best_batch_loss : 1.7898691150050634
    Epoch 3, Loss: 1.7898691150050634, fit: 0.07691666666666666
      batch rate adapted learning_rate :              1e-64
      loss :              1.7898691150050634
      loss_factor :              0.17898691150050633
      loss adapted learning_rate :              1.914817623226551e-139
    epoch : 4 ; learning_rate : 1.914817623226551e-139 ; fit : 0.07691666666666666
    batch_size :          60000
    best_batch_loss : 1.7898691150050638
    Epoch 4, Loss: 1.7898691150050638, fit: 0.07691666666666666
      batch rate adapted learning_rate :              1e-64
      loss :              1.7898691150050638
      loss_factor :              0.17898691150050638
      loss adapted learning_rate :              1.9148176232266105e-139
    epoch : 5 ; learning_rate : 1.9148176232266105e-139 ; fit : 0.07691666666666666
    batch_size :          60000
    best_batch_loss : 1.7898691150050638
    Epoch 5, Loss: 1.7898691150050638, fit: 0.07691666666666666
      batch rate adapted learning_rate :              1e-64
      loss :              1.7898691150050638
      loss_factor :              0.17898691150050638
      loss adapted learning_rate :              1.9148176232266105e-139
    epoch : 6 ; learning_rate : 1.9148176232266105e-139 ; fit : 0.07691666666666666
    batch_size :          60000
    best_batch_loss : 1.7898691150050647
    Epoch 6, Loss: 1.7898691150050647, fit: 0.07691666666666666
      batch rate adapted learning_rate :              1e-64
      loss :              1.7898691150050647
      loss_factor :              0.17898691150050647
      loss adapted learning_rate :              1.9148176232266993e-139
    epoch : 7 ; learning_rate : 1.9148176232266993e-139 ; fit : 0.07691666666666666
    batch_size :          60000
    best_batch_loss : 1.7898691150050643
    Epoch 7, Loss: 1.7898691150050643, fit: 0.07691666666666666
      batch rate adapted learning_rate :              1e-64
      loss :              1.7898691150050643
      loss_factor :              0.17898691150050644
      loss adapted learning_rate :              1.9148176232266695e-139
    epoch : 8 ; learning_rate : 1.9148176232266695e-139 ; fit : 0.07691666666666666
    batch_size :          60000
    best_batch_loss : 1.789869115005065
    Epoch 8, Loss: 1.789869115005065, fit: 0.07691666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.789869115005065_fit_0.07691666666666666_2024-01-03_182714
  self.fit : 0.07691666666666666
  self.loss : 1.789869115005065
  current_accuracy : 0.0754
   Accuracy mean: 0.0754
   Accuracy mean: 0.0754
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-33
      loss_factor :              1
      loss adapted learning_rate :              1e-33
    epoch : 0 ; learning_rate : 1e-33 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6874690905279421
    Epoch 0, Loss: 1.6874690905279421, fit: 0.1253
    Epoch 0, Loss: 1.6874690905279421
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.6874690905279421_fit_0.1253_2024-01-03_182717
  self.fit : 0.1253
  self.loss : 1.6874690905279421
  current_accuracy : 0.1288
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-33
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-33
      loss_factor :              1
      loss adapted learning_rate :              1e-33
    epoch : 0 ; learning_rate : 1e-33 ; fit : 0.1253
    batch_size :          60000
    best_batch_loss : 1.6874690905279421
    Epoch 0, Loss: 1.6874690905279421, fit: 0.1253
    Epoch 0, Loss: 1.6874690905279421
      batch rate adapted learning_rate :              1e-33
      loss :              1.6874690905279421
      loss_factor :              0.16874690905279421
      loss adapted learning_rate :              5.2915469984680866e-111
    epoch : 1 ; learning_rate : 5.2915469984680866e-111 ; fit : 0.1253
    batch_size :          60000
    best_batch_loss : 1.6874690905279421
    Epoch 1, Loss: 1.6874690905279421, fit: 0.1253
      batch rate adapted learning_rate :              1e-33
      loss :              1.6874690905279421
      loss_factor :              0.16874690905279421
      loss adapted learning_rate :              5.2915469984680866e-111
    epoch : 2 ; learning_rate : 5.2915469984680866e-111 ; fit : 0.1253
    batch_size :          60000
    best_batch_loss : 1.6874690905279417
    Epoch 2, Loss: 1.6874690905279417, fit: 0.1253
      batch rate adapted learning_rate :              1e-33
      loss :              1.6874690905279417
      loss_factor :              0.16874690905279416
      loss adapted learning_rate :              5.291546998467912e-111
    epoch : 3 ; learning_rate : 5.291546998467912e-111 ; fit : 0.1253
    batch_size :          60000
    best_batch_loss : 1.6874690905279432
    Epoch 3, Loss: 1.6874690905279432, fit: 0.1253
      batch rate adapted learning_rate :              1e-33
      loss :              1.6874690905279432
      loss_factor :              0.16874690905279432
      loss adapted learning_rate :              5.2915469984684345e-111
    epoch : 4 ; learning_rate : 5.2915469984684345e-111 ; fit : 0.1253
    batch_size :          60000
    best_batch_loss : 1.6874690905279421
    Epoch 4, Loss: 1.6874690905279421, fit: 0.1253
      batch rate adapted learning_rate :              1e-33
      loss :              1.6874690905279421
      loss_factor :              0.16874690905279421
      loss adapted learning_rate :              5.2915469984680866e-111
    epoch : 5 ; learning_rate : 5.2915469984680866e-111 ; fit : 0.1253
    batch_size :          60000
    best_batch_loss : 1.6874690905279424
    Epoch 5, Loss: 1.6874690905279424, fit: 0.1253
      batch rate adapted learning_rate :              1e-33
      loss :              1.6874690905279424
      loss_factor :              0.16874690905279424
      loss adapted learning_rate :              5.291546998468173e-111
    epoch : 6 ; learning_rate : 5.291546998468173e-111 ; fit : 0.1253
    batch_size :          60000
    best_batch_loss : 1.6874690905279421
    Epoch 6, Loss: 1.6874690905279421, fit: 0.1253
      batch rate adapted learning_rate :              1e-33
      loss :              1.6874690905279421
      loss_factor :              0.16874690905279421
      loss adapted learning_rate :              5.2915469984680866e-111
    epoch : 7 ; learning_rate : 5.2915469984680866e-111 ; fit : 0.1253
    batch_size :          60000
    best_batch_loss : 1.6874690905279428
    Epoch 7, Loss: 1.6874690905279428, fit: 0.1253
      batch rate adapted learning_rate :              1e-33
      loss :              1.6874690905279428
      loss_factor :              0.16874690905279427
      loss adapted learning_rate :              5.29154699846826e-111
    epoch : 8 ; learning_rate : 5.29154699846826e-111 ; fit : 0.1253
    batch_size :          60000
    best_batch_loss : 1.6874690905279426
    Epoch 8, Loss: 1.6874690905279426, fit: 0.1253
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.6874690905279426_fit_0.1253_2024-01-03_182738
  self.fit : 0.1253
  self.loss : 1.6874690905279426
  current_accuracy : 0.1288
   Accuracy mean: 0.1288
   Accuracy mean: 0.1288
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-17
      loss_factor :              1
      loss adapted learning_rate :              1e-17
    epoch : 0 ; learning_rate : 1e-17 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7452242401081093
    Epoch 0, Loss: 1.7452242401081093, fit: 0.10143333333333333
    Epoch 0, Loss: 1.7452242401081093
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7452242401081093_fit_0.10143333333333333_2024-01-03_182741
  self.fit : 0.10143333333333333
  self.loss : 1.7452242401081093
  current_accuracy : 0.1052
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-17
      loss_factor :              1
      loss adapted learning_rate :              1e-17
    epoch : 0 ; learning_rate : 1e-17 ; fit : 0.10143333333333333
    batch_size :          60000
    best_batch_loss : 1.7452242401081093
    Epoch 0, Loss: 1.7452242401081093, fit: 0.10143333333333333
    Epoch 0, Loss: 1.7452242401081093
      batch rate adapted learning_rate :              1e-17
      loss :              1.7452242401081093
      loss_factor :              0.17452242401081092
      loss adapted learning_rate :              1.5315234270572414e-93
    epoch : 1 ; learning_rate : 1.5315234270572414e-93 ; fit : 0.10143333333333333
    batch_size :          60000
    best_batch_loss : 1.7452242401081093
    Epoch 1, Loss: 1.7452242401081093, fit: 0.10143333333333333
      batch rate adapted learning_rate :              1e-17
      loss :              1.7452242401081093
      loss_factor :              0.17452242401081092
      loss adapted learning_rate :              1.5315234270572414e-93
    epoch : 2 ; learning_rate : 1.5315234270572414e-93 ; fit : 0.10143333333333333
    batch_size :          60000
    best_batch_loss : 1.7452242401081106
    Epoch 2, Loss: 1.7452242401081106, fit: 0.10143333333333333
      batch rate adapted learning_rate :              1e-17
      loss :              1.7452242401081106
      loss_factor :              0.17452242401081106
      loss adapted learning_rate :              1.5315234270573631e-93
    epoch : 3 ; learning_rate : 1.5315234270573631e-93 ; fit : 0.10143333333333333
    batch_size :          60000
    best_batch_loss : 1.745224240108109
    Epoch 3, Loss: 1.745224240108109, fit: 0.10143333333333333
      batch rate adapted learning_rate :              1e-17
      loss :              1.745224240108109
      loss_factor :              0.17452242401081092
      loss adapted learning_rate :              1.5315234270572414e-93
    epoch : 4 ; learning_rate : 1.5315234270572414e-93 ; fit : 0.10143333333333333
    batch_size :          60000
    best_batch_loss : 1.7452242401081095
    Epoch 4, Loss: 1.7452242401081095, fit: 0.10143333333333333
      batch rate adapted learning_rate :              1e-17
      loss :              1.7452242401081095
      loss_factor :              0.17452242401081094
      loss adapted learning_rate :              1.531523427057266e-93
    epoch : 5 ; learning_rate : 1.531523427057266e-93 ; fit : 0.10143333333333333
    batch_size :          60000
    best_batch_loss : 1.7452242401081102
    Epoch 5, Loss: 1.7452242401081102, fit: 0.10143333333333333
      batch rate adapted learning_rate :              1e-17
      loss :              1.7452242401081102
      loss_factor :              0.17452242401081103
      loss adapted learning_rate :              1.531523427057339e-93
    epoch : 6 ; learning_rate : 1.531523427057339e-93 ; fit : 0.10143333333333333
    batch_size :          60000
    best_batch_loss : 1.7452242401081093
    Epoch 6, Loss: 1.7452242401081093, fit: 0.10143333333333333
      batch rate adapted learning_rate :              1e-17
      loss :              1.7452242401081093
      loss_factor :              0.17452242401081092
      loss adapted learning_rate :              1.5315234270572414e-93
    epoch : 7 ; learning_rate : 1.5315234270572414e-93 ; fit : 0.10143333333333333
    batch_size :          60000
    best_batch_loss : 1.74522424010811
    Epoch 7, Loss: 1.74522424010811, fit: 0.10143333333333333
      batch rate adapted learning_rate :              1e-17
      loss :              1.74522424010811
      loss_factor :              0.174522424010811
      loss adapted learning_rate :              1.5315234270573144e-93
    epoch : 8 ; learning_rate : 1.5315234270573144e-93 ; fit : 0.10143333333333333
    batch_size :          60000
    best_batch_loss : 1.7452242401081095
    Epoch 8, Loss: 1.7452242401081095, fit: 0.10143333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7452242401081095_fit_0.10143333333333333_2024-01-03_182802
  self.fit : 0.10143333333333333
  self.loss : 1.7452242401081095
  current_accuracy : 0.1052
   Accuracy mean: 0.1052
   Accuracy mean: 0.1052
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-10
      loss_factor :              1
      loss adapted learning_rate :              1e-10
    epoch : 0 ; learning_rate : 1e-10 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7247470919573837
    Epoch 0, Loss: 1.7247470919573837, fit: 0.10871666666666667
    Epoch 0, Loss: 1.7247470919573837
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7247470919573837_fit_0.10871666666666667_2024-01-03_182805
  self.fit : 0.10871666666666667
  self.loss : 1.7247470919573837
  current_accuracy : 0.1058
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-10
      loss_factor :              1
      loss adapted learning_rate :              1e-10
    epoch : 0 ; learning_rate : 1e-10 ; fit : 0.10871666666666667
    batch_size :          60000
    best_batch_loss : 1.7247470917950842
    Epoch 0, Loss: 1.7247470917950842, fit: 0.10871666666666667
    Epoch 0, Loss: 1.7247470917950842
      batch rate adapted learning_rate :              1e-10
      loss :              1.7247470917950842
      loss_factor :              0.1724747091795084
      loss adapted learning_rate :              4.704810469193081e-87
    epoch : 1 ; learning_rate : 4.704810469193081e-87 ; fit : 0.10871666666666667
    batch_size :          60000
    best_batch_loss : 1.7247470916327865
    Epoch 1, Loss: 1.7247470916327865, fit: 0.10871666666666667
      batch rate adapted learning_rate :              1e-10
      loss :              1.7247470916327865
      loss_factor :              0.17247470916327864
      loss adapted learning_rate :              4.704810424921095e-87
    epoch : 2 ; learning_rate : 4.704810424921095e-87 ; fit : 0.10871666666666667
    batch_size :          60000
    best_batch_loss : 1.7247470916327856
    Epoch 2, Loss: 1.7247470916327856, fit: 0.10871666666666667
      batch rate adapted learning_rate :              1e-10
      loss :              1.7247470916327856
      loss_factor :              0.17247470916327856
      loss adapted learning_rate :              4.7048104249208685e-87
    epoch : 3 ; learning_rate : 4.7048104249208685e-87 ; fit : 0.10871666666666667
    batch_size :          60000
    best_batch_loss : 1.7247470916327858
    Epoch 3, Loss: 1.7247470916327858, fit: 0.10871666666666667
      batch rate adapted learning_rate :              1e-10
      loss :              1.7247470916327858
      loss_factor :              0.1724747091632786
      loss adapted learning_rate :              4.7048104249209435e-87
    epoch : 4 ; learning_rate : 4.7048104249209435e-87 ; fit : 0.10871666666666667
    batch_size :          60000
    best_batch_loss : 1.7247470916327852
    Epoch 4, Loss: 1.7247470916327852, fit: 0.10871666666666667
      batch rate adapted learning_rate :              1e-10
      loss :              1.7247470916327852
      loss_factor :              0.1724747091632785
      loss adapted learning_rate :              4.704810424920716e-87
    epoch : 5 ; learning_rate : 4.704810424920716e-87 ; fit : 0.10871666666666667
    batch_size :          60000
    best_batch_loss : 1.7247470916327863
    Epoch 5, Loss: 1.7247470916327863, fit: 0.10871666666666667
      batch rate adapted learning_rate :              1e-10
      loss :              1.7247470916327863
      loss_factor :              0.17247470916327862
      loss adapted learning_rate :              4.704810424921019e-87
    epoch : 6 ; learning_rate : 4.704810424921019e-87 ; fit : 0.10871666666666667
    batch_size :          60000
    best_batch_loss : 1.7247470916327865
    Epoch 6, Loss: 1.7247470916327865, fit: 0.10871666666666667
      batch rate adapted learning_rate :              1e-10
      loss :              1.7247470916327865
      loss_factor :              0.17247470916327864
      loss adapted learning_rate :              4.704810424921095e-87
    epoch : 7 ; learning_rate : 4.704810424921095e-87 ; fit : 0.10871666666666667
    batch_size :          60000
    best_batch_loss : 1.7247470916327856
    Epoch 7, Loss: 1.7247470916327856, fit: 0.10871666666666667
      batch rate adapted learning_rate :              1e-10
      loss :              1.7247470916327856
      loss_factor :              0.17247470916327856
      loss adapted learning_rate :              4.7048104249208685e-87
    epoch : 8 ; learning_rate : 4.7048104249208685e-87 ; fit : 0.10871666666666667
    batch_size :          60000
    best_batch_loss : 1.7247470916327852
    Epoch 8, Loss: 1.7247470916327852, fit: 0.10871666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7247470916327852_fit_0.10871666666666667_2024-01-03_182826
  self.fit : 0.10871666666666667
  self.loss : 1.7247470916327852
  current_accuracy : 0.1058
   Accuracy mean: 0.1058
   Accuracy mean: 0.1058
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-07
      loss_factor :              1
      loss adapted learning_rate :              1e-07
    epoch : 0 ; learning_rate : 1e-07 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.8231836955091134
    Epoch 0, Loss: 1.8231836955091134, fit: 0.060533333333333335
    Epoch 0, Loss: 1.8231836955091134
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.8231836955091134_fit_0.060533333333333335_2024-01-03_182829
  self.fit : 0.060533333333333335
  self.loss : 1.8231836955091134
  current_accuracy : 0.0585
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          1.0
      batch rate adapted learning_rate :              1e-07
      loss_factor :              1
      loss adapted learning_rate :              1e-07
    epoch : 0 ; learning_rate : 1e-07 ; fit : 0.060533333333333335
    batch_size :          60000
    best_batch_loss : 1.8231835998773067
    Epoch 0, Loss: 1.8231835998773067, fit: 0.060533333333333335
    Epoch 0, Loss: 1.8231835998773067
      batch rate adapted learning_rate :              1e-07
      loss :              1.8231835998773067
      loss_factor :              0.18231835998773066
      loss adapted learning_rate :              1.2107111920646386e-81
    epoch : 1 ; learning_rate : 1.2107111920646386e-81 ; fit : 0.060533333333333335
    batch_size :          60000
    best_batch_loss : 1.8231835042458382
    Epoch 1, Loss: 1.8231835042458382, fit: 0.060533333333333335
      batch rate adapted learning_rate :              1e-07
      loss :              1.8231835042458382
      loss_factor :              0.18231835042458383
      loss adapted learning_rate :              1.210704841536454e-81
    epoch : 2 ; learning_rate : 1.210704841536454e-81 ; fit : 0.060533333333333335
    batch_size :          60000
    best_batch_loss : 1.8231835042458389
    Epoch 2, Loss: 1.8231835042458389, fit: 0.060533333333333335
      batch rate adapted learning_rate :              1e-07
      loss :              1.8231835042458389
      loss_factor :              0.18231835042458389
      loss adapted learning_rate :              1.210704841536491e-81
    epoch : 3 ; learning_rate : 1.210704841536491e-81 ; fit : 0.060533333333333335
    batch_size :          60000
    best_batch_loss : 1.8231835042458382
    Epoch 3, Loss: 1.8231835042458382, fit: 0.060533333333333335
      batch rate adapted learning_rate :              1e-07
      loss :              1.8231835042458382
      loss_factor :              0.18231835042458383
      loss adapted learning_rate :              1.210704841536454e-81
    epoch : 4 ; learning_rate : 1.210704841536454e-81 ; fit : 0.060533333333333335
    batch_size :          60000
    best_batch_loss : 1.823183504245839
    Epoch 4, Loss: 1.823183504245839, fit: 0.060533333333333335
      batch rate adapted learning_rate :              1e-07
      loss :              1.823183504245839
      loss_factor :              0.1823183504245839
      loss adapted learning_rate :              1.2107048415365095e-81
    epoch : 5 ; learning_rate : 1.2107048415365095e-81 ; fit : 0.060533333333333335
    batch_size :          60000
    best_batch_loss : 1.823183504245839
    Epoch 5, Loss: 1.823183504245839, fit: 0.060533333333333335
      batch rate adapted learning_rate :              1e-07
      loss :              1.823183504245839
      loss_factor :              0.1823183504245839
      loss adapted learning_rate :              1.2107048415365095e-81
    epoch : 6 ; learning_rate : 1.2107048415365095e-81 ; fit : 0.060533333333333335
    batch_size :          60000
    best_batch_loss : 1.8231835042458382
    Epoch 6, Loss: 1.8231835042458382, fit: 0.060533333333333335
      batch rate adapted learning_rate :              1e-07
      loss :              1.8231835042458382
      loss_factor :              0.18231835042458383
      loss adapted learning_rate :              1.210704841536454e-81
    epoch : 7 ; learning_rate : 1.210704841536454e-81 ; fit : 0.060533333333333335
    batch_size :          60000
    best_batch_loss : 1.8231835042458384
    Epoch 7, Loss: 1.8231835042458384, fit: 0.060533333333333335
      batch rate adapted learning_rate :              1e-07
      loss :              1.8231835042458384
      loss_factor :              0.18231835042458383
      loss adapted learning_rate :              1.210704841536454e-81
    epoch : 8 ; learning_rate : 1.210704841536454e-81 ; fit : 0.060533333333333335
    batch_size :          60000
    best_batch_loss : 1.8231835042458384
    Epoch 8, Loss: 1.8231835042458384, fit: 0.060533333333333335
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.8231835042458384_fit_0.060533333333333335_2024-01-03_182852
  self.fit : 0.060533333333333335
  self.loss : 1.8231835042458384
  current_accuracy : 0.0585
   Accuracy mean: 0.0585
   Accuracy mean: 0.0585
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.0001
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6232845993598637
    Epoch 0, Loss: 1.6232845993598637, fit: 0.15596666666666667
    Epoch 0, Loss: 1.6232845993598637
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6232845993598637_fit_0.15596666666666667_2024-01-03_182854
  self.fit : 0.15596666666666667
  self.loss : 1.6232845993598637
  current_accuracy : 0.1507
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.0001
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    epoch : 0 ; learning_rate : 0.0001 ; fit : 0.15596666666666667
    batch_size :          60000
    best_batch_loss : 1.623143491133188
    Epoch 0, Loss: 1.623143491133188, fit: 0.15591666666666668
    Epoch 0, Loss: 1.623143491133188
      batch rate adapted learning_rate :              0.0001
      loss :              1.623143491133188
      loss_factor :              0.1623143491133188
      loss adapted learning_rate :              1.0856540479088183e-83
    epoch : 1 ; learning_rate : 1.0856540479088183e-83 ; fit : 0.15591666666666668
    batch_size :          60000
    best_batch_loss : 1.6230027684237402
    Epoch 1, Loss: 1.6230027684237402, fit: 0.156
      batch rate adapted learning_rate :              0.0001
      loss :              1.6230027684237402
      loss_factor :              0.16230027684237402
      loss adapted learning_rate :              1.0762819628834564e-83
    epoch : 2 ; learning_rate : 1.0762819628834564e-83 ; fit : 0.156
    batch_size :          60000
    best_batch_loss : 1.623002768423741
    Epoch 2, Loss: 1.623002768423741, fit: 0.156
      batch rate adapted learning_rate :              0.0001
      loss :              1.623002768423741
      loss_factor :              0.16230027684237408
      loss adapted learning_rate :              1.0762819628834934e-83
    epoch : 3 ; learning_rate : 1.0762819628834934e-83 ; fit : 0.156
    batch_size :          60000
    best_batch_loss : 1.6230027684237414
    Epoch 3, Loss: 1.6230027684237414, fit: 0.156
      batch rate adapted learning_rate :              0.0001
      loss :              1.6230027684237414
      loss_factor :              0.16230027684237414
      loss adapted learning_rate :              1.0762819628835301e-83
    epoch : 4 ; learning_rate : 1.0762819628835301e-83 ; fit : 0.156
    batch_size :          60000
    best_batch_loss : 1.6230027684237402
    Epoch 4, Loss: 1.6230027684237402, fit: 0.156
      batch rate adapted learning_rate :              0.0001
      loss :              1.6230027684237402
      loss_factor :              0.16230027684237402
      loss adapted learning_rate :              1.0762819628834564e-83
    epoch : 5 ; learning_rate : 1.0762819628834564e-83 ; fit : 0.156
    batch_size :          60000
    best_batch_loss : 1.6230027684237402
    Epoch 5, Loss: 1.6230027684237402, fit: 0.156
      batch rate adapted learning_rate :              0.0001
      loss :              1.6230027684237402
      loss_factor :              0.16230027684237402
      loss adapted learning_rate :              1.0762819628834564e-83
    epoch : 6 ; learning_rate : 1.0762819628834564e-83 ; fit : 0.156
    batch_size :          60000
    best_batch_loss : 1.623002768423741
    Epoch 6, Loss: 1.623002768423741, fit: 0.156
      batch rate adapted learning_rate :              0.0001
      loss :              1.623002768423741
      loss_factor :              0.16230027684237408
      loss adapted learning_rate :              1.0762819628834934e-83
    epoch : 7 ; learning_rate : 1.0762819628834934e-83 ; fit : 0.156
    batch_size :          60000
    best_batch_loss : 1.6230027684237405
    Epoch 7, Loss: 1.6230027684237405, fit: 0.156
      batch rate adapted learning_rate :              0.0001
      loss :              1.6230027684237405
      loss_factor :              0.16230027684237405
      loss adapted learning_rate :              1.0762819628834749e-83
    epoch : 8 ; learning_rate : 1.0762819628834749e-83 ; fit : 0.156
    batch_size :          60000
    best_batch_loss : 1.62300276842374
    Epoch 8, Loss: 1.62300276842374, fit: 0.156
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.62300276842374_fit_0.156_2024-01-03_182916
  self.fit : 0.156
  self.loss : 1.62300276842374
  current_accuracy : 0.1507
   Accuracy mean: 0.1507
   Accuracy mean: 0.1507
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.001
      loss_factor :              1
      loss adapted learning_rate :              0.001
    epoch : 0 ; learning_rate : 0.001 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7527690838014935
    Epoch 0, Loss: 1.7527690838014935, fit: 0.09358333333333334
    Epoch 0, Loss: 1.7527690838014935
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7527690838014935_fit_0.09358333333333334_2024-01-03_182919
  self.fit : 0.09358333333333334
  self.loss : 1.7527690838014935
  current_accuracy : 0.0874
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.001
      loss_factor :              1
      loss adapted learning_rate :              0.001
    epoch : 0 ; learning_rate : 0.001 ; fit : 0.09358333333333334
    batch_size :          60000
    best_batch_loss : 1.752470393966051
    Epoch 0, Loss: 1.752470393966051, fit: 0.09388333333333333
    Epoch 0, Loss: 1.752470393966051
      batch rate adapted learning_rate :              0.001
      loss :              1.752470393966051
      loss_factor :              0.1752470393966051
      loss adapted learning_rate :              2.317762745416055e-79
    epoch : 1 ; learning_rate : 2.317762745416055e-79 ; fit : 0.09388333333333333
    batch_size :          60000
    best_batch_loss : 1.7521511729866657
    Epoch 1, Loss: 1.7521511729866657, fit: 0.0938
      batch rate adapted learning_rate :              0.001
      loss :              1.7521511729866657
      loss_factor :              0.17521511729866657
      loss adapted learning_rate :              2.2759219941366944e-79
    epoch : 2 ; learning_rate : 2.2759219941366944e-79 ; fit : 0.0938
    batch_size :          60000
    best_batch_loss : 1.7521511729866666
    Epoch 2, Loss: 1.7521511729866666, fit: 0.0938
      batch rate adapted learning_rate :              0.001
      loss :              1.7521511729866666
      loss_factor :              0.17521511729866665
      loss adapted learning_rate :              2.2759219941368026e-79
    epoch : 3 ; learning_rate : 2.2759219941368026e-79 ; fit : 0.0938
    batch_size :          60000
    best_batch_loss : 1.7521511729866663
    Epoch 3, Loss: 1.7521511729866663, fit: 0.0938
      batch rate adapted learning_rate :              0.001
      loss :              1.7521511729866663
      loss_factor :              0.17521511729866662
      loss adapted learning_rate :              2.2759219941367666e-79
    epoch : 4 ; learning_rate : 2.2759219941367666e-79 ; fit : 0.0938
    batch_size :          60000
    best_batch_loss : 1.7521511729866663
    Epoch 4, Loss: 1.7521511729866663, fit: 0.0938
      batch rate adapted learning_rate :              0.001
      loss :              1.7521511729866663
      loss_factor :              0.17521511729866662
      loss adapted learning_rate :              2.2759219941367666e-79
    epoch : 5 ; learning_rate : 2.2759219941367666e-79 ; fit : 0.0938
    batch_size :          60000
    best_batch_loss : 1.7521511729866655
    Epoch 5, Loss: 1.7521511729866655, fit: 0.0938
      batch rate adapted learning_rate :              0.001
      loss :              1.7521511729866655
      loss_factor :              0.17521511729866654
      loss adapted learning_rate :              2.2759219941366582e-79
    epoch : 6 ; learning_rate : 2.2759219941366582e-79 ; fit : 0.0938
    batch_size :          60000
    best_batch_loss : 1.7521511729866668
    Epoch 6, Loss: 1.7521511729866668, fit: 0.0938
      batch rate adapted learning_rate :              0.001
      loss :              1.7521511729866668
      loss_factor :              0.17521511729866668
      loss adapted learning_rate :              2.2759219941368388e-79
    epoch : 7 ; learning_rate : 2.2759219941368388e-79 ; fit : 0.0938
    batch_size :          60000
    best_batch_loss : 1.7521511729866666
    Epoch 7, Loss: 1.7521511729866666, fit: 0.0938
      batch rate adapted learning_rate :              0.001
      loss :              1.7521511729866666
      loss_factor :              0.17521511729866665
      loss adapted learning_rate :              2.2759219941368026e-79
    epoch : 8 ; learning_rate : 2.2759219941368026e-79 ; fit : 0.0938
    batch_size :          60000
    best_batch_loss : 1.752151172986666
    Epoch 8, Loss: 1.752151172986666, fit: 0.0938
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.752151172986666_fit_0.0938_2024-01-03_182941
  self.fit : 0.0938
  self.loss : 1.752151172986666
  current_accuracy : 0.0872
   Accuracy mean: 0.0874
   Accuracy mean: 0.0872
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.02
      loss_factor :              1
      loss adapted learning_rate :              0.02
    epoch : 0 ; learning_rate : 0.02 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.702919464870605
    Epoch 0, Loss: 1.702919464870605, fit: 0.11746666666666666
    Epoch 0, Loss: 1.702919464870605
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.702919464870605_fit_0.11746666666666666_2024-01-03_182943
  self.fit : 0.11746666666666666
  self.loss : 1.702919464870605
  current_accuracy : 0.1258
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.02
      loss_factor :              1
      loss adapted learning_rate :              0.02
    epoch : 0 ; learning_rate : 0.02 ; fit : 0.11746666666666666
    batch_size :          60000
    best_batch_loss : 1.688129018214797
    Epoch 0, Loss: 1.688129018214797, fit: 0.12386666666666667
    Epoch 0, Loss: 1.688129018214797
      batch rate adapted learning_rate :              0.02
      loss :              1.688129018214797
      loss_factor :              0.1688129018214797
      loss adapted learning_rate :              1.100508808632397e-79
    epoch : 1 ; learning_rate : 1.100508808632397e-79 ; fit : 0.12386666666666667
    batch_size :          60000
    best_batch_loss : 1.672129655168041
    Epoch 1, Loss: 1.672129655168041, fit: 0.13135
      batch rate adapted learning_rate :              0.02
      loss :              1.672129655168041
      loss_factor :              0.1672129655168041
      loss adapted learning_rate :              4.246439198006847e-80
    epoch : 2 ; learning_rate : 4.246439198006847e-80 ; fit : 0.13135
    batch_size :          60000
    best_batch_loss : 1.6721296551680407
    Epoch 2, Loss: 1.6721296551680407, fit: 0.13135
      batch rate adapted learning_rate :              0.02
      loss :              1.6721296551680407
      loss_factor :              0.16721296551680406
      loss adapted learning_rate :              4.246439198006776e-80
    epoch : 3 ; learning_rate : 4.246439198006776e-80 ; fit : 0.13135
    batch_size :          60000
    best_batch_loss : 1.672129655168041
    Epoch 3, Loss: 1.672129655168041, fit: 0.13135
      batch rate adapted learning_rate :              0.02
      loss :              1.672129655168041
      loss_factor :              0.1672129655168041
      loss adapted learning_rate :              4.246439198006847e-80
    epoch : 4 ; learning_rate : 4.246439198006847e-80 ; fit : 0.13135
    batch_size :          60000
    best_batch_loss : 1.6721296551680402
    Epoch 4, Loss: 1.6721296551680402, fit: 0.13135
      batch rate adapted learning_rate :              0.02
      loss :              1.6721296551680402
      loss_factor :              0.16721296551680404
      loss adapted learning_rate :              4.246439198006706e-80
    epoch : 5 ; learning_rate : 4.246439198006706e-80 ; fit : 0.13135
    batch_size :          60000
    best_batch_loss : 1.672129655168041
    Epoch 5, Loss: 1.672129655168041, fit: 0.13135
      batch rate adapted learning_rate :              0.02
      loss :              1.672129655168041
      loss_factor :              0.1672129655168041
      loss adapted learning_rate :              4.246439198006847e-80
    epoch : 6 ; learning_rate : 4.246439198006847e-80 ; fit : 0.13135
    batch_size :          60000
    best_batch_loss : 1.6721296551680414
    Epoch 6, Loss: 1.6721296551680414, fit: 0.13135
      batch rate adapted learning_rate :              0.02
      loss :              1.6721296551680414
      loss_factor :              0.16721296551680415
      loss adapted learning_rate :              4.2464391980069876e-80
    epoch : 7 ; learning_rate : 4.2464391980069876e-80 ; fit : 0.13135
    batch_size :          60000
    best_batch_loss : 1.6721296551680402
    Epoch 7, Loss: 1.6721296551680402, fit: 0.13135
      batch rate adapted learning_rate :              0.02
      loss :              1.6721296551680402
      loss_factor :              0.16721296551680404
      loss adapted learning_rate :              4.246439198006706e-80
    epoch : 8 ; learning_rate : 4.246439198006706e-80 ; fit : 0.13135
    batch_size :          60000
    best_batch_loss : 1.6721296551680411
    Epoch 8, Loss: 1.6721296551680411, fit: 0.13135
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.6721296551680411_fit_0.13135_2024-01-03_183005
  self.fit : 0.13135
  self.loss : 1.6721296551680411
  current_accuracy : 0.1314
   Accuracy mean: 0.1258
   Accuracy mean: 0.1314
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.025
      loss_factor :              1
      loss adapted learning_rate :              0.025
    epoch : 0 ; learning_rate : 0.025 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7028828345855922
    Epoch 0, Loss: 1.7028828345855922, fit: 0.11568333333333333
    Epoch 0, Loss: 1.7028828345855922
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7028828345855922_fit_0.11568333333333333_2024-01-03_183008
  self.fit : 0.11568333333333333
  self.loss : 1.7028828345855922
  current_accuracy : 0.1404
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.025
      loss_factor :              1
      loss adapted learning_rate :              0.025
    epoch : 0 ; learning_rate : 0.025 ; fit : 0.11568333333333333
    batch_size :          60000
    best_batch_loss : 1.6554387719621348
    Epoch 0, Loss: 1.6554387719621348, fit: 0.13655
    Epoch 0, Loss: 1.6554387719621348
      batch rate adapted learning_rate :              0.025
      loss :              1.6554387719621348
      loss_factor :              0.1655438771962135
      loss adapted learning_rate :              1.9464904000633786e-80
    epoch : 1 ; learning_rate : 1.9464904000633786e-80 ; fit : 0.13655
    batch_size :          60000
    best_batch_loss : 1.6253825720676365
    Epoch 1, Loss: 1.6253825720676365, fit: 0.15125
      batch rate adapted learning_rate :              0.025
      loss :              1.6253825720676365
      loss_factor :              0.16253825720676365
      loss adapted learning_rate :              3.1153000839447557e-81
    epoch : 2 ; learning_rate : 3.1153000839447557e-81 ; fit : 0.15125
    batch_size :          60000
    best_batch_loss : 1.6253825720676367
    Epoch 2, Loss: 1.6253825720676367, fit: 0.15125
      batch rate adapted learning_rate :              0.025
      loss :              1.6253825720676367
      loss_factor :              0.16253825720676368
      loss adapted learning_rate :              3.115300083944809e-81
    epoch : 3 ; learning_rate : 3.115300083944809e-81 ; fit : 0.15125
    batch_size :          60000
    best_batch_loss : 1.6253825720676365
    Epoch 3, Loss: 1.6253825720676365, fit: 0.15125
      batch rate adapted learning_rate :              0.025
      loss :              1.6253825720676365
      loss_factor :              0.16253825720676365
      loss adapted learning_rate :              3.1153000839447557e-81
    epoch : 4 ; learning_rate : 3.1153000839447557e-81 ; fit : 0.15125
    batch_size :          60000
    best_batch_loss : 1.6253825720676367
    Epoch 4, Loss: 1.6253825720676367, fit: 0.15125
      batch rate adapted learning_rate :              0.025
      loss :              1.6253825720676367
      loss_factor :              0.16253825720676368
      loss adapted learning_rate :              3.115300083944809e-81
    epoch : 5 ; learning_rate : 3.115300083944809e-81 ; fit : 0.15125
    batch_size :          60000
    best_batch_loss : 1.6253825720676374
    Epoch 5, Loss: 1.6253825720676374, fit: 0.15125
      batch rate adapted learning_rate :              0.025
      loss :              1.6253825720676374
      loss_factor :              0.16253825720676374
      loss adapted learning_rate :              3.1153000839449153e-81
    epoch : 6 ; learning_rate : 3.1153000839449153e-81 ; fit : 0.15125
    batch_size :          60000
    best_batch_loss : 1.6253825720676371
    Epoch 6, Loss: 1.6253825720676371, fit: 0.15125
      batch rate adapted learning_rate :              0.025
      loss :              1.6253825720676371
      loss_factor :              0.1625382572067637
      loss adapted learning_rate :              3.115300083944862e-81
    epoch : 7 ; learning_rate : 3.115300083944862e-81 ; fit : 0.15125
    batch_size :          60000
    best_batch_loss : 1.6253825720676365
    Epoch 7, Loss: 1.6253825720676365, fit: 0.15125
      batch rate adapted learning_rate :              0.025
      loss :              1.6253825720676365
      loss_factor :              0.16253825720676365
      loss adapted learning_rate :              3.1153000839447557e-81
    epoch : 8 ; learning_rate : 3.1153000839447557e-81 ; fit : 0.15125
    batch_size :          60000
    best_batch_loss : 1.625382572067637
    Epoch 8, Loss: 1.625382572067637, fit: 0.15125
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.625382572067637_fit_0.15125_2024-01-03_183029
  self.fit : 0.15125
  self.loss : 1.625382572067637
  current_accuracy : 0.1568
   Accuracy mean: 0.1404
   Accuracy mean: 0.1568
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.03
      loss_factor :              1
      loss adapted learning_rate :              0.03
    epoch : 0 ; learning_rate : 0.03 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.6747677678672432
    Epoch 0, Loss: 1.6747677678672432, fit: 0.13518333333333332
    Epoch 0, Loss: 1.6747677678672432
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6747677678672432_fit_0.13518333333333332_2024-01-03_183032
  self.fit : 0.13518333333333332
  self.loss : 1.6747677678672432
  current_accuracy : 0.1599
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.03
      loss_factor :              1
      loss adapted learning_rate :              0.03
    epoch : 0 ; learning_rate : 0.03 ; fit : 0.13518333333333332
    batch_size :          60000
    best_batch_loss : 1.6143603029776745
    Epoch 0, Loss: 1.6143603029776745, fit: 0.1574
    Epoch 0, Loss: 1.6143603029776745
      batch rate adapted learning_rate :              0.03
      loss :              1.6143603029776745
      loss_factor :              0.16143603029776746
      loss adapted learning_rate :              1.8930767845913643e-81
    epoch : 1 ; learning_rate : 1.8930767845913643e-81 ; fit : 0.1574
    batch_size :          60000
    best_batch_loss : 1.5729902475678563
    Epoch 1, Loss: 1.5729902475678563, fit: 0.17721666666666666
      batch rate adapted learning_rate :              0.03
      loss :              1.5729902475678563
      loss_factor :              0.15729902475678564
      loss adapted learning_rate :              1.4116409826339904e-82
    epoch : 2 ; learning_rate : 1.4116409826339904e-82 ; fit : 0.17721666666666666
    batch_size :          60000
    best_batch_loss : 1.5729902475678565
    Epoch 2, Loss: 1.5729902475678565, fit: 0.17721666666666666
      batch rate adapted learning_rate :              0.03
      loss :              1.5729902475678565
      loss_factor :              0.15729902475678564
      loss adapted learning_rate :              1.4116409826339904e-82
    epoch : 3 ; learning_rate : 1.4116409826339904e-82 ; fit : 0.17721666666666666
    batch_size :          60000
    best_batch_loss : 1.5729902475678563
    Epoch 3, Loss: 1.5729902475678563, fit: 0.17721666666666666
      batch rate adapted learning_rate :              0.03
      loss :              1.5729902475678563
      loss_factor :              0.15729902475678564
      loss adapted learning_rate :              1.4116409826339904e-82
    epoch : 4 ; learning_rate : 1.4116409826339904e-82 ; fit : 0.17721666666666666
    batch_size :          60000
    best_batch_loss : 1.5729902475678565
    Epoch 4, Loss: 1.5729902475678565, fit: 0.17721666666666666
      batch rate adapted learning_rate :              0.03
      loss :              1.5729902475678565
      loss_factor :              0.15729902475678564
      loss adapted learning_rate :              1.4116409826339904e-82
    epoch : 5 ; learning_rate : 1.4116409826339904e-82 ; fit : 0.17721666666666666
    batch_size :          60000
    best_batch_loss : 1.5729902475678563
    Epoch 5, Loss: 1.5729902475678563, fit: 0.17721666666666666
      batch rate adapted learning_rate :              0.03
      loss :              1.5729902475678563
      loss_factor :              0.15729902475678564
      loss adapted learning_rate :              1.4116409826339904e-82
    epoch : 6 ; learning_rate : 1.4116409826339904e-82 ; fit : 0.17721666666666666
    batch_size :          60000
    best_batch_loss : 1.5729902475678565
    Epoch 6, Loss: 1.5729902475678565, fit: 0.17721666666666666
      batch rate adapted learning_rate :              0.03
      loss :              1.5729902475678565
      loss_factor :              0.15729902475678564
      loss adapted learning_rate :              1.4116409826339904e-82
    epoch : 7 ; learning_rate : 1.4116409826339904e-82 ; fit : 0.17721666666666666
    batch_size :          60000
    best_batch_loss : 1.5729902475678572
    Epoch 7, Loss: 1.5729902475678572, fit: 0.17721666666666666
      batch rate adapted learning_rate :              0.03
      loss :              1.5729902475678572
      loss_factor :              0.15729902475678573
      loss adapted learning_rate :              1.4116409826340653e-82
    epoch : 8 ; learning_rate : 1.4116409826340653e-82 ; fit : 0.17721666666666666
    batch_size :          60000
    best_batch_loss : 1.5729902475678563
    Epoch 8, Loss: 1.5729902475678563, fit: 0.17721666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.5729902475678563_fit_0.17721666666666666_2024-01-03_183056
  self.fit : 0.17721666666666666
  self.loss : 1.5729902475678563
  current_accuracy : 0.183
   Accuracy mean: 0.1599
   Accuracy mean: 0.183
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.1
      loss_factor :              1
      loss adapted learning_rate :              0.1
    epoch : 0 ; learning_rate : 0.1 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.797957031750971
    Epoch 0, Loss: 1.797957031750971, fit: 0.06973333333333333
    Epoch 0, Loss: 1.797957031750971
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.797957031750971_fit_0.06973333333333333_2024-01-03_183059
  self.fit : 0.06973333333333333
  self.loss : 1.797957031750971
  current_accuracy : 0.148
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.1
      loss_factor :              1
      loss adapted learning_rate :              0.1
    epoch : 0 ; learning_rate : 0.1 ; fit : 0.06973333333333333
    batch_size :          60000
    best_batch_loss : 1.6423747618515367
    Epoch 0, Loss: 1.6423747618515367, fit: 0.1438
    Epoch 0, Loss: 1.6423747618515367
      batch rate adapted learning_rate :              0.1
      loss :              1.6423747618515367
      loss_factor :              0.16423747618515366
      loss adapted learning_rate :              3.52554503743264e-80
    epoch : 1 ; learning_rate : 3.52554503743264e-80 ; fit : 0.1438
    batch_size :          60000
    best_batch_loss : 1.5737430607922898
    Epoch 1, Loss: 1.5737430607922898, fit: 0.17676666666666666
      batch rate adapted learning_rate :              0.1
      loss :              1.5737430607922898
      loss_factor :              0.15737430607922898
      loss adapted learning_rate :              4.936087114333955e-82
    epoch : 2 ; learning_rate : 4.936087114333955e-82 ; fit : 0.17676666666666666
    batch_size :          60000
    best_batch_loss : 1.5737430607922893
    Epoch 2, Loss: 1.5737430607922893, fit: 0.17676666666666666
      batch rate adapted learning_rate :              0.1
      loss :              1.5737430607922893
      loss_factor :              0.15737430607922892
      loss adapted learning_rate :              4.9360871143337804e-82
    epoch : 3 ; learning_rate : 4.9360871143337804e-82 ; fit : 0.17676666666666666
    batch_size :          60000
    best_batch_loss : 1.5737430607922889
    Epoch 3, Loss: 1.5737430607922889, fit: 0.17676666666666666
      batch rate adapted learning_rate :              0.1
      loss :              1.5737430607922889
      loss_factor :              0.1573743060792289
      loss adapted learning_rate :              4.936087114333694e-82
    epoch : 4 ; learning_rate : 4.936087114333694e-82 ; fit : 0.17676666666666666
    batch_size :          60000
    best_batch_loss : 1.5737430607922887
    Epoch 4, Loss: 1.5737430607922887, fit: 0.17676666666666666
      batch rate adapted learning_rate :              0.1
      loss :              1.5737430607922887
      loss_factor :              0.15737430607922887
      loss adapted learning_rate :              4.9360871143336066e-82
    epoch : 5 ; learning_rate : 4.9360871143336066e-82 ; fit : 0.17676666666666666
    batch_size :          60000
    best_batch_loss : 1.5737430607922893
    Epoch 5, Loss: 1.5737430607922893, fit: 0.17676666666666666
      batch rate adapted learning_rate :              0.1
      loss :              1.5737430607922893
      loss_factor :              0.15737430607922892
      loss adapted learning_rate :              4.9360871143337804e-82
    epoch : 6 ; learning_rate : 4.9360871143337804e-82 ; fit : 0.17676666666666666
    batch_size :          60000
    best_batch_loss : 1.573743060792289
    Epoch 6, Loss: 1.573743060792289, fit: 0.17676666666666666
      batch rate adapted learning_rate :              0.1
      loss :              1.573743060792289
      loss_factor :              0.15737430607922892
      loss adapted learning_rate :              4.9360871143337804e-82
    epoch : 7 ; learning_rate : 4.9360871143337804e-82 ; fit : 0.17676666666666666
    batch_size :          60000
    best_batch_loss : 1.5737430607922898
    Epoch 7, Loss: 1.5737430607922898, fit: 0.17676666666666666
      batch rate adapted learning_rate :              0.1
      loss :              1.5737430607922898
      loss_factor :              0.15737430607922898
      loss adapted learning_rate :              4.936087114333955e-82
    epoch : 8 ; learning_rate : 4.936087114333955e-82 ; fit : 0.17676666666666666
    batch_size :          60000
    best_batch_loss : 1.5737430607922902
    Epoch 8, Loss: 1.5737430607922902, fit: 0.17676666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.5737430607922902_fit_0.17676666666666666_2024-01-03_183121
  self.fit : 0.17676666666666666
  self.loss : 1.5737430607922902
  current_accuracy : 0.1787
   Accuracy mean: 0.148
   Accuracy mean: 0.1787
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.9
      loss_factor :              1
      loss adapted learning_rate :              0.9
    epoch : 0 ; learning_rate : 0.9 ; fit : 0.0
    batch_size :          60000
    best_batch_loss : 1.7990367946802903
    Epoch 0, Loss: 1.7990367946802903, fit: 0.07868333333333333
    Epoch 0, Loss: 1.7990367946802903
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7990367946802903_fit_0.07868333333333333_2024-01-03_183124
  self.fit : 0.07868333333333333
  self.loss : 1.7990367946802903
  current_accuracy : 0.3026
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          1.0
      batch rate adapted learning_rate :              0.9
      loss_factor :              1
      loss adapted learning_rate :              0.9
    epoch : 0 ; learning_rate : 0.9 ; fit : 0.07868333333333333
    batch_size :          60000
    best_batch_loss : 1.3594772783893068
    Epoch 0, Loss: 1.3594772783893068, fit: 0.2985
    Epoch 0, Loss: 1.3594772783893068
      batch rate adapted learning_rate :              0.9
      loss :              1.3594772783893068
      loss_factor :              0.13594772783893067
      loss adapted learning_rate :              1.9563107895752148e-87
    epoch : 1 ; learning_rate : 1.9563107895752148e-87 ; fit : 0.2985
    batch_size :          60000
    best_batch_loss : 1.0825146750436578
    Epoch 1, Loss: 1.0825146750436578, fit: 0.4325
      batch rate adapted learning_rate :              0.9
      loss :              1.0825146750436578
      loss_factor :              0.10825146750436579
      loss adapted learning_rate :              2.4981687109999193e-97
    epoch : 2 ; learning_rate : 2.4981687109999193e-97 ; fit : 0.4325
    batch_size :          60000
    best_batch_loss : 1.0825146750436585
    Epoch 2, Loss: 1.0825146750436585, fit: 0.4325
      batch rate adapted learning_rate :              0.9
      loss :              1.0825146750436585
      loss_factor :              0.10825146750436584
      loss adapted learning_rate :              2.4981687110000477e-97
    epoch : 3 ; learning_rate : 2.4981687110000477e-97 ; fit : 0.4325
    batch_size :          60000
    best_batch_loss : 1.0825146750436585
    Epoch 3, Loss: 1.0825146750436585, fit: 0.4325
      batch rate adapted learning_rate :              0.9
      loss :              1.0825146750436585
      loss_factor :              0.10825146750436584
      loss adapted learning_rate :              2.4981687110000477e-97
    epoch : 4 ; learning_rate : 2.4981687110000477e-97 ; fit : 0.4325
    batch_size :          60000
    best_batch_loss : 1.082514675043658
    Epoch 4, Loss: 1.082514675043658, fit: 0.4325
      batch rate adapted learning_rate :              0.9
      loss :              1.082514675043658
      loss_factor :              0.1082514675043658
      loss adapted learning_rate :              2.4981687109999516e-97
    epoch : 5 ; learning_rate : 2.4981687109999516e-97 ; fit : 0.4325
    batch_size :          60000
    best_batch_loss : 1.082514675043658
    Epoch 5, Loss: 1.082514675043658, fit: 0.4325
      batch rate adapted learning_rate :              0.9
      loss :              1.082514675043658
      loss_factor :              0.1082514675043658
      loss adapted learning_rate :              2.4981687109999516e-97
    epoch : 6 ; learning_rate : 2.4981687109999516e-97 ; fit : 0.4325
    batch_size :          60000
    best_batch_loss : 1.0825146750436585
    Epoch 6, Loss: 1.0825146750436585, fit: 0.4325
      batch rate adapted learning_rate :              0.9
      loss :              1.0825146750436585
      loss_factor :              0.10825146750436584
      loss adapted learning_rate :              2.4981687110000477e-97
    epoch : 7 ; learning_rate : 2.4981687110000477e-97 ; fit : 0.4325
    batch_size :          60000
    best_batch_loss : 1.082514675043658
    Epoch 7, Loss: 1.082514675043658, fit: 0.4325
      batch rate adapted learning_rate :              0.9
      loss :              1.082514675043658
      loss_factor :              0.1082514675043658
      loss adapted learning_rate :              2.4981687109999516e-97
    epoch : 8 ; learning_rate : 2.4981687109999516e-97 ; fit : 0.4325
    batch_size :          60000
    best_batch_loss : 1.0825146750436578
    Epoch 8, Loss: 1.0825146750436578, fit: 0.4325
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.0825146750436578_fit_0.4325_2024-01-03_183145
  self.fit : 0.4325
  self.loss : 1.0825146750436578
  current_accuracy : 0.4449
   Accuracy mean: 0.3026
   Accuracy mean: 0.4449
  Error saving file: doc/out/test_combinations_results/20240103183146.
  normalized_accuracies :      [0.04373706 0.04373706 0.18193582 0.18193582 0.12085921 0.12085921
   0.12241201 0.12241201 0.         0.         0.23861284 0.23861284
   0.07479296 0.07427536 0.17417184 0.1886646  0.21195652 0.25439959
   0.26242236 0.32220497 0.23162526 0.3110766  0.63172878 1.        ]
batch_rate :  0.5
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.5
      batch rate adapted learning_rate :              9.999999999999999e-33
      loss_factor :              1
      loss adapted learning_rate :              9.999999999999999e-33
    epoch : 0 ; learning_rate : 9.999999999999999e-33 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.6649865670190152
    Epoch 0, Loss: 1.6666183208585243, fit: 0.1352
    Epoch 0, Loss: 1.6666183208585243
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.6682500746980338_fit_0.1352_2024-01-03_183149
  self.fit : 0.1352
  self.loss : 1.6682500746980338
  current_accuracy : 0.1379
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-64
    batch_rate :          0.5
      batch rate adapted learning_rate :              9.999999999999999e-33
      loss_factor :              1
      loss adapted learning_rate :              9.999999999999999e-33
    epoch : 0 ; learning_rate : 9.999999999999999e-33 ; fit : 0.1352
    batch_size :          30000
    best_batch_loss : 1.6645991190224818
    Epoch 0, Loss: 1.666618320858525, fit: 0.135
    Epoch 0, Loss: 1.666618320858525
      batch rate adapted learning_rate :              9.999999999999999e-33
      loss :              1.6686375226945682
      loss_factor :              0.16686375226945682
      loss adapted learning_rate :              1.7226638289872403e-110
    epoch : 1 ; learning_rate : 1.7226638289872403e-110 ; fit : 0.135
    batch_size :          30000
    best_batch_loss : 1.6650399142054948
    Epoch 1, Loss: 1.666618320858525, fit: 0.13586666666666666
      batch rate adapted learning_rate :              9.999999999999999e-33
      loss :              1.6681967275115552
      loss_factor :              0.16681967275115553
      loss adapted learning_rate :              1.677747076671573e-110
    epoch : 2 ; learning_rate : 1.677747076671573e-110 ; fit : 0.13586666666666666
    batch_size :          30000
    best_batch_loss : 1.6658022915403605
    Epoch 2, Loss: 1.6666183208585248, fit: 0.13683333333333333
      batch rate adapted learning_rate :              9.999999999999999e-33
      loss :              1.6658022915403605
      loss_factor :              0.16658022915403606
      loss adapted learning_rate :              1.4532673552543762e-110
    epoch : 3 ; learning_rate : 1.4532673552543762e-110 ; fit : 0.13683333333333333
    batch_size :          30000
    best_batch_loss : 1.6644587963391104
    Epoch 3, Loss: 1.6666183208585246, fit: 0.1376
      batch rate adapted learning_rate :              9.999999999999999e-33
      loss :              1.6644587963391104
      loss_factor :              0.16644587963391105
      loss adapted learning_rate :              1.3406174468450257e-110
    epoch : 4 ; learning_rate : 1.3406174468450257e-110 ; fit : 0.1376
    batch_size :          30000
    best_batch_loss : 1.660202234089128
    Epoch 4, Loss: 1.6666183208585248, fit: 0.13296666666666668
      batch rate adapted learning_rate :              9.999999999999999e-33
      loss :              1.6730344076279213
      loss_factor :              0.16730344076279213
      loss adapted learning_rate :              2.2412343784251974e-110
    epoch : 5 ; learning_rate : 2.2412343784251974e-110 ; fit : 0.13296666666666668
    batch_size :          30000
    best_batch_loss : 1.6655612706283816
    Epoch 5, Loss: 1.6666183208585246, fit: 0.1374
      batch rate adapted learning_rate :              9.999999999999999e-33
      loss :              1.6655612706283816
      loss_factor :              0.16655612706283815
      loss adapted learning_rate :              1.43239026738188e-110
    epoch : 6 ; learning_rate : 1.43239026738188e-110 ; fit : 0.1374
    batch_size :          30000
    best_batch_loss : 1.6630240367806592
    Epoch 6, Loss: 1.666618320858525, fit: 0.13846666666666665
      batch rate adapted learning_rate :              9.999999999999999e-33
      loss :              1.6630240367806592
      loss_factor :              0.16630240367806592
      loss adapted learning_rate :              1.2298514029233578e-110
    epoch : 7 ; learning_rate : 1.2298514029233578e-110 ; fit : 0.13846666666666665
    batch_size :          30000
    best_batch_loss : 1.666027972361285
    Epoch 7, Loss: 1.6666183208585248, fit: 0.1368
      batch rate adapted learning_rate :              9.999999999999999e-33
      loss :              1.666027972361285
      loss_factor :              0.1666027972361285
      loss adapted learning_rate :              1.473088662984236e-110
    epoch : 8 ; learning_rate : 1.473088662984236e-110 ; fit : 0.1368
    batch_size :          30000
    best_batch_loss : 1.6659187669651923
    Epoch 8, Loss: 1.6666183208585248, fit: 0.1365
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.6659187669651923_fit_0.1365_2024-01-03_183210
  self.fit : 0.1365
  self.loss : 1.6659187669651923
  current_accuracy : 0.1379
   Accuracy mean: 0.1379
   Accuracy mean: 0.1379
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.5
      batch rate adapted learning_rate :              3.1622776601683796e-17
      loss_factor :              1
      loss adapted learning_rate :              3.1622776601683796e-17
    epoch : 0 ; learning_rate : 3.1622776601683796e-17 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7861301063400445
    Epoch 0, Loss: 1.7862670285365143, fit: 0.07523333333333333
    Epoch 0, Loss: 1.7862670285365143
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.786403950732984_fit_0.07523333333333333_2024-01-03_183213
  self.fit : 0.07523333333333333
  self.loss : 1.786403950732984
  current_accuracy : 0.0711
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-33
    batch_rate :          0.5
      batch rate adapted learning_rate :              3.1622776601683796e-17
      loss_factor :              1
      loss adapted learning_rate :              3.1622776601683796e-17
    epoch : 0 ; learning_rate : 3.1622776601683796e-17 ; fit : 0.07523333333333333
    batch_size :          30000
    best_batch_loss : 1.7854483399403174
    Epoch 0, Loss: 1.7862670285365143, fit: 0.07486666666666666
    Epoch 0, Loss: 1.7862670285365143
      batch rate adapted learning_rate :              3.1622776601683796e-17
      loss :              1.7854483399403174
      loss_factor :              0.17854483399403173
      loss adapted learning_rate :              4.7285598017768935e-92
    epoch : 1 ; learning_rate : 4.7285598017768935e-92 ; fit : 0.07486666666666666
    batch_size :          30000
    best_batch_loss : 1.7857826605102838
    Epoch 1, Loss: 1.7862670285365145, fit: 0.0763
      batch rate adapted learning_rate :              3.1622776601683796e-17
      loss :              1.7857826605102838
      loss_factor :              0.1785782660510284
      loss adapted learning_rate :              4.817926564697198e-92
    epoch : 2 ; learning_rate : 4.817926564697198e-92 ; fit : 0.0763
    batch_size :          30000
    best_batch_loss : 1.786030305218348
    Epoch 2, Loss: 1.7862670285365143, fit: 0.07556666666666667
      batch rate adapted learning_rate :              3.1622776601683796e-17
      loss :              1.7865037518546807
      loss_factor :              0.17865037518546806
      loss adapted learning_rate :              5.0164127218443795e-92
    epoch : 3 ; learning_rate : 5.0164127218443795e-92 ; fit : 0.07556666666666667
    batch_size :          30000
    best_batch_loss : 1.7844077465166284
    Epoch 3, Loss: 1.7862670285365145, fit: 0.07623333333333333
      batch rate adapted learning_rate :              3.1622776601683796e-17
      loss :              1.7844077465166284
      loss_factor :              0.17844077465166283
      loss adapted learning_rate :              4.4607716749181195e-92
    epoch : 4 ; learning_rate : 4.4607716749181195e-92 ; fit : 0.07623333333333333
    batch_size :          30000
    best_batch_loss : 1.784046452466129
    Epoch 4, Loss: 1.7862670285365145, fit: 0.0763
      batch rate adapted learning_rate :              3.1622776601683796e-17
      loss :              1.784046452466129
      loss_factor :              0.1784046452466129
      loss adapted learning_rate :              4.371352427982202e-92
    epoch : 5 ; learning_rate : 4.371352427982202e-92 ; fit : 0.0763
    batch_size :          30000
    best_batch_loss : 1.7860049284736577
    Epoch 5, Loss: 1.786267028536514, fit: 0.07526666666666666
      batch rate adapted learning_rate :              3.1622776601683796e-17
      loss :              1.7860049284736577
      loss_factor :              0.17860049284736576
      loss adapted learning_rate :              4.8782639912101724e-92
    epoch : 6 ; learning_rate : 4.8782639912101724e-92 ; fit : 0.07526666666666666
    batch_size :          30000
    best_batch_loss : 1.783920716273796
    Epoch 6, Loss: 1.7862670285365145, fit: 0.0765
      batch rate adapted learning_rate :              3.1622776601683796e-17
      loss :              1.783920716273796
      loss_factor :              0.1783920716273796
      loss adapted learning_rate :              4.340651202936006e-92
    epoch : 7 ; learning_rate : 4.340651202936006e-92 ; fit : 0.0765
    batch_size :          30000
    best_batch_loss : 1.785672576262554
    Epoch 7, Loss: 1.7862670285365145, fit: 0.0756
      batch rate adapted learning_rate :              3.1622776601683796e-17
      loss :              1.7868614808104748
      loss_factor :              0.1786861480810475
      loss adapted learning_rate :              5.117863390404271e-92
    epoch : 8 ; learning_rate : 5.117863390404271e-92 ; fit : 0.0756
    batch_size :          30000
    best_batch_loss : 1.7816271534891541
    Epoch 8, Loss: 1.786267028536514, fit: 0.07313333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.790906903583874_fit_0.07313333333333333_2024-01-03_183236
  self.fit : 0.07313333333333333
  self.loss : 1.790906903583874
  current_accuracy : 0.0711
   Accuracy mean: 0.0711
   Accuracy mean: 0.0711
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.5
      batch rate adapted learning_rate :              3.1622776601683795e-09
      loss_factor :              1
      loss adapted learning_rate :              3.1622776601683795e-09
    epoch : 0 ; learning_rate : 3.1622776601683795e-09 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7397071258229602
    Epoch 0, Loss: 1.7422892561657815, fit: 0.1084
    Epoch 0, Loss: 1.7422892561657815
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7397071258229602_fit_0.1084_2024-01-03_183239
  self.fit : 0.1084
  self.loss : 1.7397071258229602
  current_accuracy : 0.103
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          0.5
      batch rate adapted learning_rate :              3.1622776601683795e-09
      loss_factor :              1
      loss adapted learning_rate :              3.1622776601683795e-09
    epoch : 0 ; learning_rate : 3.1622776601683795e-09 ; fit : 0.1084
    batch_size :          30000
    best_batch_loss : 1.7386456244507158
    Epoch 0, Loss: 1.7422892440422837, fit: 0.10916666666666666
    Epoch 0, Loss: 1.7422892440422837
      batch rate adapted learning_rate :              3.1622776601683795e-09
      loss :              1.7386456244507158
      loss_factor :              0.1738645624450716
      loss adapted learning_rate :              3.3197638563391266e-85
    epoch : 1 ; learning_rate : 3.3197638563391266e-85 ; fit : 0.10916666666666666
    batch_size :          30000
    best_batch_loss : 1.7379372458898101
    Epoch 1, Loss: 1.7422892349733765, fit: 0.10486666666666666
      batch rate adapted learning_rate :              3.1622776601683795e-09
      loss :              1.7466412240569429
      loss_factor :              0.1746641224056943
      loss adapted learning_rate :              5.252555040272883e-85
    epoch : 2 ; learning_rate : 5.252555040272883e-85 ; fit : 0.10486666666666666
    batch_size :          30000
    best_batch_loss : 1.7393184568124103
    Epoch 2, Loss: 1.742289234973376, fit: 0.1092
      batch rate adapted learning_rate :              3.1622776601683795e-09
      loss :              1.7393184568124103
      loss_factor :              0.17393184568124104
      loss adapted learning_rate :              3.450726592728768e-85
    epoch : 3 ; learning_rate : 3.450726592728768e-85 ; fit : 0.1092
    batch_size :          30000
    best_batch_loss : 1.7407578065273348
    Epoch 3, Loss: 1.7422892349733758, fit: 0.10653333333333333
      batch rate adapted learning_rate :              3.1622776601683795e-09
      loss :              1.743820663419417
      loss_factor :              0.1743820663419417
      loss adapted learning_rate :              4.4687077539882836e-85
    epoch : 4 ; learning_rate : 4.4687077539882836e-85 ; fit : 0.10653333333333333
    batch_size :          30000
    best_batch_loss : 1.7413677877168874
    Epoch 4, Loss: 1.7422892349733758, fit: 0.10723333333333333
      batch rate adapted learning_rate :              3.1622776601683795e-09
      loss :              1.7432106822298645
      loss_factor :              0.17432106822298646
      loss adapted learning_rate :              4.3150701078108524e-85
    epoch : 5 ; learning_rate : 4.3150701078108524e-85 ; fit : 0.10723333333333333
    batch_size :          30000
    best_batch_loss : 1.7404128098470555
    Epoch 5, Loss: 1.7422892349733763, fit: 0.10836666666666667
      batch rate adapted learning_rate :              3.1622776601683795e-09
      loss :              1.7404128098470555
      loss_factor :              0.17404128098470556
      loss adapted learning_rate :              3.674744209093488e-85
    epoch : 6 ; learning_rate : 3.674744209093488e-85 ; fit : 0.10836666666666667
    batch_size :          30000
    best_batch_loss : 1.739812623585725
    Epoch 6, Loss: 1.7422892349733763, fit: 0.10893333333333333
      batch rate adapted learning_rate :              3.1622776601683795e-09
      loss :              1.739812623585725
      loss_factor :              0.1739812623585725
      loss adapted learning_rate :              3.5501586689595415e-85
    epoch : 7 ; learning_rate : 3.5501586689595415e-85 ; fit : 0.10893333333333333
    batch_size :          30000
    best_batch_loss : 1.741719569618222
    Epoch 7, Loss: 1.7422892349733765, fit: 0.10793333333333334
      batch rate adapted learning_rate :              3.1622776601683795e-09
      loss :              1.741719569618222
      loss_factor :              0.1741719569618222
      loss adapted learning_rate :              3.96116699209993e-85
    epoch : 8 ; learning_rate : 3.96116699209993e-85 ; fit : 0.10793333333333334
    batch_size :          30000
    best_batch_loss : 1.73908587928308
    Epoch 8, Loss: 1.742289234973376, fit: 0.10906666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.73908587928308_fit_0.10906666666666667_2024-01-03_183259
  self.fit : 0.10906666666666667
  self.loss : 1.73908587928308
  current_accuracy : 0.103
   Accuracy mean: 0.103
   Accuracy mean: 0.103
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.5
      batch rate adapted learning_rate :              1e-05
      loss_factor :              1
      loss adapted learning_rate :              1e-05
    epoch : 0 ; learning_rate : 1e-05 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7825521571430902
    Epoch 0, Loss: 1.7859480277314006, fit: 0.0801
    Epoch 0, Loss: 1.7859480277314006
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7893438983197107_fit_0.0801_2024-01-03_183302
  self.fit : 0.0801
  self.loss : 1.7893438983197107
  current_accuracy : 0.0864
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.5
      batch rate adapted learning_rate :              1e-05
      loss_factor :              1
      loss adapted learning_rate :              1e-05
    epoch : 0 ; learning_rate : 1e-05 ; fit : 0.0801
    batch_size :          30000
    best_batch_loss : 1.7827822716273534
    Epoch 0, Loss: 1.7859318521180607, fit: 0.08006666666666666
    Epoch 0, Loss: 1.7859318521180607
      batch rate adapted learning_rate :              1e-05
      loss :              1.7890814326087683
      loss_factor :              0.17890814326087684
      loss adapted learning_rate :              1.8323602224521522e-80
    epoch : 1 ; learning_rate : 1.8323602224521522e-80 ; fit : 0.08006666666666666
    batch_size :          30000
    best_batch_loss : 1.7856044592917995
    Epoch 1, Loss: 1.7859203277263154, fit: 0.08083333333333333
      batch rate adapted learning_rate :              1e-05
      loss :              1.7856044592917995
      loss_factor :              0.17856044592917994
      loss adapted learning_rate :              1.5084335550997488e-80
    epoch : 2 ; learning_rate : 1.5084335550997488e-80 ; fit : 0.08083333333333333
    batch_size :          30000
    best_batch_loss : 1.785517364033404
    Epoch 2, Loss: 1.7859203277263158, fit: 0.08073333333333334
      batch rate adapted learning_rate :              1e-05
      loss :              1.7863232914192275
      loss_factor :              0.17863232914192276
      loss adapted learning_rate :              1.5703848327394766e-80
    epoch : 3 ; learning_rate : 1.5703848327394766e-80 ; fit : 0.08073333333333334
    batch_size :          30000
    best_batch_loss : 1.7850475001148858
    Epoch 3, Loss: 1.7859203277263158, fit: 0.0817
      batch rate adapted learning_rate :              1e-05
      loss :              1.7850475001148858
      loss_factor :              0.17850475001148858
      loss adapted learning_rate :              1.462102157072707e-80
    epoch : 4 ; learning_rate : 1.462102157072707e-80 ; fit : 0.0817
    batch_size :          30000
    best_batch_loss : 1.7857277614886078
    Epoch 4, Loss: 1.785920327726316, fit: 0.08126666666666667
      batch rate adapted learning_rate :              1e-05
      loss :              1.7857277614886078
      loss_factor :              0.17857277614886077
      loss adapted learning_rate :              1.5188854980613963e-80
    epoch : 5 ; learning_rate : 1.5188854980613963e-80 ; fit : 0.08126666666666667
    batch_size :          30000
    best_batch_loss : 1.7853013953191235
    Epoch 5, Loss: 1.7859203277263156, fit: 0.08153333333333333
      batch rate adapted learning_rate :              1e-05
      loss :              1.7853013953191235
      loss_factor :              0.17853013953191235
      loss adapted learning_rate :              1.483045382931752e-80
    epoch : 6 ; learning_rate : 1.483045382931752e-80 ; fit : 0.08153333333333333
    batch_size :          30000
    best_batch_loss : 1.785798428611128
    Epoch 6, Loss: 1.7859203277263158, fit: 0.0813
      batch rate adapted learning_rate :              1e-05
      loss :              1.785798428611128
      loss_factor :              0.17857984286111278
      loss adapted learning_rate :              1.5249080172073178e-80
    epoch : 7 ; learning_rate : 1.5249080172073178e-80 ; fit : 0.0813
    batch_size :          30000
    best_batch_loss : 1.7855081202947858
    Epoch 7, Loss: 1.785920327726316, fit: 0.08116666666666666
      batch rate adapted learning_rate :              1e-05
      loss :              1.7863325351578463
      loss_factor :              0.17863325351578463
      loss adapted learning_rate :              1.5711976724948072e-80
    epoch : 8 ; learning_rate : 1.5711976724948072e-80 ; fit : 0.08116666666666666
    batch_size :          30000
    best_batch_loss : 1.7846653817747622
    Epoch 8, Loss: 1.7859203277263156, fit: 0.0817
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7846653817747622_fit_0.0817_2024-01-03_183323
  self.fit : 0.0817
  self.loss : 1.7846653817747622
  current_accuracy : 0.0863
   Accuracy mean: 0.0864
   Accuracy mean: 0.0863
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.00031622776601683794
      loss_factor :              1
      loss adapted learning_rate :              0.00031622776601683794
    epoch : 0 ; learning_rate : 0.00031622776601683794 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.8071200857651444
    Epoch 0, Loss: 1.8115789981423815, fit: 0.0622
    Epoch 0, Loss: 1.8115789981423815
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.8160379105196185_fit_0.0622_2024-01-03_183326
  self.fit : 0.0622
  self.loss : 1.8160379105196185
  current_accuracy : 0.0669
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.00031622776601683794
      loss_factor :              1
      loss adapted learning_rate :              0.00031622776601683794
    epoch : 0 ; learning_rate : 0.00031622776601683794 ; fit : 0.0622
    batch_size :          30000
    best_batch_loss : 1.804407142484009
    Epoch 0, Loss: 1.8103690441399527, fit: 0.06233333333333333
    Epoch 0, Loss: 1.8103690441399527
      batch rate adapted learning_rate :              0.00031622776601683794
      loss :              1.8163309457958965
      loss_factor :              0.18163309457958965
      loss adapted learning_rate :              2.627230826107856e-78
    epoch : 1 ; learning_rate : 2.627230826107856e-78 ; fit : 0.06233333333333333
    batch_size :          30000
    best_batch_loss : 1.8066372195115827
    Epoch 1, Loss: 1.809471002743109, fit: 0.06373333333333334
      batch rate adapted learning_rate :              0.00031622776601683794
      loss :              1.812304785974635
      loss_factor :              0.1812304785974635
      loss adapted learning_rate :              2.104377911088964e-78
    epoch : 2 ; learning_rate : 2.104377911088964e-78 ; fit : 0.06373333333333334
    batch_size :          30000
    best_batch_loss : 1.8081793711367438
    Epoch 2, Loss: 1.8094710027431091, fit: 0.0651
      batch rate adapted learning_rate :              0.00031622776601683794
      loss :              1.8107626343494745
      loss_factor :              0.18107626343494745
      loss adapted learning_rate :              1.932646441327948e-78
    epoch : 3 ; learning_rate : 1.932646441327948e-78 ; fit : 0.0651
    batch_size :          30000
    best_batch_loss : 1.8082186922611616
    Epoch 3, Loss: 1.8094710027431096, fit: 0.0655
      batch rate adapted learning_rate :              0.00031622776601683794
      loss :              1.8082186922611616
      loss_factor :              0.18082186922611615
      loss adapted learning_rate :              1.6791729912033184e-78
    epoch : 4 ; learning_rate : 1.6791729912033184e-78 ; fit : 0.0655
    batch_size :          30000
    best_batch_loss : 1.8084393434833466
    Epoch 4, Loss: 1.8094710027431096, fit: 0.06476666666666667
      batch rate adapted learning_rate :              0.00031622776601683794
      loss :              1.8105026620028726
      loss_factor :              0.18105026620028725
      loss adapted learning_rate :              1.9050955832705835e-78
    epoch : 5 ; learning_rate : 1.9050955832705835e-78 ; fit : 0.06476666666666667
    batch_size :          30000
    best_batch_loss : 1.8086953051318517
    Epoch 5, Loss: 1.8094710027431091, fit: 0.06536666666666667
      batch rate adapted learning_rate :              0.00031622776601683794
      loss :              1.8086953051318517
      loss_factor :              0.18086953051318516
      loss adapted learning_rate :              1.7240153484845201e-78
    epoch : 6 ; learning_rate : 1.7240153484845201e-78 ; fit : 0.06536666666666667
    batch_size :          30000
    best_batch_loss : 1.8090873401160286
    Epoch 6, Loss: 1.8094710027431091, fit: 0.06573333333333334
      batch rate adapted learning_rate :              0.00031622776601683794
      loss :              1.8090873401160286
      loss_factor :              0.18090873401160285
      loss adapted learning_rate :              1.761787188153772e-78
    epoch : 7 ; learning_rate : 1.761787188153772e-78 ; fit : 0.06573333333333334
    batch_size :          30000
    best_batch_loss : 1.8074872436205816
    Epoch 7, Loss: 1.8094710027431096, fit: 0.06456666666666666
      batch rate adapted learning_rate :              0.00031622776601683794
      loss :              1.8114547618656378
      loss_factor :              0.1811454761865638
      loss adapted learning_rate :              2.0079332472867442e-78
    epoch : 8 ; learning_rate : 2.0079332472867442e-78 ; fit : 0.06456666666666666
    batch_size :          30000
    best_batch_loss : 1.8087078986766925
    Epoch 8, Loss: 1.8094710027431093, fit: 0.0644
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.8102341068095262_fit_0.0644_2024-01-03_183346
  self.fit : 0.0644
  self.loss : 1.8102341068095262
  current_accuracy : 0.067
   Accuracy mean: 0.0669
   Accuracy mean: 0.067
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.01
      loss_factor :              1
      loss adapted learning_rate :              0.01
    epoch : 0 ; learning_rate : 0.01 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.7929423439637715
    Epoch 0, Loss: 1.7968206815253478, fit: 0.07183333333333333
    Epoch 0, Loss: 1.7968206815253478
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.800699019086924_fit_0.07183333333333333_2024-01-03_183349
  self.fit : 0.07183333333333333
  self.loss : 1.800699019086924
  current_accuracy : 0.0769
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.01
      loss_factor :              1
      loss adapted learning_rate :              0.01
    epoch : 0 ; learning_rate : 0.01 ; fit : 0.07183333333333333
    batch_size :          30000
    best_batch_loss : 1.7756562148220465
    Epoch 0, Loss: 1.7812684413511846, fit: 0.0822
    Epoch 0, Loss: 1.7812684413511846
      batch rate adapted learning_rate :              0.01
      loss :              1.7756562148220465
      loss_factor :              0.17756562148220464
      loss adapted learning_rate :              8.627571624553822e-78
    epoch : 1 ; learning_rate : 8.627571624553822e-78 ; fit : 0.0822
    batch_size :          30000
    best_batch_loss : 1.7607685818608791
    Epoch 1, Loss: 1.7644580487123607, fit: 0.08893333333333334
      batch rate adapted learning_rate :              0.01
      loss :              1.7607685818608791
      loss_factor :              0.17607685818608793
      loss adapted learning_rate :              3.7173035925351425e-78
    epoch : 2 ; learning_rate : 3.7173035925351425e-78 ; fit : 0.08893333333333334
    batch_size :          30000
    best_batch_loss : 1.7628602387331258
    Epoch 2, Loss: 1.7644580487123607, fit: 0.08773333333333333
      batch rate adapted learning_rate :              0.01
      loss :              1.7628602387331258
      loss_factor :              0.17628602387331258
      loss adapted learning_rate :              4.185894108556832e-78
    epoch : 3 ; learning_rate : 4.185894108556832e-78 ; fit : 0.08773333333333333
    batch_size :          30000
    best_batch_loss : 1.7637019725820908
    Epoch 3, Loss: 1.7644580487123607, fit: 0.08776666666666667
      batch rate adapted learning_rate :              0.01
      loss :              1.7637019725820908
      loss_factor :              0.17637019725820907
      loss adapted learning_rate :              4.390561489748293e-78
    epoch : 4 ; learning_rate : 4.390561489748293e-78 ; fit : 0.08776666666666667
    batch_size :          30000
    best_batch_loss : 1.7635226304679636
    Epoch 4, Loss: 1.7644580487123602, fit: 0.08746666666666666
      batch rate adapted learning_rate :              0.01
      loss :              1.7635226304679636
      loss_factor :              0.17635226304679635
      loss adapted learning_rate :              4.346140019760929e-78
    epoch : 5 ; learning_rate : 4.346140019760929e-78 ; fit : 0.08746666666666666
    batch_size :          30000
    best_batch_loss : 1.7615851291071807
    Epoch 5, Loss: 1.76445804871236, fit: 0.08906666666666667
      batch rate adapted learning_rate :              0.01
      loss :              1.7615851291071807
      loss_factor :              0.17615851291071807
      loss adapted learning_rate :              3.8937094662304777e-78
    epoch : 6 ; learning_rate : 3.8937094662304777e-78 ; fit : 0.08906666666666667
    batch_size :          30000
    best_batch_loss : 1.7612845420977556
    Epoch 6, Loss: 1.7644580487123607, fit: 0.08583333333333333
      batch rate adapted learning_rate :              0.01
      loss :              1.7676315553269655
      loss_factor :              0.17676315553269656
      loss adapted learning_rate :              5.484974184251956e-78
    epoch : 7 ; learning_rate : 5.484974184251956e-78 ; fit : 0.08583333333333333
    batch_size :          30000
    best_batch_loss : 1.7638443074998449
    Epoch 7, Loss: 1.7644580487123607, fit: 0.0874
      batch rate adapted learning_rate :              0.01
      loss :              1.7638443074998449
      loss_factor :              0.1763844307499845
      loss adapted learning_rate :              4.426136278635479e-78
    epoch : 8 ; learning_rate : 4.426136278635479e-78 ; fit : 0.0874
    batch_size :          30000
    best_batch_loss : 1.7598003611021449
    Epoch 8, Loss: 1.7644580487123607, fit: 0.0898
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7598003611021449_fit_0.0898_2024-01-03_183410
  self.fit : 0.0898
  self.loss : 1.7598003611021449
  current_accuracy : 0.0863
   Accuracy mean: 0.0769
   Accuracy mean: 0.0863
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.03162277660168379
      loss_factor :              1
      loss adapted learning_rate :              0.03162277660168379
    epoch : 0 ; learning_rate : 0.03162277660168379 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.678994944104094
    Epoch 0, Loss: 1.6965586468854206, fit: 0.12783333333333333
    Epoch 0, Loss: 1.6965586468854206
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.678994944104094_fit_0.12783333333333333_2024-01-03_183413
  self.fit : 0.12783333333333333
  self.loss : 1.678994944104094
  current_accuracy : 0.1473
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.03162277660168379
      loss_factor :              1
      loss adapted learning_rate :              0.03162277660168379
    epoch : 0 ; learning_rate : 0.03162277660168379 ; fit : 0.12783333333333333
    batch_size :          30000
    best_batch_loss : 1.6155759165871946
    Epoch 0, Loss: 1.6345123649348419, fit: 0.1571
    Epoch 0, Loss: 1.6345123649348419
      batch rate adapted learning_rate :              0.03162277660168379
      loss :              1.6155759165871946
      loss_factor :              0.16155759165871947
      loss adapted learning_rate :              2.151478701865913e-81
    epoch : 1 ; learning_rate : 2.151478701865913e-81 ; fit : 0.1571
    batch_size :          30000
    best_batch_loss : 1.5862097104457609
    Epoch 1, Loss: 1.5867024973723005, fit: 0.17073333333333332
      batch rate adapted learning_rate :              0.03162277660168379
      loss :              1.5862097104457609
      loss_factor :              0.1586209710445761
      loss adapted learning_rate :              3.4360574856761707e-82
    epoch : 2 ; learning_rate : 3.4360574856761707e-82 ; fit : 0.17073333333333332
    batch_size :          30000
    best_batch_loss : 1.5855210812020033
    Epoch 2, Loss: 1.5867024973723007, fit: 0.17023333333333332
      batch rate adapted learning_rate :              0.03162277660168379
      loss :              1.5878839135425982
      loss_factor :              0.15878839135425982
      loss adapted learning_rate :              3.818342666679509e-82
    epoch : 3 ; learning_rate : 3.818342666679509e-82 ; fit : 0.17023333333333332
    batch_size :          30000
    best_batch_loss : 1.585404335065781
    Epoch 3, Loss: 1.5867024973723005, fit: 0.17193333333333333
      batch rate adapted learning_rate :              0.03162277660168379
      loss :              1.585404335065781
      loss_factor :              0.1585404335065781
      loss adapted learning_rate :              3.265909436732265e-82
    epoch : 4 ; learning_rate : 3.265909436732265e-82 ; fit : 0.17193333333333333
    batch_size :          30000
    best_batch_loss : 1.5853643218186229
    Epoch 4, Loss: 1.5867024973723005, fit: 0.1702
      batch rate adapted learning_rate :              0.03162277660168379
      loss :              1.5880406729259784
      loss_factor :              0.15880406729259783
      loss adapted learning_rate :              3.8562229871329224e-82
    epoch : 5 ; learning_rate : 3.8562229871329224e-82 ; fit : 0.1702
    batch_size :          30000
    best_batch_loss : 1.5848210899229431
    Epoch 5, Loss: 1.586702497372301, fit: 0.17126666666666668
      batch rate adapted learning_rate :              0.03162277660168379
      loss :              1.5848210899229431
      loss_factor :              0.1584821089922943
      loss adapted learning_rate :              3.147923661465382e-82
    epoch : 6 ; learning_rate : 3.147923661465382e-82 ; fit : 0.17126666666666668
    batch_size :          30000
    best_batch_loss : 1.5866337377935238
    Epoch 6, Loss: 1.5867024973723005, fit: 0.1708
      batch rate adapted learning_rate :              0.03162277660168379
      loss :              1.5867712569510775
      loss_factor :              0.15867712569510775
      loss adapted learning_rate :              3.5598565586371957e-82
    epoch : 7 ; learning_rate : 3.5598565586371957e-82 ; fit : 0.1708
    batch_size :          30000
    best_batch_loss : 1.5848562097561028
    Epoch 7, Loss: 1.5867024973723005, fit: 0.17126666666666668
      batch rate adapted learning_rate :              0.03162277660168379
      loss :              1.5848562097561028
      loss_factor :              0.15848562097561028
      loss adapted learning_rate :              3.15490715711119e-82
    epoch : 8 ; learning_rate : 3.15490715711119e-82 ; fit : 0.17126666666666668
    batch_size :          30000
    best_batch_loss : 1.5847207186226067
    Epoch 8, Loss: 1.586702497372301, fit: 0.1706
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.588684276121995_fit_0.1706_2024-01-03_183434
  self.fit : 0.1706
  self.loss : 1.588684276121995
  current_accuracy : 0.1759
   Accuracy mean: 0.1473
   Accuracy mean: 0.1759
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.1414213562373095
      loss_factor :              1
      loss adapted learning_rate :              0.1414213562373095
    epoch : 0 ; learning_rate : 0.1414213562373095 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.3787829238989306
    Epoch 0, Loss: 1.5252192233128594, fit: 0.28036666666666665
    Epoch 0, Loss: 1.5252192233128594
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.3787829238989306_fit_0.28036666666666665_2024-01-03_183436
  self.fit : 0.28036666666666665
  self.loss : 1.3787829238989306
  current_accuracy : 0.3158
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.1414213562373095
      loss_factor :              1
      loss adapted learning_rate :              0.1414213562373095
    epoch : 0 ; learning_rate : 0.1414213562373095 ; fit : 0.28036666666666665
    batch_size :          30000
    best_batch_loss : 1.1879216834779402
    Epoch 0, Loss: 1.2523019435111165, fit: 0.3783
    Epoch 0, Loss: 1.2523019435111165
      batch rate adapted learning_rate :              0.1414213562373095
      loss :              1.1879216834779402
      loss_factor :              0.11879216834779402
      loss adapted learning_rate :              4.2588855421378e-94
    epoch : 1 ; learning_rate : 4.2588855421378e-94 ; fit : 0.3783
    batch_size :          30000
    best_batch_loss : 1.0876784095788263
    Epoch 1, Loss: 1.08918554881383, fit: 0.4273
      batch rate adapted learning_rate :              0.1414213562373095
      loss :              1.090692688048834
      loss_factor :              0.10906926880488339
      loss adapted learning_rate :              8.332113234534381e-98
    epoch : 2 ; learning_rate : 8.332113234534381e-98 ; fit : 0.4273
    batch_size :          30000
    best_batch_loss : 1.0865894542676169
    Epoch 2, Loss: 1.0891855488138305, fit: 0.4267666666666667
      batch rate adapted learning_rate :              0.1414213562373095
      loss :              1.0917816433600438
      loss_factor :              0.10917816433600439
      loss adapted learning_rate :              9.206483966632079e-98
    epoch : 3 ; learning_rate : 9.206483966632079e-98 ; fit : 0.4267666666666667
    batch_size :          30000
    best_batch_loss : 1.089029166504514
    Epoch 3, Loss: 1.08918554881383, fit: 0.4289
      batch rate adapted learning_rate :              0.1414213562373095
      loss :              1.089029166504514
      loss_factor :              0.1089029166504514
      loss adapted learning_rate :              7.152635954692309e-98
    epoch : 4 ; learning_rate : 7.152635954692309e-98 ; fit : 0.4289
    batch_size :          30000
    best_batch_loss : 1.085084309379575
    Epoch 4, Loss: 1.08918554881383, fit: 0.42596666666666666
      batch rate adapted learning_rate :              0.1414213562373095
      loss :              1.0932867882480848
      loss_factor :              0.10932867882480848
      loss adapted learning_rate :              1.0566349976002522e-97
    epoch : 5 ; learning_rate : 1.0566349976002522e-97 ; fit : 0.42596666666666666
    batch_size :          30000
    best_batch_loss : 1.0873691290567604
    Epoch 5, Loss: 1.0891855488138305, fit: 0.42896666666666666
      batch rate adapted learning_rate :              0.1414213562373095
      loss :              1.0873691290567604
      loss_factor :              0.10873691290567604
      loss adapted learning_rate :              6.140657833875744e-98
    epoch : 6 ; learning_rate : 6.140657833875744e-98 ; fit : 0.42896666666666666
    batch_size :          30000
    best_batch_loss : 1.08274572138523
    Epoch 6, Loss: 1.08918554881383, fit: 0.4322666666666667
      batch rate adapted learning_rate :              0.1414213562373095
      loss :              1.08274572138523
      loss_factor :              0.10827457213852301
      loss adapted learning_rate :              4.010168498219682e-98
    epoch : 7 ; learning_rate : 4.010168498219682e-98 ; fit : 0.4322666666666667
    batch_size :          30000
    best_batch_loss : 1.0868422466653733
    Epoch 7, Loss: 1.08918554881383, fit: 0.42633333333333334
      batch rate adapted learning_rate :              0.1414213562373095
      loss :              1.091528850962287
      loss_factor :              0.1091528850962287
      loss adapted learning_rate :              8.995740764582216e-98
    epoch : 8 ; learning_rate : 8.995740764582216e-98 ; fit : 0.42633333333333334
    batch_size :          30000
    best_batch_loss : 1.088205233670763
    Epoch 8, Loss: 1.08918554881383, fit: 0.42773333333333335
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.0901658639568972_fit_0.42773333333333335_2024-01-03_183457
  self.fit : 0.42773333333333335
  self.loss : 1.0901658639568972
  current_accuracy : 0.429
   Accuracy mean: 0.3158
   Accuracy mean: 0.429
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.15811388300841897
      loss_factor :              1
      loss adapted learning_rate :              0.15811388300841897
    epoch : 0 ; learning_rate : 0.15811388300841897 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.5423230309359868
    Epoch 0, Loss: 1.6646035730207756, fit: 0.19956666666666667
    Epoch 0, Loss: 1.6646035730207756
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.5423230309359868_fit_0.19956666666666667_2024-01-03_183459
  self.fit : 0.19956666666666667
  self.loss : 1.5423230309359868
  current_accuracy : 0.2419
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.15811388300841897
      loss_factor :              1
      loss adapted learning_rate :              0.15811388300841897
    epoch : 0 ; learning_rate : 0.15811388300841897 ; fit : 0.19956666666666667
    batch_size :          30000
    best_batch_loss : 1.2405320647857974
    Epoch 0, Loss: 1.358147456353255, fit: 0.35783333333333334
    Epoch 0, Loss: 1.358147456353255
      batch rate adapted learning_rate :              0.15811388300841897
      loss :              1.2405320647857974
      loss_factor :              0.12405320647857973
      loss adapted learning_rate :              3.628853427209849e-92
    epoch : 1 ; learning_rate : 3.628853427209849e-92 ; fit : 0.35783333333333334
    batch_size :          30000
    best_batch_loss : 1.0773640487376985
    Epoch 1, Loss: 1.0783122705767996, fit: 0.43766666666666665
      batch rate adapted learning_rate :              0.15811388300841897
      loss :              1.0773640487376985
      loss_factor :              0.10773640487376986
      loss adapted learning_rate :              2.7240678711393278e-98
    epoch : 2 ; learning_rate : 2.7240678711393278e-98 ; fit : 0.43766666666666665
    batch_size :          30000
    best_batch_loss : 1.0778260903586523
    Epoch 2, Loss: 1.0783122705767998, fit: 0.43683333333333335
      batch rate adapted learning_rate :              0.15811388300841897
      loss :              1.0787984507949473
      loss_factor :              0.10787984507949473
      loss adapted learning_rate :              3.1117264809342574e-98
    epoch : 3 ; learning_rate : 3.1117264809342574e-98 ; fit : 0.43683333333333335
    batch_size :          30000
    best_batch_loss : 1.0764958321975782
    Epoch 3, Loss: 1.0783122705767996, fit: 0.43703333333333333
      batch rate adapted learning_rate :              0.15811388300841897
      loss :              1.0764958321975782
      loss_factor :              0.10764958321975782
      loss adapted learning_rate :              2.5130740027769837e-98
    epoch : 4 ; learning_rate : 2.5130740027769837e-98 ; fit : 0.43703333333333333
    batch_size :          30000
    best_batch_loss : 1.0781768732614634
    Epoch 4, Loss: 1.0783122705767996, fit: 0.4363666666666667
      batch rate adapted learning_rate :              0.15811388300841897
      loss :              1.0784476678921355
      loss_factor :              0.10784476678921355
      loss adapted learning_rate :              3.012156745971923e-98
    epoch : 5 ; learning_rate : 3.012156745971923e-98 ; fit : 0.4363666666666667
    batch_size :          30000
    best_batch_loss : 1.0708510515587106
    Epoch 5, Loss: 1.0783122705767998, fit: 0.4330333333333333
      batch rate adapted learning_rate :              0.15811388300841897
      loss :              1.085773489594889
      loss_factor :              0.1085773489594889
      loss adapted learning_rate :              5.927797639266161e-98
    epoch : 6 ; learning_rate : 5.927797639266161e-98 ; fit : 0.4330333333333333
    batch_size :          30000
    best_batch_loss : 1.075951015640949
    Epoch 6, Loss: 1.0783122705767996, fit: 0.4357666666666667
      batch rate adapted learning_rate :              0.15811388300841897
      loss :              1.08067352551265
      loss_factor :              0.10806735255126501
      loss adapted learning_rate :              3.701870513253101e-98
    epoch : 7 ; learning_rate : 3.701870513253101e-98 ; fit : 0.4357666666666667
    batch_size :          30000
    best_batch_loss : 1.0766943267864235
    Epoch 7, Loss: 1.0783122705767996, fit: 0.4372
      batch rate adapted learning_rate :              0.15811388300841897
      loss :              1.0766943267864235
      loss_factor :              0.10766943267864235
      loss adapted learning_rate :              2.5598379660370958e-98
    epoch : 8 ; learning_rate : 2.5598379660370958e-98 ; fit : 0.4372
    batch_size :          30000
    best_batch_loss : 1.0757277196161137
    Epoch 8, Loss: 1.0783122705767996, fit: 0.4357
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.0808968215374852_fit_0.4357_2024-01-03_183521
  self.fit : 0.4357
  self.loss : 1.0808968215374852
  current_accuracy : 0.4457
   Accuracy mean: 0.2419
   Accuracy mean: 0.4457
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.17320508075688773
      loss_factor :              1
      loss adapted learning_rate :              0.17320508075688773
    epoch : 0 ; learning_rate : 0.17320508075688773 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.6164770091682508
    Epoch 0, Loss: 1.698326689082696, fit: 0.1633
    Epoch 0, Loss: 1.698326689082696
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6164770091682508_fit_0.1633_2024-01-03_183523
  self.fit : 0.1633
  self.loss : 1.6164770091682508
  current_accuracy : 0.2829
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.17320508075688773
      loss_factor :              1
      loss adapted learning_rate :              0.17320508075688773
    epoch : 0 ; learning_rate : 0.17320508075688773 ; fit : 0.1633
    batch_size :          30000
    best_batch_loss : 1.1977326359438158
    Epoch 0, Loss: 1.2980114863873742, fit: 0.3756333333333333
    Epoch 0, Loss: 1.2980114863873742
      batch rate adapted learning_rate :              0.17320508075688773
      loss :              1.1977326359438158
      loss_factor :              0.11977326359438159
      loss adapted learning_rate :              1.1872685647257865e-93
    epoch : 1 ; learning_rate : 1.1872685647257865e-93 ; fit : 0.3756333333333333
    batch_size :          30000
    best_batch_loss : 1.1442555225404385
    Epoch 1, Loss: 1.1474773844826784, fit: 0.39753333333333335
      batch rate adapted learning_rate :              0.17320508075688773
      loss :              1.1506992464249182
      loss_factor :              0.11506992464249181
      loss adapted learning_rate :              2.1614415485236223e-95
    epoch : 2 ; learning_rate : 2.1614415485236223e-95 ; fit : 0.39753333333333335
    batch_size :          30000
    best_batch_loss : 1.1466626347950122
    Epoch 2, Loss: 1.1474773844826784, fit: 0.3994666666666667
      batch rate adapted learning_rate :              0.17320508075688773
      loss :              1.1482921341703447
      loss_factor :              0.11482921341703448
      loss adapted learning_rate :              1.753071454248771e-95
    epoch : 3 ; learning_rate : 1.753071454248771e-95 ; fit : 0.3994666666666667
    batch_size :          30000
    best_batch_loss : 1.1439340227014139
    Epoch 3, Loss: 1.1474773844826787, fit: 0.4018333333333333
      batch rate adapted learning_rate :              0.17320508075688773
      loss :              1.1439340227014139
      loss_factor :              0.11439340227014139
      loss adapted learning_rate :              1.1985559647201612e-95
    epoch : 4 ; learning_rate : 1.1985559647201612e-95 ; fit : 0.4018333333333333
    batch_size :          30000
    best_batch_loss : 1.14673767069785
    Epoch 4, Loss: 1.1474773844826784, fit: 0.39903333333333335
      batch rate adapted learning_rate :              0.17320508075688773
      loss :              1.1482170982675068
      loss_factor :              0.11482170982675069
      loss adapted learning_rate :              1.7416528691199676e-95
    epoch : 5 ; learning_rate : 1.7416528691199676e-95 ; fit : 0.39903333333333335
    batch_size :          30000
    best_batch_loss : 1.1469221480960874
    Epoch 5, Loss: 1.1474773844826782, fit: 0.39986666666666665
      batch rate adapted learning_rate :              0.17320508075688773
      loss :              1.1469221480960874
      loss_factor :              0.11469221480960874
      loss adapted learning_rate :              1.5558028794974143e-95
    epoch : 6 ; learning_rate : 1.5558028794974143e-95 ; fit : 0.39986666666666665
    batch_size :          30000
    best_batch_loss : 1.146319610105475
    Epoch 6, Loss: 1.1474773844826787, fit: 0.39916666666666667
      batch rate adapted learning_rate :              0.17320508075688773
      loss :              1.146319610105475
      loss_factor :              0.11463196101054748
      loss adapted learning_rate :              1.4761579258273108e-95
    epoch : 7 ; learning_rate : 1.4761579258273108e-95 ; fit : 0.39916666666666667
    batch_size :          30000
    best_batch_loss : 1.1456784309480448
    Epoch 7, Loss: 1.1474773844826784, fit: 0.40073333333333333
      batch rate adapted learning_rate :              0.17320508075688773
      loss :              1.1456784309480448
      loss_factor :              0.11456784309480447
      loss adapted learning_rate :              1.3958357660686813e-95
    epoch : 8 ; learning_rate : 1.3958357660686813e-95 ; fit : 0.40073333333333333
    batch_size :          30000
    best_batch_loss : 1.1449365943426981
    Epoch 8, Loss: 1.1474773844826784, fit: 0.39816666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.150018174622659_fit_0.39816666666666667_2024-01-03_183544
  self.fit : 0.39816666666666667
  self.loss : 1.150018174622659
  current_accuracy : 0.4053
   Accuracy mean: 0.2829
   Accuracy mean: 0.4053
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.31622776601683794
      loss_factor :              1
      loss adapted learning_rate :              0.31622776601683794
    epoch : 0 ; learning_rate : 0.31622776601683794 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.5971236014746695
    Epoch 0, Loss: 1.637219775821523, fit: 0.1781
    Epoch 0, Loss: 1.637219775821523
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.5971236014746695_fit_0.1781_2024-01-03_183547
  self.fit : 0.1781
  self.loss : 1.5971236014746695
  current_accuracy : 0.3777
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.31622776601683794
      loss_factor :              1
      loss adapted learning_rate :              0.31622776601683794
    epoch : 0 ; learning_rate : 0.31622776601683794 ; fit : 0.1781
    batch_size :          30000
    best_batch_loss : 1.2111065798150027
    Epoch 0, Loss: 1.2417393725931238, fit: 0.3433333333333333
    Epoch 0, Loss: 1.2417393725931238
      batch rate adapted learning_rate :              0.31622776601683794
      loss :              1.2723721653712448
      loss_factor :              0.12723721653712447
      loss adapted learning_rate :              9.149892337742868e-91
    epoch : 1 ; learning_rate : 9.149892337742868e-91 ; fit : 0.3433333333333333
    batch_size :          30000
    best_batch_loss : 0.9084852286737704
    Epoch 1, Loss: 0.9098430208758224, fit: 0.5301333333333333
      batch rate adapted learning_rate :              0.31622776601683794
      loss :              0.9084852286737704
      loss_factor :              0.09084852286737703
      loss adapted learning_rate :              2.1467775158689416e-105
    epoch : 2 ; learning_rate : 2.1467775158689416e-105 ; fit : 0.5301333333333333
    batch_size :          30000
    best_batch_loss : 0.909549103254325
    Epoch 2, Loss: 0.9098430208758224, fit: 0.5297333333333333
      batch rate adapted learning_rate :              0.31622776601683794
      loss :              0.909549103254325
      loss_factor :              0.0909549103254325
      loss adapted learning_rate :              2.4133204970133315e-105
    epoch : 3 ; learning_rate : 2.4133204970133315e-105 ; fit : 0.5297333333333333
    batch_size :          30000
    best_batch_loss : 0.9091216670741905
    Epoch 3, Loss: 0.9098430208758224, fit: 0.529
      batch rate adapted learning_rate :              0.31622776601683794
      loss :              0.9105643746774542
      loss_factor :              0.09105643746774542
      loss adapted learning_rate :              2.6981461949103804e-105
    epoch : 4 ; learning_rate : 2.6981461949103804e-105 ; fit : 0.529
    batch_size :          30000
    best_batch_loss : 0.908489522825727
    Epoch 4, Loss: 0.9098430208758226, fit: 0.5289666666666667
      batch rate adapted learning_rate :              0.31622776601683794
      loss :              0.911196518925918
      loss_factor :              0.0911196518925918
      loss adapted learning_rate :              2.8920459736082692e-105
    epoch : 5 ; learning_rate : 2.8920459736082692e-105 ; fit : 0.5289666666666667
    batch_size :          30000
    best_batch_loss : 0.9046588099095273
    Epoch 5, Loss: 0.9098430208758223, fit: 0.5264333333333333
      batch rate adapted learning_rate :              0.31622776601683794
      loss :              0.9150272318421172
      loss_factor :              0.09150272318421172
      loss adapted learning_rate :              4.399485758973105e-105
    epoch : 6 ; learning_rate : 4.399485758973105e-105 ; fit : 0.5264333333333333
    batch_size :          30000
    best_batch_loss : 0.9077781116592526
    Epoch 6, Loss: 0.9098430208758226, fit: 0.5301666666666667
      batch rate adapted learning_rate :              0.31622776601683794
      loss :              0.9077781116592526
      loss_factor :              0.09077781116592526
      loss adapted learning_rate :              1.9859608493782742e-105
    epoch : 7 ; learning_rate : 1.9859608493782742e-105 ; fit : 0.5301666666666667
    batch_size :          30000
    best_batch_loss : 0.9064919183802431
    Epoch 7, Loss: 0.9098430208758224, fit: 0.5313333333333333
      batch rate adapted learning_rate :              0.31622776601683794
      loss :              0.9064919183802431
      loss_factor :              0.0906491918380243
      loss adapted learning_rate :              1.7234300442080325e-105
    epoch : 8 ; learning_rate : 1.7234300442080325e-105 ; fit : 0.5313333333333333
    batch_size :          30000
    best_batch_loss : 0.9066759315454135
    Epoch 8, Loss: 0.9098430208758227, fit: 0.5304333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_0.9066759315454135_fit_0.5304333333333333_2024-01-03_183608
  self.fit : 0.5304333333333333
  self.loss : 0.9066759315454135
  current_accuracy : 0.5369
   Accuracy mean: 0.3777
   Accuracy mean: 0.5369
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.9486832980505138
      loss_factor :              1
      loss adapted learning_rate :              0.9486832980505138
    epoch : 0 ; learning_rate : 0.9486832980505138 ; fit : 0.0
    batch_size :          30000
    best_batch_loss : 1.3715634883024497
    Epoch 0, Loss: 1.5601540973750119, fit: 0.2904333333333333
    Epoch 0, Loss: 1.5601540973750119
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.3715634883024497_fit_0.2904333333333333_2024-01-03_183610
  self.fit : 0.2904333333333333
  self.loss : 1.3715634883024497
  current_accuracy : 0.3214
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.5
      batch rate adapted learning_rate :              0.9486832980505138
      loss_factor :              1
      loss adapted learning_rate :              0.9486832980505138
    epoch : 0 ; learning_rate : 0.9486832980505138 ; fit : 0.2904333333333333
    batch_size :          30000
    best_batch_loss : 0.9802951363852725
    Epoch 0, Loss: 1.1579631365224814, fit: 0.4846666666666667
    Epoch 0, Loss: 1.1579631365224814
      batch rate adapted learning_rate :              0.9486832980505138
      loss :              0.9802951363852725
      loss_factor :              0.09802951363852726
      loss adapted learning_rate :              1.2966000951588263e-101
    epoch : 1 ; learning_rate : 1.2966000951588263e-101 ; fit : 0.4846666666666667
    batch_size :          30000
    best_batch_loss : 0.9629564077182511
    Epoch 1, Loss: 0.9717885217022225, fit: 0.4846666666666667
      batch rate adapted learning_rate :              0.9486832980505138
      loss :              0.980620635686194
      loss_factor :              0.09806206356861939
      loss adapted learning_rate :              1.3403680365350857e-101
    epoch : 2 ; learning_rate : 1.3403680365350857e-101 ; fit : 0.4846666666666667
    batch_size :          30000
    best_batch_loss : 0.969968651069428
    Epoch 2, Loss: 0.9717885217022222, fit: 0.49033333333333334
      batch rate adapted learning_rate :              0.9486832980505138
      loss :              0.969968651069428
      loss_factor :              0.0969968651069428
      loss adapted learning_rate :              4.496670701971953e-102
    epoch : 3 ; learning_rate : 4.496670701971953e-102 ; fit : 0.49033333333333334
    batch_size :          30000
    best_batch_loss : 0.9702904665768114
    Epoch 3, Loss: 0.9717885217022224, fit: 0.4891333333333333
      batch rate adapted learning_rate :              0.9486832980505138
      loss :              0.9732865768276334
      loss_factor :              0.09732865768276334
      loss adapted learning_rate :              6.326953390637514e-102
    epoch : 4 ; learning_rate : 6.326953390637514e-102 ; fit : 0.4891333333333333
    batch_size :          30000
    best_batch_loss : 0.9711175239691127
    Epoch 4, Loss: 0.9717885217022224, fit: 0.4897
      batch rate adapted learning_rate :              0.9486832980505138
      loss :              0.9711175239691127
      loss_factor :              0.09711175239691126
      loss adapted learning_rate :              5.061746197330743e-102
    epoch : 5 ; learning_rate : 5.061746197330743e-102 ; fit : 0.4897
    batch_size :          30000
    best_batch_loss : 0.9684215867634849
    Epoch 5, Loss: 0.9717885217022222, fit: 0.4874
      batch rate adapted learning_rate :              0.9486832980505138
      loss :              0.9751554566409594
      loss_factor :              0.09751554566409594
      loss adapted learning_rate :              7.664905087270873e-102
    epoch : 6 ; learning_rate : 7.664905087270873e-102 ; fit : 0.4874
    batch_size :          30000
    best_batch_loss : 0.9690553815698747
    Epoch 6, Loss: 0.9717885217022225, fit: 0.48883333333333334
      batch rate adapted learning_rate :              0.9486832980505138
      loss :              0.9745216618345703
      loss_factor :              0.09745216618345703
      loss adapted learning_rate :              7.182422822186563e-102
    epoch : 7 ; learning_rate : 7.182422822186563e-102 ; fit : 0.48883333333333334
    batch_size :          30000
    best_batch_loss : 0.9675520709553538
    Epoch 7, Loss: 0.9717885217022223, fit: 0.4874
      batch rate adapted learning_rate :              0.9486832980505138
      loss :              0.9760249724490908
      loss_factor :              0.09760249724490908
      loss adapted learning_rate :              8.379424955626804e-102
    epoch : 8 ; learning_rate : 8.379424955626804e-102 ; fit : 0.4874
    batch_size :          30000
    best_batch_loss : 0.9707033378947393
    Epoch 8, Loss: 0.9717885217022225, fit: 0.4902
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_0.9707033378947393_fit_0.4902_2024-01-03_183631
  self.fit : 0.4902
  self.loss : 0.9707033378947393
  current_accuracy : 0.5007
   Accuracy mean: 0.3214
   Accuracy mean: 0.5007
  Error saving file: doc/out/test_combinations_results/20240103183631.
  normalized_accuracies :      [1.51063830e-01 1.51063830e-01 8.93617021e-03 8.93617021e-03
   7.68085106e-02 7.68085106e-02 4.14893617e-02 4.12765957e-02
   0.00000000e+00 2.12765957e-04 2.12765957e-02 4.12765957e-02
   1.71063830e-01 2.31914894e-01 5.29574468e-01 7.70425532e-01
   3.72340426e-01 8.05957447e-01 4.59574468e-01 7.20000000e-01
   6.61276596e-01 1.00000000e+00 5.41489362e-01 9.22978723e-01]
batch_rate :  0.2
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.2
      batch rate adapted learning_rate :              1.584893192461111e-13
      loss_factor :              1
      loss adapted learning_rate :              1.584893192461111e-13
    epoch : 0 ; learning_rate : 1.584893192461111e-13 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7121859341101413
    Epoch 0, Loss: 1.7160420385499355, fit: 0.10783333333333334
    Epoch 0, Loss: 1.7160420385499355
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7174644665949164_fit_0.10783333333333334_2024-01-03_183635
  self.fit : 0.10783333333333334
  self.loss : 1.7174644665949164
  current_accuracy : 0.1143
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-64
    batch_rate :          0.2
      batch rate adapted learning_rate :              1.584893192461111e-13
      loss_factor :              1
      loss adapted learning_rate :              1.584893192461111e-13
    epoch : 0 ; learning_rate : 1.584893192461111e-13 ; fit : 0.10783333333333334
    batch_size :          12000
    best_batch_loss : 1.7085341460959347
    Epoch 0, Loss: 1.7160420385498676, fit: 0.11166666666666666
    Epoch 0, Loss: 1.7160420385498676
      batch rate adapted learning_rate :              1.584893192461111e-13
      loss :              1.712488145247781
      loss_factor :              0.17124881452477808
      loss adapted learning_rate :              3.653906044091588e-90
    epoch : 1 ; learning_rate : 3.653906044091588e-90 ; fit : 0.11166666666666666
    batch_size :          12000
    best_batch_loss : 1.7105436288549627
    Epoch 1, Loss: 1.716042038549844, fit: 0.109
      batch rate adapted learning_rate :              1.584893192461111e-13
      loss :              1.7177273444108088
      loss_factor :              0.17177273444108088
      loss adapted learning_rate :              4.959329019233303e-90
    epoch : 2 ; learning_rate : 4.959329019233303e-90 ; fit : 0.109
    batch_size :          12000
    best_batch_loss : 1.7071076948757422
    Epoch 2, Loss: 1.716042038549844, fit: 0.10491666666666667
      batch rate adapted learning_rate :              1.584893192461111e-13
      loss :              1.7257926063777707
      loss_factor :              0.17257926063777707
      loss adapted learning_rate :              7.922466395574548e-90
    epoch : 3 ; learning_rate : 7.922466395574548e-90 ; fit : 0.10491666666666667
    batch_size :          12000
    best_batch_loss : 1.7125034277130329
    Epoch 3, Loss: 1.716042038549844, fit: 0.105
      batch rate adapted learning_rate :              1.584893192461111e-13
      loss :              1.724552273221096
      loss_factor :              0.1724552273221096
      loss adapted learning_rate :              7.372865108757695e-90
    epoch : 4 ; learning_rate : 7.372865108757695e-90 ; fit : 0.105
    batch_size :          12000
    best_batch_loss : 1.7033456511168477
    Epoch 4, Loss: 1.7160420385498436, fit: 0.11033333333333334
      batch rate adapted learning_rate :              1.584893192461111e-13
      loss :              1.715258077494391
      loss_factor :              0.1715258077494391
      loss adapted learning_rate :              4.294843254069493e-90
    epoch : 5 ; learning_rate : 4.294843254069493e-90 ; fit : 0.11033333333333334
    batch_size :          12000
    best_batch_loss : 1.7051681975158008
    Epoch 5, Loss: 1.716042038549844, fit: 0.1105
      batch rate adapted learning_rate :              1.584893192461111e-13
      loss :              1.7143548505765942
      loss_factor :              0.17143548505765943
      loss adapted learning_rate :              4.074478740155444e-90
    epoch : 6 ; learning_rate : 4.074478740155444e-90 ; fit : 0.1105
    batch_size :          12000
    best_batch_loss : 1.7104080144769294
    Epoch 6, Loss: 1.7160420385498443, fit: 0.11233333333333333
      batch rate adapted learning_rate :              1.584893192461111e-13
      loss :              1.7104080144769294
      loss_factor :              0.17104080144769293
      loss adapted learning_rate :              3.235730069185844e-90
    epoch : 7 ; learning_rate : 3.235730069185844e-90 ; fit : 0.11233333333333333
    batch_size :          12000
    best_batch_loss : 1.710096487085303
    Epoch 7, Loss: 1.716042038549844, fit: 0.10866666666666666
      batch rate adapted learning_rate :              1.584893192461111e-13
      loss :              1.7162405743169002
      loss_factor :              0.17162405743169
      loss adapted learning_rate :              4.547958659430811e-90
    epoch : 8 ; learning_rate : 4.547958659430811e-90 ; fit : 0.10866666666666666
    batch_size :          12000
    best_batch_loss : 1.705717139701959
    Epoch 8, Loss: 1.716042038549844, fit: 0.107
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7213637050944335_fit_0.107_2024-01-03_183655
  self.fit : 0.107
  self.loss : 1.7213637050944335
  current_accuracy : 0.1143
   Accuracy mean: 0.1143
   Accuracy mean: 0.1143
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.2
      batch rate adapted learning_rate :              2.511886431509578e-07
      loss_factor :              1
      loss adapted learning_rate :              2.511886431509578e-07
    epoch : 0 ; learning_rate : 2.511886431509578e-07 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.6707364099228526
    Epoch 0, Loss: 1.6766996228798812, fit: 0.13408333333333333
    Epoch 0, Loss: 1.6766996228798812
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.6747518669914931_fit_0.13408333333333333_2024-01-03_183658
  self.fit : 0.13408333333333333
  self.loss : 1.6747518669914931
  current_accuracy : 0.1366
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-33
    batch_rate :          0.2
      batch rate adapted learning_rate :              2.511886431509578e-07
      loss_factor :              1
      loss adapted learning_rate :              2.511886431509578e-07
    epoch : 0 ; learning_rate : 2.511886431509578e-07 ; fit : 0.13408333333333333
    batch_size :          12000
    best_batch_loss : 1.6655685611567397
    Epoch 0, Loss: 1.6766976909807012, fit: 0.13891666666666666
    Epoch 0, Loss: 1.6766976909807012
      batch rate adapted learning_rate :              2.511886431509578e-07
      loss :              1.6655685611567397
      loss_factor :              0.16655685611567397
      loss adapted learning_rate :              3.599576943099273e-85
    epoch : 1 ; learning_rate : 3.599576943099273e-85 ; fit : 0.13891666666666666
    batch_size :          12000
    best_batch_loss : 1.669713065725814
    Epoch 1, Loss: 1.6766965327732746, fit: 0.13691666666666666
      batch rate adapted learning_rate :              2.511886431509578e-07
      loss :              1.669713065725814
      loss_factor :              0.16697130657258138
      loss adapted learning_rate :              4.6151365919601454e-85
    epoch : 2 ; learning_rate : 4.6151365919601454e-85 ; fit : 0.13691666666666666
    batch_size :          12000
    best_batch_loss : 1.6666625440996308
    Epoch 2, Loss: 1.6766965327732741, fit: 0.138
      batch rate adapted learning_rate :              2.511886431509578e-07
      loss :              1.6666625440996308
      loss_factor :              0.16666625440996308
      loss adapted learning_rate :              3.8438597830149567e-85
    epoch : 3 ; learning_rate : 3.8438597830149567e-85 ; fit : 0.138
    batch_size :          12000
    best_batch_loss : 1.6655380769023826
    Epoch 3, Loss: 1.6766965327732744, fit: 0.1315
      batch rate adapted learning_rate :              2.511886431509578e-07
      loss :              1.6792377885107477
      loss_factor :              0.16792377885107476
      loss adapted learning_rate :              8.151168621914768e-85
    epoch : 4 ; learning_rate : 8.151168621914768e-85 ; fit : 0.1315
    batch_size :          12000
    best_batch_loss : 1.6740745335562217
    Epoch 4, Loss: 1.6766965327732746, fit: 0.13358333333333333
      batch rate adapted learning_rate :              2.511886431509578e-07
      loss :              1.676187404952349
      loss_factor :              0.1676187404952349
      loss adapted learning_rate :              6.796061491432677e-85
    epoch : 5 ; learning_rate : 6.796061491432677e-85 ; fit : 0.13358333333333333
    batch_size :          12000
    best_batch_loss : 1.6693031857727674
    Epoch 5, Loss: 1.6766965327732744, fit: 0.13333333333333333
      batch rate adapted learning_rate :              2.511886431509578e-07
      loss :              1.6768751344037103
      loss_factor :              0.16768751344037103
      loss adapted learning_rate :              7.080639484054747e-85
    epoch : 6 ; learning_rate : 7.080639484054747e-85 ; fit : 0.13333333333333333
    batch_size :          12000
    best_batch_loss : 1.6665706425467728
    Epoch 6, Loss: 1.6766965327732741, fit: 0.1315
      batch rate adapted learning_rate :              2.511886431509578e-07
      loss :              1.6813349150159145
      loss_factor :              0.16813349150159146
      loss adapted learning_rate :              9.23470812505871e-85
    epoch : 7 ; learning_rate : 9.23470812505871e-85 ; fit : 0.1315
    batch_size :          12000
    best_batch_loss : 1.6698208277050128
    Epoch 7, Loss: 1.6766965327732744, fit: 0.13733333333333334
      batch rate adapted learning_rate :              2.511886431509578e-07
      loss :              1.6698208277050128
      loss_factor :              0.16698208277050128
      loss adapted learning_rate :              4.6450176807817933e-85
    epoch : 8 ; learning_rate : 4.6450176807817933e-85 ; fit : 0.13733333333333334
    batch_size :          12000
    best_batch_loss : 1.6697794276454088
    Epoch 8, Loss: 1.6766965327732741, fit: 0.13408333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.67561413561283_fit_0.13408333333333333_2024-01-03_183719
  self.fit : 0.13408333333333333
  self.loss : 1.67561413561283
  current_accuracy : 0.1366
   Accuracy mean: 0.1366
   Accuracy mean: 0.1366
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.0003981071705534971
      loss_factor :              1
      loss adapted learning_rate :              0.0003981071705534971
    epoch : 0 ; learning_rate : 0.0003981071705534971 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.7248214088376372
    Epoch 0, Loss: 1.7377521304258874, fit: 0.09741666666666667
    Epoch 0, Loss: 1.7377521304258874
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7335760305557673_fit_0.09741666666666667_2024-01-03_183722
  self.fit : 0.09741666666666667
  self.loss : 1.7335760305557673
  current_accuracy : 0.0965
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.0003981071705534971
      loss_factor :              1
      loss adapted learning_rate :              0.0003981071705534971
    epoch : 0 ; learning_rate : 0.0003981071705534971 ; fit : 0.09741666666666667
    batch_size :          12000
    best_batch_loss : 1.73115853848526
    Epoch 0, Loss: 1.736781262327601, fit: 0.096
    Epoch 0, Loss: 1.736781262327601
      batch rate adapted learning_rate :              0.0003981071705534971
      loss :              1.7386349185675216
      loss_factor :              0.17386349185675215
      loss adapted learning_rate :              4.176762397035418e-80
    epoch : 1 ; learning_rate : 4.176762397035418e-80 ; fit : 0.096
    batch_size :          12000
    best_batch_loss : 1.7318582010602463
    Epoch 1, Loss: 1.7362334039689356, fit: 0.09916666666666667
      batch rate adapted learning_rate :              0.0003981071705534971
      loss :              1.7318582010602463
      loss_factor :              0.17318582010602462
      loss adapted learning_rate :              2.8263962691200058e-80
    epoch : 2 ; learning_rate : 2.8263962691200058e-80 ; fit : 0.09916666666666667
    batch_size :          12000
    best_batch_loss : 1.72966567771204
    Epoch 2, Loss: 1.736233403968936, fit: 0.09933333333333333
      batch rate adapted learning_rate :              0.0003981071705534971
      loss :              1.72966567771204
      loss_factor :              0.172966567771204
      loss adapted learning_rate :              2.4900998738406074e-80
    epoch : 3 ; learning_rate : 2.4900998738406074e-80 ; fit : 0.09933333333333333
    batch_size :          12000
    best_batch_loss : 1.7302878235988144
    Epoch 3, Loss: 1.736233403968936, fit: 0.09841666666666667
      batch rate adapted learning_rate :              0.0003981071705534971
      loss :              1.733994367120328
      loss_factor :              0.17339943671203278
      loss adapted learning_rate :              3.19718843764003e-80
    epoch : 4 ; learning_rate : 3.19718843764003e-80 ; fit : 0.09841666666666667
    batch_size :          12000
    best_batch_loss : 1.7284217247411788
    Epoch 4, Loss: 1.736233403968936, fit: 0.10133333333333333
      batch rate adapted learning_rate :              0.0003981071705534971
      loss :              1.7284217247411788
      loss_factor :              0.17284217247411787
      loss adapted learning_rate :              2.3172433117148676e-80
    epoch : 5 ; learning_rate : 2.3172433117148676e-80 ; fit : 0.10133333333333333
    batch_size :          12000
    best_batch_loss : 1.732906009531016
    Epoch 5, Loss: 1.736233403968936, fit: 0.09316666666666666
      batch rate adapted learning_rate :              0.0003981071705534971
      loss :              1.7456484289390255
      loss_factor :              0.17456484289390256
      loss adapted learning_rate :              6.24709614236357e-80
    epoch : 6 ; learning_rate : 6.24709614236357e-80 ; fit : 0.09316666666666666
    batch_size :          12000
    best_batch_loss : 1.7306248926225387
    Epoch 6, Loss: 1.7362334039689356, fit: 0.09625
      batch rate adapted learning_rate :              0.0003981071705534971
      loss :              1.7366969653709992
      loss_factor :              0.1736696965370999
      loss adapted learning_rate :              3.735979994345204e-80
    epoch : 7 ; learning_rate : 3.735979994345204e-80 ; fit : 0.09625
    batch_size :          12000
    best_batch_loss : 1.7342980303442739
    Epoch 7, Loss: 1.736233403968936, fit: 0.09583333333333334
      batch rate adapted learning_rate :              0.0003981071705534971
      loss :              1.7378299365538747
      loss_factor :              0.17378299365538746
      loss adapted learning_rate :              3.987745450051971e-80
    epoch : 8 ; learning_rate : 3.987745450051971e-80 ; fit : 0.09583333333333334
    batch_size :          12000
    best_batch_loss : 1.7315056624086747
    Epoch 8, Loss: 1.7362334039689358, fit: 0.0995
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7332503030451931_fit_0.0995_2024-01-03_183742
  self.fit : 0.0995
  self.loss : 1.7332503030451931
  current_accuracy : 0.0966
   Accuracy mean: 0.0965
   Accuracy mean: 0.0966
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.009999999999999997
      loss_factor :              1
      loss adapted learning_rate :              0.009999999999999997
    epoch : 0 ; learning_rate : 0.009999999999999997 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.687228650462157
    Epoch 0, Loss: 1.711502004465691, fit: 0.12158333333333333
    Epoch 0, Loss: 1.711502004465691
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.687228650462157_fit_0.12158333333333333_2024-01-03_183744
  self.fit : 0.12158333333333333
  self.loss : 1.687228650462157
  current_accuracy : 0.1309
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.009999999999999997
      loss_factor :              1
      loss adapted learning_rate :              0.009999999999999997
    epoch : 0 ; learning_rate : 0.009999999999999997 ; fit : 0.12158333333333333
    batch_size :          12000
    best_batch_loss : 1.6444510003849355
    Epoch 0, Loss: 1.6605706481291544, fit: 0.14241666666666666
    Epoch 0, Loss: 1.6605706481291544
      batch rate adapted learning_rate :              0.009999999999999997
      loss :              1.6444510003849355
      loss_factor :              0.16444510003849355
      loss adapted learning_rate :              4.0003108146844728e-81
    epoch : 1 ; learning_rate : 4.0003108146844728e-81 ; fit : 0.14241666666666666
    batch_size :          12000
    best_batch_loss : 1.6230531468348324
    Epoch 1, Loss: 1.6281838446712102, fit: 0.15141666666666667
      batch rate adapted learning_rate :              0.009999999999999997
      loss :              1.6230531468348324
      loss_factor :              0.16230531468348325
      loss adapted learning_rate :              1.079627907295969e-81
    epoch : 2 ; learning_rate : 1.079627907295969e-81 ; fit : 0.15141666666666667
    batch_size :          12000
    best_batch_loss : 1.6213955381755456
    Epoch 2, Loss: 1.6281838446712102, fit: 0.14725
      batch rate adapted learning_rate :              0.009999999999999997
      loss :              1.6324251946754542
      loss_factor :              0.16324251946754542
      loss adapted learning_rate :              1.9201223409730488e-81
    epoch : 3 ; learning_rate : 1.9201223409730488e-81 ; fit : 0.14725
    batch_size :          12000
    best_batch_loss : 1.6202061980280953
    Epoch 3, Loss: 1.6281838446712102, fit: 0.15133333333333332
      batch rate adapted learning_rate :              0.009999999999999997
      loss :              1.6236920687474707
      loss_factor :              0.16236920687474707
      loss adapted learning_rate :              1.1229668309058913e-81
    epoch : 4 ; learning_rate : 1.1229668309058913e-81 ; fit : 0.15133333333333332
    batch_size :          12000
    best_batch_loss : 1.615982534339328
    Epoch 4, Loss: 1.6281838446712102, fit: 0.14641666666666667
      batch rate adapted learning_rate :              0.009999999999999997
      loss :              1.6357132650981803
      loss_factor :              0.16357132650981804
      loss adapted learning_rate :              2.348105295811888e-81
    epoch : 5 ; learning_rate : 2.348105295811888e-81 ; fit : 0.14641666666666667
    batch_size :          12000
    best_batch_loss : 1.6234108266668505
    Epoch 5, Loss: 1.6281838446712102, fit: 0.15291666666666667
      batch rate adapted learning_rate :              0.009999999999999997
      loss :              1.6239432010223271
      loss_factor :              0.1623943201022327
      loss adapted learning_rate :              1.1404691192976445e-81
    epoch : 6 ; learning_rate : 1.1404691192976445e-81 ; fit : 0.15291666666666667
    batch_size :          12000
    best_batch_loss : 1.619214942332695
    Epoch 6, Loss: 1.6281838446712105, fit: 0.15283333333333332
      batch rate adapted learning_rate :              0.009999999999999997
      loss :              1.6246207003080346
      loss_factor :              0.16246207003080346
      loss adapted learning_rate :              1.189044901678175e-81
    epoch : 7 ; learning_rate : 1.189044901678175e-81 ; fit : 0.15283333333333332
    batch_size :          12000
    best_batch_loss : 1.6213450334685715
    Epoch 7, Loss: 1.6281838446712102, fit: 0.15416666666666667
      batch rate adapted learning_rate :              0.009999999999999997
      loss :              1.6213450334685715
      loss_factor :              0.16213450334685714
      loss adapted learning_rate :              9.717276402677072e-82
    epoch : 8 ; learning_rate : 9.717276402677072e-82 ; fit : 0.15416666666666667
    batch_size :          12000
    best_batch_loss : 1.6226231415641923
    Epoch 8, Loss: 1.62818384467121, fit: 0.1505
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.6278444838177166_fit_0.1505_2024-01-03_183804
  self.fit : 0.1505
  self.loss : 1.6278444838177166
  current_accuracy : 0.1539
   Accuracy mean: 0.1309
   Accuracy mean: 0.1539
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.03981071705534972
      loss_factor :              1
      loss adapted learning_rate :              0.03981071705534972
    epoch : 0 ; learning_rate : 0.03981071705534972 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.5950000820107877
    Epoch 0, Loss: 1.6906609973823887, fit: 0.16516666666666666
    Epoch 0, Loss: 1.6906609973823887
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.5950000820107877_fit_0.16516666666666666_2024-01-03_183807
  self.fit : 0.16516666666666666
  self.loss : 1.5950000820107877
  current_accuracy : 0.191
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.03981071705534972
      loss_factor :              1
      loss adapted learning_rate :              0.03981071705534972
    epoch : 0 ; learning_rate : 0.03981071705534972 ; fit : 0.16516666666666666
    batch_size :          12000
    best_batch_loss : 1.349374813587936
    Epoch 0, Loss: 1.4573835376217086, fit: 0.2891666666666667
    Epoch 0, Loss: 1.4573835376217086
      batch rate adapted learning_rate :              0.03981071705534972
      loss :              1.349374813587936
      loss_factor :              0.1349374813587936
      loss adapted learning_rate :              4.104497242037329e-89
    epoch : 1 ; learning_rate : 4.104497242037329e-89 ; fit : 0.2891666666666667
    batch_size :          12000
    best_batch_loss : 1.3010219317703098
    Epoch 1, Loss: 1.3177241654820617, fit: 0.3005833333333333
      batch rate adapted learning_rate :              0.03981071705534972
      loss :              1.3278959423573702
      loss_factor :              0.13278959423573702
      loss adapted learning_rate :              8.249058498390881e-90
    epoch : 2 ; learning_rate : 8.249058498390881e-90 ; fit : 0.3005833333333333
    batch_size :          12000
    best_batch_loss : 1.2980157557411625
    Epoch 2, Loss: 1.3177241654820615, fit: 0.30691666666666667
      batch rate adapted learning_rate :              0.03981071705534972
      loss :              1.315831388107247
      loss_factor :              0.1315831388107247
      loss adapted learning_rate :              3.311495683515316e-90
    epoch : 3 ; learning_rate : 3.311495683515316e-90 ; fit : 0.30691666666666667
    batch_size :          12000
    best_batch_loss : 1.3042291215351345
    Epoch 3, Loss: 1.3177241654820615, fit: 0.30083333333333334
      batch rate adapted learning_rate :              0.03981071705534972
      loss :              1.3264591972602386
      loss_factor :              0.13264591972602385
      loss adapted learning_rate :              7.4026895160758255e-90
    epoch : 4 ; learning_rate : 7.4026895160758255e-90 ; fit : 0.30083333333333334
    batch_size :          12000
    best_batch_loss : 1.302032162632794
    Epoch 4, Loss: 1.317724165482062, fit: 0.304
      batch rate adapted learning_rate :              0.03981071705534972
      loss :              1.32185127965136
      loss_factor :              0.13218512796513598
      loss adapted learning_rate :              5.22708521181737e-90
    epoch : 5 ; learning_rate : 5.22708521181737e-90 ; fit : 0.304
    batch_size :          12000
    best_batch_loss : 1.3058209977321984
    Epoch 5, Loss: 1.317724165482062, fit: 0.30075
      batch rate adapted learning_rate :              0.03981071705534972
      loss :              1.3280516520090258
      loss_factor :              0.13280516520090258
      loss adapted learning_rate :              8.346350923261927e-90
    epoch : 6 ; learning_rate : 8.346350923261927e-90 ; fit : 0.30075
    batch_size :          12000
    best_batch_loss : 1.3123461304012227
    Epoch 6, Loss: 1.3177241654820617, fit: 0.3055833333333333
      batch rate adapted learning_rate :              0.03981071705534972
      loss :              1.3173485585534774
      loss_factor :              0.13173485585534775
      loss adapted learning_rate :              3.7159516723484436e-90
    epoch : 7 ; learning_rate : 3.7159516723484436e-90 ; fit : 0.3055833333333333
    batch_size :          12000
    best_batch_loss : 1.3093481257647737
    Epoch 7, Loss: 1.3177241654820617, fit: 0.30725
      batch rate adapted learning_rate :              0.03981071705534972
      loss :              1.3123417432474138
      loss_factor :              0.13123417432474138
      loss adapted learning_rate :              2.539184848913558e-90
    epoch : 8 ; learning_rate : 2.539184848913558e-90 ; fit : 0.30725
    batch_size :          12000
    best_batch_loss : 1.299528656541514
    Epoch 8, Loss: 1.317724165482062, fit: 0.3055
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.318245034774623_fit_0.3055_2024-01-03_183827
  self.fit : 0.3055
  self.loss : 1.318245034774623
  current_accuracy : 0.3124
   Accuracy mean: 0.191
   Accuracy mean: 0.3124
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.15848931924611134
      loss_factor :              1
      loss adapted learning_rate :              0.15848931924611134
    epoch : 0 ; learning_rate : 0.15848931924611134 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.125668195557767
    Epoch 0, Loss: 1.4180735911734685, fit: 0.41408333333333336
    Epoch 0, Loss: 1.4180735911734685
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.125668195557767_fit_0.41408333333333336_2024-01-03_183829
  self.fit : 0.41408333333333336
  self.loss : 1.125668195557767
  current_accuracy : 0.5116
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.15848931924611134
      loss_factor :              1
      loss adapted learning_rate :              0.15848931924611134
    epoch : 0 ; learning_rate : 0.15848931924611134 ; fit : 0.41408333333333336
    batch_size :          12000
    best_batch_loss : 0.7189163351544384
    Epoch 0, Loss: 0.8104173400542087, fit: 0.62
    Epoch 0, Loss: 0.8104173400542087
      batch rate adapted learning_rate :              0.15848931924611134
      loss :              0.7189163351544384
      loss_factor :              0.07189163351544384
      loss adapted learning_rate :              7.376242269739155e-116
    epoch : 1 ; learning_rate : 7.376242269739155e-116 ; fit : 0.62
    batch_size :          12000
    best_batch_loss : 0.6936227560122693
    Epoch 1, Loss: 0.7028031739022909, fit: 0.62725
      batch rate adapted learning_rate :              0.15848931924611134
      loss :              0.7061021943730074
      loss_factor :              0.07061021943730074
      loss adapted learning_rate :              1.2211137360768828e-116
    epoch : 2 ; learning_rate : 1.2211137360768828e-116 ; fit : 0.62725
    batch_size :          12000
    best_batch_loss : 0.6855150121106675
    Epoch 2, Loss: 0.7028031739022909, fit: 0.638
      batch rate adapted learning_rate :              0.15848931924611134
      loss :              0.6855150121106675
      loss_factor :              0.06855150121106675
      loss adapted learning_rate :              6.334276041252318e-118
    epoch : 3 ; learning_rate : 6.334276041252318e-118 ; fit : 0.638
    batch_size :          12000
    best_batch_loss : 0.6976025683709453
    Epoch 3, Loss: 0.7028031739022909, fit: 0.6315833333333334
      batch rate adapted learning_rate :              0.15848931924611134
      loss :              0.6994413153508274
      loss_factor :              0.06994413153508275
      loss adapted learning_rate :              4.7329096107849106e-117
    epoch : 4 ; learning_rate : 4.7329096107849106e-117 ; fit : 0.6315833333333334
    batch_size :          12000
    best_batch_loss : 0.6877071102437553
    Epoch 4, Loss: 0.702803173902291, fit: 0.6274166666666666
      batch rate adapted learning_rate :              0.15848931924611134
      loss :              0.7063902744615509
      loss_factor :              0.07063902744615509
      loss adapted learning_rate :              1.2719531847700202e-116
    epoch : 5 ; learning_rate : 1.2719531847700202e-116 ; fit : 0.6274166666666666
    batch_size :          12000
    best_batch_loss : 0.6954750015024247
    Epoch 5, Loss: 0.7028031739022909, fit: 0.6294166666666666
      batch rate adapted learning_rate :              0.15848931924611134
      loss :              0.7003307354025518
      loss_factor :              0.07003307354025518
      loss adapted learning_rate :              5.3742598863387246e-117
    epoch : 6 ; learning_rate : 5.3742598863387246e-117 ; fit : 0.6294166666666666
    batch_size :          12000
    best_batch_loss : 0.6938791517799585
    Epoch 6, Loss: 0.7028031739022909, fit: 0.6255833333333334
      batch rate adapted learning_rate :              0.15848931924611134
      loss :              0.7089223922295826
      loss_factor :              0.07089223922295826
      loss adapted learning_rate :              1.819153859214901e-116
    epoch : 7 ; learning_rate : 1.819153859214901e-116 ; fit : 0.6255833333333334
    batch_size :          12000
    best_batch_loss : 0.6970899742187396
    Epoch 7, Loss: 0.7028031739022909, fit: 0.6321666666666667
      batch rate adapted learning_rate :              0.15848931924611134
      loss :              0.6970899742187396
      loss_factor :              0.06970899742187396
      loss adapted learning_rate :              3.379742875983019e-117
    epoch : 8 ; learning_rate : 3.379742875983019e-117 ; fit : 0.6321666666666667
    batch_size :          12000
    best_batch_loss : 0.6945442561066896
    Epoch 8, Loss: 0.702803173902291, fit: 0.6311666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_0.6967251514386279_fit_0.6311666666666667_2024-01-03_183850
  self.fit : 0.6311666666666667
  self.loss : 0.6967251514386279
  current_accuracy : 0.6431
   Accuracy mean: 0.5116
   Accuracy mean: 0.6431
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.251188643150958
      loss_factor :              1
      loss adapted learning_rate :              0.251188643150958
    epoch : 0 ; learning_rate : 0.251188643150958 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.0350554920650898
    Epoch 0, Loss: 1.378255717233467, fit: 0.464
    Epoch 0, Loss: 1.378255717233467
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.0350554920650898_fit_0.464_2024-01-03_183852
  self.fit : 0.464
  self.loss : 1.0350554920650898
  current_accuracy : 0.5256
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.251188643150958
      loss_factor :              1
      loss adapted learning_rate :              0.251188643150958
    epoch : 0 ; learning_rate : 0.251188643150958 ; fit : 0.464
    batch_size :          12000
    best_batch_loss : 0.5801990848584837
    Epoch 0, Loss: 0.6983043622994469, fit: 0.6934166666666667
    Epoch 0, Loss: 0.6983043622994469
      batch rate adapted learning_rate :              0.251188643150958
      loss :              0.5801990848584837
      loss_factor :              0.05801990848584837
      loss adapted learning_rate :              5.724056949057108e-125
    epoch : 1 ; learning_rate : 5.724056949057108e-125 ; fit : 0.6934166666666667
    batch_size :          12000
    best_batch_loss : 0.5487911614290741
    Epoch 1, Loss: 0.5534597172430108, fit: 0.70825
      batch rate adapted learning_rate :              0.251188643150958
      loss :              0.5534683703676678
      loss_factor :              0.05534683703676678
      loss adapted learning_rate :              5.120126424489403e-127
    epoch : 2 ; learning_rate : 5.120126424489403e-127 ; fit : 0.70825
    batch_size :          12000
    best_batch_loss : 0.5431614741484493
    Epoch 2, Loss: 0.5534597172430108, fit: 0.7036666666666667
      batch rate adapted learning_rate :              0.251188643150958
      loss :              0.5627230050844855
      loss_factor :              0.056272300508448544
      loss adapted learning_rate :              2.6882337067416352e-126
    epoch : 3 ; learning_rate : 2.6882337067416352e-126 ; fit : 0.7036666666666667
    batch_size :          12000
    best_batch_loss : 0.5468018359969357
    Epoch 3, Loss: 0.5534597172430108, fit: 0.7098333333333333
      batch rate adapted learning_rate :              0.251188643150958
      loss :              0.5518548268287803
      loss_factor :              0.055185482682878026
      loss adapted learning_rate :              3.823706492068186e-127
    epoch : 4 ; learning_rate : 3.823706492068186e-127 ; fit : 0.7098333333333333
    batch_size :          12000
    best_batch_loss : 0.5444021032091552
    Epoch 4, Loss: 0.5534597172430108, fit: 0.7066666666666667
      batch rate adapted learning_rate :              0.251188643150958
      loss :              0.5572005552121209
      loss_factor :              0.055720055521212085
      loss adapted learning_rate :              1.0026608313328573e-126
    epoch : 5 ; learning_rate : 1.0026608313328573e-126 ; fit : 0.7066666666666667
    batch_size :          12000
    best_batch_loss : 0.5433456934010955
    Epoch 5, Loss: 0.5534597172430108, fit: 0.7038333333333333
      batch rate adapted learning_rate :              0.251188643150958
      loss :              0.5627422699299072
      loss_factor :              0.056274226992990714
      loss adapted learning_rate :              2.697452499469651e-126
    epoch : 6 ; learning_rate : 2.697452499469651e-126 ; fit : 0.7038333333333333
    batch_size :          12000
    best_batch_loss : 0.5381176597002572
    Epoch 6, Loss: 0.5534597172430108, fit: 0.7073333333333334
      batch rate adapted learning_rate :              0.251188643150958
      loss :              0.554904502715213
      loss_factor :              0.055490450271521295
      loss adapted learning_rate :              6.634754942016006e-127
    epoch : 7 ; learning_rate : 6.634754942016006e-127 ; fit : 0.7073333333333334
    batch_size :          12000
    best_batch_loss : 0.5420526203399878
    Epoch 7, Loss: 0.5534597172430108, fit: 0.713
      batch rate adapted learning_rate :              0.251188643150958
      loss :              0.5452347114173532
      loss_factor :              0.05452347114173532
      loss adapted learning_rate :              1.1437988732309136e-127
    epoch : 8 ; learning_rate : 1.1437988732309136e-127 ; fit : 0.713
    batch_size :          12000
    best_batch_loss : 0.542424479187856
    Epoch 8, Loss: 0.5534597172430108, fit: 0.706
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_0.5584851698654671_fit_0.706_2024-01-03_183913
  self.fit : 0.706
  self.loss : 0.5584851698654671
  current_accuracy : 0.7198
   Accuracy mean: 0.5256
   Accuracy mean: 0.7198
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.45730505192732634
      loss_factor :              1
      loss adapted learning_rate :              0.45730505192732634
    epoch : 0 ; learning_rate : 0.45730505192732634 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.1265972984901933
    Epoch 0, Loss: 1.3877570413097318, fit: 0.41558333333333336
    Epoch 0, Loss: 1.3877570413097318
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.1265972984901933_fit_0.41558333333333336_2024-01-03_183916
  self.fit : 0.41558333333333336
  self.loss : 1.1265972984901933
  current_accuracy : 0.5425
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.45730505192732634
      loss_factor :              1
      loss adapted learning_rate :              0.45730505192732634
    epoch : 0 ; learning_rate : 0.45730505192732634 ; fit : 0.41558333333333336
    batch_size :          12000
    best_batch_loss : 0.4906893043675879
    Epoch 0, Loss: 0.6245918583617124, fit: 0.7394166666666667
    Epoch 0, Loss: 0.6245918583617124
      batch rate adapted learning_rate :              0.45730505192732634
      loss :              0.4906893043675879
      loss_factor :              0.04906893043675879
      loss adapted learning_rate :              5.506367192557567e-132
    epoch : 1 ; learning_rate : 5.506367192557567e-132 ; fit : 0.7394166666666667
    batch_size :          12000
    best_batch_loss : 0.4654840974018227
    Epoch 1, Loss: 0.4706286900199168, fit: 0.7491666666666666
      batch rate adapted learning_rate :              0.45730505192732634
      loss :              0.4695215898329684
      loss_factor :              0.04695215898329684
      loss adapted learning_rate :              6.695186899800191e-134
    epoch : 2 ; learning_rate : 6.695186899800191e-134 ; fit : 0.7491666666666666
    batch_size :          12000
    best_batch_loss : 0.4663893675330585
    Epoch 2, Loss: 0.4706286900199167, fit: 0.751
      batch rate adapted learning_rate :              0.45730505192732634
      loss :              0.47010811531666713
      loss_factor :              0.04701081153166671
      loss adapted learning_rate :              7.585440956397349e-134
    epoch : 3 ; learning_rate : 7.585440956397349e-134 ; fit : 0.751
    batch_size :          12000
    best_batch_loss : 0.45431428342598285
    Epoch 3, Loss: 0.4706286900199167, fit: 0.7510833333333333
      batch rate adapted learning_rate :              0.45730505192732634
      loss :              0.4702513976288528
      loss_factor :              0.04702513976288528
      loss adapted learning_rate :              7.820157460132076e-134
    epoch : 4 ; learning_rate : 7.820157460132076e-134 ; fit : 0.7510833333333333
    batch_size :          12000
    best_batch_loss : 0.46546810170827335
    Epoch 4, Loss: 0.4706286900199167, fit: 0.7509166666666667
      batch rate adapted learning_rate :              0.45730505192732634
      loss :              0.46711339855131245
      loss_factor :              0.04671133985513125
      loss adapted learning_rate :              4.003487381820442e-134
    epoch : 5 ; learning_rate : 4.003487381820442e-134 ; fit : 0.7509166666666667
    batch_size :          12000
    best_batch_loss : 0.4579425092374184
    Epoch 5, Loss: 0.4706286900199167, fit: 0.743
      batch rate adapted learning_rate :              0.45730505192732634
      loss :              0.48475529273321427
      loss_factor :              0.048475529273321424
      loss adapted learning_rate :              1.631030022409428e-132
    epoch : 6 ; learning_rate : 1.631030022409428e-132 ; fit : 0.743
    batch_size :          12000
    best_batch_loss : 0.4453064753171341
    Epoch 6, Loss: 0.4706286900199168, fit: 0.7415833333333334
      batch rate adapted learning_rate :              0.45730505192732634
      loss :              0.4884173897295847
      loss_factor :              0.04884173897295847
      loss adapted learning_rate :              3.461926366528766e-132
    epoch : 7 ; learning_rate : 3.461926366528766e-132 ; fit : 0.7415833333333334
    batch_size :          12000
    best_batch_loss : 0.4658232092426312
    Epoch 7, Loss: 0.4706286900199168, fit: 0.7500833333333333
      batch rate adapted learning_rate :              0.45730505192732634
      loss :              0.47135830035611403
      loss_factor :              0.047135830035611406
      loss adapted learning_rate :              9.892861201826386e-134
    epoch : 8 ; learning_rate : 9.892861201826386e-134 ; fit : 0.7500833333333333
    batch_size :          12000
    best_batch_loss : 0.4650010634013638
    Epoch 8, Loss: 0.47062869001991675, fit: 0.749
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_0.4701633352925675_fit_0.749_2024-01-03_183936
  self.fit : 0.749
  self.loss : 0.4701633352925675
  current_accuracy : 0.7549
   Accuracy mean: 0.5425
   Accuracy mean: 0.7549
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.4781762498950185
      loss_factor :              1
      loss adapted learning_rate :              0.4781762498950185
    epoch : 0 ; learning_rate : 0.4781762498950185 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 0.8123756679122183
    Epoch 0, Loss: 1.266472648130562, fit: 0.576
    Epoch 0, Loss: 1.266472648130562
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_0.8123756679122183_fit_0.576_2024-01-03_183939
  self.fit : 0.576
  self.loss : 0.8123756679122183
  current_accuracy : 0.6358
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.4781762498950185
      loss_factor :              1
      loss adapted learning_rate :              0.4781762498950185
    epoch : 0 ; learning_rate : 0.4781762498950185 ; fit : 0.576
    batch_size :          12000
    best_batch_loss : 0.42659420393028774
    Epoch 0, Loss: 0.5397317743195893, fit: 0.7745833333333333
    Epoch 0, Loss: 0.5397317743195893
      batch rate adapted learning_rate :              0.4781762498950185
      loss :              0.42659420393028774
      loss_factor :              0.04265942039302877
      loss adapted learning_rate :              4.7982519432823535e-138
    epoch : 1 ; learning_rate : 4.7982519432823535e-138 ; fit : 0.7745833333333333
    batch_size :          12000
    best_batch_loss : 0.4055875578008648
    Epoch 1, Loss: 0.41653123515268314, fit: 0.7778333333333334
      batch rate adapted learning_rate :              0.4781762498950185
      loss :              0.421150378128465
      loss_factor :              0.0421150378128465
      loss adapted learning_rate :              1.3283346377677378e-138
    epoch : 2 ; learning_rate : 1.3283346377677378e-138 ; fit : 0.7778333333333334
    batch_size :          12000
    best_batch_loss : 0.40740451870386135
    Epoch 2, Loss: 0.4165312351526831, fit: 0.7771666666666667
      batch rate adapted learning_rate :              0.4781762498950185
      loss :              0.4219097721741347
      loss_factor :              0.042190977217413474
      loss adapted learning_rate :              1.5905468070035172e-138
    epoch : 3 ; learning_rate : 1.5905468070035172e-138 ; fit : 0.7771666666666667
    batch_size :          12000
    best_batch_loss : 0.4080037231548388
    Epoch 3, Loss: 0.4165312351526831, fit: 0.78475
      batch rate adapted learning_rate :              0.4781762498950185
      loss :              0.4080037231548388
      loss_factor :              0.04080037231548388
      loss adapted learning_rate :              5.571865432314389e-140
    epoch : 4 ; learning_rate : 5.571865432314389e-140 ; fit : 0.78475
    batch_size :          12000
    best_batch_loss : 0.4005954832034326
    Epoch 4, Loss: 0.4165312351526831, fit: 0.7773333333333333
      batch rate adapted learning_rate :              0.4781762498950185
      loss :              0.42002321851403857
      loss_factor :              0.04200232185140386
      loss adapted learning_rate :              1.0160564791304444e-138
    epoch : 5 ; learning_rate : 1.0160564791304444e-138 ; fit : 0.7773333333333333
    batch_size :          12000
    best_batch_loss : 0.40681908101050135
    Epoch 5, Loss: 0.4165312351526831, fit: 0.7836666666666666
      batch rate adapted learning_rate :              0.4781762498950185
      loss :              0.40681908101050135
      loss_factor :              0.040681908101050136
      loss adapted learning_rate :              4.1660017047904725e-140
    epoch : 6 ; learning_rate : 4.1660017047904725e-140 ; fit : 0.7836666666666666
    batch_size :          12000
    best_batch_loss : 0.40637226960989475
    Epoch 6, Loss: 0.4165312351526831, fit: 0.7793333333333333
      batch rate adapted learning_rate :              0.4781762498950185
      loss :              0.4179010231023196
      loss_factor :              0.04179010231023196
      loss adapted learning_rate :              6.122537303674847e-139
    epoch : 7 ; learning_rate : 6.122537303674847e-139 ; fit : 0.7793333333333333
    batch_size :          12000
    best_batch_loss : 0.4137220136051669
    Epoch 7, Loss: 0.4165312351526831, fit: 0.779
      batch rate adapted learning_rate :              0.4781762498950185
      loss :              0.4180786619043984
      loss_factor :              0.041807866190439844
      loss adapted learning_rate :              6.388343215932626e-139
    epoch : 8 ; learning_rate : 6.388343215932626e-139 ; fit : 0.779
    batch_size :          12000
    best_batch_loss : 0.40529596200283835
    Epoch 8, Loss: 0.41653123515268314, fit: 0.7778333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_0.419720377156452_fit_0.7778333333333334_2024-01-03_184000
  self.fit : 0.7778333333333334
  self.loss : 0.419720377156452
  current_accuracy : 0.7866
   Accuracy mean: 0.6358
   Accuracy mean: 0.7866
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.4959344196412831
      loss_factor :              1
      loss adapted learning_rate :              0.4959344196412831
    epoch : 0 ; learning_rate : 0.4959344196412831 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.162738408814681
    Epoch 0, Loss: 1.394092949463407, fit: 0.40041666666666664
    Epoch 0, Loss: 1.394092949463407
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.162738408814681_fit_0.40041666666666664_2024-01-03_184003
  self.fit : 0.40041666666666664
  self.loss : 1.162738408814681
  current_accuracy : 0.4514
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.4959344196412831
      loss_factor :              1
      loss adapted learning_rate :              0.4959344196412831
    epoch : 0 ; learning_rate : 0.4959344196412831 ; fit : 0.40041666666666664
    batch_size :          12000
    best_batch_loss : 0.46833652099204665
    Epoch 0, Loss: 0.7001062784689009, fit: 0.75125
    Epoch 0, Loss: 0.7001062784689009
      batch rate adapted learning_rate :              0.4959344196412831
      loss :              0.46833652099204665
      loss_factor :              0.04683365209920466
      loss adapted learning_rate :              5.63932078828154e-134
    epoch : 1 ; learning_rate : 5.63932078828154e-134 ; fit : 0.75125
    batch_size :          12000
    best_batch_loss : 0.4401295329160653
    Epoch 1, Loss: 0.454946146670278, fit: 0.7525833333333334
      batch rate adapted learning_rate :              0.4959344196412831
      loss :              0.46580827648720174
      loss_factor :              0.04658082764872017
      loss adapted learning_rate :              3.2820434982577824e-134
    epoch : 2 ; learning_rate : 3.2820434982577824e-134 ; fit : 0.7525833333333334
    batch_size :          12000
    best_batch_loss : 0.4407429197409683
    Epoch 2, Loss: 0.45494614667027794, fit: 0.7515833333333334
      batch rate adapted learning_rate :              0.4959344196412831
      loss :              0.4678526103635045
      loss_factor :              0.04678526103635045
      loss adapted learning_rate :              5.085456521608765e-134
    epoch : 3 ; learning_rate : 5.085456521608765e-134 ; fit : 0.7515833333333334
    batch_size :          12000
    best_batch_loss : 0.4485553878815444
    Epoch 3, Loss: 0.454946146670278, fit: 0.7535
      batch rate adapted learning_rate :              0.4959344196412831
      loss :              0.46333466074213614
      loss_factor :              0.04633346607421361
      loss adapted learning_rate :              1.9270954486943265e-134
    epoch : 4 ; learning_rate : 1.9270954486943265e-134 ; fit : 0.7535
    batch_size :          12000
    best_batch_loss : 0.44299286801358023
    Epoch 4, Loss: 0.45494614667027794, fit: 0.7566666666666667
      batch rate adapted learning_rate :              0.4959344196412831
      loss :              0.45606879032951914
      loss_factor :              0.045606879032951915
      loss adapted learning_rate :              3.966976387839947e-135
    epoch : 5 ; learning_rate : 3.966976387839947e-135 ; fit : 0.7566666666666667
    batch_size :          12000
    best_batch_loss : 0.4490671599221435
    Epoch 5, Loss: 0.454946146670278, fit: 0.7613333333333333
      batch rate adapted learning_rate :              0.4959344196412831
      loss :              0.4506003647415477
      loss_factor :              0.04506003647415477
      loss adapted learning_rate :              1.1873483252279363e-135
    epoch : 6 ; learning_rate : 1.1873483252279363e-135 ; fit : 0.7613333333333333
    batch_size :          12000
    best_batch_loss : 0.4494527360580958
    Epoch 6, Loss: 0.454946146670278, fit: 0.7580833333333333
      batch rate adapted learning_rate :              0.4959344196412831
      loss :              0.45456273777394274
      loss_factor :              0.04545627377739427
      loss adapted learning_rate :              2.8497545759818977e-135
    epoch : 7 ; learning_rate : 2.8497545759818977e-135 ; fit : 0.7580833333333333
    batch_size :          12000
    best_batch_loss : 0.4423893831865798
    Epoch 7, Loss: 0.4549461466702781, fit: 0.7595
      batch rate adapted learning_rate :              0.4959344196412831
      loss :              0.4532902631333067
      loss_factor :              0.045329026313330675
      loss adapted learning_rate :              2.153095453562838e-135
    epoch : 8 ; learning_rate : 2.153095453562838e-135 ; fit : 0.7595
    batch_size :          12000
    best_batch_loss : 0.4459943704750434
    Epoch 8, Loss: 0.45494614667027794, fit: 0.7595833333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_0.45189810754708437_fit_0.7595833333333334_2024-01-03_184024
  self.fit : 0.7595833333333334
  self.loss : 0.45189810754708437
  current_accuracy : 0.7609
   Accuracy mean: 0.4514
   Accuracy mean: 0.7609
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.6309573444801932
      loss_factor :              1
      loss adapted learning_rate :              0.6309573444801932
    epoch : 0 ; learning_rate : 0.6309573444801932 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.0254251341896576
    Epoch 0, Loss: 1.2905263861470473, fit: 0.47491666666666665
    Epoch 0, Loss: 1.2905263861470473
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.0254251341896576_fit_0.47491666666666665_2024-01-03_184026
  self.fit : 0.47491666666666665
  self.loss : 1.0254251341896576
  current_accuracy : 0.6064
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.6309573444801932
      loss_factor :              1
      loss adapted learning_rate :              0.6309573444801932
    epoch : 0 ; learning_rate : 0.6309573444801932 ; fit : 0.47491666666666665
    batch_size :          12000
    best_batch_loss : 0.38587178161143726
    Epoch 0, Loss: 0.503768738572306, fit: 0.7955
    Epoch 0, Loss: 0.503768738572306
      batch rate adapted learning_rate :              0.6309573444801932
      loss :              0.38587178161143726
      loss_factor :              0.03858717816114372
      loss adapted learning_rate :              2.781647055239421e-142
    epoch : 1 ; learning_rate : 2.781647055239421e-142 ; fit : 0.7955
    batch_size :          12000
    best_batch_loss : 0.37409047132380274
    Epoch 1, Loss: 0.3828608789108661, fit: 0.79425
      batch rate adapted learning_rate :              0.6309573444801932
      loss :              0.38591485668202746
      loss_factor :              0.038591485668202745
      loss adapted learning_rate :              2.8128709380572324e-142
    epoch : 2 ; learning_rate : 2.8128709380572324e-142 ; fit : 0.79425
    batch_size :          12000
    best_batch_loss : 0.37890872593368735
    Epoch 2, Loss: 0.3828608789108661, fit: 0.7980833333333334
      batch rate adapted learning_rate :              0.6309573444801932
      loss :              0.37890872593368735
      loss_factor :              0.037890872593368735
      loss adapted learning_rate :              4.502572151416299e-143
    epoch : 3 ; learning_rate : 4.502572151416299e-143 ; fit : 0.7980833333333334
    batch_size :          12000
    best_batch_loss : 0.3809446321393696
    Epoch 3, Loss: 0.3828608789108662, fit: 0.7966666666666666
      batch rate adapted learning_rate :              0.6309573444801932
      loss :              0.3818728648508573
      loss_factor :              0.03818728648508573
      loss adapted learning_rate :              9.814765647106336e-143
    epoch : 4 ; learning_rate : 9.814765647106336e-143 ; fit : 0.7966666666666666
    batch_size :          12000
    best_batch_loss : 0.3706139006217529
    Epoch 4, Loss: 0.3828608789108661, fit: 0.7945
      batch rate adapted learning_rate :              0.6309573444801932
      loss :              0.38404130014343674
      loss_factor :              0.038404130014343674
      loss adapted learning_rate :              1.728992579771236e-142
    epoch : 5 ; learning_rate : 1.728992579771236e-142 ; fit : 0.7945
    batch_size :          12000
    best_batch_loss : 0.3765225963620037
    Epoch 5, Loss: 0.3828608789108662, fit: 0.7975
      batch rate adapted learning_rate :              0.6309573444801932
      loss :              0.37918007219661143
      loss_factor :              0.03791800721966114
      loss adapted learning_rate :              4.836714943060307e-143
    epoch : 6 ; learning_rate : 4.836714943060307e-143 ; fit : 0.7975
    batch_size :          12000
    best_batch_loss : 0.36958606498479235
    Epoch 6, Loss: 0.3828608789108661, fit: 0.7939166666666667
      batch rate adapted learning_rate :              0.6309573444801932
      loss :              0.3882632955753109
      loss_factor :              0.03882632955753109
      loss adapted learning_rate :              5.159809299505963e-142
    epoch : 7 ; learning_rate : 5.159809299505963e-142 ; fit : 0.7939166666666667
    batch_size :          12000
    best_batch_loss : 0.3676575062996057
    Epoch 7, Loss: 0.3828608789108661, fit: 0.7970833333333334
      batch rate adapted learning_rate :              0.6309573444801932
      loss :              0.379388010793891
      loss_factor :              0.0379388010793891
      loss adapted learning_rate :              5.109286357061188e-143
    epoch : 8 ; learning_rate : 5.109286357061188e-143 ; fit : 0.7970833333333334
    batch_size :          12000
    best_batch_loss : 0.37825878591338324
    Epoch 8, Loss: 0.38286087891086606, fit: 0.7973333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_0.3823744311707607_fit_0.7973333333333333_2024-01-03_184047
  self.fit : 0.7973333333333333
  self.loss : 0.3823744311707607
  current_accuracy : 0.8033
   Accuracy mean: 0.6064
   Accuracy mean: 0.8033
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.9791483623609768
      loss_factor :              1
      loss adapted learning_rate :              0.9791483623609768
    epoch : 0 ; learning_rate : 0.9791483623609768 ; fit : 0.0
    batch_size :          12000
    best_batch_loss : 1.315601820321225
    Epoch 0, Loss: 1.6389039237597207, fit: 0.10233333333333333
    Epoch 0, Loss: 1.6389039237597207
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7953333333333332_fit_0.10233333333333333_2024-01-03_184050
  self.fit : 0.10233333333333333
  self.loss : 1.7953333333333332
  current_accuracy : 0.101
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.2
      batch rate adapted learning_rate :              0.9791483623609768
      loss_factor :              1
      loss adapted learning_rate :              0.9791483623609768
    epoch : 0 ; learning_rate : 0.9791483623609768 ; fit : 0.10233333333333333
    batch_size :          12000
    best_batch_loss : 1.7856666666666667
    Epoch 0, Loss: 1.7956333333333334, fit: 0.10066666666666667
    Epoch 0, Loss: 1.7956333333333334
      batch rate adapted learning_rate :              0.9791483623609768
      loss :              1.7986666666666666
      loss_factor :              0.17986666666666667
      loss adapted learning_rate :              3.06137921953776e-75
    epoch : 1 ; learning_rate : 3.06137921953776e-75 ; fit : 0.10066666666666667
    batch_size :          12000
    best_batch_loss : 1.791
    Epoch 1, Loss: 1.7956333333333334, fit: 0.10108333333333333
      batch rate adapted learning_rate :              0.9791483623609768
      loss :              1.7978333333333334
      loss_factor :              0.17978333333333335
      loss adapted learning_rate :              2.922747763033842e-75
    epoch : 2 ; learning_rate : 2.922747763033842e-75 ; fit : 0.10108333333333333
    batch_size :          12000
    best_batch_loss : 1.7911666666666666
    Epoch 2, Loss: 1.7956333333333334, fit: 0.10441666666666667
      batch rate adapted learning_rate :              0.9791483623609768
      loss :              1.7911666666666666
      loss_factor :              0.17911666666666665
      loss adapted learning_rate :              2.0158042254735179e-75
    epoch : 3 ; learning_rate : 2.0158042254735179e-75 ; fit : 0.10441666666666667
    batch_size :          12000
    best_batch_loss : 1.7916666666666667
    Epoch 3, Loss: 1.7956333333333332, fit: 0.10416666666666667
      batch rate adapted learning_rate :              0.9791483623609768
      loss :              1.7916666666666667
      loss_factor :              0.17916666666666667
      loss adapted learning_rate :              2.0728596069430574e-75
    epoch : 4 ; learning_rate : 2.0728596069430574e-75 ; fit : 0.10416666666666667
    batch_size :          12000
    best_batch_loss : 1.7878333333333334
    Epoch 4, Loss: 1.7956333333333334, fit: 0.10366666666666667
      batch rate adapted learning_rate :              0.9791483623609768
      loss :              1.7926666666666666
      loss_factor :              0.17926666666666666
      loss adapted learning_rate :              2.1918095690393151e-75
    epoch : 5 ; learning_rate : 2.1918095690393151e-75 ; fit : 0.10366666666666667
    batch_size :          12000
    best_batch_loss : 1.7836666666666667
    Epoch 5, Loss: 1.795633333333333, fit: 0.1005
      batch rate adapted learning_rate :              0.9791483623609768
      loss :              1.799
      loss_factor :              0.1799
      loss adapted learning_rate :              3.118637066927666e-75
    epoch : 6 ; learning_rate : 3.118637066927666e-75 ; fit : 0.1005
    batch_size :          12000
    best_batch_loss : 1.79
    Epoch 6, Loss: 1.7956333333333334, fit: 0.10183333333333333
      batch rate adapted learning_rate :              0.9791483623609768
      loss :              1.7963333333333333
      loss_factor :              0.17963333333333334
      loss adapted learning_rate :              2.6886940712005625e-75
    epoch : 7 ; learning_rate : 2.6886940712005625e-75 ; fit : 0.10183333333333333
    batch_size :          12000
    best_batch_loss : 1.7853333333333334
    Epoch 7, Loss: 1.7956333333333334, fit: 0.1015
      batch rate adapted learning_rate :              0.9791483623609768
      loss :              1.797
      loss_factor :              0.1797
      loss adapted learning_rate :              2.790334140157443e-75
    epoch : 8 ; learning_rate : 2.790334140157443e-75 ; fit : 0.1015
    batch_size :          12000
    best_batch_loss : 1.7923333333333333
    Epoch 8, Loss: 1.7956333333333334, fit: 0.10341666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7931666666666666_fit_0.10341666666666667_2024-01-03_184112
  self.fit : 0.10341666666666667
  self.loss : 1.7931666666666666
  current_accuracy : 0.101
   Accuracy mean: 0.101
   Accuracy mean: 0.101
  Error saving file: doc/out/test_combinations_results/20240103184112.
  normalized_accuracies :      [2.51839276e-02 2.51839276e-02 5.67345784e-02 5.67345784e-02
   0.00000000e+00 1.41482739e-04 4.86700623e-02 8.12110922e-02
   1.33701188e-01 3.05461234e-01 5.87294850e-01 7.73344652e-01
   6.07102434e-01 8.81861913e-01 6.31013016e-01 9.31522354e-01
   7.63016412e-01 9.76372383e-01 5.02122241e-01 9.40011319e-01
   7.21420487e-01 1.00000000e+00 6.36672326e-03 6.36672326e-03]
batch_rate :  0.1
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.1
      batch rate adapted learning_rate :              3.981071705534969e-07
      loss_factor :              1
      loss adapted learning_rate :              3.981071705534969e-07
    epoch : 0 ; learning_rate : 3.981071705534969e-07 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.7092503828323264
    Epoch 0, Loss: 1.722727276906049, fit: 0.112
    Epoch 0, Loss: 1.722727276906049
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7214939510967575_fit_0.112_2024-01-03_184116
  self.fit : 0.112
  self.loss : 1.7214939510967575
  current_accuracy : 0.1058
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-64
    batch_rate :          0.1
      batch rate adapted learning_rate :              3.981071705534969e-07
      loss_factor :              1
      loss adapted learning_rate :              3.981071705534969e-07
    epoch : 0 ; learning_rate : 3.981071705534969e-07 ; fit : 0.112
    batch_size :          6000
    best_batch_loss : 1.7093352615522208
    Epoch 0, Loss: 1.7227241069610035, fit: 0.106
    Epoch 0, Loss: 1.7227241069610035
      batch rate adapted learning_rate :              3.981071705534969e-07
      loss :              1.730014991189886
      loss_factor :              0.1730014991189886
      loss adapted learning_rate :              2.540894575038949e-83
    epoch : 1 ; learning_rate : 2.540894575038949e-83 ; fit : 0.106
    batch_size :          6000
    best_batch_loss : 1.712491409377493
    Epoch 1, Loss: 1.7227221289151529, fit: 0.114
      batch rate adapted learning_rate :              3.981071705534969e-07
      loss :              1.7145949211105722
      loss_factor :              0.1714594921110572
      loss adapted learning_rate :              1.037894701035154e-83
    epoch : 2 ; learning_rate : 1.037894701035154e-83 ; fit : 0.114
    batch_size :          6000
    best_batch_loss : 1.7124345156669214
    Epoch 2, Loss: 1.7227221289151529, fit: 0.112
      batch rate adapted learning_rate :              3.981071705534969e-07
      loss :              1.717701704155404
      loss_factor :              0.17177017041554038
      loss adapted learning_rate :              1.2438690225609064e-83
    epoch : 3 ; learning_rate : 1.2438690225609064e-83 ; fit : 0.112
    batch_size :          6000
    best_batch_loss : 1.7086384774059657
    Epoch 3, Loss: 1.7227221289151529, fit: 0.11666666666666667
      batch rate adapted learning_rate :              3.981071705534969e-07
      loss :              1.7086384774059657
      loss_factor :              0.17086384774059657
      loss adapted learning_rate :              7.328552714523803e-84
    epoch : 4 ; learning_rate : 7.328552714523803e-84 ; fit : 0.11666666666666667
    batch_size :          6000
    best_batch_loss : 1.7090220935789324
    Epoch 4, Loss: 1.7227221289151533, fit: 0.1115
      batch rate adapted learning_rate :              3.981071705534969e-07
      loss :              1.7209374700545235
      loss_factor :              0.17209374700545235
      loss adapted learning_rate :              1.501443729183968e-83
    epoch : 5 ; learning_rate : 1.501443729183968e-83 ; fit : 0.1115
    batch_size :          6000
    best_batch_loss : 1.7136923224449956
    Epoch 5, Loss: 1.7227221289151526, fit: 0.1085
      batch rate adapted learning_rate :              3.981071705534969e-07
      loss :              1.724627619292603
      loss_factor :              0.1724627619292603
      loss adapted learning_rate :              1.8600888483159052e-83
    epoch : 6 ; learning_rate : 1.8600888483159052e-83 ; fit : 0.1085
    batch_size :          6000
    best_batch_loss : 1.7130051755407782
    Epoch 6, Loss: 1.7227221289151529, fit: 0.112
      batch rate adapted learning_rate :              3.981071705534969e-07
      loss :              1.718141061634584
      loss_factor :              0.1718141061634584
      loss adapted learning_rate :              1.2760911905919447e-83
    epoch : 7 ; learning_rate : 1.2760911905919447e-83 ; fit : 0.112
    batch_size :          6000
    best_batch_loss : 1.7162506879350012
    Epoch 7, Loss: 1.7227221289151526, fit: 0.10283333333333333
      batch rate adapted learning_rate :              3.981071705534969e-07
      loss :              1.7370706016843933
      loss_factor :              0.17370706016843934
      loss adapted learning_rate :              3.8172186036721714e-83
    epoch : 8 ; learning_rate : 3.8172186036721714e-83 ; fit : 0.10283333333333333
    batch_size :          6000
    best_batch_loss : 1.7095256438272315
    Epoch 8, Loss: 1.722722128915153, fit: 0.11516666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7095256438272315_fit_0.11516666666666667_2024-01-03_184136
  self.fit : 0.11516666666666667
  self.loss : 1.7095256438272315
  current_accuracy : 0.1058
   Accuracy mean: 0.1058
   Accuracy mean: 0.1058
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.0005011872336272721
      loss_factor :              1
      loss adapted learning_rate :              0.0005011872336272721
    epoch : 0 ; learning_rate : 0.0005011872336272721 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.6989771740202204
    Epoch 0, Loss: 1.709981700624617, fit: 0.12033333333333333
    Epoch 0, Loss: 1.709981700624617
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7138946904729222_fit_0.12033333333333333_2024-01-03_184139
  self.fit : 0.12033333333333333
  self.loss : 1.7138946904729222
  current_accuracy : 0.125
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-33
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.0005011872336272721
      loss_factor :              1
      loss adapted learning_rate :              0.0005011872336272721
    epoch : 0 ; learning_rate : 0.0005011872336272721 ; fit : 0.12033333333333333
    batch_size :          6000
    best_batch_loss : 1.692475233535247
    Epoch 0, Loss: 1.7069387635202644, fit: 0.1255
    Epoch 0, Loss: 1.7069387635202644
      batch rate adapted learning_rate :              0.0005011872336272721
      loss :              1.7001238754715826
      loss_factor :              0.17001238754715825
      loss adapted learning_rate :              5.598306041669737e-81
    epoch : 1 ; learning_rate : 5.598306041669737e-81 ; fit : 0.1255
    batch_size :          6000
    best_batch_loss : 1.6981968395388116
    Epoch 1, Loss: 1.705349763738762, fit: 0.122
      batch rate adapted learning_rate :              0.0005011872336272721
      loss :              1.7088035038571914
      loss_factor :              0.17088035038571914
      loss adapted learning_rate :              9.315637616482295e-81
    epoch : 2 ; learning_rate : 9.315637616482295e-81 ; fit : 0.122
    batch_size :          6000
    best_batch_loss : 1.69097147221821
    Epoch 2, Loss: 1.7053497637387616, fit: 0.12466666666666666
      batch rate adapted learning_rate :              0.0005011872336272721
      loss :              1.7043545324469487
      loss_factor :              0.17043545324469486
      loss adapted learning_rate :              7.177843556244708e-81
    epoch : 3 ; learning_rate : 7.177843556244708e-81 ; fit : 0.12466666666666666
    batch_size :          6000
    best_batch_loss : 1.6933232510132639
    Epoch 3, Loss: 1.7053497637387622, fit: 0.12016666666666667
      batch rate adapted learning_rate :              0.0005011872336272721
      loss :              1.713929836930803
      loss_factor :              0.1713929836930803
      loss adapted learning_rate :              1.2569092597470218e-80
    epoch : 4 ; learning_rate : 1.2569092597470218e-80 ; fit : 0.12016666666666667
    batch_size :          6000
    best_batch_loss : 1.6904036671939635
    Epoch 4, Loss: 1.705349763738762, fit: 0.11816666666666667
      batch rate adapted learning_rate :              0.0005011872336272721
      loss :              1.7162020425710753
      loss_factor :              0.17162020425710753
      loss adapted learning_rate :              1.4349654744899312e-80
    epoch : 5 ; learning_rate : 1.4349654744899312e-80 ; fit : 0.11816666666666667
    batch_size :          6000
    best_batch_loss : 1.6935121613253807
    Epoch 5, Loss: 1.7053497637387618, fit: 0.1225
      batch rate adapted learning_rate :              0.0005011872336272721
      loss :              1.7077794086470743
      loss_factor :              0.17077794086470743
      loss adapted learning_rate :              8.773588872690813e-81
    epoch : 6 ; learning_rate : 8.773588872690813e-81 ; fit : 0.1225
    batch_size :          6000
    best_batch_loss : 1.6914235186151156
    Epoch 6, Loss: 1.7053497637387618, fit: 0.12266666666666666
      batch rate adapted learning_rate :              0.0005011872336272721
      loss :              1.7061567599459573
      loss_factor :              0.17061567599459573
      loss adapted learning_rate :              7.977983246179112e-81
    epoch : 7 ; learning_rate : 7.977983246179112e-81 ; fit : 0.12266666666666666
    batch_size :          6000
    best_batch_loss : 1.6914710381896345
    Epoch 7, Loss: 1.7053497637387618, fit: 0.12233333333333334
      batch rate adapted learning_rate :              0.0005011872336272721
      loss :              1.709443261869413
      loss_factor :              0.1709443261869413
      loss adapted learning_rate :              9.670948495705766e-81
    epoch : 8 ; learning_rate : 9.670948495705766e-81 ; fit : 0.12233333333333334
    batch_size :          6000
    best_batch_loss : 1.695136975598225
    Epoch 8, Loss: 1.7053497637387618, fit: 0.12383333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.705893570727756_fit_0.12383333333333334_2024-01-03_184159
  self.fit : 0.12383333333333334
  self.loss : 1.705893570727756
  current_accuracy : 0.1282
   Accuracy mean: 0.125
   Accuracy mean: 0.1282
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.019952623149688792
      loss_factor :              1
      loss adapted learning_rate :              0.019952623149688792
    epoch : 0 ; learning_rate : 0.019952623149688792 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.5238140607670103
    Epoch 0, Loss: 1.6127613238002512, fit: 0.20183333333333334
    Epoch 0, Loss: 1.6127613238002512
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.5238140607670103_fit_0.20183333333333334_2024-01-03_184202
  self.fit : 0.20183333333333334
  self.loss : 1.5238140607670103
  current_accuracy : 0.2354
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.019952623149688792
      loss_factor :              1
      loss adapted learning_rate :              0.019952623149688792
    epoch : 0 ; learning_rate : 0.019952623149688792 ; fit : 0.20183333333333334
    batch_size :          6000
    best_batch_loss : 1.3126653490612175
    Epoch 0, Loss: 1.3995725655613969, fit: 0.3085
    Epoch 0, Loss: 1.3995725655613969
      batch rate adapted learning_rate :              0.019952623149688792
      loss :              1.3126653490612175
      loss_factor :              0.13126653490612175
      loss adapted learning_rate :              1.304373945689711e-90
    epoch : 1 ; learning_rate : 1.304373945689711e-90 ; fit : 0.3085
    batch_size :          6000
    best_batch_loss : 1.2753025614265348
    Epoch 1, Loss: 1.292887220486087, fit: 0.3215
      batch rate adapted learning_rate :              0.019952623149688792
      loss :              1.2912584850661801
      loss_factor :              0.129125848506618
      loss adapted learning_rate :              2.519525948869819e-91
    epoch : 2 ; learning_rate : 2.519525948869819e-91 ; fit : 0.3215
    batch_size :          6000
    best_batch_loss : 1.2810781877345987
    Epoch 2, Loss: 1.2928872204860873, fit: 0.31466666666666665
      batch rate adapted learning_rate :              0.019952623149688792
      loss :              1.3047850225480717
      loss_factor :              0.13047850225480717
      loss adapted learning_rate :              7.143257161803542e-91
    epoch : 3 ; learning_rate : 7.143257161803542e-91 ; fit : 0.31466666666666665
    batch_size :          6000
    best_batch_loss : 1.2707967509760207
    Epoch 3, Loss: 1.2928872204860873, fit: 0.3145
      batch rate adapted learning_rate :              0.019952623149688792
      loss :              1.3060429352580085
      loss_factor :              0.13060429352580086
      loss adapted learning_rate :              7.86584583764647e-91
    epoch : 4 ; learning_rate : 7.86584583764647e-91 ; fit : 0.3145
    batch_size :          6000
    best_batch_loss : 1.2785614759950392
    Epoch 4, Loss: 1.2928872204860868, fit: 0.3283333333333333
      batch rate adapted learning_rate :              0.019952623149688792
      loss :              1.2785974467357373
      loss_factor :              0.12785974467357372
      loss adapted learning_rate :              9.405530248818708e-92
    epoch : 5 ; learning_rate : 9.405530248818708e-92 ; fit : 0.3283333333333333
    batch_size :          6000
    best_batch_loss : 1.2794709571818583
    Epoch 5, Loss: 1.292887220486087, fit: 0.31416666666666665
      batch rate adapted learning_rate :              0.019952623149688792
      loss :              1.3077287042575494
      loss_factor :              0.13077287042575494
      loss adapted learning_rate :              8.948817426624529e-91
    epoch : 6 ; learning_rate : 8.948817426624529e-91 ; fit : 0.31416666666666665
    batch_size :          6000
    best_batch_loss : 1.2805353037401384
    Epoch 6, Loss: 1.2928872204860868, fit: 0.319
      batch rate adapted learning_rate :              0.019952623149688792
      loss :              1.2960705238414267
      loss_factor :              0.12960705238414266
      loss adapted learning_rate :              3.654797924834755e-91
    epoch : 7 ; learning_rate : 3.654797924834755e-91 ; fit : 0.319
    batch_size :          6000
    best_batch_loss : 1.2817747831568078
    Epoch 7, Loss: 1.292887220486087, fit: 0.32233333333333336
      batch rate adapted learning_rate :              0.019952623149688792
      loss :              1.283952707560164
      loss_factor :              0.1283952707560164
      loss adapted learning_rate :              1.4285740116377227e-91
    epoch : 8 ; learning_rate : 1.4285740116377227e-91 ; fit : 0.32233333333333336
    batch_size :          6000
    best_batch_loss : 1.2650384559374634
    Epoch 8, Loss: 1.2928872204860873, fit: 0.33416666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.2650384559374634_fit_0.33416666666666667_2024-01-03_184223
  self.fit : 0.33416666666666667
  self.loss : 1.2650384559374634
  current_accuracy : 0.338
   Accuracy mean: 0.2354
   Accuracy mean: 0.338
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.09999999999999999
      loss_factor :              1
      loss adapted learning_rate :              0.09999999999999999
    epoch : 0 ; learning_rate : 0.09999999999999999 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 0.8871965548947063
    Epoch 0, Loss: 1.1907180461510045, fit: 0.5343333333333333
    Epoch 0, Loss: 1.1907180461510045
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_0.8871965548947063_fit_0.5343333333333333_2024-01-03_184225
  self.fit : 0.5343333333333333
  self.loss : 0.8871965548947063
  current_accuracy : 0.5445
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.09999999999999999
      loss_factor :              1
      loss adapted learning_rate :              0.09999999999999999
    epoch : 0 ; learning_rate : 0.09999999999999999 ; fit : 0.5343333333333333
    batch_size :          6000
    best_batch_loss : 0.6944149570563762
    Epoch 0, Loss: 0.7635933673446966, fit: 0.6331666666666667
    Epoch 0, Loss: 0.7635933673446966
      batch rate adapted learning_rate :              0.09999999999999999
      loss :              0.6944149570563762
      loss_factor :              0.06944149570563762
      loss adapted learning_rate :              1.4517995487957876e-117
    epoch : 1 ; learning_rate : 1.4517995487957876e-117 ; fit : 0.6331666666666667
    batch_size :          6000
    best_batch_loss : 0.657502302317617
    Epoch 1, Loss: 0.6719368075099522, fit: 0.6516666666666666
      batch rate adapted learning_rate :              0.09999999999999999
      loss :              0.657502302317617
      loss_factor :              0.0657502302317617
      loss adapted learning_rate :              6.162086487067034e-120
    epoch : 2 ; learning_rate : 6.162086487067034e-120 ; fit : 0.6516666666666666
    batch_size :          6000
    best_batch_loss : 0.6554373508048759
    Epoch 2, Loss: 0.6719368075099522, fit: 0.645
      batch rate adapted learning_rate :              0.09999999999999999
      loss :              0.6757144004931245
      loss_factor :              0.06757144004931245
      loss adapted learning_rate :              9.46929070314431e-119
    epoch : 3 ; learning_rate : 9.46929070314431e-119 ; fit : 0.645
    batch_size :          6000
    best_batch_loss : 0.6603084825666369
    Epoch 3, Loss: 0.6719368075099522, fit: 0.6506666666666666
      batch rate adapted learning_rate :              0.09999999999999999
      loss :              0.6603084825666369
      loss_factor :              0.0660308482566637
      loss adapted learning_rate :              9.43382121841204e-120
    epoch : 4 ; learning_rate : 9.43382121841204e-120 ; fit : 0.6506666666666666
    batch_size :          6000
    best_batch_loss : 0.6572012317698304
    Epoch 4, Loss: 0.6719368075099522, fit: 0.644
      batch rate adapted learning_rate :              0.09999999999999999
      loss :              0.6712131971119303
      loss_factor :              0.06712131971119303
      loss adapted learning_rate :              4.853429013866534e-119
    epoch : 5 ; learning_rate : 4.853429013866534e-119 ; fit : 0.644
    batch_size :          6000
    best_batch_loss : 0.6505741585002331
    Epoch 5, Loss: 0.6719368075099522, fit: 0.6455
      batch rate adapted learning_rate :              0.09999999999999999
      loss :              0.66910323300138
      loss_factor :              0.06691032330013799
      loss adapted learning_rate :              3.542524256934445e-119
    epoch : 6 ; learning_rate : 3.542524256934445e-119 ; fit : 0.6455
    batch_size :          6000
    best_batch_loss : 0.6523644345852349
    Epoch 6, Loss: 0.6719368075099522, fit: 0.6406666666666667
      batch rate adapted learning_rate :              0.09999999999999999
      loss :              0.6793572411562381
      loss_factor :              0.0679357241156238
      loss adapted learning_rate :              1.6211413509945167e-118
    epoch : 7 ; learning_rate : 1.6211413509945167e-118 ; fit : 0.6406666666666667
    batch_size :          6000
    best_batch_loss : 0.6562359058403645
    Epoch 7, Loss: 0.6719368075099521, fit: 0.637
      batch rate adapted learning_rate :              0.09999999999999999
      loss :              0.685166576433211
      loss_factor :              0.0685166576433211
      loss adapted learning_rate :              3.798541890629999e-118
    epoch : 8 ; learning_rate : 3.798541890629999e-118 ; fit : 0.637
    batch_size :          6000
    best_batch_loss : 0.6462832158276791
    Epoch 8, Loss: 0.6719368075099521, fit: 0.6565
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_0.6462832158276791_fit_0.6565_2024-01-03_184246
  self.fit : 0.6565
  self.loss : 0.6462832158276791
  current_accuracy : 0.6495
   Accuracy mean: 0.5445
   Accuracy mean: 0.6495
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.19952623149688795
      loss_factor :              1
      loss adapted learning_rate :              0.19952623149688795
    epoch : 0 ; learning_rate : 0.19952623149688795 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 0.696999339918134
    Epoch 0, Loss: 1.0587435274095516, fit: 0.626
    Epoch 0, Loss: 1.0587435274095516
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_0.7097739373726708_fit_0.626_2024-01-03_184248
  self.fit : 0.626
  self.loss : 0.7097739373726708
  current_accuracy : 0.6583
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.19952623149688795
      loss_factor :              1
      loss adapted learning_rate :              0.19952623149688795
    epoch : 0 ; learning_rate : 0.19952623149688795 ; fit : 0.626
    batch_size :          6000
    best_batch_loss : 0.49292040784439095
    Epoch 0, Loss: 0.5766440947514491, fit: 0.7368333333333333
    Epoch 0, Loss: 0.5766440947514491
      batch rate adapted learning_rate :              0.19952623149688795
      loss :              0.49292040784439095
      loss_factor :              0.04929204078443909
      loss adapted learning_rate :              3.781637662822772e-132
    epoch : 1 ; learning_rate : 3.781637662822772e-132 ; fit : 0.7368333333333333
    batch_size :          6000
    best_batch_loss : 0.48006847375479506
    Epoch 1, Loss: 0.497199726459872, fit: 0.7353333333333333
      batch rate adapted learning_rate :              0.19952623149688795
      loss :              0.5006156641416113
      loss_factor :              0.05006156641416113
      loss adapted learning_rate :              1.780096086012404e-131
    epoch : 2 ; learning_rate : 1.780096086012404e-131 ; fit : 0.7353333333333333
    batch_size :          6000
    best_batch_loss : 0.4783306794297344
    Epoch 2, Loss: 0.4971997264598719, fit: 0.729
      batch rate adapted learning_rate :              0.19952623149688795
      loss :              0.5108260533056941
      loss_factor :              0.05108260533056941
      loss adapted learning_rate :              1.3406145159358911e-130
    epoch : 3 ; learning_rate : 1.3406145159358911e-130 ; fit : 0.729
    batch_size :          6000
    best_batch_loss : 0.48338727551355487
    Epoch 3, Loss: 0.49719972645987187, fit: 0.7338333333333333
      batch rate adapted learning_rate :              0.19952623149688795
      loss :              0.5046051510902702
      loss_factor :              0.050460515109027014
      loss adapted learning_rate :              3.937023029471295e-131
    epoch : 4 ; learning_rate : 3.937023029471295e-131 ; fit : 0.7338333333333333
    batch_size :          6000
    best_batch_loss : 0.4636948749850939
    Epoch 4, Loss: 0.49719972645987187, fit: 0.7538333333333334
      batch rate adapted learning_rate :              0.19952623149688795
      loss :              0.4636948749850939
      loss_factor :              0.04636948749850939
      loss adapted learning_rate :              8.379721511667898e-135
    epoch : 5 ; learning_rate : 8.379721511667898e-135 ; fit : 0.7538333333333334
    batch_size :          6000
    best_batch_loss : 0.48804876592943863
    Epoch 5, Loss: 0.497199726459872, fit: 0.741
      batch rate adapted learning_rate :              0.19952623149688795
      loss :              0.48937916450332286
      loss_factor :              0.04893791645033228
      loss adapted learning_rate :              1.838855132669717e-132
    epoch : 6 ; learning_rate : 1.838855132669717e-132 ; fit : 0.741
    batch_size :          6000
    best_batch_loss : 0.48330217339969334
    Epoch 6, Loss: 0.4971997264598719, fit: 0.74
      batch rate adapted learning_rate :              0.19952623149688795
      loss :              0.4903955453127888
      loss_factor :              0.04903955453127888
      loss adapted learning_rate :              2.262828529068359e-132
    epoch : 7 ; learning_rate : 2.262828529068359e-132 ; fit : 0.74
    batch_size :          6000
    best_batch_loss : 0.48457410997347833
    Epoch 7, Loss: 0.4971997264598719, fit: 0.7413333333333333
      batch rate adapted learning_rate :              0.19952623149688795
      loss :              0.4912791394768614
      loss_factor :              0.04912791394768614
      loss adapted learning_rate :              2.709145726312369e-132
    epoch : 8 ; learning_rate : 2.709145726312369e-132 ; fit : 0.7413333333333333
    batch_size :          6000
    best_batch_loss : 0.4829192210770618
    Epoch 8, Loss: 0.497199726459872, fit: 0.7383333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_0.4929025752964764_fit_0.7383333333333333_2024-01-03_184309
  self.fit : 0.7383333333333333
  self.loss : 0.4929025752964764
  current_accuracy : 0.7491
   Accuracy mean: 0.6583
   Accuracy mean: 0.7491
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.39810717055349726
      loss_factor :              1
      loss adapted learning_rate :              0.39810717055349726
    epoch : 0 ; learning_rate : 0.39810717055349726 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 0.4180574738593535
    Epoch 0, Loss: 0.9622394566243306, fit: 0.7795
    Epoch 0, Loss: 0.9622394566243306
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_0.4180574738593535_fit_0.7795_2024-01-03_184311
  self.fit : 0.7795
  self.loss : 0.4180574738593535
  current_accuracy : 0.802
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.39810717055349726
      loss_factor :              1
      loss adapted learning_rate :              0.39810717055349726
    epoch : 0 ; learning_rate : 0.39810717055349726 ; fit : 0.7795
    batch_size :          6000
    best_batch_loss : 0.325540505936346
    Epoch 0, Loss: 0.36774203771615227, fit: 0.8258333333333333
    Epoch 0, Loss: 0.36774203771615227
      batch rate adapted learning_rate :              0.39810717055349726
      loss :              0.325540505936346
      loss_factor :              0.0325540505936346
      loss adapted learning_rate :              7.252761732846035e-150
    epoch : 1 ; learning_rate : 7.252761732846035e-150 ; fit : 0.8258333333333333
    batch_size :          6000
    best_batch_loss : 0.3303969784396494
    Epoch 1, Loss: 0.34165989003752834, fit: 0.82
      batch rate adapted learning_rate :              0.39810717055349726
      loss :              0.33886257935029457
      loss_factor :              0.03388625793502946
      loss adapted learning_rate :              4.0027754501610165e-148
    epoch : 2 ; learning_rate : 4.0027754501610165e-148 ; fit : 0.82
    batch_size :          6000
    best_batch_loss : 0.3300366138855181
    Epoch 2, Loss: 0.34165989003752834, fit: 0.8195
      batch rate adapted learning_rate :              0.39810717055349726
      loss :              0.337909907738203
      loss_factor :              0.0337909907738203
      loss adapted learning_rate :              3.0205955330880332e-148
    epoch : 3 ; learning_rate : 3.0205955330880332e-148 ; fit : 0.8195
    batch_size :          6000
    best_batch_loss : 0.3265866131520629
    Epoch 3, Loss: 0.3416598900375284, fit: 0.8135
      batch rate adapted learning_rate :              0.39810717055349726
      loss :              0.3505967290409764
      loss_factor :              0.03505967290409764
      loss adapted learning_rate :              1.2044422856488024e-146
    epoch : 4 ; learning_rate : 1.2044422856488024e-146 ; fit : 0.8135
    batch_size :          6000
    best_batch_loss : 0.3218975036979088
    Epoch 4, Loss: 0.34165989003752834, fit: 0.8035
      batch rate adapted learning_rate :              0.39810717055349726
      loss :              0.36841023185015054
      loss_factor :              0.03684102318501505
      loss adapted learning_rate :              1.7106782529910472e-144
    epoch : 5 ; learning_rate : 1.7106782529910472e-144 ; fit : 0.8035
    batch_size :          6000
    best_batch_loss : 0.32641648930596845
    Epoch 5, Loss: 0.34165989003752834, fit: 0.8166666666666667
      batch rate adapted learning_rate :              0.39810717055349726
      loss :              0.3435481178894255
      loss_factor :              0.03435481178894255
      loss adapted learning_rate :              1.5803648525176923e-147
    epoch : 6 ; learning_rate : 1.5803648525176923e-147 ; fit : 0.8166666666666667
    batch_size :          6000
    best_batch_loss : 0.32739671686642674
    Epoch 6, Loss: 0.34165989003752834, fit: 0.8251666666666667
      batch rate adapted learning_rate :              0.39810717055349726
      loss :              0.32739671686642674
      loss_factor :              0.03273967168664267
      loss adapted learning_rate :              1.2806545574634634e-149
    epoch : 7 ; learning_rate : 1.2806545574634634e-149 ; fit : 0.8251666666666667
    batch_size :          6000
    best_batch_loss : 0.31766022954458795
    Epoch 7, Loss: 0.3416598900375283, fit: 0.8196666666666667
      batch rate adapted learning_rate :              0.39810717055349726
      loss :              0.33521264910416815
      loss_factor :              0.03352126491041681
      loss adapted learning_rate :              1.355313668205559e-148
    epoch : 8 ; learning_rate : 1.355313668205559e-148 ; fit : 0.8196666666666667
    batch_size :          6000
    best_batch_loss : 0.3228765821763534
    Epoch 8, Loss: 0.3416598900375284, fit: 0.8273333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_0.3228765821763534_fit_0.8273333333333334_2024-01-03_184332
  self.fit : 0.8273333333333334
  self.loss : 0.3228765821763534
  current_accuracy : 0.8324
   Accuracy mean: 0.802
   Accuracy mean: 0.8324
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.5011872336272722
      loss_factor :              1
      loss adapted learning_rate :              0.5011872336272722
    epoch : 0 ; learning_rate : 0.5011872336272722 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 0.6128423762110181
    Epoch 0, Loss: 1.0372766559914202, fit: 0.6761666666666667
    Epoch 0, Loss: 1.0372766559914202
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_0.6128423762110181_fit_0.6761666666666667_2024-01-03_184334
  self.fit : 0.6761666666666667
  self.loss : 0.6128423762110181
  current_accuracy : 0.666
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.5011872336272722
      loss_factor :              1
      loss adapted learning_rate :              0.5011872336272722
    epoch : 0 ; learning_rate : 0.5011872336272722 ; fit : 0.6761666666666667
    batch_size :          6000
    best_batch_loss : 0.627766287477541
    Epoch 0, Loss: 0.9044774727163496, fit: 0.22866666666666666
    Epoch 0, Loss: 0.9044774727163496
      batch rate adapted learning_rate :              0.5011872336272722
      loss :              1.514446686806725
      loss_factor :              0.1514446686806725
      loss adapted learning_rate :              5.31372339216615e-83
    epoch : 1 ; learning_rate : 5.31372339216615e-83 ; fit : 0.22866666666666666
    batch_size :          6000
    best_batch_loss : 1.5460654923658492
    Epoch 1, Loss: 1.5624117501509427, fit: 0.20316666666666666
      batch rate adapted learning_rate :              0.5011872336272722
      loss :              1.5626012995981937
      loss_factor :              0.15626012995981936
      loss adapted learning_rate :              1.2156767677244989e-81
    epoch : 2 ; learning_rate : 1.2156767677244989e-81 ; fit : 0.20316666666666666
    batch_size :          6000
    best_batch_loss : 1.5531561363666797
    Epoch 2, Loss: 1.5624117501509425, fit: 0.20116666666666666
      batch rate adapted learning_rate :              0.5011872336272722
      loss :              1.5691100337539157
      loss_factor :              0.15691100337539157
      loss adapted learning_rate :              1.8422134684876315e-81
    epoch : 3 ; learning_rate : 1.8422134684876315e-81 ; fit : 0.20116666666666666
    batch_size :          6000
    best_batch_loss : 1.5523503709591095
    Epoch 3, Loss: 1.5624117501509427, fit: 0.19766666666666666
      batch rate adapted learning_rate :              0.5011872336272722
      loss :              1.5713115817738432
      loss_factor :              0.15713115817738432
      loss adapted learning_rate :              2.119488966677711e-81
    epoch : 4 ; learning_rate : 2.119488966677711e-81 ; fit : 0.19766666666666666
    batch_size :          6000
    best_batch_loss : 1.5552437564627544
    Epoch 4, Loss: 1.5624117501509427, fit: 0.20333333333333334
      batch rate adapted learning_rate :              0.5011872336272722
      loss :              1.561218637302993
      loss_factor :              0.1561218637302993
      loss adapted learning_rate :              1.112686221473589e-81
    epoch : 5 ; learning_rate : 1.112686221473589e-81 ; fit : 0.20333333333333334
    batch_size :          6000
    best_batch_loss : 1.5457739343965182
    Epoch 5, Loss: 1.5624117501509427, fit: 0.20366666666666666
      batch rate adapted learning_rate :              0.5011872336272722
      loss :              1.561888143122342
      loss_factor :              0.1561888143122342
      loss adapted learning_rate :              1.161429366201828e-81
    epoch : 6 ; learning_rate : 1.161429366201828e-81 ; fit : 0.20366666666666666
    batch_size :          6000
    best_batch_loss : 1.5459625175253249
    Epoch 6, Loss: 1.5624117501509427, fit: 0.2055
      batch rate adapted learning_rate :              0.5011872336272722
      loss :              1.5585773828255443
      loss_factor :              0.15585773828255442
      loss adapted learning_rate :              9.393706175719408e-82
    epoch : 7 ; learning_rate : 9.393706175719408e-82 ; fit : 0.2055
    batch_size :          6000
    best_batch_loss : 1.5467501774394075
    Epoch 7, Loss: 1.562411750150943, fit: 0.203
      batch rate adapted learning_rate :              0.5011872336272722
      loss :              1.5625434775258191
      loss_factor :              0.1562543477525819
      loss adapted learning_rate :              1.2111865403673509e-81
    epoch : 8 ; learning_rate : 1.2111865403673509e-81 ; fit : 0.203
    batch_size :          6000
    best_batch_loss : 1.5388743215196974
    Epoch 8, Loss: 1.5624117501509427, fit: 0.19816666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.5724010686135037_fit_0.19816666666666666_2024-01-03_184355
  self.fit : 0.19816666666666666
  self.loss : 1.5724010686135037
  current_accuracy : 0.1952
   Accuracy mean: 0.666
   Accuracy mean: 0.1952
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.6762433378062414
      loss_factor :              1
      loss adapted learning_rate :              0.6762433378062414
    epoch : 0 ; learning_rate : 0.6762433378062414 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 0.7244614686169375
    Epoch 0, Loss: 1.1442240569505109, fit: 0.34
    Epoch 0, Loss: 1.1442240569505109
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.2829828770541825_fit_0.34_2024-01-03_184357
  self.fit : 0.34
  self.loss : 1.2829828770541825
  current_accuracy : 0.2518
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.6762433378062414
      loss_factor :              1
      loss adapted learning_rate :              0.6762433378062414
    epoch : 0 ; learning_rate : 0.6762433378062414 ; fit : 0.34
    batch_size :          6000
    best_batch_loss : 0.9357398431533682
    Epoch 0, Loss: 1.6040226133059434, fit: 0.09766666666666667
    Epoch 0, Loss: 1.6040226133059434
      batch rate adapted learning_rate :              0.6762433378062414
      loss :              1.8046666666666666
      loss_factor :              0.18046666666666666
      loss adapted learning_rate :              2.949868600056669e-75
    epoch : 1 ; learning_rate : 2.949868600056669e-75 ; fit : 0.09766666666666667
    batch_size :          6000
    best_batch_loss : 1.7833333333333334
    Epoch 1, Loss: 1.8025666666666667, fit: 0.10116666666666667
      batch rate adapted learning_rate :              0.6762433378062414
      loss :              1.7976666666666667
      loss_factor :              0.17976666666666669
      loss adapted learning_rate :              1.999951926066056e-75
    epoch : 2 ; learning_rate : 1.999951926066056e-75 ; fit : 0.10116666666666667
    batch_size :          6000
    best_batch_loss : 1.7913333333333334
    Epoch 2, Loss: 1.8025666666666664, fit: 0.10033333333333333
      batch rate adapted learning_rate :              0.6762433378062414
      loss :              1.7993333333333332
      loss_factor :              0.17993333333333333
      loss adapted learning_rate :              2.1941461373849043e-75
    epoch : 3 ; learning_rate : 2.1941461373849043e-75 ; fit : 0.10033333333333333
    batch_size :          6000
    best_batch_loss : 1.7933333333333332
    Epoch 3, Loss: 1.8025666666666667, fit: 0.09133333333333334
      batch rate adapted learning_rate :              0.6762433378062414
      loss :              1.8173333333333332
      loss_factor :              0.18173333333333333
      loss adapted learning_rate :              5.936934313010381e-75
    epoch : 4 ; learning_rate : 5.936934313010381e-75 ; fit : 0.09133333333333334
    batch_size :          6000
    best_batch_loss : 1.7953333333333332
    Epoch 4, Loss: 1.8025666666666662, fit: 0.10233333333333333
      batch rate adapted learning_rate :              0.6762433378062414
      loss :              1.7953333333333332
      loss_factor :              0.17953333333333332
      loss adapted learning_rate :              1.7563555408628218e-75
    epoch : 5 ; learning_rate : 1.7563555408628218e-75 ; fit : 0.10233333333333333
    batch_size :          6000
    best_batch_loss : 1.7853333333333334
    Epoch 5, Loss: 1.8025666666666662, fit: 0.099
      batch rate adapted learning_rate :              0.6762433378062414
      loss :              1.802
      loss_factor :              0.1802
      loss adapted learning_rate :              2.54437818796534e-75
    epoch : 6 ; learning_rate : 2.54437818796534e-75 ; fit : 0.099
    batch_size :          6000
    best_batch_loss : 1.7936666666666667
    Epoch 6, Loss: 1.8025666666666667, fit: 0.09683333333333333
      batch rate adapted learning_rate :              0.6762433378062414
      loss :              1.8063333333333333
      loss_factor :              0.18063333333333334
      loss adapted learning_rate :              3.2351366435919445e-75
    epoch : 7 ; learning_rate : 3.2351366435919445e-75 ; fit : 0.09683333333333333
    batch_size :          6000
    best_batch_loss : 1.7963333333333333
    Epoch 7, Loss: 1.8025666666666664, fit: 0.10083333333333333
      batch rate adapted learning_rate :              0.6762433378062414
      loss :              1.7983333333333333
      loss_factor :              0.17983333333333335
      loss adapted learning_rate :              2.0754985270055585e-75
    epoch : 8 ; learning_rate : 2.0754985270055585e-75 ; fit : 0.10083333333333333
    batch_size :          6000
    best_batch_loss : 1.7946666666666666
    Epoch 8, Loss: 1.8025666666666667, fit: 0.0995
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.801_fit_0.0995_2024-01-03_184418
  self.fit : 0.0995
  self.loss : 1.801
  current_accuracy : 0.098
   Accuracy mean: 0.2518
   Accuracy mean: 0.098
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.6915028921812392
      loss_factor :              1
      loss adapted learning_rate :              0.6915028921812392
    epoch : 0 ; learning_rate : 0.6915028921812392 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.030951658636826
    Epoch 0, Loss: 1.409988630403829, fit: 0.10783333333333334
    Epoch 0, Loss: 1.409988630403829
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.782685231525204_fit_0.10783333333333334_2024-01-03_184420
  self.fit : 0.10783333333333334
  self.loss : 1.782685231525204
  current_accuracy : 0.1028
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.6915028921812392
      loss_factor :              1
      loss adapted learning_rate :              0.6915028921812392
    epoch : 0 ; learning_rate : 0.6915028921812392 ; fit : 0.10783333333333334
    batch_size :          6000
    best_batch_loss : 1.7778833092073651
    Epoch 0, Loss: 1.791208937575345, fit: 0.1025
    Epoch 0, Loss: 1.791208937575345
      batch rate adapted learning_rate :              0.6915028921812392
      loss :              1.7949378895394175
      loss_factor :              0.17949378895394175
      loss adapted learning_rate :              1.7568574461426339e-75
    epoch : 1 ; learning_rate : 1.7568574461426339e-75 ; fit : 0.1025
    batch_size :          6000
    best_batch_loss : 1.7753333333333334
    Epoch 1, Loss: 1.7911592209495477, fit: 0.10083333333333333
      batch rate adapted learning_rate :              0.6915028921812392
      loss :              1.7983333333010985
      loss_factor :              0.17983333333010984
      loss adapted learning_rate :              2.122332526373594e-75
    epoch : 2 ; learning_rate : 2.122332526373594e-75 ; fit : 0.10083333333333333
    batch_size :          6000
    best_batch_loss : 1.781
    Epoch 2, Loss: 1.7911592209495475, fit: 0.10783333333333334
      batch rate adapted learning_rate :              0.6915028921812392
      loss :              1.7843333333333333
      loss_factor :              0.17843333333333333
      loss adapted learning_rate :              9.713882977326262e-76
    epoch : 3 ; learning_rate : 9.713882977326262e-76 ; fit : 0.10783333333333334
    batch_size :          6000
    best_batch_loss : 1.783333333301099
    Epoch 3, Loss: 1.7911592209495477, fit: 0.10833333333333334
      batch rate adapted learning_rate :              0.6915028921812392
      loss :              1.783333333301099
      loss_factor :              0.1783333333301099
      loss adapted learning_rate :              9.184314147098759e-76
    epoch : 4 ; learning_rate : 9.184314147098759e-76 ; fit : 0.10833333333333334
    batch_size :          6000
    best_batch_loss : 1.7776666666666725
    Epoch 4, Loss: 1.791159220949548, fit: 0.10083333333333333
      batch rate adapted learning_rate :              0.6915028921812392
      loss :              1.798258875715824
      loss_factor :              0.1798258875715824
      loss adapted learning_rate :              2.113563277479956e-75
    epoch : 5 ; learning_rate : 2.113563277479956e-75 ; fit : 0.10083333333333333
    batch_size :          6000
    best_batch_loss : 1.783592209038234
    Epoch 5, Loss: 1.791159220949548, fit: 0.10466666666666667
      batch rate adapted learning_rate :              0.6915028921812392
      loss :              1.7906666666666666
      loss_factor :              0.17906666666666665
      loss adapted learning_rate :              1.3844233913613348e-75
    epoch : 6 ; learning_rate : 1.3844233913613348e-75 ; fit : 0.10466666666666667
    batch_size :          6000
    best_batch_loss : 1.783
    Epoch 6, Loss: 1.7911592209495482, fit: 0.10566666666666667
      batch rate adapted learning_rate :              0.6915028921812392
      loss :              1.7886666666666666
      loss_factor :              0.17886666666666667
      loss adapted learning_rate :              1.2380419553019878e-75
    epoch : 7 ; learning_rate : 1.2380419553019878e-75 ; fit : 0.10566666666666667
    batch_size :          6000
    best_batch_loss : 1.7783333333333333
    Epoch 7, Loss: 1.7911592209495477, fit: 0.104
      batch rate adapted learning_rate :              0.6915028921812392
      loss :              1.7919999999999994
      loss_factor :              0.17919999999999994
      loss adapted learning_rate :              1.4914013555098373e-75
    epoch : 8 ; learning_rate : 1.4914013555098373e-75 ; fit : 0.104
    batch_size :          6000
    best_batch_loss : 1.776
    Epoch 8, Loss: 1.7911592209495477, fit: 0.107
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7859255423824876_fit_0.107_2024-01-03_184442
  self.fit : 0.107
  self.loss : 1.7859255423824876
  current_accuracy : 0.1028
   Accuracy mean: 0.1028
   Accuracy mean: 0.1028
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.7042261140012369
      loss_factor :              1
      loss adapted learning_rate :              0.7042261140012369
    epoch : 0 ; learning_rate : 0.7042261140012369 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 1.0969076018822321
    Epoch 0, Loss: 1.5083273715452163, fit: 0.0945
    Epoch 0, Loss: 1.5083273715452163
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.811_fit_0.0945_2024-01-03_184445
  self.fit : 0.0945
  self.loss : 1.811
  current_accuracy : 0.0982
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.7042261140012369
      loss_factor :              1
      loss adapted learning_rate :              0.7042261140012369
    epoch : 0 ; learning_rate : 0.7042261140012369 ; fit : 0.0945
    batch_size :          6000
    best_batch_loss : 1.7926666666666666
    Epoch 0, Loss: 1.8052666666123993, fit: 0.09833333333333333
    Epoch 0, Loss: 1.8052666666123993
      batch rate adapted learning_rate :              0.7042261140012369
      loss :              1.8033333333333332
      loss_factor :              0.18033333333333332
      loss adapted learning_rate :              2.853074841486838e-75
    epoch : 1 ; learning_rate : 2.853074841486838e-75 ; fit : 0.09833333333333333
    batch_size :          6000
    best_batch_loss : 1.7973333333333332
    Epoch 1, Loss: 1.8052666666666666, fit: 0.10133333333333333
      batch rate adapted learning_rate :              0.7042261140012369
      loss :              1.7973333333333332
      loss_factor :              0.17973333333333333
      loss adapted learning_rate :              2.0444429976483733e-75
    epoch : 2 ; learning_rate : 2.0444429976483733e-75 ; fit : 0.10133333333333333
    batch_size :          6000
    best_batch_loss : 1.7903333333333333
    Epoch 2, Loss: 1.8052666666666668, fit: 0.09333333333333334
      batch rate adapted learning_rate :              0.7042261140012369
      loss :              1.8133333333333332
      loss_factor :              0.18133333333333332
      loss adapted learning_rate :              4.959942289020439e-75
    epoch : 3 ; learning_rate : 4.959942289020439e-75 ; fit : 0.09333333333333334
    batch_size :          6000
    best_batch_loss : 1.7973333333333332
    Epoch 3, Loss: 1.8052666666666668, fit: 0.09283333333333334
      batch rate adapted learning_rate :              0.7042261140012369
      loss :              1.8143333333333334
      loss_factor :              0.18143333333333334
      loss adapted learning_rate :              5.24107150865074e-75
    epoch : 4 ; learning_rate : 5.24107150865074e-75 ; fit : 0.09283333333333334
    batch_size :          6000
    best_batch_loss : 1.79
    Epoch 4, Loss: 1.8052666666666668, fit: 0.08966666666666667
      batch rate adapted learning_rate :              0.7042261140012369
      loss :              1.8206666666666667
      loss_factor :              0.18206666666666665
      loss adapted learning_rate :              7.426022040417952e-75
    epoch : 5 ; learning_rate : 7.426022040417952e-75 ; fit : 0.08966666666666667
    batch_size :          6000
    best_batch_loss : 1.786
    Epoch 5, Loss: 1.8052666666666666, fit: 0.09383333333333334
      batch rate adapted learning_rate :              0.7042261140012369
      loss :              1.8123333333333334
      loss_factor :              0.18123333333333333
      loss adapted learning_rate :              4.6937499927431895e-75
    epoch : 6 ; learning_rate : 4.6937499927431895e-75 ; fit : 0.09383333333333334
    batch_size :          6000
    best_batch_loss : 1.7946666666666666
    Epoch 6, Loss: 1.8052666666666666, fit: 0.10266666666666667
      batch rate adapted learning_rate :              0.7042261140012369
      loss :              1.7946666666666666
      loss_factor :              0.17946666666666666
      loss adapted learning_rate :              1.762348426484747e-75
    epoch : 7 ; learning_rate : 1.762348426484747e-75 ; fit : 0.10266666666666667
    batch_size :          6000
    best_batch_loss : 1.7906666666666666
    Epoch 7, Loss: 1.8052666666666666, fit: 0.10166666666666667
      batch rate adapted learning_rate :              0.7042261140012369
      loss :              1.7966666666666666
      loss_factor :              0.17966666666666667
      loss adapted learning_rate :              1.9699861469034573e-75
    epoch : 8 ; learning_rate : 1.9699861469034573e-75 ; fit : 0.10166666666666667
    batch_size :          6000
    best_batch_loss : 1.798
    Epoch 8, Loss: 1.8052666666666666, fit: 0.09683333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.8063333333333333_fit_0.09683333333333333_2024-01-03_184506
  self.fit : 0.09683333333333333
  self.loss : 1.8063333333333333
  current_accuracy : 0.0982
   Accuracy mean: 0.0982
   Accuracy mean: 0.0982
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.7943282347242815
      loss_factor :              1
      loss adapted learning_rate :              0.7943282347242815
    epoch : 0 ; learning_rate : 0.7943282347242815 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 0.9974835015787016
    Epoch 0, Loss: 1.4723447852849059, fit: 0.10266666666666667
    Epoch 0, Loss: 1.4723447852849059
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7946666666666624_fit_0.10266666666666667_2024-01-03_184509
  self.fit : 0.10266666666666667
  self.loss : 1.7946666666666624
  current_accuracy : 0.1009
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.7943282347242815
      loss_factor :              1
      loss adapted learning_rate :              0.7943282347242815
    epoch : 0 ; learning_rate : 0.7943282347242815 ; fit : 0.10266666666666667
    batch_size :          6000
    best_batch_loss : 1.7846666666666666
    Epoch 0, Loss: 1.8016987094934773, fit: 0.09633333333333334
    Epoch 0, Loss: 1.8016987094934773
      batch rate adapted learning_rate :              0.7943282347242815
      loss :              1.8073333333333332
      loss_factor :              0.18073333333333333
      loss adapted learning_rate :              4.0162972925777725e-75
    epoch : 1 ; learning_rate : 4.0162972925777725e-75 ; fit : 0.09633333333333334
    batch_size :          6000
    best_batch_loss : 1.7806666666666666
    Epoch 1, Loss: 1.8017, fit: 0.09966666666666667
      batch rate adapted learning_rate :              0.7943282347242815
      loss :              1.8006666666666666
      loss_factor :              0.18006666666666665
      loss adapted learning_rate :              2.7754442822102036e-75
    epoch : 2 ; learning_rate : 2.7754442822102036e-75 ; fit : 0.09966666666666667
    batch_size :          6000
    best_batch_loss : 1.789
    Epoch 2, Loss: 1.8017, fit: 0.10066666666666667
      batch rate adapted learning_rate :              0.7943282347242815
      loss :              1.7986666666666666
      loss_factor :              0.17986666666666667
      loss adapted learning_rate :              2.4835255256041905e-75
    epoch : 3 ; learning_rate : 2.4835255256041905e-75 ; fit : 0.10066666666666667
    batch_size :          6000
    best_batch_loss : 1.7916666666666667
    Epoch 3, Loss: 1.8016999999999999, fit: 0.09683333333333333
      batch rate adapted learning_rate :              0.7943282347242815
      loss :              1.8063333333333333
      loss_factor :              0.18063333333333334
      loss adapted learning_rate :              3.8000527850412914e-75
    epoch : 4 ; learning_rate : 3.8000527850412914e-75 ; fit : 0.09683333333333333
    batch_size :          6000
    best_batch_loss : 1.783
    Epoch 4, Loss: 1.8016999999999996, fit: 0.1025
      batch rate adapted learning_rate :              0.7943282347242815
      loss :              1.795
      loss_factor :              0.1795
      loss adapted learning_rate :              2.0250944873175517e-75
    epoch : 5 ; learning_rate : 2.0250944873175517e-75 ; fit : 0.1025
    batch_size :          6000
    best_batch_loss : 1.7883333333333333
    Epoch 5, Loss: 1.8016999999999999, fit: 0.093
      batch rate adapted learning_rate :              0.7943282347242815
      loss :              1.814
      loss_factor :              0.1814
      loss adapted learning_rate :              5.804011608719703e-75
    epoch : 6 ; learning_rate : 5.804011608719703e-75 ; fit : 0.093
    batch_size :          6000
    best_batch_loss : 1.7883333333333333
    Epoch 6, Loss: 1.8017, fit: 0.09833333333333333
      batch rate adapted learning_rate :              0.7943282347242815
      loss :              1.8033333333333332
      loss_factor :              0.18033333333333332
      loss adapted learning_rate :              3.218111139756057e-75
    epoch : 7 ; learning_rate : 3.218111139756057e-75 ; fit : 0.09833333333333333
    batch_size :          6000
    best_batch_loss : 1.791
    Epoch 7, Loss: 1.8017, fit: 0.103
      batch rate adapted learning_rate :              0.7943282347242815
      loss :              1.794
      loss_factor :              0.1794
      loss adapted learning_rate :              1.915331145297823e-75
    epoch : 8 ; learning_rate : 1.915331145297823e-75 ; fit : 0.103
    batch_size :          6000
    best_batch_loss : 1.795
    Epoch 8, Loss: 1.8016999999999999, fit: 0.10016666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7996666666666667_fit_0.10016666666666667_2024-01-03_184537
  self.fit : 0.10016666666666667
  self.loss : 1.7996666666666667
  current_accuracy : 0.1009
   Accuracy mean: 0.1009
   Accuracy mean: 0.1009
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.9895192582062144
      loss_factor :              1
      loss adapted learning_rate :              0.9895192582062144
    epoch : 0 ; learning_rate : 0.9895192582062144 ; fit : 0.0
    batch_size :          6000
    best_batch_loss : 0.8931356335603433
    Epoch 0, Loss: 1.3986871236974157, fit: 0.1015
    Epoch 0, Loss: 1.3986871236974157
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.797_fit_0.1015_2024-01-03_184539
  self.fit : 0.1015
  self.loss : 1.797
  current_accuracy : 0.1028
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.1
      batch rate adapted learning_rate :              0.9895192582062144
      loss_factor :              1
      loss adapted learning_rate :              0.9895192582062144
    epoch : 0 ; learning_rate : 0.9895192582062144 ; fit : 0.1015
    batch_size :          6000
    best_batch_loss : 1.7796666666666667
    Epoch 0, Loss: 1.7911666666666668, fit: 0.10616666666666667
    Epoch 0, Loss: 1.7911666666666668
      batch rate adapted learning_rate :              0.9895192582062144
      loss :              1.7876666666666667
      loss_factor :              0.17876666666666668
      loss adapted learning_rate :              1.6752455588842755e-75
    epoch : 1 ; learning_rate : 1.6752455588842755e-75 ; fit : 0.10616666666666667
    batch_size :          6000
    best_batch_loss : 1.7783333333333333
    Epoch 1, Loss: 1.7911666666666668, fit: 0.10616666666666667
      batch rate adapted learning_rate :              0.9895192582062144
      loss :              1.7876666666666667
      loss_factor :              0.17876666666666668
      loss adapted learning_rate :              1.6752455588842755e-75
    epoch : 2 ; learning_rate : 1.6752455588842755e-75 ; fit : 0.10616666666666667
    batch_size :          6000
    best_batch_loss : 1.779
    Epoch 2, Loss: 1.7911666666666666, fit: 0.10883333333333334
      batch rate adapted learning_rate :              0.9895192582062144
      loss :              1.7823333333333333
      loss_factor :              0.17823333333333333
      loss adapted learning_rate :              1.2425594854192166e-75
    epoch : 3 ; learning_rate : 1.2425594854192166e-75 ; fit : 0.10883333333333334
    batch_size :          6000
    best_batch_loss : 1.776
    Epoch 3, Loss: 1.7911666666666664, fit: 0.10266666666666667
      batch rate adapted learning_rate :              0.9895192582062144
      loss :              1.7946666666666666
      loss_factor :              0.17946666666666666
      loss adapted learning_rate :              2.476303665832269e-75
    epoch : 4 ; learning_rate : 2.476303665832269e-75 ; fit : 0.10266666666666667
    batch_size :          6000
    best_batch_loss : 1.78
    Epoch 4, Loss: 1.7911666666666668, fit: 0.105
      batch rate adapted learning_rate :              0.9895192582062144
      loss :              1.79
      loss_factor :              0.179
      loss adapted learning_rate :              1.9086546246243601e-75
    epoch : 5 ; learning_rate : 1.9086546246243601e-75 ; fit : 0.105
    batch_size :          6000
    best_batch_loss : 1.775
    Epoch 5, Loss: 1.7911666666666668, fit: 0.1125
      batch rate adapted learning_rate :              0.9895192582062144
      loss :              1.775
      loss_factor :              0.1775
      loss adapted learning_rate :              8.227350322400494e-76
    epoch : 6 ; learning_rate : 8.227350322400494e-76 ; fit : 0.1125
    batch_size :          6000
    best_batch_loss : 1.7803333333333333
    Epoch 6, Loss: 1.7911666666666668, fit: 0.09766666666666667
      batch rate adapted learning_rate :              0.9895192582062144
      loss :              1.8046666666666666
      loss_factor :              0.18046666666666666
      loss adapted learning_rate :              4.316422248835852e-75
    epoch : 7 ; learning_rate : 4.316422248835852e-75 ; fit : 0.09766666666666667
    batch_size :          6000
    best_batch_loss : 1.777
    Epoch 7, Loss: 1.7911666666666666, fit: 0.1115
      batch rate adapted learning_rate :              0.9895192582062144
      loss :              1.777
      loss_factor :              0.1777
      loss adapted learning_rate :              9.208036485934044e-76
    epoch : 8 ; learning_rate : 9.208036485934044e-76 ; fit : 0.1115
    batch_size :          6000
    best_batch_loss : 1.7836666666666667
    Epoch 8, Loss: 1.791166666666667, fit: 0.10633333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7873333333333334_fit_0.10633333333333334_2024-01-03_184600
  self.fit : 0.10633333333333334
  self.loss : 1.7873333333333334
  current_accuracy : 0.1028
   Accuracy mean: 0.1028
   Accuracy mean: 0.1028
  Error saving file: doc/out/test_combinations_results/20240103184600.
  normalized_accuracies :      [1.06209150e-02 1.06209150e-02 3.67647059e-02 4.11220044e-02
   1.87091503e-01 3.26797386e-01 6.07979303e-01 7.50953159e-01
   7.62935730e-01 8.86574074e-01 9.58605664e-01 1.00000000e+00
   7.73420479e-01 1.32352941e-01 2.09422658e-01 0.00000000e+00
   6.53594771e-03 6.53594771e-03 2.72331155e-04 2.72331155e-04
   3.94880174e-03 3.94880174e-03 6.53594771e-03 6.53594771e-03]
batch_rate :  0.01
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.2290867652767773
      loss_factor :              1
      loss adapted learning_rate :              0.2290867652767773
    epoch : 0 ; learning_rate : 0.2290867652767773 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 0.21709666994773394
    Epoch 0, Loss: 0.4391710278811507, fit: 0.87
    Epoch 0, Loss: 0.4391710278811507
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_0.24881707548783163_fit_0.87_2024-01-03_184605
  self.fit : 0.87
  self.loss : 0.24881707548783163
  current_accuracy : 0.8684
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-64
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.2290867652767773
      loss_factor :              1
      loss adapted learning_rate :              0.2290867652767773
    epoch : 0 ; learning_rate : 0.2290867652767773 ; fit : 0.87
    batch_size :          600
    best_batch_loss : 0.14889860453181594
    Epoch 0, Loss: 0.23296762686344585, fit: 0.8483333333333334
    Epoch 0, Loss: 0.23296762686344585
      batch rate adapted learning_rate :              0.2290867652767773
      loss :              0.28076374872769433
      loss_factor :              0.028076374872769434
      loss adapted learning_rate :              1.563519288259841e-156
    epoch : 1 ; learning_rate : 1.563519288259841e-156 ; fit : 0.8483333333333334
    batch_size :          600
    best_batch_loss : 0.14032781859574187
    Epoch 1, Loss: 0.2047810767124667, fit: 0.8766666666666667
      batch rate adapted learning_rate :              0.2290867652767773
      loss :              0.23055736051278075
      loss_factor :              0.023055736051278076
      loss adapted learning_rate :              4.344093427665823e-165
    epoch : 2 ; learning_rate : 4.344093427665823e-165 ; fit : 0.8766666666666667
    batch_size :          600
    best_batch_loss : 0.1667462065916481
    Epoch 2, Loss: 0.20478107671246676, fit: 0.895
      batch rate adapted learning_rate :              0.2290867652767773
      loss :              0.19300091588242058
      loss_factor :              0.019300091588242058
      loss adapted learning_rate :              8.240185622540479e-173
    epoch : 3 ; learning_rate : 8.240185622540479e-173 ; fit : 0.895
    batch_size :          600
    best_batch_loss : 0.1562188657231738
    Epoch 3, Loss: 0.20478107671246665, fit: 0.8983333333333333
      batch rate adapted learning_rate :              0.2290867652767773
      loss :              0.18550977936670654
      loss_factor :              0.018550977936670654
      loss adapted learning_rate :              1.5728267614304226e-174
    epoch : 4 ; learning_rate : 1.5728267614304226e-174 ; fit : 0.8983333333333333
    batch_size :          600
    best_batch_loss : 0.14345807870805108
    Epoch 4, Loss: 0.20478107671246662, fit: 0.9166666666666666
      batch rate adapted learning_rate :              0.2290867652767773
      loss :              0.1616167894417428
      loss_factor :              0.01616167894417428
      loss adapted learning_rate :              1.6167672920515605e-180
    epoch : 5 ; learning_rate : 1.6167672920515605e-180 ; fit : 0.9166666666666666
    batch_size :          600
    best_batch_loss : 0.1479633362891562
    Epoch 5, Loss: 0.20478107671246668, fit: 0.895
      batch rate adapted learning_rate :              0.2290867652767773
      loss :              0.19173933890264935
      loss_factor :              0.019173933890264935
      loss adapted learning_rate :              4.276835669882947e-173
    epoch : 6 ; learning_rate : 4.276835669882947e-173 ; fit : 0.895
    batch_size :          600
    best_batch_loss : 0.13598242190426987
    Epoch 6, Loss: 0.20478107671246673, fit: 0.9083333333333333
      batch rate adapted learning_rate :              0.2290867652767773
      loss :              0.16895182266267714
      loss_factor :              0.016895182266267714
      loss adapted learning_rate :              1.3686365479690209e-178
    epoch : 7 ; learning_rate : 1.3686365479690209e-178 ; fit : 0.9083333333333333
    batch_size :          600
    best_batch_loss : 0.13908908738634765
    Epoch 7, Loss: 0.20478107671246668, fit: 0.88
      batch rate adapted learning_rate :              0.2290867652767773
      loss :              0.21656175204606182
      loss_factor :              0.021656175204606182
      loss adapted learning_rate :              8.282770618073345e-168
    epoch : 8 ; learning_rate : 8.282770618073345e-168 ; fit : 0.88
    batch_size :          600
    best_batch_loss : 0.1359957383407098
    Epoch 8, Loss: 0.20478107671246662, fit: 0.8583333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_0.2546567001680152_fit_0.8583333333333333_2024-01-03_184627
  self.fit : 0.8583333333333333
  self.loss : 0.2546567001680152
  current_accuracy : 0.8964
   Accuracy mean: 0.8684
   Accuracy mean: 0.8964
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.4677351412871982
      loss_factor :              1
      loss adapted learning_rate :              0.4677351412871982
    epoch : 0 ; learning_rate : 0.4677351412871982 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 0.4993844902963962
    Epoch 0, Loss: 1.0793415474912897, fit: 0.24
    Epoch 0, Loss: 1.0793415474912897
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.5062652361543478_fit_0.24_2024-01-03_184630
  self.fit : 0.24
  self.loss : 1.5062652361543478
  current_accuracy : 0.2683
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-33
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.4677351412871982
      loss_factor :              1
      loss adapted learning_rate :              0.4677351412871982
    epoch : 0 ; learning_rate : 0.4677351412871982 ; fit : 0.24
    batch_size :          600
    best_batch_loss : 1.1841170551299625
    Epoch 0, Loss: 1.5404724760747737, fit: 0.30833333333333335
    Epoch 0, Loss: 1.5404724760747737
      batch rate adapted learning_rate :              0.4677351412871982
      loss :              1.3707254881550515
      loss_factor :              0.13707254881550515
      loss adapted learning_rate :              2.317655276143454e-87
    epoch : 1 ; learning_rate : 2.317655276143454e-87 ; fit : 0.30833333333333335
    batch_size :          600
    best_batch_loss : 1.271806088946682
    Epoch 1, Loss: 1.3396207939815663, fit: 0.315
      batch rate adapted learning_rate :              0.4677351412871982
      loss :              1.355046508245767
      loss_factor :              0.1355046508245767
      loss adapted learning_rate :              7.335338216669644e-88
    epoch : 2 ; learning_rate : 7.335338216669644e-88 ; fit : 0.315
    batch_size :          600
    best_batch_loss : 1.2566854297527337
    Epoch 2, Loss: 1.3396207939815674, fit: 0.35333333333333333
      batch rate adapted learning_rate :              0.4677351412871982
      loss :              1.2865578327855738
      loss_factor :              0.12865578327855737
      loss adapted learning_rate :              4.101392378954224e-90
    epoch : 3 ; learning_rate : 4.101392378954224e-90 ; fit : 0.35333333333333333
    batch_size :          600
    best_batch_loss : 1.268286626894612
    Epoch 3, Loss: 1.3396207939815667, fit: 0.3333333333333333
      batch rate adapted learning_rate :              0.4677351412871982
      loss :              1.3168271854893503
      loss_factor :              0.13168271854893504
      loss adapted learning_rate :              4.196414544344913e-89
    epoch : 4 ; learning_rate : 4.196414544344913e-89 ; fit : 0.3333333333333333
    batch_size :          600
    best_batch_loss : 1.2286719089073173
    Epoch 4, Loss: 1.3396207939815667, fit: 0.38
      batch rate adapted learning_rate :              0.4677351412871982
      loss :              1.2286719089073173
      loss_factor :              0.12286719089073173
      loss adapted learning_rate :              4.1076450893379164e-92
    epoch : 5 ; learning_rate : 4.1076450893379164e-92 ; fit : 0.38
    batch_size :          600
    best_batch_loss : 1.2323925742520705
    Epoch 5, Loss: 1.339620793981567, fit: 0.3283333333333333
      batch rate adapted learning_rate :              0.4677351412871982
      loss :              1.3357660420154083
      loss_factor :              0.13357660420154083
      loss adapted learning_rate :              1.750007294365659e-88
    epoch : 6 ; learning_rate : 1.750007294365659e-88 ; fit : 0.3283333333333333
    batch_size :          600
    best_batch_loss : 1.2554337671477969
    Epoch 6, Loss: 1.3396207939815667, fit: 0.305
      batch rate adapted learning_rate :              0.4677351412871982
      loss :              1.3786719948905122
      loss_factor :              0.1378671994890512
      loss adapted learning_rate :              4.131382265784198e-87
    epoch : 7 ; learning_rate : 4.131382265784198e-87 ; fit : 0.305
    batch_size :          600
    best_batch_loss : 1.2399839615213262
    Epoch 7, Loss: 1.3396207939815672, fit: 0.325
      batch rate adapted learning_rate :              0.4677351412871982
      loss :              1.340355265788275
      loss_factor :              0.1340355265788275
      loss adapted learning_rate :              2.465996678976528e-88
    epoch : 8 ; learning_rate : 2.465996678976528e-88 ; fit : 0.325
    batch_size :          600
    best_batch_loss : 1.2551532022716259
    Epoch 8, Loss: 1.3396207939815663, fit: 0.315
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.3548539332918677_fit_0.315_2024-01-03_184652
  self.fit : 0.315
  self.loss : 1.3548539332918677
  current_accuracy : 0.3365
   Accuracy mean: 0.2683
   Accuracy mean: 0.3365
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.6760829753919818
      loss_factor :              1
      loss adapted learning_rate :              0.6760829753919818
    epoch : 0 ; learning_rate : 0.6760829753919818 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 0.8108125757372968
    Epoch 0, Loss: 1.7326744737769302, fit: 0.08666666666666667
    Epoch 0, Loss: 1.7326744737769302
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.8266666666666667_fit_0.08666666666666667_2024-01-03_184655
  self.fit : 0.08666666666666667
  self.loss : 1.8266666666666667
  current_accuracy : 0.0974
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.6760829753919818
      loss_factor :              1
      loss adapted learning_rate :              0.6760829753919818
    epoch : 0 ; learning_rate : 0.6760829753919818 ; fit : 0.08666666666666667
    batch_size :          600
    best_batch_loss : 1.7566666666666666
    Epoch 0, Loss: 1.8049666666666675, fit: 0.10666666666666667
    Epoch 0, Loss: 1.8049666666666675
      batch rate adapted learning_rate :              0.6760829753919818
      loss :              1.7866666666666666
      loss_factor :              0.17866666666666667
      loss adapted learning_rate :              1.082314551059602e-75
    epoch : 1 ; learning_rate : 1.082314551059602e-75 ; fit : 0.10666666666666667
    batch_size :          600
    best_batch_loss : 1.74
    Epoch 1, Loss: 1.8049666666666673, fit: 0.11833333333333333
      batch rate adapted learning_rate :              0.6760829753919818
      loss :              1.7633333333333334
      loss_factor :              0.17633333333333334
      loss adapted learning_rate :              2.906977909695042e-76
    epoch : 2 ; learning_rate : 2.906977909695042e-76 ; fit : 0.11833333333333333
    batch_size :          600
    best_batch_loss : 1.7533333333333334
    Epoch 2, Loss: 1.8049666666666675, fit: 0.105
      batch rate adapted learning_rate :              0.6760829753919818
      loss :              1.79
      loss_factor :              0.179
      loss adapted learning_rate :              1.30407658760572e-75
    epoch : 3 ; learning_rate : 1.30407658760572e-75 ; fit : 0.105
    batch_size :          600
    best_batch_loss : 1.7433333333333334
    Epoch 3, Loss: 1.804966666666667, fit: 0.09
      batch rate adapted learning_rate :              0.6760829753919818
      loss :              1.82
      loss_factor :              0.182
      loss adapted learning_rate :              6.872880590021905e-75
    epoch : 4 ; learning_rate : 6.872880590021905e-75 ; fit : 0.09
    batch_size :          600
    best_batch_loss : 1.7233333333333334
    Epoch 4, Loss: 1.8049666666666673, fit: 0.10333333333333333
      batch rate adapted learning_rate :              0.6760829753919818
      loss :              1.7933333333333332
      loss_factor :              0.17933333333333332
      loss adapted learning_rate :              1.5707320112414308e-75
    epoch : 5 ; learning_rate : 1.5707320112414308e-75 ; fit : 0.10333333333333333
    batch_size :          600
    best_batch_loss : 1.7533333333333334
    Epoch 5, Loss: 1.8049666666666673, fit: 0.09333333333333334
      batch rate adapted learning_rate :              0.6760829753919818
      loss :              1.8133333333333332
      loss_factor :              0.18133333333333332
      loss adapted learning_rate :              4.761727055931875e-75
    epoch : 6 ; learning_rate : 4.761727055931875e-75 ; fit : 0.09333333333333334
    batch_size :          600
    best_batch_loss : 1.7466666666666666
    Epoch 6, Loss: 1.804966666666667, fit: 0.11166666666666666
      batch rate adapted learning_rate :              0.6760829753919818
      loss :              1.7766666666666666
      loss_factor :              0.17766666666666667
      loss adapted learning_rate :              6.174409517826702e-76
    epoch : 7 ; learning_rate : 6.174409517826702e-76 ; fit : 0.11166666666666666
    batch_size :          600
    best_batch_loss : 1.7566666666666666
    Epoch 7, Loss: 1.8049666666666673, fit: 0.08166666666666667
      batch rate adapted learning_rate :              0.6760829753919818
      loss :              1.8366666666666667
      loss_factor :              0.18366666666666667
      loss adapted learning_rate :              1.710150820960217e-74
    epoch : 8 ; learning_rate : 1.710150820960217e-74 ; fit : 0.08166666666666667
    batch_size :          600
    best_batch_loss : 1.7366666666666666
    Epoch 8, Loss: 1.8049666666666673, fit: 0.09166666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.8166666666666667_fit_0.09166666666666666_2024-01-03_184719
  self.fit : 0.09166666666666666
  self.loss : 1.8166666666666667
  current_accuracy : 0.0974
   Accuracy mean: 0.0974
   Accuracy mean: 0.0974
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.7943282347242815
      loss_factor :              1
      loss adapted learning_rate :              0.7943282347242815
    epoch : 0 ; learning_rate : 0.7943282347242815 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.105688920179602
    Epoch 0, Loss: 1.7704449678661518, fit: 0.09666666666666666
    Epoch 0, Loss: 1.7704449678661518
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.8066666666666666_fit_0.09666666666666666_2024-01-03_184722
  self.fit : 0.09666666666666666
  self.loss : 1.8066666666666666
  current_accuracy : 0.1028
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.7943282347242815
      loss_factor :              1
      loss adapted learning_rate :              0.7943282347242815
    epoch : 0 ; learning_rate : 0.7943282347242815 ; fit : 0.09666666666666666
    batch_size :          600
    best_batch_loss : 1.7266666666666666
    Epoch 0, Loss: 1.7911666666666681, fit: 0.12
    Epoch 0, Loss: 1.7911666666666681
      batch rate adapted learning_rate :              0.7943282347242815
      loss :              1.76
      loss_factor :              0.176
      loss adapted learning_rate :              2.8266157947955457e-76
    epoch : 1 ; learning_rate : 2.8266157947955457e-76 ; fit : 0.12
    batch_size :          600
    best_batch_loss : 1.7266666666666666
    Epoch 1, Loss: 1.791166666666668, fit: 0.08666666666666667
      batch rate adapted learning_rate :              0.7943282347242815
      loss :              1.8266666666666667
      loss_factor :              0.18266666666666667
      loss adapted learning_rate :              1.1639394185515033e-74
    epoch : 2 ; learning_rate : 1.1639394185515033e-74 ; fit : 0.08666666666666667
    batch_size :          600
    best_batch_loss : 1.7333333333333334
    Epoch 2, Loss: 1.7911666666666681, fit: 0.085
      batch rate adapted learning_rate :              0.7943282347242815
      loss :              1.83
      loss_factor :              0.183
      loss adapted learning_rate :              1.3967187813539247e-74
    epoch : 3 ; learning_rate : 1.3967187813539247e-74 ; fit : 0.085
    batch_size :          600
    best_batch_loss : 1.7233333333333334
    Epoch 3, Loss: 1.7911666666666681, fit: 0.115
      batch rate adapted learning_rate :              0.7943282347242815
      loss :              1.77
      loss_factor :              0.177
      loss adapted learning_rate :              4.981115776352936e-76
    epoch : 4 ; learning_rate : 4.981115776352936e-76 ; fit : 0.115
    batch_size :          600
    best_batch_loss : 1.7366666666666666
    Epoch 4, Loss: 1.7911666666666675, fit: 0.10833333333333334
      batch rate adapted learning_rate :              0.7943282347242815
      loss :              1.7833333333333334
      loss_factor :              0.17833333333333334
      loss adapted learning_rate :              1.0550006571617916e-75
    epoch : 5 ; learning_rate : 1.0550006571617916e-75 ; fit : 0.10833333333333334
    batch_size :          600
    best_batch_loss : 1.71
    Epoch 5, Loss: 1.7911666666666681, fit: 0.08833333333333333
      batch rate adapted learning_rate :              0.7943282347242815
      loss :              1.8233333333333333
      loss_factor :              0.18233333333333332
      loss adapted learning_rate :              9.69632494846745e-75
    epoch : 6 ; learning_rate : 9.69632494846745e-75 ; fit : 0.08833333333333333
    batch_size :          600
    best_batch_loss : 1.73
    Epoch 6, Loss: 1.791166666666668, fit: 0.10666666666666667
      batch rate adapted learning_rate :              0.7943282347242815
      loss :              1.7866666666666666
      loss_factor :              0.17866666666666667
      loss adapted learning_rate :              1.271608719715401e-75
    epoch : 7 ; learning_rate : 1.271608719715401e-75 ; fit : 0.10666666666666667
    batch_size :          600
    best_batch_loss : 1.7266666666666666
    Epoch 7, Loss: 1.7911666666666681, fit: 0.12833333333333333
      batch rate adapted learning_rate :              0.7943282347242815
      loss :              1.7433333333333334
      loss_factor :              0.17433333333333334
      loss adapted learning_rate :              1.0915494490256704e-76
    epoch : 8 ; learning_rate : 1.0915494490256704e-76 ; fit : 0.12833333333333333
    batch_size :          600
    best_batch_loss : 1.7466666666666666
    Epoch 8, Loss: 1.791166666666668, fit: 0.08833333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.8233333333333333_fit_0.08833333333333333_2024-01-03_184746
  self.fit : 0.08833333333333333
  self.loss : 1.8233333333333333
  current_accuracy : 0.1028
   Accuracy mean: 0.1028
   Accuracy mean: 0.1028
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.8511380382023764
      loss_factor :              1
      loss adapted learning_rate :              0.8511380382023764
    epoch : 0 ; learning_rate : 0.8511380382023764 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.2210006686046229
    Epoch 0, Loss: 1.7777046381979922, fit: 0.11
    Epoch 0, Loss: 1.7777046381979922
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.78_fit_0.11_2024-01-03_184749
  self.fit : 0.11
  self.loss : 1.78
  current_accuracy : 0.1009
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.8511380382023764
      loss_factor :              1
      loss adapted learning_rate :              0.8511380382023764
    epoch : 0 ; learning_rate : 0.8511380382023764 ; fit : 0.11
    batch_size :          600
    best_batch_loss : 1.7366666666666666
    Epoch 0, Loss: 1.8017000000000007, fit: 0.095
    Epoch 0, Loss: 1.8017000000000007
      batch rate adapted learning_rate :              0.8511380382023764
      loss :              1.81
      loss_factor :              0.181
      loss adapted learning_rate :              4.987208061225697e-75
    epoch : 1 ; learning_rate : 4.987208061225697e-75 ; fit : 0.095
    batch_size :          600
    best_batch_loss : 1.75
    Epoch 1, Loss: 1.8017000000000005, fit: 0.10166666666666667
      batch rate adapted learning_rate :              0.8511380382023764
      loss :              1.7966666666666666
      loss_factor :              0.17966666666666667
      loss adapted learning_rate :              2.380954229082056e-75
    epoch : 2 ; learning_rate : 2.380954229082056e-75 ; fit : 0.10166666666666667
    batch_size :          600
    best_batch_loss : 1.73
    Epoch 2, Loss: 1.8017000000000012, fit: 0.10166666666666667
      batch rate adapted learning_rate :              0.8511380382023764
      loss :              1.7966666666666666
      loss_factor :              0.17966666666666667
      loss adapted learning_rate :              2.380954229082056e-75
    epoch : 3 ; learning_rate : 2.380954229082056e-75 ; fit : 0.10166666666666667
    batch_size :          600
    best_batch_loss : 1.7433333333333334
    Epoch 3, Loss: 1.8017000000000007, fit: 0.12333333333333334
      batch rate adapted learning_rate :              0.8511380382023764
      loss :              1.7533333333333334
      loss_factor :              0.17533333333333334
      loss adapted learning_rate :              2.0722824232878566e-76
    epoch : 4 ; learning_rate : 2.0722824232878566e-76 ; fit : 0.12333333333333334
    batch_size :          600
    best_batch_loss : 1.7466666666666666
    Epoch 4, Loss: 1.801700000000001, fit: 0.10333333333333333
      batch rate adapted learning_rate :              0.8511380382023764
      loss :              1.7933333333333332
      loss_factor :              0.17933333333333332
      loss adapted learning_rate :              1.9774344440703985e-75
    epoch : 5 ; learning_rate : 1.9774344440703985e-75 ; fit : 0.10333333333333333
    batch_size :          600
    best_batch_loss : 1.7133333333333334
    Epoch 5, Loss: 1.801700000000001, fit: 0.08833333333333333
      batch rate adapted learning_rate :              0.8511380382023764
      loss :              1.8233333333333333
      loss_factor :              0.18233333333333332
      loss adapted learning_rate :              1.0389799372139912e-74
    epoch : 6 ; learning_rate : 1.0389799372139912e-74 ; fit : 0.08833333333333333
    batch_size :          600
    best_batch_loss : 1.7533333333333334
    Epoch 6, Loss: 1.801700000000001, fit: 0.12333333333333334
      batch rate adapted learning_rate :              0.8511380382023764
      loss :              1.7533333333333334
      loss_factor :              0.17533333333333334
      loss adapted learning_rate :              2.0722824232878566e-76
    epoch : 7 ; learning_rate : 2.0722824232878566e-76 ; fit : 0.12333333333333334
    batch_size :          600
    best_batch_loss : 1.7466666666666666
    Epoch 7, Loss: 1.801700000000001, fit: 0.09
      batch rate adapted learning_rate :              0.8511380382023764
      loss :              1.82
      loss_factor :              0.182
      loss adapted learning_rate :              8.652444027005465e-75
    epoch : 8 ; learning_rate : 8.652444027005465e-75 ; fit : 0.09
    batch_size :          600
    best_batch_loss : 1.7566666666666666
    Epoch 8, Loss: 1.801700000000001, fit: 0.085
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.83_fit_0.085_2024-01-03_184812
  self.fit : 0.085
  self.loss : 1.83
  current_accuracy : 0.1009
   Accuracy mean: 0.1009
   Accuracy mean: 0.1009
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9120108393559098
      loss_factor :              1
      loss adapted learning_rate :              0.9120108393559098
    epoch : 0 ; learning_rate : 0.9120108393559098 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.151022685382923
    Epoch 0, Loss: 1.7769610698001999, fit: 0.095
    Epoch 0, Loss: 1.7769610698001999
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.81_fit_0.095_2024-01-03_184815
  self.fit : 0.095
  self.loss : 1.81
  current_accuracy : 0.1032
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9120108393559098
      loss_factor :              1
      loss adapted learning_rate :              0.9120108393559098
    epoch : 0 ; learning_rate : 0.9120108393559098 ; fit : 0.095
    batch_size :          600
    best_batch_loss : 1.7266666666666666
    Epoch 0, Loss: 1.801400000000001, fit: 0.1
    Epoch 0, Loss: 1.801400000000001
      batch rate adapted learning_rate :              0.9120108393559098
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.070792776471649e-75
    epoch : 1 ; learning_rate : 3.070792776471649e-75 ; fit : 0.1
    batch_size :          600
    best_batch_loss : 1.73
    Epoch 1, Loss: 1.8014000000000006, fit: 0.09333333333333334
      batch rate adapted learning_rate :              0.9120108393559098
      loss :              1.8133333333333332
      loss_factor :              0.18133333333333332
      loss adapted learning_rate :              6.423393055484531e-75
    epoch : 2 ; learning_rate : 6.423393055484531e-75 ; fit : 0.09333333333333334
    batch_size :          600
    best_batch_loss : 1.74
    Epoch 2, Loss: 1.8014000000000008, fit: 0.08666666666666667
      batch rate adapted learning_rate :              0.9120108393559098
      loss :              1.8266666666666667
      loss_factor :              0.18266666666666667
      loss adapted learning_rate :              1.3363812586128846e-74
    epoch : 3 ; learning_rate : 1.3363812586128846e-74 ; fit : 0.08666666666666667
    batch_size :          600
    best_batch_loss : 1.74
    Epoch 3, Loss: 1.801400000000001, fit: 0.10833333333333334
      batch rate adapted learning_rate :              0.9120108393559098
      loss :              1.7833333333333334
      loss_factor :              0.17833333333333334
      loss adapted learning_rate :              1.2113028252019024e-75
    epoch : 4 ; learning_rate : 1.2113028252019024e-75 ; fit : 0.10833333333333334
    batch_size :          600
    best_batch_loss : 1.7533333333333334
    Epoch 4, Loss: 1.8014000000000008, fit: 0.09166666666666666
      batch rate adapted learning_rate :              0.9120108393559098
      loss :              1.8166666666666667
      loss_factor :              0.18166666666666667
      loss adapted learning_rate :              7.718355105769686e-75
    epoch : 5 ; learning_rate : 7.718355105769686e-75 ; fit : 0.09166666666666666
    batch_size :          600
    best_batch_loss : 1.73
    Epoch 5, Loss: 1.8014000000000008, fit: 0.11833333333333333
      batch rate adapted learning_rate :              0.9120108393559098
      loss :              1.7633333333333334
      loss_factor :              0.17633333333333334
      loss adapted learning_rate :              3.9214053006924837e-76
    epoch : 6 ; learning_rate : 3.9214053006924837e-76 ; fit : 0.11833333333333333
    batch_size :          600
    best_batch_loss : 1.7066666666666668
    Epoch 6, Loss: 1.8014000000000008, fit: 0.095
      batch rate adapted learning_rate :              0.9120108393559098
      loss :              1.81
      loss_factor :              0.181
      loss adapted learning_rate :              5.34388971683995e-75
    epoch : 7 ; learning_rate : 5.34388971683995e-75 ; fit : 0.095
    batch_size :          600
    best_batch_loss : 1.71
    Epoch 7, Loss: 1.8014000000000008, fit: 0.10166666666666667
      batch rate adapted learning_rate :              0.9120108393559098
      loss :              1.7966666666666666
      loss_factor :              0.17966666666666667
      loss adapted learning_rate :              2.551238421348546e-75
    epoch : 8 ; learning_rate : 2.551238421348546e-75 ; fit : 0.10166666666666667
    batch_size :          600
    best_batch_loss : 1.74
    Epoch 8, Loss: 1.8014000000000008, fit: 0.1
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.8_fit_0.1_2024-01-03_184838
  self.fit : 0.1
  self.loss : 1.8
  current_accuracy : 0.1032
   Accuracy mean: 0.1032
   Accuracy mean: 0.1032
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.933254300796991
      loss_factor :              1
      loss adapted learning_rate :              0.933254300796991
    epoch : 0 ; learning_rate : 0.933254300796991 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.118107688535611
    Epoch 0, Loss: 1.7931627423315122, fit: 0.08833333333333333
    Epoch 0, Loss: 1.7931627423315122
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.8233333333333333_fit_0.08833333333333333_2024-01-03_184841
  self.fit : 0.08833333333333333
  self.loss : 1.8233333333333333
  current_accuracy : 0.0892
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.933254300796991
      loss_factor :              1
      loss adapted learning_rate :              0.933254300796991
    epoch : 0 ; learning_rate : 0.933254300796991 ; fit : 0.08833333333333333
    batch_size :          600
    best_batch_loss : 1.7666666666666666
    Epoch 0, Loss: 1.8192999999999997, fit: 0.07833333333333334
    Epoch 0, Loss: 1.8192999999999997
      batch rate adapted learning_rate :              0.933254300796991
      loss :              1.8433333333333333
      loss_factor :              0.18433333333333332
      loss adapted learning_rate :              3.3914674911847513e-74
    epoch : 1 ; learning_rate : 3.3914674911847513e-74 ; fit : 0.07833333333333334
    batch_size :          600
    best_batch_loss : 1.7266666666666666
    Epoch 1, Loss: 1.8192999999999995, fit: 0.07666666666666666
      batch rate adapted learning_rate :              0.933254300796991
      loss :              1.8466666666666667
      loss_factor :              0.18466666666666667
      loss adapted learning_rate :              4.063039092211245e-74
    epoch : 2 ; learning_rate : 4.063039092211245e-74 ; fit : 0.07666666666666666
    batch_size :          600
    best_batch_loss : 1.7633333333333334
    Epoch 2, Loss: 1.8193, fit: 0.09333333333333334
      batch rate adapted learning_rate :              0.933254300796991
      loss :              1.8133333333333332
      loss_factor :              0.18133333333333332
      loss adapted learning_rate :              6.573013100342182e-75
    epoch : 3 ; learning_rate : 6.573013100342182e-75 ; fit : 0.09333333333333334
    batch_size :          600
    best_batch_loss : 1.7666666666666666
    Epoch 3, Loss: 1.8192999999999997, fit: 0.07
      batch rate adapted learning_rate :              0.933254300796991
      loss :              1.86
      loss_factor :              0.186
      loss adapted learning_rate :              8.34246554748649e-74
    epoch : 4 ; learning_rate : 8.34246554748649e-74 ; fit : 0.07
    batch_size :          600
    best_batch_loss : 1.7633333333333334
    Epoch 4, Loss: 1.8192999999999997, fit: 0.085
      batch rate adapted learning_rate :              0.933254300796991
      loss :              1.83
      loss_factor :              0.183
      loss adapted learning_rate :              1.6410014811508453e-74
    epoch : 5 ; learning_rate : 1.6410014811508453e-74 ; fit : 0.085
    batch_size :          600
    best_batch_loss : 1.7533333333333334
    Epoch 5, Loss: 1.8192999999999997, fit: 0.09
      batch rate adapted learning_rate :              0.933254300796991
      loss :              1.82
      loss_factor :              0.182
      loss adapted learning_rate :              9.487216218960827e-75
    epoch : 6 ; learning_rate : 9.487216218960827e-75 ; fit : 0.09
    batch_size :          600
    best_batch_loss : 1.7733333333333334
    Epoch 6, Loss: 1.8192999999999995, fit: 0.07666666666666666
      batch rate adapted learning_rate :              0.933254300796991
      loss :              1.8466666666666667
      loss_factor :              0.18466666666666667
      loss adapted learning_rate :              4.063039092211245e-74
    epoch : 7 ; learning_rate : 4.063039092211245e-74 ; fit : 0.07666666666666666
    batch_size :          600
    best_batch_loss : 1.76
    Epoch 7, Loss: 1.8192999999999995, fit: 0.10666666666666667
      batch rate adapted learning_rate :              0.933254300796991
      loss :              1.7866666666666666
      loss_factor :              0.17866666666666667
      loss adapted learning_rate :              1.49400997563341e-75
    epoch : 8 ; learning_rate : 1.49400997563341e-75 ; fit : 0.10666666666666667
    batch_size :          600
    best_batch_loss : 1.77
    Epoch 8, Loss: 1.8192999999999997, fit: 0.1
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.8_fit_0.1_2024-01-03_184906
  self.fit : 0.1
  self.loss : 1.8
  current_accuracy : 0.0892
   Accuracy mean: 0.0892
   Accuracy mean: 0.0892
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9616350847573034
      loss_factor :              1
      loss adapted learning_rate :              0.9616350847573034
    epoch : 0 ; learning_rate : 0.9616350847573034 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.1419975603039418
    Epoch 0, Loss: 1.782551661137493, fit: 0.11166666666666666
    Epoch 0, Loss: 1.782551661137493
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7766666666666666_fit_0.11166666666666666_2024-01-03_184909
  self.fit : 0.11166666666666666
  self.loss : 1.7766666666666666
  current_accuracy : 0.0974
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9616350847573034
      loss_factor :              1
      loss adapted learning_rate :              0.9616350847573034
    epoch : 0 ; learning_rate : 0.9616350847573034 ; fit : 0.11166666666666666
    batch_size :          600
    best_batch_loss : 1.75
    Epoch 0, Loss: 1.804966666666667, fit: 0.09333333333333334
    Epoch 0, Loss: 1.804966666666667
      batch rate adapted learning_rate :              0.9616350847573034
      loss :              1.8133333333333332
      loss_factor :              0.18133333333333332
      loss adapted learning_rate :              6.772902095881559e-75
    epoch : 1 ; learning_rate : 6.772902095881559e-75 ; fit : 0.09333333333333334
    batch_size :          600
    best_batch_loss : 1.7466666666666666
    Epoch 1, Loss: 1.8049666666666675, fit: 0.085
      batch rate adapted learning_rate :              0.9616350847573034
      loss :              1.83
      loss_factor :              0.183
      loss adapted learning_rate :              1.6909052517258343e-74
    epoch : 2 ; learning_rate : 1.6909052517258343e-74 ; fit : 0.085
    batch_size :          600
    best_batch_loss : 1.7466666666666666
    Epoch 2, Loss: 1.8049666666666673, fit: 0.09833333333333333
      batch rate adapted learning_rate :              0.9616350847573034
      loss :              1.8033333333333332
      loss_factor :              0.18033333333333332
      loss adapted learning_rate :              3.8959317362197486e-75
    epoch : 3 ; learning_rate : 3.8959317362197486e-75 ; fit : 0.09833333333333333
    batch_size :          600
    best_batch_loss : 1.74
    Epoch 3, Loss: 1.804966666666667, fit: 0.09666666666666666
      batch rate adapted learning_rate :              0.9616350847573034
      loss :              1.8066666666666666
      loss_factor :              0.18066666666666667
      loss adapted learning_rate :              4.686120822352047e-75
    epoch : 4 ; learning_rate : 4.686120822352047e-75 ; fit : 0.09666666666666666
    batch_size :          600
    best_batch_loss : 1.7433333333333334
    Epoch 4, Loss: 1.8049666666666673, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.9616350847573034
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              2.0284008198659593e-74
    epoch : 5 ; learning_rate : 2.0284008198659593e-74 ; fit : 0.08333333333333333
    batch_size :          600
    best_batch_loss : 1.7266666666666666
    Epoch 5, Loss: 1.8049666666666673, fit: 0.08166666666666667
      batch rate adapted learning_rate :              0.9616350847573034
      loss :              1.8366666666666667
      loss_factor :              0.18366666666666667
      loss adapted learning_rate :              2.432454431659623e-74
    epoch : 6 ; learning_rate : 2.432454431659623e-74 ; fit : 0.08166666666666667
    batch_size :          600
    best_batch_loss : 1.7366666666666666
    Epoch 6, Loss: 1.8049666666666673, fit: 0.10333333333333333
      batch rate adapted learning_rate :              0.9616350847573034
      loss :              1.7933333333333332
      loss_factor :              0.17933333333333332
      loss adapted learning_rate :              2.234150342101747e-75
    epoch : 7 ; learning_rate : 2.234150342101747e-75 ; fit : 0.10333333333333333
    batch_size :          600
    best_batch_loss : 1.7433333333333334
    Epoch 7, Loss: 1.8049666666666673, fit: 0.09833333333333333
      batch rate adapted learning_rate :              0.9616350847573034
      loss :              1.8033333333333332
      loss_factor :              0.18033333333333332
      loss adapted learning_rate :              3.8959317362197486e-75
    epoch : 8 ; learning_rate : 3.8959317362197486e-75 ; fit : 0.09833333333333333
    batch_size :          600
    best_batch_loss : 1.75
    Epoch 8, Loss: 1.8049666666666673, fit: 0.12333333333333334
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7533333333333334_fit_0.12333333333333334_2024-01-03_184932
  self.fit : 0.12333333333333334
  self.loss : 1.7533333333333334
  current_accuracy : 0.0974
   Accuracy mean: 0.0974
   Accuracy mean: 0.0974
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9637833073548235
      loss_factor :              1
      loss adapted learning_rate :              0.9637833073548235
    epoch : 0 ; learning_rate : 0.9637833073548235 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.5500602324892254
    Epoch 0, Loss: 1.7997239427308356, fit: 0.10666666666666667
    Epoch 0, Loss: 1.7997239427308356
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7866666666666666_fit_0.10666666666666667_2024-01-03_184934
  self.fit : 0.10666666666666667
  self.loss : 1.7866666666666666
  current_accuracy : 0.0974
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9637833073548235
      loss_factor :              1
      loss adapted learning_rate :              0.9637833073548235
    epoch : 0 ; learning_rate : 0.9637833073548235 ; fit : 0.10666666666666667
    batch_size :          600
    best_batch_loss : 1.7566666666666666
    Epoch 0, Loss: 1.8049666666666673, fit: 0.10666666666666667
    Epoch 0, Loss: 1.8049666666666673
      batch rate adapted learning_rate :              0.9637833073548235
      loss :              1.7866666666666666
      loss_factor :              0.17866666666666667
      loss adapted learning_rate :              1.5428826572858047e-75
    epoch : 1 ; learning_rate : 1.5428826572858047e-75 ; fit : 0.10666666666666667
    batch_size :          600
    best_batch_loss : 1.7366666666666666
    Epoch 1, Loss: 1.8049666666666673, fit: 0.09333333333333334
      batch rate adapted learning_rate :              0.9637833073548235
      loss :              1.8133333333333332
      loss_factor :              0.18133333333333332
      loss adapted learning_rate :              6.788032264865396e-75
    epoch : 2 ; learning_rate : 6.788032264865396e-75 ; fit : 0.09333333333333334
    batch_size :          600
    best_batch_loss : 1.76
    Epoch 2, Loss: 1.804966666666667, fit: 0.1
      batch rate adapted learning_rate :              0.9637833073548235
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.245113644043193e-75
    epoch : 3 ; learning_rate : 3.245113644043193e-75 ; fit : 0.1
    batch_size :          600
    best_batch_loss : 1.7533333333333334
    Epoch 3, Loss: 1.8049666666666673, fit: 0.09833333333333333
      batch rate adapted learning_rate :              0.9637833073548235
      loss :              1.8033333333333332
      loss_factor :              0.18033333333333332
      loss adapted learning_rate :              3.904634963386481e-75
    epoch : 4 ; learning_rate : 3.904634963386481e-75 ; fit : 0.09833333333333333
    batch_size :          600
    best_batch_loss : 1.7333333333333334
    Epoch 4, Loss: 1.804966666666667, fit: 0.08166666666666667
      batch rate adapted learning_rate :              0.9637833073548235
      loss :              1.8366666666666667
      loss_factor :              0.18366666666666667
      loss adapted learning_rate :              2.437888357335128e-74
    epoch : 5 ; learning_rate : 2.437888357335128e-74 ; fit : 0.08166666666666667
    batch_size :          600
    best_batch_loss : 1.76
    Epoch 5, Loss: 1.8049666666666673, fit: 0.09666666666666666
      batch rate adapted learning_rate :              0.9637833073548235
      loss :              1.8066666666666666
      loss_factor :              0.18066666666666667
      loss adapted learning_rate :              4.696589274267803e-75
    epoch : 6 ; learning_rate : 4.696589274267803e-75 ; fit : 0.09666666666666666
    batch_size :          600
    best_batch_loss : 1.7566666666666666
    Epoch 6, Loss: 1.8049666666666673, fit: 0.11833333333333333
      batch rate adapted learning_rate :              0.9637833073548235
      loss :              1.7633333333333334
      loss_factor :              0.17633333333333334
      loss adapted learning_rate :              4.1440132146337834e-76
    epoch : 7 ; learning_rate : 4.1440132146337834e-76 ; fit : 0.11833333333333333
    batch_size :          600
    best_batch_loss : 1.73
    Epoch 7, Loss: 1.8049666666666673, fit: 0.10166666666666667
      batch rate adapted learning_rate :              0.9637833073548235
      loss :              1.7966666666666666
      loss_factor :              0.17966666666666667
      loss adapted learning_rate :              2.6960655482061047e-75
    epoch : 8 ; learning_rate : 2.6960655482061047e-75 ; fit : 0.10166666666666667
    batch_size :          600
    best_batch_loss : 1.74
    Epoch 8, Loss: 1.8049666666666673, fit: 0.11
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.78_fit_0.11_2024-01-03_184958
  self.fit : 0.11
  self.loss : 1.78
  current_accuracy : 0.0974
   Accuracy mean: 0.0974
   Accuracy mean: 0.0974
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9655420949221489
      loss_factor :              1
      loss adapted learning_rate :              0.9655420949221489
    epoch : 0 ; learning_rate : 0.9655420949221489 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.1511060135725169
    Epoch 0, Loss: 1.8037352543138727, fit: 0.08
    Epoch 0, Loss: 1.8037352543138727
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.84_fit_0.08_2024-01-03_185000
  self.fit : 0.08
  self.loss : 1.84
  current_accuracy : 0.0892
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9655420949221489
      loss_factor :              1
      loss adapted learning_rate :              0.9655420949221489
    epoch : 0 ; learning_rate : 0.9655420949221489 ; fit : 0.08
    batch_size :          600
    best_batch_loss : 1.77
    Epoch 0, Loss: 1.8192999999999997, fit: 0.07666666666666666
    Epoch 0, Loss: 1.8192999999999997
      batch rate adapted learning_rate :              0.9655420949221489
      loss :              1.8466666666666667
      loss_factor :              0.18466666666666667
      loss adapted learning_rate :              4.203608034266752e-74
    epoch : 1 ; learning_rate : 4.203608034266752e-74 ; fit : 0.07666666666666666
    batch_size :          600
    best_batch_loss : 1.7633333333333334
    Epoch 1, Loss: 1.8192999999999997, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.9655420949221489
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              2.036641973654142e-74
    epoch : 2 ; learning_rate : 2.036641973654142e-74 ; fit : 0.08333333333333333
    batch_size :          600
    best_batch_loss : 1.7666666666666666
    Epoch 2, Loss: 1.8192999999999997, fit: 0.07833333333333334
      batch rate adapted learning_rate :              0.9655420949221489
      loss :              1.8433333333333333
      loss_factor :              0.18433333333333332
      loss adapted learning_rate :              3.5088020740996383e-74
    epoch : 3 ; learning_rate : 3.5088020740996383e-74 ; fit : 0.07833333333333334
    batch_size :          600
    best_batch_loss : 1.75
    Epoch 3, Loss: 1.8192999999999997, fit: 0.09166666666666666
      batch rate adapted learning_rate :              0.9655420949221489
      loss :              1.8166666666666667
      loss_factor :              0.18166666666666667
      loss adapted learning_rate :              8.171390554350253e-75
    epoch : 4 ; learning_rate : 8.171390554350253e-75 ; fit : 0.09166666666666666
    batch_size :          600
    best_batch_loss : 1.7633333333333334
    Epoch 4, Loss: 1.8192999999999997, fit: 0.08833333333333333
      batch rate adapted learning_rate :              0.9655420949221489
      loss :              1.8233333333333333
      loss_factor :              0.18233333333333332
      loss adapted learning_rate :              1.1786323958431198e-74
    epoch : 5 ; learning_rate : 1.1786323958431198e-74 ; fit : 0.08833333333333333
    batch_size :          600
    best_batch_loss : 1.7666666666666666
    Epoch 5, Loss: 1.8193, fit: 0.10833333333333334
      batch rate adapted learning_rate :              0.9655420949221489
      loss :              1.7833333333333334
      loss_factor :              0.17833333333333334
      loss adapted learning_rate :              1.28240127963451e-75
    epoch : 6 ; learning_rate : 1.28240127963451e-75 ; fit : 0.10833333333333334
    batch_size :          600
    best_batch_loss : 1.7633333333333334
    Epoch 6, Loss: 1.8193, fit: 0.08833333333333333
      batch rate adapted learning_rate :              0.9655420949221489
      loss :              1.8233333333333333
      loss_factor :              0.18233333333333332
      loss adapted learning_rate :              1.1786323958431198e-74
    epoch : 7 ; learning_rate : 1.1786323958431198e-74 ; fit : 0.08833333333333333
    batch_size :          600
    best_batch_loss : 1.7466666666666666
    Epoch 7, Loss: 1.8192999999999997, fit: 0.07666666666666666
      batch rate adapted learning_rate :              0.9655420949221489
      loss :              1.8466666666666667
      loss_factor :              0.18466666666666667
      loss adapted learning_rate :              4.203608034266752e-74
    epoch : 8 ; learning_rate : 4.203608034266752e-74 ; fit : 0.07666666666666666
    batch_size :          600
    best_batch_loss : 1.7733333333333334
    Epoch 8, Loss: 1.8192999999999997, fit: 0.09166666666666666
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.8166666666666667_fit_0.09166666666666666_2024-01-03_185024
  self.fit : 0.09166666666666666
  self.loss : 1.8166666666666667
  current_accuracy : 0.0892
   Accuracy mean: 0.0892
   Accuracy mean: 0.0892
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9772372209558107
      loss_factor :              1
      loss adapted learning_rate :              0.9772372209558107
    epoch : 0 ; learning_rate : 0.9772372209558107 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 1.0510944902181616
    Epoch 0, Loss: 1.7716969354688255, fit: 0.09833333333333333
    Epoch 0, Loss: 1.7716969354688255
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.8033333333333332_fit_0.09833333333333333_2024-01-03_185027
  self.fit : 0.09833333333333333
  self.loss : 1.8033333333333332
  current_accuracy : 0.1032
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9772372209558107
      loss_factor :              1
      loss adapted learning_rate :              0.9772372209558107
    epoch : 0 ; learning_rate : 0.9772372209558107 ; fit : 0.09833333333333333
    batch_size :          600
    best_batch_loss : 1.74
    Epoch 0, Loss: 1.8014000000000008, fit: 0.09666666666666666
    Epoch 0, Loss: 1.8014000000000008
      batch rate adapted learning_rate :              0.9772372209558107
      loss :              1.8066666666666666
      loss_factor :              0.18066666666666667
      loss adapted learning_rate :              4.762151217324013e-75
    epoch : 1 ; learning_rate : 4.762151217324013e-75 ; fit : 0.09666666666666666
    batch_size :          600
    best_batch_loss : 1.7433333333333334
    Epoch 1, Loss: 1.8014000000000008, fit: 0.09666666666666666
      batch rate adapted learning_rate :              0.9772372209558107
      loss :              1.8066666666666666
      loss_factor :              0.18066666666666667
      loss adapted learning_rate :              4.762151217324013e-75
    epoch : 2 ; learning_rate : 4.762151217324013e-75 ; fit : 0.09666666666666666
    batch_size :          600
    best_batch_loss : 1.7466666666666666
    Epoch 2, Loss: 1.801400000000001, fit: 0.105
      batch rate adapted learning_rate :              0.9772372209558107
      loss :              1.79
      loss_factor :              0.179
      loss adapted learning_rate :              1.8849641638239435e-75
    epoch : 3 ; learning_rate : 1.8849641638239435e-75 ; fit : 0.105
    batch_size :          600
    best_batch_loss : 1.74
    Epoch 3, Loss: 1.801400000000001, fit: 0.09
      batch rate adapted learning_rate :              0.9772372209558107
      loss :              1.82
      loss_factor :              0.182
      loss adapted learning_rate :              9.934334944405396e-75
    epoch : 4 ; learning_rate : 9.934334944405396e-75 ; fit : 0.09
    batch_size :          600
    best_batch_loss : 1.7333333333333334
    Epoch 4, Loss: 1.801400000000001, fit: 0.10166666666666667
      batch rate adapted learning_rate :              0.9772372209558107
      loss :              1.7966666666666666
      loss_factor :              0.17966666666666667
      loss adapted learning_rate :              2.7337012207388815e-75
    epoch : 5 ; learning_rate : 2.7337012207388815e-75 ; fit : 0.10166666666666667
    batch_size :          600
    best_batch_loss : 1.73
    Epoch 5, Loss: 1.801400000000001, fit: 0.08666666666666667
      batch rate adapted learning_rate :              0.9772372209558107
      loss :              1.8266666666666667
      loss_factor :              0.18266666666666667
      loss adapted learning_rate :              1.431958317761436e-74
    epoch : 6 ; learning_rate : 1.431958317761436e-74 ; fit : 0.08666666666666667
    batch_size :          600
    best_batch_loss : 1.7333333333333334
    Epoch 6, Loss: 1.8014000000000006, fit: 0.07833333333333334
      batch rate adapted learning_rate :              0.9772372209558107
      loss :              1.8433333333333333
      loss_factor :              0.18433333333333332
      loss adapted learning_rate :              3.551302429806116e-74
    epoch : 7 ; learning_rate : 3.551302429806116e-74 ; fit : 0.07833333333333334
    batch_size :          600
    best_batch_loss : 1.74
    Epoch 7, Loss: 1.8014000000000008, fit: 0.08666666666666667
      batch rate adapted learning_rate :              0.9772372209558107
      loss :              1.8266666666666667
      loss_factor :              0.18266666666666667
      loss adapted learning_rate :              1.431958317761436e-74
    epoch : 8 ; learning_rate : 1.431958317761436e-74 ; fit : 0.08666666666666667
    batch_size :          600
    best_batch_loss : 1.7533333333333334
    Epoch 8, Loss: 1.8014000000000008, fit: 0.105
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.79_fit_0.105_2024-01-03_185052
  self.fit : 0.105
  self.loss : 1.79
  current_accuracy : 0.1032
   Accuracy mean: 0.1032
   Accuracy mean: 0.1032
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9989469496904544
      loss_factor :              1
      loss adapted learning_rate :              0.9989469496904544
    epoch : 0 ; learning_rate : 0.9989469496904544 ; fit : 0.0
    batch_size :          600
    best_batch_loss : 0.9245307451172994
    Epoch 0, Loss: 1.7723424541930755, fit: 0.10666666666666667
    Epoch 0, Loss: 1.7723424541930755
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7866666666666666_fit_0.10666666666666667_2024-01-03_185055
  self.fit : 0.10666666666666667
  self.loss : 1.7866666666666666
  current_accuracy : 0.1032
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.01
      batch rate adapted learning_rate :              0.9989469496904544
      loss_factor :              1
      loss adapted learning_rate :              0.9989469496904544
    epoch : 0 ; learning_rate : 0.9989469496904544 ; fit : 0.10666666666666667
    batch_size :          600
    best_batch_loss : 1.7133333333333334
    Epoch 0, Loss: 1.801400000000001, fit: 0.09833333333333333
    Epoch 0, Loss: 1.801400000000001
      batch rate adapted learning_rate :              0.9989469496904544
      loss :              1.8033333333333332
      loss_factor :              0.18033333333333332
      loss adapted learning_rate :              4.0470956039225317e-75
    epoch : 1 ; learning_rate : 4.0470956039225317e-75 ; fit : 0.09833333333333333
    batch_size :          600
    best_batch_loss : 1.72
    Epoch 1, Loss: 1.8014000000000006, fit: 0.10833333333333334
      batch rate adapted learning_rate :              0.9989469496904544
      loss :              1.7833333333333334
      loss_factor :              0.17833333333333334
      loss adapted learning_rate :              1.3267685099460315e-75
    epoch : 2 ; learning_rate : 1.3267685099460315e-75 ; fit : 0.10833333333333334
    batch_size :          600
    best_batch_loss : 1.75
    Epoch 2, Loss: 1.8014000000000008, fit: 0.07333333333333333
      batch rate adapted learning_rate :              0.9989469496904544
      loss :              1.8533333333333333
      loss_factor :              0.18533333333333332
      loss adapted learning_rate :              6.235856481613339e-74
    epoch : 3 ; learning_rate : 6.235856481613339e-74 ; fit : 0.07333333333333333
    batch_size :          600
    best_batch_loss : 1.7366666666666666
    Epoch 3, Loss: 1.8014000000000008, fit: 0.09666666666666666
      batch rate adapted learning_rate :              0.9989469496904544
      loss :              1.8066666666666666
      loss_factor :              0.18066666666666667
      loss adapted learning_rate :              4.867944374711468e-75
    epoch : 4 ; learning_rate : 4.867944374711468e-75 ; fit : 0.09666666666666666
    batch_size :          600
    best_batch_loss : 1.7366666666666666
    Epoch 4, Loss: 1.8014000000000008, fit: 0.07333333333333333
      batch rate adapted learning_rate :              0.9989469496904544
      loss :              1.8533333333333333
      loss_factor :              0.18533333333333332
      loss adapted learning_rate :              6.235856481613339e-74
    epoch : 5 ; learning_rate : 6.235856481613339e-74 ; fit : 0.07333333333333333
    batch_size :          600
    best_batch_loss : 1.7333333333333334
    Epoch 5, Loss: 1.8014000000000008, fit: 0.09833333333333333
      batch rate adapted learning_rate :              0.9989469496904544
      loss :              1.8033333333333332
      loss_factor :              0.18033333333333332
      loss adapted learning_rate :              4.0470956039225317e-75
    epoch : 6 ; learning_rate : 4.0470956039225317e-75 ; fit : 0.09833333333333333
    batch_size :          600
    best_batch_loss : 1.7433333333333334
    Epoch 6, Loss: 1.8014000000000012, fit: 0.07833333333333334
      batch rate adapted learning_rate :              0.9989469496904544
      loss :              1.8433333333333333
      loss_factor :              0.18433333333333332
      loss adapted learning_rate :              3.630196080961118e-74
    epoch : 7 ; learning_rate : 3.630196080961118e-74 ; fit : 0.07833333333333334
    batch_size :          600
    best_batch_loss : 1.71
    Epoch 7, Loss: 1.8014000000000006, fit: 0.10333333333333333
      batch rate adapted learning_rate :              0.9989469496904544
      loss :              1.7933333333333332
      loss_factor :              0.17933333333333332
      loss adapted learning_rate :              2.320836359621471e-75
    epoch : 8 ; learning_rate : 2.320836359621471e-75 ; fit : 0.10333333333333333
    batch_size :          600
    best_batch_loss : 1.74
    Epoch 8, Loss: 1.8014000000000006, fit: 0.10333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7933333333333332_fit_0.10333333333333333_2024-01-03_185118
  self.fit : 0.10333333333333333
  self.loss : 1.7933333333333332
  current_accuracy : 0.1032
   Accuracy mean: 0.1032
   Accuracy mean: 0.1032
  Error saving file: doc/out/test_combinations_results/20240103185119.
  normalized_accuracies :      [0.96531219 1.         0.2218781  0.30636769 0.01015857 0.01015857
   0.01684836 0.01684836 0.01449455 0.01449455 0.0173439  0.0173439
   0.         0.         0.01015857 0.01015857 0.01015857 0.01015857
   0.         0.         0.0173439  0.0173439  0.0173439  0.0173439 ]
batch_rate :  0.001
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.8629785477669702
      loss_factor :              1
      loss adapted learning_rate :              0.8629785477669702
    epoch : 0 ; learning_rate : 0.8629785477669702 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 0.9872757974192731
    Epoch 0, Loss: 1.8155223193329468, fit: 0.06666666666666667
    Epoch 0, Loss: 1.8155223193329468
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.8666666666666667_fit_0.06666666666666667_2024-01-03_185128
  self.fit : 0.06666666666666667
  self.loss : 1.8666666666666667
  current_accuracy : 0.0892
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-64
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.8629785477669702
      loss_factor :              1
      loss adapted learning_rate :              0.8629785477669702
    epoch : 0 ; learning_rate : 0.8629785477669702 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5666666666666667
    Epoch 0, Loss: 1.8193000000000175, fit: 0.016666666666666666
    Epoch 0, Loss: 1.8193000000000175
      batch rate adapted learning_rate :              0.8629785477669702
      loss :              1.9666666666666666
      loss_factor :              0.19666666666666666
      loss adapted learning_rate :              2.0373971403147615e-71
    epoch : 1 ; learning_rate : 2.0373971403147615e-71 ; fit : 0.016666666666666666
    batch_size :          60
    best_batch_loss : 1.3666666666666667
    Epoch 1, Loss: 1.8193000000000175, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.8629785477669702
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              1.8203021307807614e-74
    epoch : 2 ; learning_rate : 1.8203021307807614e-74 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 2, Loss: 1.8193000000000172, fit: 0.15
      batch rate adapted learning_rate :              0.8629785477669702
      loss :              1.7
      loss_factor :              0.16999999999999998
      loss adapted learning_rate :              9.569563703420561e-78
    epoch : 3 ; learning_rate : 9.569563703420561e-78 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 3, Loss: 1.819300000000017, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.8629785477669702
      loss :              1.7666666666666666
      loss_factor :              0.17666666666666667
      loss adapted learning_rate :              4.4818930419730504e-76
    epoch : 4 ; learning_rate : 4.4818930419730504e-76 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 4, Loss: 1.8193000000000175, fit: 0.05
      batch rate adapted learning_rate :              0.8629785477669702
      loss :              1.9
      loss_factor :              0.19
      loss adapted learning_rate :              6.476794165815998e-73
    epoch : 5 ; learning_rate : 6.476794165815998e-73 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 5, Loss: 1.8193000000000177, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.8629785477669702
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              1.8203021307807614e-74
    epoch : 6 ; learning_rate : 1.8203021307807614e-74 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4666666666666666
    Epoch 6, Loss: 1.8193000000000172, fit: 0.03333333333333333
      batch rate adapted learning_rate :              0.8629785477669702
      loss :              1.9333333333333333
      loss_factor :              0.19333333333333333
      loss adapted learning_rate :              3.687006339394494e-72
    epoch : 7 ; learning_rate : 3.687006339394494e-72 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 7, Loss: 1.8193000000000175, fit: 0.05
      batch rate adapted learning_rate :              0.8629785477669702
      loss :              1.9
      loss_factor :              0.19
      loss adapted learning_rate :              6.476794165815998e-73
    epoch : 8 ; learning_rate : 6.476794165815998e-73 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 8, Loss: 1.8193000000000172, fit: 0.05
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.9_fit_0.05_2024-01-03_185231
  self.fit : 0.05
  self.loss : 1.9
  current_accuracy : 0.0892
   Accuracy mean: 0.0892
   Accuracy mean: 0.0892
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9268298233793493
      loss_factor :              1
      loss adapted learning_rate :              0.9268298233793493
    epoch : 0 ; learning_rate : 0.9268298233793493 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.1510412113626591
    Epoch 0, Loss: 1.8012077010785135, fit: 0.05
    Epoch 0, Loss: 1.8012077010785135
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.9_fit_0.05_2024-01-03_185239
  self.fit : 0.05
  self.loss : 1.9
  current_accuracy : 0.0958
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-33
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9268298233793493
      loss_factor :              1
      loss adapted learning_rate :              0.9268298233793493
    epoch : 0 ; learning_rate : 0.9268298233793493 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 0, Loss: 1.802733333333352, fit: 0.11666666666666667
    Epoch 0, Loss: 1.802733333333352
      batch rate adapted learning_rate :              0.9268298233793493
      loss :              1.7666666666666666
      loss_factor :              0.17666666666666667
      loss adapted learning_rate :              4.81350567432495e-76
    epoch : 1 ; learning_rate : 4.81350567432495e-76 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 1, Loss: 1.8027333333333513, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.9268298233793493
      loss :              1.6333333333333333
      loss_factor :              0.16333333333333333
      loss adapted learning_rate :              1.8814060267879118e-79
    epoch : 2 ; learning_rate : 1.8814060267879118e-79 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.4666666666666666
    Epoch 2, Loss: 1.802733333333351, fit: 0.03333333333333333
      batch rate adapted learning_rate :              0.9268298233793493
      loss :              1.9333333333333333
      loss_factor :              0.19333333333333333
      loss adapted learning_rate :              3.959805771744737e-72
    epoch : 3 ; learning_rate : 3.959805771744737e-72 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 3, Loss: 1.802733333333352, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.9268298233793493
      loss :              1.7666666666666666
      loss_factor :              0.17666666666666667
      loss adapted learning_rate :              4.81350567432495e-76
    epoch : 4 ; learning_rate : 4.81350567432495e-76 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 4, Loss: 1.8027333333333517, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.9268298233793493
      loss :              1.8666666666666667
      loss_factor :              0.18666666666666668
      loss adapted learning_rate :              1.1848878021699235e-73
    epoch : 5 ; learning_rate : 1.1848878021699235e-73 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 5, Loss: 1.802733333333351, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.9268298233793493
      loss :              1.7666666666666666
      loss_factor :              0.17666666666666667
      loss adapted learning_rate :              4.81350567432495e-76
    epoch : 6 ; learning_rate : 4.81350567432495e-76 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4666666666666666
    Epoch 6, Loss: 1.8027333333333515, fit: 0.03333333333333333
      batch rate adapted learning_rate :              0.9268298233793493
      loss :              1.9333333333333333
      loss_factor :              0.19333333333333333
      loss adapted learning_rate :              3.959805771744737e-72
    epoch : 7 ; learning_rate : 3.959805771744737e-72 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 7, Loss: 1.8027333333333517, fit: 0.1
      batch rate adapted learning_rate :              0.9268298233793493
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.1206891451660876e-75
    epoch : 8 ; learning_rate : 3.1206891451660876e-75 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5666666666666667
    Epoch 8, Loss: 1.8027333333333513, fit: 0.15
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7_fit_0.15_2024-01-03_185341
  self.fit : 0.15
  self.loss : 1.7
  current_accuracy : 0.0958
   Accuracy mean: 0.0958
   Accuracy mean: 0.0958
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9616122783836647
      loss_factor :              1
      loss adapted learning_rate :              0.9616122783836647
    epoch : 0 ; learning_rate : 0.9616122783836647 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4037141370579322
    Epoch 0, Loss: 1.8044349103414452, fit: 0.11666666666666667
    Epoch 0, Loss: 1.8044349103414452
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7666666666666666_fit_0.11666666666666667_2024-01-03_185348
  self.fit : 0.11666666666666667
  self.loss : 1.7666666666666666
  current_accuracy : 0.0974
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9616122783836647
      loss_factor :              1
      loss adapted learning_rate :              0.9616122783836647
    epoch : 0 ; learning_rate : 0.9616122783836647 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 0, Loss: 1.804966666666685, fit: 0.05
    Epoch 0, Loss: 1.804966666666685
      batch rate adapted learning_rate :              0.9616122783836647
      loss :              1.9
      loss_factor :              0.19
      loss adapted learning_rate :              7.217056334167576e-73
    epoch : 1 ; learning_rate : 7.217056334167576e-73 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4333333333333333
    Epoch 1, Loss: 1.8049666666666846, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.9616122783836647
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              2.0283527138143808e-74
    epoch : 2 ; learning_rate : 2.0283527138143808e-74 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 2, Loss: 1.804966666666685, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.9616122783836647
      loss :              1.7333333333333334
      loss_factor :              0.17333333333333334
      loss adapted learning_rate :              7.43376608995735e-77
    epoch : 3 ; learning_rate : 7.43376608995735e-77 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 3, Loss: 1.8049666666666848, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.9616122783836647
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              2.0283527138143808e-74
    epoch : 4 ; learning_rate : 2.0283527138143808e-74 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 4, Loss: 1.8049666666666844, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.9616122783836647
      loss :              1.7666666666666666
      loss_factor :              0.17666666666666667
      loss adapted learning_rate :              4.994148916813381e-76
    epoch : 5 ; learning_rate : 4.994148916813381e-76 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 5, Loss: 1.8049666666666848, fit: 0.15
      batch rate adapted learning_rate :              0.9616122783836647
      loss :              1.7
      loss_factor :              0.16999999999999998
      loss adapted learning_rate :              1.0663312523579362e-77
    epoch : 6 ; learning_rate : 1.0663312523579362e-77 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 6, Loss: 1.8049666666666848, fit: 0.05
      batch rate adapted learning_rate :              0.9616122783836647
      loss :              1.9
      loss_factor :              0.19
      loss adapted learning_rate :              7.217056334167576e-73
    epoch : 7 ; learning_rate : 7.217056334167576e-73 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 7, Loss: 1.8049666666666848, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.9616122783836647
      loss :              1.6333333333333333
      loss_factor :              0.16333333333333333
      loss adapted learning_rate :              1.9520122144837236e-79
    epoch : 8 ; learning_rate : 1.9520122144837236e-79 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.4666666666666666
    Epoch 8, Loss: 1.8049666666666848, fit: 0.13333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7333333333333334_fit_0.13333333333333333_2024-01-03_185449
  self.fit : 0.13333333333333333
  self.loss : 1.7333333333333334
  current_accuracy : 0.0974
   Accuracy mean: 0.0974
   Accuracy mean: 0.0974
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9772372209558107
      loss_factor :              1
      loss adapted learning_rate :              0.9772372209558107
    epoch : 0 ; learning_rate : 0.9772372209558107 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.455911745656965
    Epoch 0, Loss: 1.801721472050736, fit: 0.05
    Epoch 0, Loss: 1.801721472050736
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.9_fit_0.05_2024-01-03_185457
  self.fit : 0.05
  self.loss : 1.9
  current_accuracy : 0.0958
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9772372209558107
      loss_factor :              1
      loss adapted learning_rate :              0.9772372209558107
    epoch : 0 ; learning_rate : 0.9772372209558107 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5666666666666667
    Epoch 0, Loss: 1.8027333333333517, fit: 0.08333333333333333
    Epoch 0, Loss: 1.8027333333333517
      batch rate adapted learning_rate :              0.9772372209558107
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              2.0613107940945924e-74
    epoch : 1 ; learning_rate : 2.0613107940945924e-74 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 1, Loss: 1.8027333333333515, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.9772372209558107
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              2.0613107940945924e-74
    epoch : 2 ; learning_rate : 2.0613107940945924e-74 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5666666666666667
    Epoch 2, Loss: 1.8027333333333522, fit: 0.1
      batch rate adapted learning_rate :              0.9772372209558107
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.290413742373562e-75
    epoch : 3 ; learning_rate : 3.290413742373562e-75 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5666666666666667
    Epoch 3, Loss: 1.8027333333333515, fit: 0.05
      batch rate adapted learning_rate :              0.9772372209558107
      loss :              1.9
      loss_factor :              0.19
      loss adapted learning_rate :              7.334324065971974e-73
    epoch : 4 ; learning_rate : 7.334324065971974e-73 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 4, Loss: 1.8027333333333517, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.9772372209558107
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              2.0613107940945924e-74
    epoch : 5 ; learning_rate : 2.0613107940945924e-74 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 5, Loss: 1.802733333333352, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.9772372209558107
      loss :              1.8666666666666667
      loss_factor :              0.18666666666666668
      loss adapted learning_rate :              1.2493301723018055e-73
    epoch : 6 ; learning_rate : 1.2493301723018055e-73 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 6, Loss: 1.8027333333333515, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.9772372209558107
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              2.0613107940945924e-74
    epoch : 7 ; learning_rate : 2.0613107940945924e-74 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4666666666666666
    Epoch 7, Loss: 1.8027333333333515, fit: 0.05
      batch rate adapted learning_rate :              0.9772372209558107
      loss :              1.9
      loss_factor :              0.19
      loss adapted learning_rate :              7.334324065971974e-73
    epoch : 8 ; learning_rate : 7.334324065971974e-73 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 8, Loss: 1.8027333333333515, fit: 0.15
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7_fit_0.15_2024-01-03_185600
  self.fit : 0.15
  self.loss : 1.7
  current_accuracy : 0.0958
   Accuracy mean: 0.0958
   Accuracy mean: 0.0958
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9840111057611337
      loss_factor :              1
      loss adapted learning_rate :              0.9840111057611337
    epoch : 0 ; learning_rate : 0.9840111057611337 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.1565052248872274
    Epoch 0, Loss: 1.7995988490624166, fit: 0.06666666666666667
    Epoch 0, Loss: 1.7995988490624166
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.8666666666666667_fit_0.06666666666666667_2024-01-03_185607
  self.fit : 0.06666666666666667
  self.loss : 1.8666666666666667
  current_accuracy : 0.1032
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9840111057611337
      loss_factor :              1
      loss adapted learning_rate :              0.9840111057611337
    epoch : 0 ; learning_rate : 0.9840111057611337 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 0, Loss: 1.8014000000000185, fit: 0.11666666666666667
    Epoch 0, Loss: 1.8014000000000185
      batch rate adapted learning_rate :              0.9840111057611337
      loss :              1.7666666666666666
      loss_factor :              0.17666666666666667
      loss adapted learning_rate :              5.110477588981651e-76
    epoch : 1 ; learning_rate : 5.110477588981651e-76 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 1, Loss: 1.8014000000000183, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.9840111057611337
      loss :              1.7666666666666666
      loss_factor :              0.17666666666666667
      loss adapted learning_rate :              5.110477588981651e-76
    epoch : 2 ; learning_rate : 5.110477588981651e-76 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 2, Loss: 1.8014000000000185, fit: 0.1
      batch rate adapted learning_rate :              0.9840111057611337
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.313221800821121e-75
    epoch : 3 ; learning_rate : 3.313221800821121e-75 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 3, Loss: 1.8014000000000185, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.9840111057611337
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              2.0755991179199055e-74
    epoch : 4 ; learning_rate : 2.0755991179199055e-74 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 4, Loss: 1.8014000000000185, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.9840111057611337
      loss :              1.7666666666666666
      loss_factor :              0.17666666666666667
      loss adapted learning_rate :              5.110477588981651e-76
    epoch : 5 ; learning_rate : 5.110477588981651e-76 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 5, Loss: 1.801400000000018, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.9840111057611337
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              2.0755991179199055e-74
    epoch : 6 ; learning_rate : 2.0755991179199055e-74 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4666666666666666
    Epoch 6, Loss: 1.8014000000000179, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.9840111057611337
      loss :              1.7666666666666666
      loss_factor :              0.17666666666666667
      loss adapted learning_rate :              5.110477588981651e-76
    epoch : 7 ; learning_rate : 5.110477588981651e-76 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 7, Loss: 1.801400000000018, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.9840111057611337
      loss :              1.6666666666666667
      loss_factor :              0.16666666666666669
      loss adapted learning_rate :              1.5061733591634523e-78
    epoch : 8 ; learning_rate : 1.5061733591634523e-78 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 8, Loss: 1.8014000000000185, fit: 0.05
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.9_fit_0.05_2024-01-03_185711
  self.fit : 0.05
  self.loss : 1.9
  current_accuracy : 0.1032
   Accuracy mean: 0.1032
   Accuracy mean: 0.1032
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9908319448927676
      loss_factor :              1
      loss adapted learning_rate :              0.9908319448927676
    epoch : 0 ; learning_rate : 0.9908319448927676 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.1711417700417859
    Epoch 0, Loss: 1.8000388940987249, fit: 0.06666666666666667
    Epoch 0, Loss: 1.8000388940987249
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.8666666666666667_fit_0.06666666666666667_2024-01-03_185718
  self.fit : 0.06666666666666667
  self.loss : 1.8666666666666667
  current_accuracy : 0.1032
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9908319448927676
      loss_factor :              1
      loss adapted learning_rate :              0.9908319448927676
    epoch : 0 ; learning_rate : 0.9908319448927676 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 0, Loss: 1.801400000000018, fit: 0.08333333333333333
    Epoch 0, Loss: 1.801400000000018
      batch rate adapted learning_rate :              0.9908319448927676
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              2.0899864836744235e-74
    epoch : 1 ; learning_rate : 2.0899864836744235e-74 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 1, Loss: 1.801400000000018, fit: 0.1
      batch rate adapted learning_rate :              0.9908319448927676
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.3361879571769917e-75
    epoch : 2 ; learning_rate : 3.3361879571769917e-75 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5666666666666667
    Epoch 2, Loss: 1.8014000000000177, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.9908319448927676
      loss :              1.7333333333333334
      loss_factor :              0.17333333333333334
      loss adapted learning_rate :              7.659649401701596e-77
    epoch : 3 ; learning_rate : 7.659649401701596e-77 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.4666666666666666
    Epoch 3, Loss: 1.801400000000018, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.9908319448927676
      loss :              1.8666666666666667
      loss_factor :              0.18666666666666668
      loss adapted learning_rate :              1.266710086241168e-73
    epoch : 4 ; learning_rate : 1.266710086241168e-73 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 4, Loss: 1.8014000000000183, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.9908319448927676
      loss :              1.7666666666666666
      loss_factor :              0.17666666666666667
      loss adapted learning_rate :              5.1459017273030387e-76
    epoch : 5 ; learning_rate : 5.1459017273030387e-76 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5666666666666667
    Epoch 5, Loss: 1.8014000000000185, fit: 0.03333333333333333
      batch rate adapted learning_rate :              0.9908319448927676
      loss :              1.9333333333333333
      loss_factor :              0.19333333333333333
      loss adapted learning_rate :              4.2332496810577534e-72
    epoch : 6 ; learning_rate : 4.2332496810577534e-72 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.5666666666666667
    Epoch 6, Loss: 1.8014000000000183, fit: 0.1
      batch rate adapted learning_rate :              0.9908319448927676
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.3361879571769917e-75
    epoch : 7 ; learning_rate : 3.3361879571769917e-75 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 7, Loss: 1.8014000000000183, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.9908319448927676
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              2.0899864836744235e-74
    epoch : 8 ; learning_rate : 2.0899864836744235e-74 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 8, Loss: 1.8014000000000185, fit: 0.11666666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7666666666666666_fit_0.11666666666666667_2024-01-03_185822
  self.fit : 0.11666666666666667
  self.loss : 1.7666666666666666
  current_accuracy : 0.1032
   Accuracy mean: 0.1032
   Accuracy mean: 0.1032
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9931160484209338
      loss_factor :              1
      loss adapted learning_rate :              0.9931160484209338
    epoch : 0 ; learning_rate : 0.9931160484209338 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.006206115510545
    Epoch 0, Loss: 1.799784576483945, fit: 0.11666666666666667
    Epoch 0, Loss: 1.799784576483945
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7666666666666666_fit_0.11666666666666667_2024-01-03_185830
  self.fit : 0.11666666666666667
  self.loss : 1.7666666666666666
  current_accuracy : 0.1009
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9931160484209338
      loss_factor :              1
      loss adapted learning_rate :              0.9931160484209338
    epoch : 0 ; learning_rate : 0.9931160484209338 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4666666666666666
    Epoch 0, Loss: 1.801700000000018, fit: 0.1
    Epoch 0, Loss: 1.801700000000018
      batch rate adapted learning_rate :              0.9931160484209338
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.3438786646909063e-75
    epoch : 1 ; learning_rate : 3.3438786646909063e-75 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 1, Loss: 1.8017000000000187, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.9931160484209338
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              2.0948044000988854e-74
    epoch : 2 ; learning_rate : 2.0948044000988854e-74 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5666666666666667
    Epoch 2, Loss: 1.8017000000000185, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.9931160484209338
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              2.0948044000988854e-74
    epoch : 3 ; learning_rate : 2.0948044000988854e-74 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5666666666666667
    Epoch 3, Loss: 1.8017000000000176, fit: 0.1
      batch rate adapted learning_rate :              0.9931160484209338
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.3438786646909063e-75
    epoch : 4 ; learning_rate : 3.3438786646909063e-75 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 4, Loss: 1.8017000000000183, fit: 0.15
      batch rate adapted learning_rate :              0.9931160484209338
      loss :              1.7
      loss_factor :              0.16999999999999998
      loss adapted learning_rate :              1.1012657631924937e-77
    epoch : 5 ; learning_rate : 1.1012657631924937e-77 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 5, Loss: 1.8017000000000183, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.9931160484209338
      loss :              1.8666666666666667
      loss_factor :              0.18666666666666668
      loss adapted learning_rate :              1.2696301545656309e-73
    epoch : 6 ; learning_rate : 1.2696301545656309e-73 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 6, Loss: 1.8017000000000187, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.9931160484209338
      loss :              1.8666666666666667
      loss_factor :              0.18666666666666668
      loss adapted learning_rate :              1.2696301545656309e-73
    epoch : 7 ; learning_rate : 1.2696301545656309e-73 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.6
    Epoch 7, Loss: 1.801700000000018, fit: 0.05
      batch rate adapted learning_rate :              0.9931160484209338
      loss :              1.9
      loss_factor :              0.19
      loss adapted learning_rate :              7.453497245134104e-73
    epoch : 8 ; learning_rate : 7.453497245134104e-73 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5666666666666667
    Epoch 8, Loss: 1.8017000000000178, fit: 0.06666666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.8666666666666667_fit_0.06666666666666667_2024-01-03_185935
  self.fit : 0.06666666666666667
  self.loss : 1.8666666666666667
  current_accuracy : 0.1009
   Accuracy mean: 0.1009
   Accuracy mean: 0.1009
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9960956189881034
      loss_factor :              1
      loss adapted learning_rate :              0.9960956189881034
    epoch : 0 ; learning_rate : 0.9960956189881034 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 0, Loss: 1.7951328310402141, fit: 0.03333333333333333
    Epoch 0, Loss: 1.7951328310402141
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.9333333333333333_fit_0.03333333333333333_2024-01-03_185942
  self.fit : 0.03333333333333333
  self.loss : 1.9333333333333333
  current_accuracy : 0.101
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9960956189881034
      loss_factor :              1
      loss adapted learning_rate :              0.9960956189881034
    epoch : 0 ; learning_rate : 0.9960956189881034 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 0, Loss: 1.7956333333333523, fit: 0.05
    Epoch 0, Loss: 1.7956333333333523
      batch rate adapted learning_rate :              0.9960956189881034
      loss :              1.9
      loss_factor :              0.19
      loss adapted learning_rate :              7.475859406182042e-73
    epoch : 1 ; learning_rate : 7.475859406182042e-73 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 1, Loss: 1.795633333333352, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.9960956189881034
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              2.101089282459246e-74
    epoch : 2 ; learning_rate : 2.101089282459246e-74 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 2, Loss: 1.7956333333333518, fit: 0.05
      batch rate adapted learning_rate :              0.9960956189881034
      loss :              1.9
      loss_factor :              0.19
      loss adapted learning_rate :              7.475859406182042e-73
    epoch : 3 ; learning_rate : 7.475859406182042e-73 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 3, Loss: 1.795633333333352, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.9960956189881034
      loss :              1.6666666666666667
      loss_factor :              0.16666666666666669
      loss adapted learning_rate :              1.5246704795458908e-78
    epoch : 4 ; learning_rate : 1.5246704795458908e-78 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 4, Loss: 1.7956333333333518, fit: 0.18333333333333332
      batch rate adapted learning_rate :              0.9960956189881034
      loss :              1.6333333333333333
      loss_factor :              0.16333333333333333
      loss adapted learning_rate :              2.022011218832138e-79
    epoch : 5 ; learning_rate : 2.022011218832138e-79 ; fit : 0.18333333333333332
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 5, Loss: 1.7956333333333516, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.9960956189881034
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              2.101089282459246e-74
    epoch : 6 ; learning_rate : 2.101089282459246e-74 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.4666666666666666
    Epoch 6, Loss: 1.7956333333333516, fit: 0.15
      batch rate adapted learning_rate :              0.9960956189881034
      loss :              1.7
      loss_factor :              0.16999999999999998
      loss adapted learning_rate :              1.1045698071255842e-77
    epoch : 7 ; learning_rate : 1.1045698071255842e-77 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 7, Loss: 1.7956333333333516, fit: 0.03333333333333333
      batch rate adapted learning_rate :              0.9960956189881034
      loss :              1.9333333333333333
      loss_factor :              0.19333333333333333
      loss adapted learning_rate :              4.2557383046837144e-72
    epoch : 8 ; learning_rate : 4.2557383046837144e-72 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 8, Loss: 1.7956333333333518, fit: 0.13333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7333333333333334_fit_0.13333333333333333_2024-01-03_190047
  self.fit : 0.13333333333333333
  self.loss : 1.7333333333333334
  current_accuracy : 0.101
   Accuracy mean: 0.101
   Accuracy mean: 0.101
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9963179161031344
      loss_factor :              1
      loss adapted learning_rate :              0.9963179161031344
    epoch : 0 ; learning_rate : 0.9963179161031344 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4699841549167636
    Epoch 0, Loss: 1.802322929230382, fit: 0.05
    Epoch 0, Loss: 1.802322929230382
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.9_fit_0.05_2024-01-03_190054
  self.fit : 0.05
  self.loss : 1.9
  current_accuracy : 0.0958
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9963179161031344
      loss_factor :              1
      loss adapted learning_rate :              0.9963179161031344
    epoch : 0 ; learning_rate : 0.9963179161031344 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 0, Loss: 1.8027333333333517, fit: 0.06666666666666667
    Epoch 0, Loss: 1.8027333333333517
      batch rate adapted learning_rate :              0.9963179161031344
      loss :              1.8666666666666667
      loss_factor :              0.18666666666666668
      loss adapted learning_rate :              1.2737235208612563e-73
    epoch : 1 ; learning_rate : 1.2737235208612563e-73 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 1, Loss: 1.802733333333351, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.9963179161031344
      loss :              1.7666666666666666
      loss_factor :              0.17666666666666667
      loss adapted learning_rate :              5.174393207490848e-76
    epoch : 2 ; learning_rate : 5.174393207490848e-76 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 2, Loss: 1.802733333333352, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.9963179161031344
      loss :              1.7333333333333334
      loss_factor :              0.17333333333333334
      loss adapted learning_rate :              7.702058829774472e-77
    epoch : 3 ; learning_rate : 7.702058829774472e-77 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 3, Loss: 1.8027333333333513, fit: 0.1
      batch rate adapted learning_rate :              0.9963179161031344
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.3546595367216195e-75
    epoch : 4 ; learning_rate : 3.3546595367216195e-75 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 4, Loss: 1.8027333333333515, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.9963179161031344
      loss :              1.8666666666666667
      loss_factor :              0.18666666666666668
      loss adapted learning_rate :              1.2737235208612563e-73
    epoch : 5 ; learning_rate : 1.2737235208612563e-73 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 5, Loss: 1.8027333333333515, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.9963179161031344
      loss :              1.8666666666666667
      loss_factor :              0.18666666666666668
      loss adapted learning_rate :              1.2737235208612563e-73
    epoch : 6 ; learning_rate : 1.2737235208612563e-73 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5666666666666667
    Epoch 6, Loss: 1.8027333333333517, fit: 0.05
      batch rate adapted learning_rate :              0.9963179161031344
      loss :              1.9
      loss_factor :              0.19
      loss adapted learning_rate :              7.477527782135809e-73
    epoch : 7 ; learning_rate : 7.477527782135809e-73 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.4333333333333333
    Epoch 7, Loss: 1.8027333333333515, fit: 0.05
      batch rate adapted learning_rate :              0.9963179161031344
      loss :              1.9
      loss_factor :              0.19
      loss adapted learning_rate :              7.477527782135809e-73
    epoch : 8 ; learning_rate : 7.477527782135809e-73 ; fit : 0.05
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 8, Loss: 1.8027333333333517, fit: 0.06666666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.8666666666666667_fit_0.06666666666666667_2024-01-03_190158
  self.fit : 0.06666666666666667
  self.loss : 1.8666666666666667
  current_accuracy : 0.0958
   Accuracy mean: 0.0958
   Accuracy mean: 0.0958
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9964995828970431
      loss_factor :              1
      loss adapted learning_rate :              0.9964995828970431
    epoch : 0 ; learning_rate : 0.9964995828970431 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.349887323949233
    Epoch 0, Loss: 1.8042100343448564, fit: 0.1
    Epoch 0, Loss: 1.8042100343448564
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.8_fit_0.1_2024-01-03_190206
  self.fit : 0.1
  self.loss : 1.8
  current_accuracy : 0.0974
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9964995828970431
      loss_factor :              1
      loss adapted learning_rate :              0.9964995828970431
    epoch : 0 ; learning_rate : 0.9964995828970431 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 0, Loss: 1.8049666666666853, fit: 0.1
    Epoch 0, Loss: 1.8049666666666853
      batch rate adapted learning_rate :              0.9964995828970431
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.3552712192306276e-75
    epoch : 1 ; learning_rate : 3.3552712192306276e-75 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.4333333333333333
    Epoch 1, Loss: 1.8049666666666853, fit: 0.15
      batch rate adapted learning_rate :              0.9964995828970431
      loss :              1.7
      loss_factor :              0.16999999999999998
      loss adapted learning_rate :              1.1050177624508335e-77
    epoch : 2 ; learning_rate : 1.1050177624508335e-77 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.4666666666666666
    Epoch 2, Loss: 1.8049666666666846, fit: 0.11666666666666667
      batch rate adapted learning_rate :              0.9964995828970431
      loss :              1.7666666666666666
      loss_factor :              0.17666666666666667
      loss adapted learning_rate :              5.1753366969225195e-76
    epoch : 3 ; learning_rate : 5.1753366969225195e-76 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.4666666666666666
    Epoch 3, Loss: 1.804966666666685, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.9964995828970431
      loss :              1.8666666666666667
      loss_factor :              0.18666666666666668
      loss adapted learning_rate :              1.2739557692878089e-73
    epoch : 4 ; learning_rate : 1.2739557692878089e-73 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4333333333333333
    Epoch 4, Loss: 1.8049666666666848, fit: 0.08333333333333333
      batch rate adapted learning_rate :              0.9964995828970431
      loss :              1.8333333333333333
      loss_factor :              0.18333333333333332
      loss adapted learning_rate :              2.1019413735872405e-74
    epoch : 5 ; learning_rate : 2.1019413735872405e-74 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 5, Loss: 1.804966666666685, fit: 0.1
      batch rate adapted learning_rate :              0.9964995828970431
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.3552712192306276e-75
    epoch : 6 ; learning_rate : 3.3552712192306276e-75 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 6, Loss: 1.8049666666666846, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.9964995828970431
      loss :              1.8666666666666667
      loss_factor :              0.18666666666666668
      loss adapted learning_rate :              1.2739557692878089e-73
    epoch : 7 ; learning_rate : 1.2739557692878089e-73 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.4666666666666666
    Epoch 7, Loss: 1.8049666666666846, fit: 0.15
      batch rate adapted learning_rate :              0.9964995828970431
      loss :              1.7
      loss_factor :              0.16999999999999998
      loss adapted learning_rate :              1.1050177624508335e-77
    epoch : 8 ; learning_rate : 1.1050177624508335e-77 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 8, Loss: 1.8049666666666853, fit: 0.06666666666666667
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.8666666666666667_fit_0.06666666666666667_2024-01-03_190309
  self.fit : 0.06666666666666667
  self.loss : 1.8666666666666667
  current_accuracy : 0.0974
   Accuracy mean: 0.0974
   Accuracy mean: 0.0974
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9977000638225533
      loss_factor :              1
      loss adapted learning_rate :              0.9977000638225533
    epoch : 0 ; learning_rate : 0.9977000638225533 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.4994127145991247
    Epoch 0, Loss: 1.7947124132027232, fit: 0.2
    Epoch 0, Loss: 1.7947124132027232
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6_fit_0.2_2024-01-03_190317
  self.fit : 0.2
  self.loss : 1.6
  current_accuracy : 0.101
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9977000638225533
      loss_factor :              1
      loss adapted learning_rate :              0.9977000638225533
    epoch : 0 ; learning_rate : 0.9977000638225533 ; fit : 0.2
    batch_size :          60
    best_batch_loss : 1.5666666666666667
    Epoch 0, Loss: 1.795633333333352, fit: 0.11666666666666667
    Epoch 0, Loss: 1.795633333333352
      batch rate adapted learning_rate :              0.9977000638225533
      loss :              1.7666666666666666
      loss_factor :              0.17666666666666667
      loss adapted learning_rate :              5.18157141402063e-76
    epoch : 1 ; learning_rate : 5.18157141402063e-76 ; fit : 0.11666666666666667
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 1, Loss: 1.795633333333352, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.9977000638225533
      loss :              1.7333333333333334
      loss_factor :              0.17333333333333334
      loss adapted learning_rate :              7.71274355487511e-77
    epoch : 2 ; learning_rate : 7.71274355487511e-77 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.5666666666666667
    Epoch 2, Loss: 1.795633333333352, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.9977000638225533
      loss :              1.7333333333333334
      loss_factor :              0.17333333333333334
      loss adapted learning_rate :              7.71274355487511e-77
    epoch : 3 ; learning_rate : 7.71274355487511e-77 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 3, Loss: 1.7956333333333523, fit: 0.1
      batch rate adapted learning_rate :              0.9977000638225533
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.359313307323519e-75
    epoch : 4 ; learning_rate : 3.359313307323519e-75 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 4, Loss: 1.7956333333333516, fit: 0.1
      batch rate adapted learning_rate :              0.9977000638225533
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.359313307323519e-75
    epoch : 5 ; learning_rate : 3.359313307323519e-75 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 5, Loss: 1.7956333333333516, fit: 0.1
      batch rate adapted learning_rate :              0.9977000638225533
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.359313307323519e-75
    epoch : 6 ; learning_rate : 3.359313307323519e-75 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5666666666666667
    Epoch 6, Loss: 1.7956333333333523, fit: 0.15
      batch rate adapted learning_rate :              0.9977000638225533
      loss :              1.7
      loss_factor :              0.16999999999999998
      loss adapted learning_rate :              1.106348974996167e-77
    epoch : 7 ; learning_rate : 1.106348974996167e-77 ; fit : 0.15
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 7, Loss: 1.7956333333333518, fit: 0.1
      batch rate adapted learning_rate :              0.9977000638225533
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.359313307323519e-75
    epoch : 8 ; learning_rate : 3.359313307323519e-75 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 8, Loss: 1.7956333333333518, fit: 0.03333333333333333
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.9333333333333333_fit_0.03333333333333333_2024-01-03_190420
  self.fit : 0.03333333333333333
  self.loss : 1.9333333333333333
  current_accuracy : 0.101
   Accuracy mean: 0.101
   Accuracy mean: 0.101
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9998946450345664
      loss_factor :              1
      loss adapted learning_rate :              0.9998946450345664
    epoch : 0 ; learning_rate : 0.9998946450345664 ; fit : 0.0
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 0, Loss: 1.8026479031689675, fit: 0.08333333333333333
    Epoch 0, Loss: 1.8026479031689675
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.8333333333333333_fit_0.08333333333333333_2024-01-03_190427
  self.fit : 0.08333333333333333
  self.loss : 1.8333333333333333
  current_accuracy : 0.0958
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.001
      batch rate adapted learning_rate :              0.9998946450345664
      loss_factor :              1
      loss adapted learning_rate :              0.9998946450345664
    epoch : 0 ; learning_rate : 0.9998946450345664 ; fit : 0.08333333333333333
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 0, Loss: 1.8027333333333515, fit: 0.03333333333333333
    Epoch 0, Loss: 1.8027333333333515
      batch rate adapted learning_rate :              0.9998946450345664
      loss :              1.9333333333333333
      loss_factor :              0.19333333333333333
      loss adapted learning_rate :              4.271969337486416e-72
    epoch : 1 ; learning_rate : 4.271969337486416e-72 ; fit : 0.03333333333333333
    batch_size :          60
    best_batch_loss : 1.5666666666666667
    Epoch 1, Loss: 1.8027333333333515, fit: 0.21666666666666667
      batch rate adapted learning_rate :              0.9998946450345664
      loss :              1.5666666666666667
      loss_factor :              0.15666666666666668
      loss adapted learning_rate :              3.1449559301640964e-81
    epoch : 2 ; learning_rate : 3.1449559301640964e-81 ; fit : 0.21666666666666667
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 2, Loss: 1.8027333333333515, fit: 0.1
      batch rate adapted learning_rate :              0.9998946450345664
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.3667025880671446e-75
    epoch : 3 ; learning_rate : 3.3667025880671446e-75 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5666666666666667
    Epoch 3, Loss: 1.8027333333333513, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.9998946450345664
      loss :              1.7333333333333334
      loss_factor :              0.17333333333333334
      loss adapted learning_rate :              7.729708815991516e-77
    epoch : 4 ; learning_rate : 7.729708815991516e-77 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 4, Loss: 1.8027333333333515, fit: 0.06666666666666667
      batch rate adapted learning_rate :              0.9998946450345664
      loss :              1.8666666666666667
      loss_factor :              0.18666666666666668
      loss adapted learning_rate :              1.2782961213275098e-73
    epoch : 5 ; learning_rate : 1.2782961213275098e-73 ; fit : 0.06666666666666667
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 5, Loss: 1.8027333333333517, fit: 0.16666666666666666
      batch rate adapted learning_rate :              0.9998946450345664
      loss :              1.6666666666666667
      loss_factor :              0.16666666666666669
      loss adapted learning_rate :              1.530485446255565e-78
    epoch : 6 ; learning_rate : 1.530485446255565e-78 ; fit : 0.16666666666666666
    batch_size :          60
    best_batch_loss : 1.4333333333333333
    Epoch 6, Loss: 1.8027333333333517, fit: 0.1
      batch rate adapted learning_rate :              0.9998946450345664
      loss :              1.8
      loss_factor :              0.18
      loss adapted learning_rate :              3.3667025880671446e-75
    epoch : 7 ; learning_rate : 3.3667025880671446e-75 ; fit : 0.1
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 7, Loss: 1.8027333333333515, fit: 0.13333333333333333
      batch rate adapted learning_rate :              0.9998946450345664
      loss :              1.7333333333333334
      loss_factor :              0.17333333333333334
      loss adapted learning_rate :              7.729708815991516e-77
    epoch : 8 ; learning_rate : 7.729708815991516e-77 ; fit : 0.13333333333333333
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 8, Loss: 1.802733333333352, fit: 0.15
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7_fit_0.15_2024-01-03_190530
  self.fit : 0.15
  self.loss : 1.7
  current_accuracy : 0.0958
   Accuracy mean: 0.0958
   Accuracy mean: 0.0958
  Error saving file: doc/out/test_combinations_results/20240103190530.
  normalized_accuracies :      [0.         0.         0.47142857 0.47142857 0.58571429 0.58571429
   0.47142857 0.47142857 1.         1.         1.         1.
   0.83571429 0.83571429 0.84285714 0.84285714 0.47142857 0.47142857
   0.58571429 0.58571429 0.84285714 0.84285714 0.47142857 0.47142857]
file_path :  src/mnist_train.csv
save_file :  src/mnist_train.npz
file_path :  src/mnist_test.csv
save_file :  src/mnist_test.npz
batch_rate :  1.0
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-64
    learning_rate : 1e-64
    batch_size :          60000
    best_batch_loss : 1.7329615954576512
    Epoch 0, Loss: 1.7329615954576512, fit: 0.10308333333333333
    Epoch 0, Loss: 1.7329615954576512
    Best fit: 0.10308333333333333
    Best loss: 1.7329615954576512
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7329615954576512_fit_0.10308333333333333_2024-01-03_190723
  self.fit : 0.10308333333333333
  self.loss : 1.7329615954576512
  current_accuracy : 0.1065
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-64
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-64
    learning_rate : 1e-64
    batch_size :          60000
    best_batch_loss : 1.7329615954576514
    Epoch 0, Loss: 1.7329615954576514, fit: 0.10308333333333333
    Epoch 0, Loss: 1.7329615954576514
    epoch : 1
      loss :              1.7329615954576514
      loss_factor :              2.4428082792496217e-08
      loss adapted learning_rate :              2.442808279249622e-72
    learning_rate : 2.442808279249622e-72
    batch_size :          60000
    best_batch_loss : 1.7329615954576518
    Epoch 1, Loss: 1.7329615954576518, fit: 0.10308333333333333
    epoch : 2
      loss :              1.7329615954576518
      loss_factor :              2.4428082792496296e-08
      loss adapted learning_rate :              2.4428082792496293e-72
    learning_rate : 2.4428082792496293e-72
    batch_size :          60000
    best_batch_loss : 1.732961595457652
    Epoch 2, Loss: 1.732961595457652, fit: 0.10308333333333333
    epoch : 3
      loss :              1.732961595457652
      loss_factor :              2.4428082792496336e-08
      loss adapted learning_rate :              2.4428082792496333e-72
    learning_rate : 2.4428082792496333e-72
    batch_size :          60000
    best_batch_loss : 1.7329615954576518
    Epoch 3, Loss: 1.7329615954576518, fit: 0.10308333333333333
    epoch : 4
      loss :              1.7329615954576518
      loss_factor :              2.4428082792496296e-08
      loss adapted learning_rate :              2.4428082792496293e-72
    learning_rate : 2.4428082792496293e-72
    batch_size :          60000
    best_batch_loss : 1.7329615954576514
    Epoch 4, Loss: 1.7329615954576514, fit: 0.10308333333333333
    epoch : 5
      loss :              1.7329615954576514
      loss_factor :              2.4428082792496217e-08
      loss adapted learning_rate :              2.442808279249622e-72
    learning_rate : 2.442808279249622e-72
    batch_size :          60000
    best_batch_loss : 1.7329615954576512
    Epoch 5, Loss: 1.7329615954576512, fit: 0.10308333333333333
    epoch : 6
      loss :              1.7329615954576512
      loss_factor :              2.442808279249618e-08
      loss adapted learning_rate :              2.4428082792496177e-72
    learning_rate : 2.4428082792496177e-72
    batch_size :          60000
    best_batch_loss : 1.7329615954576516
    Epoch 6, Loss: 1.7329615954576516, fit: 0.10308333333333333
    epoch : 7
      loss :              1.7329615954576516
      loss_factor :              2.4428082792496256e-08
      loss adapted learning_rate :              2.442808279249626e-72
    learning_rate : 2.442808279249626e-72
    batch_size :          60000
    best_batch_loss : 1.732961595457652
    Epoch 7, Loss: 1.732961595457652, fit: 0.10308333333333333
    epoch : 8
      loss :              1.732961595457652
      loss_factor :              2.4428082792496336e-08
      loss adapted learning_rate :              2.4428082792496333e-72
    learning_rate : 2.4428082792496333e-72
    batch_size :          60000
    best_batch_loss : 1.7329615954576514
    Epoch 8, Loss: 1.7329615954576514, fit: 0.10308333333333333
    Best fit: 0.10308333333333333
    Best loss: 1.7329615954576512
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7329615954576514_fit_0.10308333333333333_2024-01-03_190745
  self.fit : 0.10308333333333333
  self.loss : 1.7329615954576514
  current_accuracy : 0.1065
   Accuracy mean: 0.1065
   Accuracy mean: 0.1065
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-33
    learning_rate : 1e-33
    batch_size :          60000
    best_batch_loss : 1.7308901056867587
    Epoch 0, Loss: 1.7308901056867587, fit: 0.10806666666666667
    Epoch 0, Loss: 1.7308901056867587
    Best fit: 0.10806666666666667
    Best loss: 1.7308901056867587
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7308901056867587_fit_0.10806666666666667_2024-01-03_190748
  self.fit : 0.10806666666666667
  self.loss : 1.7308901056867587
  current_accuracy : 0.1054
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-33
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-33
    learning_rate : 1e-33
    batch_size :          60000
    best_batch_loss : 1.730890105686759
    Epoch 0, Loss: 1.730890105686759, fit: 0.10806666666666667
    Epoch 0, Loss: 1.730890105686759
    epoch : 1
      loss :              1.730890105686759
      loss_factor :              2.4137648223192028e-08
      loss adapted learning_rate :              2.4137648223192027e-41
    learning_rate : 2.4137648223192027e-41
    batch_size :          60000
    best_batch_loss : 1.7308901056867578
    Epoch 1, Loss: 1.7308901056867578, fit: 0.10806666666666667
    epoch : 2
      loss :              1.7308901056867578
      loss_factor :              2.4137648223191872e-08
      loss adapted learning_rate :              2.4137648223191874e-41
    learning_rate : 2.4137648223191874e-41
    batch_size :          60000
    best_batch_loss : 1.730890105686758
    Epoch 2, Loss: 1.730890105686758, fit: 0.10806666666666667
    epoch : 3
      loss :              1.730890105686758
      loss_factor :              2.4137648223191912e-08
      loss adapted learning_rate :              2.4137648223191915e-41
    learning_rate : 2.4137648223191915e-41
    batch_size :          60000
    best_batch_loss : 1.730890105686759
    Epoch 3, Loss: 1.730890105686759, fit: 0.10806666666666667
    epoch : 4
      loss :              1.730890105686759
      loss_factor :              2.4137648223192028e-08
      loss adapted learning_rate :              2.4137648223192027e-41
    learning_rate : 2.4137648223192027e-41
    batch_size :          60000
    best_batch_loss : 1.7308901056867592
    Epoch 4, Loss: 1.7308901056867592, fit: 0.10806666666666667
    epoch : 5
      loss :              1.7308901056867592
      loss_factor :              2.4137648223192064e-08
      loss adapted learning_rate :              2.413764822319207e-41
    learning_rate : 2.413764822319207e-41
    batch_size :          60000
    best_batch_loss : 1.7308901056867587
    Epoch 5, Loss: 1.7308901056867587, fit: 0.10806666666666667
    epoch : 6
      loss :              1.7308901056867587
      loss_factor :              2.4137648223191988e-08
      loss adapted learning_rate :              2.413764822319199e-41
    learning_rate : 2.413764822319199e-41
    batch_size :          60000
    best_batch_loss : 1.7308901056867583
    Epoch 6, Loss: 1.7308901056867583, fit: 0.10806666666666667
    epoch : 7
      loss :              1.7308901056867583
      loss_factor :              2.4137648223191912e-08
      loss adapted learning_rate :              2.4137648223191915e-41
    learning_rate : 2.4137648223191915e-41
    batch_size :          60000
    best_batch_loss : 1.7308901056867592
    Epoch 7, Loss: 1.7308901056867592, fit: 0.10806666666666667
    epoch : 8
      loss :              1.7308901056867592
      loss_factor :              2.4137648223192064e-08
      loss adapted learning_rate :              2.413764822319207e-41
    learning_rate : 2.413764822319207e-41
    batch_size :          60000
    best_batch_loss : 1.7308901056867585
    Epoch 8, Loss: 1.7308901056867585, fit: 0.10806666666666667
    Best fit: 0.10806666666666667
    Best loss: 1.7308901056867578
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7308901056867585_fit_0.10806666666666667_2024-01-03_190810
  self.fit : 0.10806666666666667
  self.loss : 1.7308901056867585
  current_accuracy : 0.1054
   Accuracy mean: 0.1054
   Accuracy mean: 0.1054
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-17
    learning_rate : 1e-17
    batch_size :          60000
    best_batch_loss : 1.705115816884242
    Epoch 0, Loss: 1.705115816884242, fit: 0.11606666666666667
    Epoch 0, Loss: 1.705115816884242
    Best fit: 0.11606666666666667
    Best loss: 1.705115816884242
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.705115816884242_fit_0.11606666666666667_2024-01-03_190812
  self.fit : 0.11606666666666667
  self.loss : 1.705115816884242
  current_accuracy : 0.1183
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-17
    learning_rate : 1e-17
    batch_size :          60000
    best_batch_loss : 1.7051158168842426
    Epoch 0, Loss: 1.7051158168842426, fit: 0.11606666666666667
    Epoch 0, Loss: 1.7051158168842426
    epoch : 1
      loss :              1.7051158168842426
      loss_factor :              2.0774894639082056e-08
      loss adapted learning_rate :              2.0774894639082057e-25
    learning_rate : 2.0774894639082057e-25
    batch_size :          60000
    best_batch_loss : 1.705115816884243
    Epoch 1, Loss: 1.705115816884243, fit: 0.11606666666666667
    epoch : 2
      loss :              1.705115816884243
      loss_factor :              2.0774894639082122e-08
      loss adapted learning_rate :              2.0774894639082125e-25
    learning_rate : 2.0774894639082125e-25
    batch_size :          60000
    best_batch_loss : 1.705115816884243
    Epoch 2, Loss: 1.705115816884243, fit: 0.11606666666666667
    epoch : 3
      loss :              1.705115816884243
      loss_factor :              2.0774894639082122e-08
      loss adapted learning_rate :              2.0774894639082125e-25
    learning_rate : 2.0774894639082125e-25
    batch_size :          60000
    best_batch_loss : 1.7051158168842424
    Epoch 3, Loss: 1.7051158168842424, fit: 0.11606666666666667
    epoch : 4
      loss :              1.7051158168842424
      loss_factor :              2.077489463908202e-08
      loss adapted learning_rate :              2.077489463908202e-25
    learning_rate : 2.077489463908202e-25
    batch_size :          60000
    best_batch_loss : 1.7051158168842422
    Epoch 4, Loss: 1.7051158168842422, fit: 0.11606666666666667
    epoch : 5
      loss :              1.7051158168842422
      loss_factor :              2.077489463908202e-08
      loss adapted learning_rate :              2.077489463908202e-25
    learning_rate : 2.077489463908202e-25
    batch_size :          60000
    best_batch_loss : 1.7051158168842424
    Epoch 5, Loss: 1.7051158168842424, fit: 0.11606666666666667
    epoch : 6
      loss :              1.7051158168842424
      loss_factor :              2.077489463908202e-08
      loss adapted learning_rate :              2.077489463908202e-25
    learning_rate : 2.077489463908202e-25
    batch_size :          60000
    best_batch_loss : 1.7051158168842422
    Epoch 6, Loss: 1.7051158168842422, fit: 0.11606666666666667
    epoch : 7
      loss :              1.7051158168842422
      loss_factor :              2.077489463908202e-08
      loss adapted learning_rate :              2.077489463908202e-25
    learning_rate : 2.077489463908202e-25
    batch_size :          60000
    best_batch_loss : 1.7051158168842424
    Epoch 7, Loss: 1.7051158168842424, fit: 0.11606666666666667
    epoch : 8
      loss :              1.7051158168842424
      loss_factor :              2.077489463908202e-08
      loss adapted learning_rate :              2.077489463908202e-25
    learning_rate : 2.077489463908202e-25
    batch_size :          60000
    best_batch_loss : 1.7051158168842426
    Epoch 8, Loss: 1.7051158168842426, fit: 0.11606666666666667
    Best fit: 0.11606666666666667
    Best loss: 1.7051158168842422
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7051158168842426_fit_0.11606666666666667_2024-01-03_190833
  self.fit : 0.11606666666666667
  self.loss : 1.7051158168842426
  current_accuracy : 0.1183
   Accuracy mean: 0.1183
   Accuracy mean: 0.1183
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-10
    learning_rate : 1e-10
    batch_size :          60000
    best_batch_loss : 1.7237130048692892
    Epoch 0, Loss: 1.7237130048692892, fit: 0.10651666666666666
    Epoch 0, Loss: 1.7237130048692892
    Best fit: 0.10651666666666666
    Best loss: 1.7237130048692892
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7237130048692892_fit_0.10651666666666666_2024-01-03_190835
  self.fit : 0.10651666666666666
  self.loss : 1.7237130048692892
  current_accuracy : 0.1035
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-10
    learning_rate : 1e-10
    batch_size :          60000
    best_batch_loss : 1.723713004744089
    Epoch 0, Loss: 1.723713004744089, fit: 0.10651666666666666
    Epoch 0, Loss: 1.723713004744089
    epoch : 1
      loss :              1.723713004744089
      loss_factor :              2.315525575417363e-08
      loss adapted learning_rate :              2.3155255754173632e-18
    learning_rate : 2.3155255754173632e-18
    batch_size :          60000
    best_batch_loss : 1.7237130046188887
    Epoch 1, Loss: 1.7237130046188887, fit: 0.10651666666666666
    epoch : 2
      loss :              1.7237130046188887
      loss_factor :              2.315525573735501e-08
      loss adapted learning_rate :              2.3155255737355013e-18
    learning_rate : 2.3155255737355013e-18
    batch_size :          60000
    best_batch_loss : 1.723713004618889
    Epoch 2, Loss: 1.723713004618889, fit: 0.10651666666666666
    epoch : 3
      loss :              1.723713004618889
      loss_factor :              2.3155255737355084e-08
      loss adapted learning_rate :              2.3155255737355087e-18
    learning_rate : 2.3155255737355087e-18
    batch_size :          60000
    best_batch_loss : 1.7237130046188889
    Epoch 3, Loss: 1.7237130046188889, fit: 0.10651666666666666
    epoch : 4
      loss :              1.7237130046188889
      loss_factor :              2.3155255737355048e-08
      loss adapted learning_rate :              2.315525573735505e-18
    learning_rate : 2.315525573735505e-18
    batch_size :          60000
    best_batch_loss : 1.723713004618888
    Epoch 4, Loss: 1.723713004618888, fit: 0.10651666666666666
    epoch : 5
      loss :              1.723713004618888
      loss_factor :              2.3155255737354935e-08
      loss adapted learning_rate :              2.3155255737354936e-18
    learning_rate : 2.3155255737354936e-18
    batch_size :          60000
    best_batch_loss : 1.723713004618888
    Epoch 5, Loss: 1.723713004618888, fit: 0.10651666666666666
    epoch : 6
      loss :              1.723713004618888
      loss_factor :              2.3155255737354935e-08
      loss adapted learning_rate :              2.3155255737354936e-18
    learning_rate : 2.3155255737354936e-18
    batch_size :          60000
    best_batch_loss : 1.7237130046188887
    Epoch 6, Loss: 1.7237130046188887, fit: 0.10651666666666666
    epoch : 7
      loss :              1.7237130046188887
      loss_factor :              2.315525573735501e-08
      loss adapted learning_rate :              2.3155255737355013e-18
    learning_rate : 2.3155255737355013e-18
    batch_size :          60000
    best_batch_loss : 1.7237130046188895
    Epoch 7, Loss: 1.7237130046188895, fit: 0.10651666666666666
    epoch : 8
      loss :              1.7237130046188895
      loss_factor :              2.315525573735512e-08
      loss adapted learning_rate :              2.315525573735512e-18
    learning_rate : 2.315525573735512e-18
    batch_size :          60000
    best_batch_loss : 1.7237130046188884
    Epoch 8, Loss: 1.7237130046188884, fit: 0.10651666666666666
    Best fit: 0.10651666666666666
    Best loss: 1.723713004618888
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7237130046188884_fit_0.10651666666666666_2024-01-03_190857
  self.fit : 0.10651666666666666
  self.loss : 1.7237130046188884
  current_accuracy : 0.1035
   Accuracy mean: 0.1035
   Accuracy mean: 0.1035
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-07
    learning_rate : 1e-07
    batch_size :          60000
    best_batch_loss : 1.6525101404908205
    Epoch 0, Loss: 1.6525101404908205, fit: 0.1429
    Epoch 0, Loss: 1.6525101404908205
    Best fit: 0.1429
    Best loss: 1.6525101404908205
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.6525101404908205_fit_0.1429_2024-01-03_190900
  self.fit : 0.1429
  self.loss : 1.6525101404908205
  current_accuracy : 0.1433
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-07
    learning_rate : 1e-07
    batch_size :          60000
    best_batch_loss : 1.6525099880082956
    Epoch 0, Loss: 1.6525099880082956, fit: 0.1429
    Epoch 0, Loss: 1.6525099880082956
    epoch : 1
      loss :              1.6525099880082956
      loss_factor :              1.5185913823861735e-08
      loss adapted learning_rate :              1.5185913823861735e-15
    learning_rate : 1.5185913823861735e-15
    batch_size :          60000
    best_batch_loss : 1.652509835526098
    Epoch 1, Loss: 1.652509835526098, fit: 0.1429
    epoch : 2
      loss :              1.652509835526098
      loss_factor :              1.518589981135612e-08
      loss adapted learning_rate :              1.518589981135612e-15
    learning_rate : 1.518589981135612e-15
    batch_size :          60000
    best_batch_loss : 1.6525098355260954
    Epoch 2, Loss: 1.6525098355260954, fit: 0.1429
    epoch : 3
      loss :              1.6525098355260954
      loss_factor :              1.5185899811355865e-08
      loss adapted learning_rate :              1.5185899811355865e-15
    learning_rate : 1.5185899811355865e-15
    batch_size :          60000
    best_batch_loss : 1.6525098355260934
    Epoch 3, Loss: 1.6525098355260934, fit: 0.1429
    epoch : 4
      loss :              1.6525098355260934
      loss_factor :              1.5185899811355687e-08
      loss adapted learning_rate :              1.5185899811355685e-15
    learning_rate : 1.5185899811355685e-15
    batch_size :          60000
    best_batch_loss : 1.652509835526091
    Epoch 4, Loss: 1.652509835526091, fit: 0.1429
    epoch : 5
      loss :              1.652509835526091
      loss_factor :              1.5185899811355458e-08
      loss adapted learning_rate :              1.5185899811355458e-15
    learning_rate : 1.5185899811355458e-15
    batch_size :          60000
    best_batch_loss : 1.6525098355260894
    Epoch 5, Loss: 1.6525098355260894, fit: 0.1429
    epoch : 6
      loss :              1.6525098355260894
      loss_factor :              1.518589981135533e-08
      loss adapted learning_rate :              1.5185899811355328e-15
    learning_rate : 1.5185899811355328e-15
    batch_size :          60000
    best_batch_loss : 1.652509835526086
    Epoch 6, Loss: 1.652509835526086, fit: 0.1429
    epoch : 7
      loss :              1.652509835526086
      loss_factor :              1.5185899811355025e-08
      loss adapted learning_rate :              1.5185899811355025e-15
    learning_rate : 1.5185899811355025e-15
    batch_size :          60000
    best_batch_loss : 1.6525098355260839
    Epoch 7, Loss: 1.6525098355260839, fit: 0.1429
    epoch : 8
      loss :              1.6525098355260839
      loss_factor :              1.518589981135482e-08
      loss adapted learning_rate :              1.518589981135482e-15
    learning_rate : 1.518589981135482e-15
    batch_size :          60000
    best_batch_loss : 1.6525098355260817
    Epoch 8, Loss: 1.6525098355260817, fit: 0.1429
    Best fit: 0.1429
    Best loss: 1.6525098355260817
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.6525098355260817_fit_0.1429_2024-01-03_190922
  self.fit : 0.1429
  self.loss : 1.6525098355260817
  current_accuracy : 0.1433
   Accuracy mean: 0.1433
   Accuracy mean: 0.1433
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    learning_rate : 0.0001
    batch_size :          60000
    best_batch_loss : 1.677723565129083
    Epoch 0, Loss: 1.677723565129083, fit: 0.13665
    Epoch 0, Loss: 1.677723565129083
    Best fit: 0.13665
    Best loss: 1.677723565129083
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.677723565129083_fit_0.13665_2024-01-03_190925
  self.fit : 0.13665
  self.loss : 1.677723565129083
  current_accuracy : 0.1354
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    learning_rate : 0.0001
    batch_size :          60000
    best_batch_loss : 1.677570395609172
    Epoch 0, Loss: 1.677570395609172, fit: 0.1367
    Epoch 0, Loss: 1.677570395609172
    epoch : 1
      loss :              1.677570395609172
      loss_factor :              1.765255342335994e-08
      loss adapted learning_rate :              1.7652553423359941e-12
    learning_rate : 1.7652553423359941e-12
    batch_size :          60000
    best_batch_loss : 1.677419443630311
    Epoch 1, Loss: 1.677419443630311, fit: 0.13668333333333332
    epoch : 2
      loss :              1.677419443630311
      loss_factor :              1.7636675644708755e-08
      loss adapted learning_rate :              1.7636675644708756e-12
    learning_rate : 1.7636675644708756e-12
    batch_size :          60000
    best_batch_loss : 1.6774194436276688
    Epoch 2, Loss: 1.6774194436276688, fit: 0.13668333333333332
    epoch : 3
      loss :              1.6774194436276688
      loss_factor :              1.7636675644430968e-08
      loss adapted learning_rate :              1.7636675644430968e-12
    learning_rate : 1.7636675644430968e-12
    batch_size :          60000
    best_batch_loss : 1.6774194436250303
    Epoch 3, Loss: 1.6774194436250303, fit: 0.13668333333333332
    epoch : 4
      loss :              1.6774194436250303
      loss_factor :              1.7636675644153555e-08
      loss adapted learning_rate :              1.7636675644153556e-12
    learning_rate : 1.7636675644153556e-12
    batch_size :          60000
    best_batch_loss : 1.6774194436223897
    Epoch 4, Loss: 1.6774194436223897, fit: 0.13668333333333332
    epoch : 5
      loss :              1.6774194436223897
      loss_factor :              1.7636675643875913e-08
      loss adapted learning_rate :              1.7636675643875913e-12
    learning_rate : 1.7636675643875913e-12
    batch_size :          60000
    best_batch_loss : 1.6774194436197505
    Epoch 5, Loss: 1.6774194436197505, fit: 0.13668333333333332
    epoch : 6
      loss :              1.6774194436197505
      loss_factor :              1.7636675643598414e-08
      loss adapted learning_rate :              1.7636675643598414e-12
    learning_rate : 1.7636675643598414e-12
    batch_size :          60000
    best_batch_loss : 1.6774194436171102
    Epoch 6, Loss: 1.6774194436171102, fit: 0.13668333333333332
    epoch : 7
      loss :              1.6774194436171102
      loss_factor :              1.763667564332083e-08
      loss adapted learning_rate :              1.763667564332083e-12
    learning_rate : 1.763667564332083e-12
    batch_size :          60000
    best_batch_loss : 1.6774194436144705
    Epoch 7, Loss: 1.6774194436144705, fit: 0.13668333333333332
    epoch : 8
      loss :              1.6774194436144705
      loss_factor :              1.763667564304327e-08
      loss adapted learning_rate :              1.763667564304327e-12
    learning_rate : 1.763667564304327e-12
    batch_size :          60000
    best_batch_loss : 1.6774194436118313
    Epoch 8, Loss: 1.6774194436118313, fit: 0.13668333333333332
    Best fit: 0.1367
    Best loss: 1.6774194436118313
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6774194436118313_fit_0.13668333333333332_2024-01-03_190949
  self.fit : 0.13668333333333332
  self.loss : 1.6774194436118313
  current_accuracy : 0.1354
   Accuracy mean: 0.1354
   Accuracy mean: 0.1354
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.001
    learning_rate : 0.001
    batch_size :          60000
    best_batch_loss : 1.753427917612778
    Epoch 0, Loss: 1.753427917612778, fit: 0.09618333333333333
    Epoch 0, Loss: 1.753427917612778
    Best fit: 0.09618333333333333
    Best loss: 1.753427917612778
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.753427917612778_fit_0.09618333333333333_2024-01-03_190951
  self.fit : 0.09618333333333333
  self.loss : 1.753427917612778
  current_accuracy : 0.0997
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.001
    learning_rate : 0.001
    batch_size :          60000
    best_batch_loss : 1.7526153043704822
    Epoch 0, Loss: 1.7526153043704822, fit: 0.09606666666666666
    Epoch 0, Loss: 1.7526153043704822
    epoch : 1
      loss :              1.7526153043704822
      loss_factor :              2.7344248843972895e-08
      loss adapted learning_rate :              2.7344248843972895e-11
    learning_rate : 2.7344248843972895e-11
    batch_size :          60000
    best_batch_loss : 1.7519624563447698
    Epoch 1, Loss: 1.7519624563447698, fit: 0.09606666666666666
    epoch : 2
      loss :              1.7519624563447698
      loss_factor :              2.7242562268618983e-08
      loss adapted learning_rate :              2.7242562268618984e-11
    learning_rate : 2.7242562268618984e-11
    batch_size :          60000
    best_batch_loss : 1.7519624563298952
    Epoch 2, Loss: 1.7519624563298952, fit: 0.09606666666666666
    epoch : 3
      loss :              1.7519624563298952
      loss_factor :              2.724256226630604e-08
      loss adapted learning_rate :              2.7242562266306042e-11
    learning_rate : 2.7242562266306042e-11
    batch_size :          60000
    best_batch_loss : 1.751962456315075
    Epoch 3, Loss: 1.751962456315075, fit: 0.09606666666666666
    epoch : 4
      loss :              1.751962456315075
      loss_factor :              2.7242562264001555e-08
      loss adapted learning_rate :              2.7242562264001556e-11
    learning_rate : 2.7242562264001556e-11
    batch_size :          60000
    best_batch_loss : 1.751962456300256
    Epoch 4, Loss: 1.751962456300256, fit: 0.09606666666666666
    epoch : 5
      loss :              1.751962456300256
      loss_factor :              2.7242562261697242e-08
      loss adapted learning_rate :              2.7242562261697244e-11
    learning_rate : 2.7242562261697244e-11
    batch_size :          60000
    best_batch_loss : 1.751962456285436
    Epoch 5, Loss: 1.751962456285436, fit: 0.09606666666666666
    epoch : 6
      loss :              1.751962456285436
      loss_factor :              2.7242562259392713e-08
      loss adapted learning_rate :              2.7242562259392715e-11
    learning_rate : 2.7242562259392715e-11
    batch_size :          60000
    best_batch_loss : 1.7519624562706153
    Epoch 6, Loss: 1.7519624562706153, fit: 0.09606666666666666
    epoch : 7
      loss :              1.7519624562706153
      loss_factor :              2.7242562257088185e-08
      loss adapted learning_rate :              2.7242562257088187e-11
    learning_rate : 2.7242562257088187e-11
    batch_size :          60000
    best_batch_loss : 1.7519624562557967
    Epoch 7, Loss: 1.7519624562557967, fit: 0.09606666666666666
    epoch : 8
      loss :              1.7519624562557967
      loss_factor :              2.7242562254783914e-08
      loss adapted learning_rate :              2.7242562254783914e-11
    learning_rate : 2.7242562254783914e-11
    batch_size :          60000
    best_batch_loss : 1.7519624562409761
    Epoch 8, Loss: 1.7519624562409761, fit: 0.09606666666666666
    Best fit: 0.09606666666666666
    Best loss: 1.7519624562409761
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7519624562409761_fit_0.09606666666666666_2024-01-03_191013
  self.fit : 0.09606666666666666
  self.loss : 1.7519624562409761
  current_accuracy : 0.1002
   Accuracy mean: 0.0997
   Accuracy mean: 0.1002
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.02
    learning_rate : 0.02
    batch_size :          60000
    best_batch_loss : 1.7593515385084832
    Epoch 0, Loss: 1.7593515385084832, fit: 0.09093333333333334
    Epoch 0, Loss: 1.7593515385084832
    Best fit: 0.09093333333333334
    Best loss: 1.7593515385084832
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7593515385084832_fit_0.09093333333333334_2024-01-03_191016
  self.fit : 0.09093333333333334
  self.loss : 1.7593515385084832
  current_accuracy : 0.0912
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.02
    learning_rate : 0.02
    batch_size :          60000
    best_batch_loss : 1.7451506086618753
    Epoch 0, Loss: 1.7451506086618753, fit: 0.09451666666666667
    Epoch 0, Loss: 1.7451506086618753
    epoch : 1
      loss :              1.7451506086618753
      loss_factor :              2.6201679537405988e-08
      loss adapted learning_rate :              5.240335907481198e-10
    learning_rate : 5.240335907481198e-10
    batch_size :          60000
    best_batch_loss : 1.7250791441267894
    Epoch 1, Loss: 1.7250791441267894, fit: 0.10283333333333333
    epoch : 2
      loss :              1.7250791441267894
      loss_factor :              2.333943006721942e-08
      loss adapted learning_rate :              4.667886013443884e-10
    learning_rate : 4.667886013443884e-10
    batch_size :          60000
    best_batch_loss : 1.7250791435476867
    Epoch 2, Loss: 1.7250791435476867, fit: 0.10283333333333333
    epoch : 3
      loss :              1.7250791435476867
      loss_factor :              2.3339429988869828e-08
      loss adapted learning_rate :              4.667885997773966e-10
    learning_rate : 4.667885997773966e-10
    batch_size :          60000
    best_batch_loss : 1.7250791430318457
    Epoch 3, Loss: 1.7250791430318457, fit: 0.10283333333333333
    epoch : 4
      loss :              1.7250791430318457
      loss_factor :              2.3339429919079207e-08
      loss adapted learning_rate :              4.667885983815842e-10
    learning_rate : 4.667885983815842e-10
    batch_size :          60000
    best_batch_loss : 1.7250791425160057
    Epoch 4, Loss: 1.7250791425160057, fit: 0.10283333333333333
    epoch : 5
      loss :              1.7250791425160057
      loss_factor :              2.3339429849288698e-08
      loss adapted learning_rate :              4.66788596985774e-10
    learning_rate : 4.66788596985774e-10
    batch_size :          60000
    best_batch_loss : 1.7250791420001637
    Epoch 5, Loss: 1.7250791420001637, fit: 0.10283333333333333
    epoch : 6
      loss :              1.7250791420001637
      loss_factor :              2.3339429779497964e-08
      loss adapted learning_rate :              4.667885955899593e-10
    learning_rate : 4.667885955899593e-10
    batch_size :          60000
    best_batch_loss : 1.725079141484323
    Epoch 6, Loss: 1.725079141484323, fit: 0.10283333333333333
    epoch : 7
      loss :              1.725079141484323
      loss_factor :              2.3339429709707382e-08
      loss adapted learning_rate :              4.667885941941477e-10
    learning_rate : 4.667885941941477e-10
    batch_size :          60000
    best_batch_loss : 1.7250791409684811
    Epoch 7, Loss: 1.7250791409684811, fit: 0.10283333333333333
    epoch : 8
      loss :              1.7250791409684811
      loss_factor :              2.333942963991665e-08
      loss adapted learning_rate :              4.66788592798333e-10
    learning_rate : 4.66788592798333e-10
    batch_size :          60000
    best_batch_loss : 1.7250791404526409
    Epoch 8, Loss: 1.7250791404526409, fit: 0.10283333333333333
    Best fit: 0.10283333333333333
    Best loss: 1.7250791404526409
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7250791404526409_fit_0.10283333333333333_2024-01-03_191039
  self.fit : 0.10283333333333333
  self.loss : 1.7250791404526409
  current_accuracy : 0.1003
   Accuracy mean: 0.0912
   Accuracy mean: 0.1003
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.025
    learning_rate : 0.025
    batch_size :          60000
    best_batch_loss : 1.7057817011303746
    Epoch 0, Loss: 1.7057817011303746, fit: 0.12468333333333333
    Epoch 0, Loss: 1.7057817011303746
    Best fit: 0.12468333333333333
    Best loss: 1.7057817011303746
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7057817011303746_fit_0.12468333333333333_2024-01-03_191041
  self.fit : 0.12468333333333333
  self.loss : 1.7057817011303746
  current_accuracy : 0.138
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.025
    learning_rate : 0.025
    batch_size :          60000
    best_batch_loss : 1.657882613212357
    Epoch 0, Loss: 1.657882613212357, fit: 0.13845
    Epoch 0, Loss: 1.657882613212357
    epoch : 1
      loss :              1.657882613212357
      loss_factor :              1.5686923178757925e-08
      loss adapted learning_rate :              3.9217307946894816e-10
    learning_rate : 3.9217307946894816e-10
    batch_size :          60000
    best_batch_loss : 1.6278705629748003
    Epoch 1, Loss: 1.6278705629748003, fit: 0.15241666666666667
    epoch : 2
      loss :              1.6278705629748003
      loss_factor :              1.3067686237688432e-08
      loss adapted learning_rate :              3.2669215594221085e-10
    learning_rate : 3.2669215594221085e-10
    batch_size :          60000
    best_batch_loss : 1.6278705625682839
    Epoch 2, Loss: 1.6278705625682839, fit: 0.15241666666666667
    epoch : 3
      loss :              1.6278705625682839
      loss_factor :              1.3067686205055433e-08
      loss adapted learning_rate :              3.266921551263858e-10
    learning_rate : 3.266921551263858e-10
    batch_size :          60000
    best_batch_loss : 1.627870562229642
    Epoch 3, Loss: 1.627870562229642, fit: 0.15241666666666667
    epoch : 4
      loss :              1.627870562229642
      loss_factor :              1.306768617787107e-08
      loss adapted learning_rate :              3.2669215444677677e-10
    learning_rate : 3.2669215444677677e-10
    batch_size :          60000
    best_batch_loss : 1.6278705618910017
    Epoch 4, Loss: 1.6278705618910017, fit: 0.15241666666666667
    epoch : 5
      loss :              1.6278705618910017
      loss_factor :              1.3067686150686796e-08
      loss adapted learning_rate :              3.2669215376716993e-10
    learning_rate : 3.2669215376716993e-10
    batch_size :          60000
    best_batch_loss : 1.6278705615523605
    Epoch 5, Loss: 1.6278705615523605, fit: 0.15241666666666667
    epoch : 6
      loss :              1.6278705615523605
      loss_factor :              1.3067686123502478e-08
      loss adapted learning_rate :              3.2669215308756196e-10
    learning_rate : 3.2669215308756196e-10
    batch_size :          60000
    best_batch_loss : 1.6278705612137188
    Epoch 6, Loss: 1.6278705612137188, fit: 0.15241666666666667
    epoch : 7
      loss :              1.6278705612137188
      loss_factor :              1.3067686096318094e-08
      loss adapted learning_rate :              3.266921524079524e-10
    learning_rate : 3.266921524079524e-10
    batch_size :          60000
    best_batch_loss : 1.6278705608750788
    Epoch 7, Loss: 1.6278705608750788, fit: 0.15241666666666667
    epoch : 8
      loss :              1.6278705608750788
      loss_factor :              1.3067686069133865e-08
      loss adapted learning_rate :              3.2669215172834664e-10
    learning_rate : 3.2669215172834664e-10
    batch_size :          60000
    best_batch_loss : 1.6278705605364374
    Epoch 8, Loss: 1.6278705605364374, fit: 0.15241666666666667
    Best fit: 0.15241666666666667
    Best loss: 1.6278705605364374
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.6278705605364374_fit_0.15241666666666667_2024-01-03_191104
  self.fit : 0.15241666666666667
  self.loss : 1.6278705605364374
  current_accuracy : 0.1521
   Accuracy mean: 0.138
   Accuracy mean: 0.1521
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.03
    learning_rate : 0.03
    batch_size :          60000
    best_batch_loss : 1.719610068090329
    Epoch 0, Loss: 1.719610068090329, fit: 0.11641666666666667
    Epoch 0, Loss: 1.719610068090329
    Best fit: 0.11641666666666667
    Best loss: 1.719610068090329
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.719610068090329_fit_0.11641666666666667_2024-01-03_191107
  self.fit : 0.11641666666666667
  self.loss : 1.719610068090329
  current_accuracy : 0.1211
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.03
    learning_rate : 0.03
    batch_size :          60000
    best_batch_loss : 1.694273268508306
    Epoch 0, Loss: 1.694273268508306, fit: 0.12038333333333333
    Epoch 0, Loss: 1.694273268508306
    epoch : 1
      loss :              1.694273268508306
      loss_factor :              1.9491020932203753e-08
      loss adapted learning_rate :              5.847306279661126e-10
    learning_rate : 5.847306279661126e-10
    batch_size :          60000
    best_batch_loss : 1.6774663105116396
    Epoch 1, Loss: 1.6774663105116396, fit: 0.12631666666666666
    epoch : 2
      loss :              1.6774663105116396
      loss_factor :              1.7641603928569268e-08
      loss adapted learning_rate :              5.29248117857078e-10
    learning_rate : 5.29248117857078e-10
    batch_size :          60000
    best_batch_loss : 1.6774663101078118
    Epoch 2, Loss: 1.6774663101078118, fit: 0.12631666666666666
    epoch : 3
      loss :              1.6774663101078118
      loss_factor :              1.7641603886099443e-08
      loss adapted learning_rate :              5.292481165829833e-10
    learning_rate : 5.292481165829833e-10
    batch_size :          60000
    best_batch_loss : 1.677466309742301
    Epoch 3, Loss: 1.677466309742301, fit: 0.12631666666666666
    epoch : 4
      loss :              1.677466309742301
      loss_factor :              1.764160384765933e-08
      loss adapted learning_rate :              5.292481154297799e-10
    learning_rate : 5.292481154297799e-10
    batch_size :          60000
    best_batch_loss : 1.67746630937679
    Epoch 4, Loss: 1.67746630937679, fit: 0.12631666666666666
    epoch : 5
      loss :              1.67746630937679
      loss_factor :              1.7641603809219215e-08
      loss adapted learning_rate :              5.292481142765765e-10
    learning_rate : 5.292481142765765e-10
    batch_size :          60000
    best_batch_loss : 1.677466309011279
    Epoch 5, Loss: 1.677466309011279, fit: 0.12631666666666666
    epoch : 6
      loss :              1.677466309011279
      loss_factor :              1.7641603770779103e-08
      loss adapted learning_rate :              5.292481131233731e-10
    learning_rate : 5.292481131233731e-10
    batch_size :          60000
    best_batch_loss : 1.677466308645768
    Epoch 6, Loss: 1.677466308645768, fit: 0.12631666666666666
    epoch : 7
      loss :              1.677466308645768
      loss_factor :              1.764160373233899e-08
      loss adapted learning_rate :              5.292481119701698e-10
    learning_rate : 5.292481119701698e-10
    batch_size :          60000
    best_batch_loss : 1.6774663082802566
    Epoch 7, Loss: 1.6774663082802566, fit: 0.12631666666666666
    epoch : 8
      loss :              1.6774663082802566
      loss_factor :              1.764160369389882e-08
      loss adapted learning_rate :              5.292481108169646e-10
    learning_rate : 5.292481108169646e-10
    batch_size :          60000
    best_batch_loss : 1.6774663079147456
    Epoch 8, Loss: 1.6774663079147456, fit: 0.12631666666666666
    Best fit: 0.12631666666666666
    Best loss: 1.6774663079147456
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6774663079147456_fit_0.12631666666666666_2024-01-03_191129
  self.fit : 0.12631666666666666
  self.loss : 1.6774663079147456
  current_accuracy : 0.131
   Accuracy mean: 0.1211
   Accuracy mean: 0.131
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.1
    learning_rate : 0.1
    batch_size :          60000
    best_batch_loss : 1.7684387381200068
    Epoch 0, Loss: 1.7684387381200068, fit: 0.08235
    Epoch 0, Loss: 1.7684387381200068
    Best fit: 0.08235
    Best loss: 1.7684387381200068
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.7684387381200068_fit_0.08235_2024-01-03_191132
  self.fit : 0.08235
  self.loss : 1.7684387381200068
  current_accuracy : 0.1403
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.1
    learning_rate : 0.1
    batch_size :          60000
    best_batch_loss : 1.647260913920876
    Epoch 0, Loss: 1.647260913920876, fit: 0.1406
    Epoch 0, Loss: 1.647260913920876
    epoch : 1
      loss :              1.647260913920876
      loss_factor :              1.4710381518972608e-08
      loss adapted learning_rate :              1.471038151897261e-09
    learning_rate : 1.471038151897261e-09
    batch_size :          60000
    best_batch_loss : 1.5559591182406574
    Epoch 1, Loss: 1.5559591182406574, fit: 0.18551666666666666
    epoch : 2
      loss :              1.5559591182406574
      loss_factor :              8.317289536337707e-09
      loss adapted learning_rate :              8.317289536337708e-10
    learning_rate : 8.317289536337708e-10
    batch_size :          60000
    best_batch_loss : 1.5559591163134079
    Epoch 2, Loss: 1.5559591163134079, fit: 0.18551666666666666
    epoch : 3
      loss :              1.5559591163134079
      loss_factor :              8.317289433317702e-09
      loss adapted learning_rate :              8.317289433317703e-10
    learning_rate : 8.317289433317703e-10
    batch_size :          60000
    best_batch_loss : 1.5559591152237358
    Epoch 3, Loss: 1.5559591152237358, fit: 0.18551666666666666
    epoch : 4
      loss :              1.5559591152237358
      loss_factor :              8.317289375069904e-09
      loss adapted learning_rate :              8.317289375069904e-10
    learning_rate : 8.317289375069904e-10
    batch_size :          60000
    best_batch_loss : 1.5559591141340627
    Epoch 4, Loss: 1.5559591141340627, fit: 0.18551666666666666
    epoch : 5
      loss :              1.5559591141340627
      loss_factor :              8.317289316822075e-09
      loss adapted learning_rate :              8.317289316822075e-10
    learning_rate : 8.317289316822075e-10
    batch_size :          60000
    best_batch_loss : 1.5559591130443897
    Epoch 5, Loss: 1.5559591130443897, fit: 0.18551666666666666
    epoch : 6
      loss :              1.5559591130443897
      loss_factor :              8.317289258574231e-09
      loss adapted learning_rate :              8.317289258574232e-10
    learning_rate : 8.317289258574232e-10
    batch_size :          60000
    best_batch_loss : 1.5559591119547174
    Epoch 6, Loss: 1.5559591119547174, fit: 0.18551666666666666
    epoch : 7
      loss :              1.5559591119547174
      loss_factor :              8.31728920032642e-09
      loss adapted learning_rate :              8.317289200326419e-10
    learning_rate : 8.317289200326419e-10
    batch_size :          60000
    best_batch_loss : 1.5559591108650452
    Epoch 7, Loss: 1.5559591108650452, fit: 0.18551666666666666
    epoch : 8
      loss :              1.5559591108650452
      loss_factor :              8.317289142078637e-09
      loss adapted learning_rate :              8.317289142078637e-10
    learning_rate : 8.317289142078637e-10
    batch_size :          60000
    best_batch_loss : 1.5559591097753733
    Epoch 8, Loss: 1.5559591097753733, fit: 0.18551666666666666
    Best fit: 0.18551666666666666
    Best loss: 1.5559591097753733
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.5559591097753733_fit_0.18551666666666666_2024-01-03_191153
  self.fit : 0.18551666666666666
  self.loss : 1.5559591097753733
  current_accuracy : 0.1847
   Accuracy mean: 0.1403
   Accuracy mean: 0.1847
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.9
    learning_rate : 0.9
    batch_size :          60000
    best_batch_loss : 1.6084801798704373
    Epoch 0, Loss: 1.6084801798704373, fit: 0.16771666666666665
    Epoch 0, Loss: 1.6084801798704373
    Best fit: 0.16771666666666665
    Best loss: 1.6084801798704373
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.6084801798704373_fit_0.16771666666666665_2024-01-03_191156
  self.fit : 0.16771666666666665
  self.loss : 1.6084801798704373
  current_accuracy : 0.3257
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          1.0
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.9
    learning_rate : 0.9
    batch_size :          60000
    best_batch_loss : 1.3271367099576166
    Epoch 0, Loss: 1.3271367099576166, fit: 0.32293333333333335
    Epoch 0, Loss: 1.3271367099576166
    epoch : 1
      loss :              1.3271367099576166
      loss_factor :              1.6949489555538886e-09
      loss adapted learning_rate :              1.5254540599984997e-09
    learning_rate : 1.5254540599984997e-09
    batch_size :          60000
    best_batch_loss : 1.3841371227668327
    Epoch 1, Loss: 1.3841371227668327, fit: 0.29051666666666665
    epoch : 2
      loss :              1.3841371227668327
      loss_factor :              2.5810187912524747e-09
      loss adapted learning_rate :              2.3229169121272275e-09
    learning_rate : 2.3229169121272275e-09
    batch_size :          60000
    best_batch_loss : 1.3841371202659425
    Epoch 2, Loss: 1.3841371202659425, fit: 0.29051666666666665
    epoch : 3
      loss :              1.3841371202659425
      loss_factor :              2.5810187446180476e-09
      loss adapted learning_rate :              2.322916870156243e-09
    learning_rate : 2.322916870156243e-09
    batch_size :          60000
    best_batch_loss : 1.3841371164576601
    Epoch 3, Loss: 1.3841371164576601, fit: 0.29051666666666665
    epoch : 4
      loss :              1.3841371164576601
      loss_factor :              2.5810186736045064e-09
      loss adapted learning_rate :              2.322916806244056e-09
    learning_rate : 2.322916806244056e-09
    batch_size :          60000
    best_batch_loss : 1.3841371126493776
    Epoch 4, Loss: 1.3841371126493776, fit: 0.29051666666666665
    epoch : 5
      loss :              1.3841371126493776
      loss_factor :              2.5810186025909615e-09
      loss adapted learning_rate :              2.3229167423318656e-09
    learning_rate : 2.3229167423318656e-09
    batch_size :          60000
    best_batch_loss : 1.3841371088410948
    Epoch 5, Loss: 1.3841371088410948, fit: 0.29051666666666665
    epoch : 6
      loss :              1.3841371088410948
      loss_factor :              2.581018531577413e-09
      loss adapted learning_rate :              2.3229166784196716e-09
    learning_rate : 2.3229166784196716e-09
    batch_size :          60000
    best_batch_loss : 1.3841371050328124
    Epoch 6, Loss: 1.3841371050328124, fit: 0.29051666666666665
    epoch : 7
      loss :              1.3841371050328124
      loss_factor :              2.581018460563877e-09
      loss adapted learning_rate :              2.3229166145074892e-09
    learning_rate : 2.3229166145074892e-09
    batch_size :          60000
    best_batch_loss : 1.3841371012245305
    Epoch 7, Loss: 1.3841371012245305, fit: 0.29051666666666665
    epoch : 8
      loss :              1.3841371012245305
      loss_factor :              2.5810183895503474e-09
      loss adapted learning_rate :              2.3229165505953126e-09
    learning_rate : 2.3229165505953126e-09
    batch_size :          60000
    best_batch_loss : 1.3841370974162481
    Epoch 8, Loss: 1.3841370974162481, fit: 0.29051666666666665
    Best fit: 0.32293333333333335
    Best loss: 1.3271367099576166
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.3841370974162481_fit_0.29051666666666665_2024-01-03_191217
  self.fit : 0.29051666666666665
  self.loss : 1.3841370974162481
  current_accuracy : 0.2862
   Accuracy mean: 0.3257
   Accuracy mean: 0.2862
  Error saving file: doc/out/test_combinations_results/20240103191217.
  normalized_accuracies :      [0.0652452  0.0652452  0.06055437 0.06055437 0.11556503 0.11556503
   0.05245203 0.05245203 0.22217484 0.22217484 0.18848614 0.18848614
   0.03624733 0.03837953 0.         0.03880597 0.19957356 0.25970149
   0.12750533 0.16972281 0.20938166 0.39872068 1.         0.8315565 ]
batch_rate :  0.5
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-64
    learning_rate : 1e-64
    batch_size :          30000
    best_batch_loss : 1.7628262280787068
    Epoch 0, Loss: 1.7671935483185295, fit: 0.0841
    Epoch 0, Loss: 1.7671935483185295
    Best fit: 0.0841
    Best loss: 1.7671935483185295
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7715608685583524_fit_0.0841_2024-01-03_191220
  self.fit : 0.0841
  self.loss : 1.7715608685583524
  current_accuracy : 0.0848
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-64
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-64
    learning_rate : 1e-64
    batch_size :          30000
    best_batch_loss : 1.765975948702496
    Epoch 0, Loss: 1.76719354831853, fit: 0.08503333333333334
    Epoch 0, Loss: 1.76719354831853
    epoch : 1
      loss :              1.768411147934564
      loss_factor :              2.9911103804981367e-08
      loss adapted learning_rate :              2.991110380498137e-72
    learning_rate : 2.991110380498137e-72
    batch_size :          30000
    best_batch_loss : 1.7671177549367778
    Epoch 1, Loss: 1.76719354831853, fit: 0.0856
    epoch : 2
      loss :              1.7672693417002823
      loss_factor :              2.971853758704663e-08
      loss adapted learning_rate :              2.971853758704663e-72
    learning_rate : 2.971853758704663e-72
    batch_size :          30000
    best_batch_loss : 1.762891964286534
    Epoch 2, Loss: 1.7671935483185304, fit: 0.08353333333333333
    epoch : 3
      loss :              1.7714951323505268
      loss_factor :              3.0436845046860903e-08
      loss adapted learning_rate :              3.04368450468609e-72
    learning_rate : 3.04368450468609e-72
    batch_size :          30000
    best_batch_loss : 1.7632153688810002
    Epoch 3, Loss: 1.76719354831853, fit: 0.08383333333333333
    epoch : 4
      loss :              1.77117172775606
      loss_factor :              3.0381325092493967e-08
      loss adapted learning_rate :              3.0381325092493967e-72
    learning_rate : 3.0381325092493967e-72
    batch_size :          30000
    best_batch_loss : 1.7643141781445393
    Epoch 4, Loss: 1.7671935483185295, fit: 0.08453333333333334
    epoch : 5
      loss :              1.77007291849252
      loss_factor :              3.019336907628233e-08
      loss adapted learning_rate :              3.019336907628233e-72
    learning_rate : 3.019336907628233e-72
    batch_size :          30000
    best_batch_loss : 1.7658932807199565
    Epoch 5, Loss: 1.76719354831853, fit: 0.0865
    epoch : 6
      loss :              1.7658932807199565
      loss_factor :              2.9487947222499986e-08
      loss adapted learning_rate :              2.9487947222499987e-72
    learning_rate : 2.9487947222499987e-72
    batch_size :          30000
    best_batch_loss : 1.7651720719970558
    Epoch 6, Loss: 1.7671935483185301, fit: 0.08703333333333334
    epoch : 7
      loss :              1.7651720719970558
      loss_factor :              2.936773654992976e-08
      loss adapted learning_rate :              2.936773654992976e-72
    learning_rate : 2.936773654992976e-72
    batch_size :          30000
    best_batch_loss : 1.7622967159741316
    Epoch 7, Loss: 1.7671935483185297, fit: 0.0886
    epoch : 8
      loss :              1.7622967159741316
      loss_factor :              2.8892845762011737e-08
      loss adapted learning_rate :              2.8892845762011737e-72
    learning_rate : 2.8892845762011737e-72
    batch_size :          30000
    best_batch_loss : 1.7670216337332212
    Epoch 8, Loss: 1.7671935483185301, fit: 0.08643333333333333
    Best fit: 0.0886
    Best loss: 1.7671935483185295
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7670216337332212_fit_0.08643333333333333_2024-01-03_191242
  self.fit : 0.08643333333333333
  self.loss : 1.7670216337332212
  current_accuracy : 0.0848
   Accuracy mean: 0.0848
   Accuracy mean: 0.0848
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-33
    learning_rate : 1e-33
    batch_size :          30000
    best_batch_loss : 1.7938322593628564
    Epoch 0, Loss: 1.7943956484278583, fit: 0.07536666666666667
    Epoch 0, Loss: 1.7943956484278583
    Best fit: 0.07536666666666667
    Best loss: 1.7943956484278583
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7938322593628564_fit_0.07536666666666667_2024-01-03_191245
  self.fit : 0.07536666666666667
  self.loss : 1.7938322593628564
  current_accuracy : 0.0755
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-33
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-33
    learning_rate : 1e-33
    batch_size :          30000
    best_batch_loss : 1.794129274658702
    Epoch 0, Loss: 1.7943956484278587, fit: 0.07503333333333333
    Epoch 0, Loss: 1.7943956484278587
    epoch : 1
      loss :              1.7946620221970153
      loss_factor :              3.465985373994815e-08
      loss adapted learning_rate :              3.465985373994815e-41
    learning_rate : 3.465985373994815e-41
    batch_size :          30000
    best_batch_loss : 1.7932315183458138
    Epoch 1, Loss: 1.7943956484278583, fit: 0.0757
    epoch : 2
      loss :              1.7932315183458138
      loss_factor :              3.438457299619982e-08
      loss adapted learning_rate :              3.438457299619982e-41
    learning_rate : 3.438457299619982e-41
    batch_size :          30000
    best_batch_loss : 1.790017529769885
    Epoch 2, Loss: 1.794395648427858, fit: 0.07793333333333333
    epoch : 3
      loss :              1.790017529769885
      loss_factor :              3.377324890898905e-08
      loss adapted learning_rate :              3.3773248908989047e-41
    learning_rate : 3.3773248908989047e-41
    batch_size :          30000
    best_batch_loss : 1.792763422641887
    Epoch 3, Loss: 1.7943956484278583, fit: 0.07583333333333334
    epoch : 4
      loss :              1.792763422641887
      loss_factor :              3.4294922677878163e-08
      loss adapted learning_rate :              3.4294922677878164e-41
    learning_rate : 3.4294922677878164e-41
    batch_size :          30000
    best_batch_loss : 1.7939073811266235
    Epoch 4, Loss: 1.7943956484278578, fit: 0.07513333333333333
    epoch : 5
      loss :              1.7939073811266235
      loss_factor :              3.451438728453351e-08
      loss adapted learning_rate :              3.451438728453351e-41
    learning_rate : 3.451438728453351e-41
    batch_size :          30000
    best_batch_loss : 1.79190013707885
    Epoch 5, Loss: 1.7943956484278576, fit: 0.07643333333333334
    epoch : 6
      loss :              1.79190013707885
      loss_factor :              3.413013663362838e-08
      loss adapted learning_rate :              3.4130136633628383e-41
    learning_rate : 3.4130136633628383e-41
    batch_size :          30000
    best_batch_loss : 1.7917241267772241
    Epoch 6, Loss: 1.7943956484278583, fit: 0.07663333333333333
    epoch : 7
      loss :              1.7917241267772241
      loss_factor :              3.4096626947911286e-08
      loss adapted learning_rate :              3.409662694791129e-41
    learning_rate : 3.409662694791129e-41
    batch_size :          30000
    best_batch_loss : 1.792988222995758
    Epoch 7, Loss: 1.7943956484278585, fit: 0.0744
    epoch : 8
      loss :              1.7958030738599589
      loss_factor :              3.4880853731079354e-08
      loss adapted learning_rate :              3.4880853731079355e-41
    learning_rate : 3.4880853731079355e-41
    batch_size :          30000
    best_batch_loss : 1.7923266820210264
    Epoch 8, Loss: 1.7943956484278583, fit: 0.07483333333333334
    Best fit: 0.07793333333333333
    Best loss: 1.7943956484278576
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7964646148346899_fit_0.07483333333333334_2024-01-03_191306
  self.fit : 0.07483333333333334
  self.loss : 1.7964646148346899
  current_accuracy : 0.0755
   Accuracy mean: 0.0755
   Accuracy mean: 0.0755
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-17
    learning_rate : 1e-17
    batch_size :          30000
    best_batch_loss : 1.758885364837258
    Epoch 0, Loss: 1.762094543747585, fit: 0.08616666666666667
    Epoch 0, Loss: 1.762094543747585
    Best fit: 0.08616666666666667
    Best loss: 1.762094543747585
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7653037226579118_fit_0.08616666666666667_2024-01-03_191309
  self.fit : 0.08616666666666667
  self.loss : 1.7653037226579118
  current_accuracy : 0.0904
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-17
    learning_rate : 1e-17
    batch_size :          30000
    best_batch_loss : 1.761388778073772
    Epoch 0, Loss: 1.762094543747585, fit: 0.0888
    Epoch 0, Loss: 1.762094543747585
    epoch : 1
      loss :              1.761388778073772
      loss_factor :              2.874433402498887e-08
      loss adapted learning_rate :              2.8744334024988873e-25
    learning_rate : 2.8744334024988873e-25
    batch_size :          30000
    best_batch_loss : 1.7604637268946977
    Epoch 1, Loss: 1.762094543747585, fit: 0.0888
    epoch : 2
      loss :              1.7604637268946977
      loss_factor :              2.859372998100013e-08
      loss adapted learning_rate :              2.859372998100013e-25
    learning_rate : 2.859372998100013e-25
    batch_size :          30000
    best_batch_loss : 1.7598866572059129
    Epoch 2, Loss: 1.762094543747585, fit: 0.0896
    epoch : 3
      loss :              1.7598866572059129
      loss_factor :              2.8500139546216394e-08
      loss adapted learning_rate :              2.8500139546216396e-25
    learning_rate : 2.8500139546216396e-25
    batch_size :          30000
    best_batch_loss : 1.7616766502420689
    Epoch 3, Loss: 1.7620945437475846, fit: 0.08733333333333333
    epoch : 4
      loss :              1.7625124372531005
      loss_factor :              2.8928232743043944e-08
      loss adapted learning_rate :              2.8928232743043945e-25
    learning_rate : 2.8928232743043945e-25
    batch_size :          30000
    best_batch_loss : 1.7600706529777237
    Epoch 4, Loss: 1.7620945437475848, fit: 0.08956666666666667
    epoch : 5
      loss :              1.7600706529777237
      loss_factor :              2.8529950403411166e-08
      loss adapted learning_rate :              2.852995040341117e-25
    learning_rate : 2.852995040341117e-25
    batch_size :          30000
    best_batch_loss : 1.7599937704142172
    Epoch 5, Loss: 1.7620945437475846, fit: 0.0891
    epoch : 6
      loss :              1.7599937704142172
      loss_factor :              2.851749053647588e-08
      loss adapted learning_rate :              2.8517490536475883e-25
    learning_rate : 2.8517490536475883e-25
    batch_size :          30000
    best_batch_loss : 1.760707070110711
    Epoch 6, Loss: 1.7620945437475846, fit: 0.08876666666666666
    epoch : 7
      loss :              1.760707070110711
      loss_factor :              2.8633278764512667e-08
      loss adapted learning_rate :              2.863327876451267e-25
    learning_rate : 2.863327876451267e-25
    batch_size :          30000
    best_batch_loss : 1.7609338085618613
    Epoch 7, Loss: 1.762094543747585, fit: 0.08806666666666667
    epoch : 8
      loss :              1.763255278933309
      loss_factor :              2.905038733725791e-08
      loss adapted learning_rate :              2.905038733725791e-25
    learning_rate : 2.905038733725791e-25
    batch_size :          30000
    best_batch_loss : 1.75917814819764
    Epoch 8, Loss: 1.762094543747585, fit: 0.08676666666666667
    Best fit: 0.0896
    Best loss: 1.7620945437475846
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7650109392975302_fit_0.08676666666666667_2024-01-03_191330
  self.fit : 0.08676666666666667
  self.loss : 1.7650109392975302
  current_accuracy : 0.0904
   Accuracy mean: 0.0904
   Accuracy mean: 0.0904
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-10
    learning_rate : 1e-10
    batch_size :          30000
    best_batch_loss : 1.7667579687724708
    Epoch 0, Loss: 1.7667903831187766, fit: 0.09116666666666666
    Epoch 0, Loss: 1.7667903831187766
    Best fit: 0.09116666666666666
    Best loss: 1.7667903831187766
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7667579687724708_fit_0.09116666666666666_2024-01-03_191332
  self.fit : 0.09116666666666666
  self.loss : 1.7667579687724708
  current_accuracy : 0.0922
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-10
    learning_rate : 1e-10
    batch_size :          30000
    best_batch_loss : 1.7664155301800668
    Epoch 0, Loss: 1.766790382970191, fit: 0.09193333333333334
    Epoch 0, Loss: 1.766790382970191
    epoch : 1
      loss :              1.767165235760315
      loss_factor :              2.9701035691860906e-08
      loss adapted learning_rate :              2.9701035691860907e-18
    learning_rate : 2.9701035691860907e-18
    batch_size :          30000
    best_batch_loss : 1.7660870196749296
    Epoch 1, Loss: 1.7667903828601705, fit: 0.0915
    epoch : 2
      loss :              1.7674937460454112
      loss_factor :              2.9756295164538774e-08
      loss adapted learning_rate :              2.9756295164538774e-18
    learning_rate : 2.9756295164538774e-18
    batch_size :          30000
    best_batch_loss : 1.763719670435389
    Epoch 2, Loss: 1.7667903828601703, fit: 0.09293333333333334
    epoch : 3
      loss :              1.763719670435389
      loss_factor :              2.91269885717908e-08
      loss adapted learning_rate :              2.9126988571790805e-18
    learning_rate : 2.9126988571790805e-18
    batch_size :          30000
    best_batch_loss : 1.7666085399086882
    Epoch 3, Loss: 1.76679038286017, fit: 0.09153333333333333
    epoch : 4
      loss :              1.7666085399086882
      loss_factor :              2.9607603433819526e-08
      loss adapted learning_rate :              2.9607603433819526e-18
    learning_rate : 2.9607603433819526e-18
    batch_size :          30000
    best_batch_loss : 1.7660703591801477
    Epoch 4, Loss: 1.76679038286017, fit: 0.09166666666666666
    epoch : 5
      loss :              1.7660703591801477
      loss_factor :              2.9517530194700336e-08
      loss adapted learning_rate :              2.9517530194700337e-18
    learning_rate : 2.9517530194700337e-18
    batch_size :          30000
    best_batch_loss : 1.7655007671022585
    Epoch 5, Loss: 1.76679038286017, fit: 0.09273333333333333
    epoch : 6
      loss :              1.7655007671022585
      loss_factor :              2.9422468460742735e-08
      loss adapted learning_rate :              2.9422468460742737e-18
    learning_rate : 2.9422468460742737e-18
    batch_size :          30000
    best_batch_loss : 1.76437350983965
    Epoch 6, Loss: 1.76679038286017, fit: 0.09283333333333334
    epoch : 7
      loss :              1.76437350983965
      loss_factor :              2.9235147339080577e-08
      loss adapted learning_rate :              2.923514733908058e-18
    learning_rate : 2.923514733908058e-18
    batch_size :          30000
    best_batch_loss : 1.766046602066898
    Epoch 7, Loss: 1.76679038286017, fit: 0.0918
    epoch : 8
      loss :              1.7675341636534423
      loss_factor :              2.9763100292098953e-08
      loss adapted learning_rate :              2.9763100292098956e-18
    learning_rate : 2.9763100292098956e-18
    batch_size :          30000
    best_batch_loss : 1.7638218306597713
    Epoch 8, Loss: 1.7667903828601705, fit: 0.09006666666666667
    Best fit: 0.09293333333333334
    Best loss: 1.76679038286017
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7697589350605698_fit_0.09006666666666667_2024-01-03_191354
  self.fit : 0.09006666666666667
  self.loss : 1.7697589350605698
  current_accuracy : 0.0922
   Accuracy mean: 0.0922
   Accuracy mean: 0.0922
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-07
    learning_rate : 1e-07
    batch_size :          30000
    best_batch_loss : 1.7743477095737292
    Epoch 0, Loss: 1.7762630730278985, fit: 0.08496666666666666
    Epoch 0, Loss: 1.7762630730278985
    Best fit: 0.08496666666666666
    Best loss: 1.7762630730278985
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7781784364820679_fit_0.08496666666666666_2024-01-03_191357
  self.fit : 0.08496666666666666
  self.loss : 1.7781784364820679
  current_accuracy : 0.088
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-07
    learning_rate : 1e-07
    batch_size :          30000
    best_batch_loss : 1.7717483439620962
    Epoch 0, Loss: 1.776262953190979, fit: 0.08756666666666667
    Epoch 0, Loss: 1.776262953190979
    epoch : 1
      loss :              1.7717483439620962
      loss_factor :              3.048037845301343e-08
      loss adapted learning_rate :              3.048037845301343e-15
    learning_rate : 3.048037845301343e-15
    batch_size :          30000
    best_batch_loss : 1.7726863118422085
    Epoch 1, Loss: 1.7762628900907305, fit: 0.08773333333333333
    epoch : 2
      loss :              1.7726863118422085
      loss_factor :              3.064212727957179e-08
      loss adapted learning_rate :              3.064212727957179e-15
    learning_rate : 3.064212727957179e-15
    batch_size :          30000
    best_batch_loss : 1.771981004526208
    Epoch 2, Loss: 1.7762628900907274, fit: 0.08363333333333334
    epoch : 3
      loss :              1.7805447756552464
      loss_factor :              3.202793911404875e-08
      loss adapted learning_rate :              3.2027939114048747e-15
    learning_rate : 3.2027939114048747e-15
    batch_size :          30000
    best_batch_loss : 1.7744825703710048
    Epoch 3, Loss: 1.7762628900907247, fit: 0.08636666666666666
    epoch : 4
      loss :              1.7744825703710048
      loss_factor :              3.0954042819187666e-08
      loss adapted learning_rate :              3.0954042819187663e-15
    learning_rate : 3.0954042819187663e-15
    batch_size :          30000
    best_batch_loss : 1.7740071756638778
    Epoch 4, Loss: 1.7762628900907211, fit: 0.08486666666666667
    epoch : 5
      loss :              1.7785186045175643
      loss_factor :              3.1665337818856625e-08
      loss adapted learning_rate :              3.1665337818856623e-15
    learning_rate : 3.1665337818856623e-15
    batch_size :          30000
    best_batch_loss : 1.7737120462072704
    Epoch 5, Loss: 1.7762628900907178, fit: 0.08446666666666666
    epoch : 6
      loss :              1.7788137339741652
      loss_factor :              3.1717922905844306e-08
      loss adapted learning_rate :              3.1717922905844306e-15
    learning_rate : 3.1717922905844306e-15
    batch_size :          30000
    best_batch_loss : 1.7762553919182034
    Epoch 6, Loss: 1.7762628900907147, fit: 0.0851
    epoch : 7
      loss :              1.7762553919182034
      loss_factor :              3.1264687534746615e-08
      loss adapted learning_rate :              3.1264687534746612e-15
    learning_rate : 3.1264687534746612e-15
    batch_size :          30000
    best_batch_loss : 1.7751611629244415
    Epoch 7, Loss: 1.7762628900907114, fit: 0.08503333333333334
    epoch : 8
      loss :              1.7773646172569812
      loss_factor :              3.146047694932184e-08
      loss adapted learning_rate :              3.146047694932184e-15
    learning_rate : 3.146047694932184e-15
    batch_size :          30000
    best_batch_loss : 1.7750537604387029
    Epoch 8, Loss: 1.7762628900907087, fit: 0.08546666666666666
    Best fit: 0.08773333333333333
    Best loss: 1.7762628900907087
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7774720197427143_fit_0.08546666666666666_2024-01-03_191418
  self.fit : 0.08546666666666666
  self.loss : 1.7774720197427143
  current_accuracy : 0.088
   Accuracy mean: 0.088
   Accuracy mean: 0.088
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    learning_rate : 0.0001
    batch_size :          30000
    best_batch_loss : 1.7135342043448991
    Epoch 0, Loss: 1.7143119689044797, fit: 0.10843333333333334
    Epoch 0, Loss: 1.7143119689044797
    Best fit: 0.10843333333333334
    Best loss: 1.7143119689044797
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7150897334640605_fit_0.10843333333333334_2024-01-03_191421
  self.fit : 0.10843333333333334
  self.loss : 1.7150897334640605
  current_accuracy : 0.1148
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    learning_rate : 0.0001
    batch_size :          30000
    best_batch_loss : 1.7136160441650687
    Epoch 0, Loss: 1.714111313800837, fit: 0.1092
    Epoch 0, Loss: 1.714111313800837
    epoch : 1
      loss :              1.7146065834366053
      loss_factor :              2.196063375842779e-08
      loss adapted learning_rate :              2.1960633758427793e-12
    learning_rate : 2.1960633758427793e-12
    batch_size :          30000
    best_batch_loss : 1.7136910268533756
    Epoch 1, Loss: 1.7139512825848304, fit: 0.1094
    epoch : 2
      loss :              1.7142115383162855
      loss_factor :              2.19100889155398e-08
      loss adapted learning_rate :              2.1910088915539804e-12
    learning_rate : 2.1910088915539804e-12
    batch_size :          30000
    best_batch_loss : 1.7135370546004955
    Epoch 2, Loss: 1.7139512825803027, fit: 0.10966666666666666
    epoch : 3
      loss :              1.7135370546004955
      loss_factor :              2.1824032676733467e-08
      loss adapted learning_rate :              2.1824032676733466e-12
    learning_rate : 2.1824032676733466e-12
    batch_size :          30000
    best_batch_loss : 1.7129888008119167
    Epoch 3, Loss: 1.7139512825758336, fit: 0.10913333333333333
    epoch : 4
      loss :              1.7149137643397507
      loss_factor :              2.2000009137615356e-08
      loss adapted learning_rate :              2.2000009137615357e-12
    learning_rate : 2.2000009137615357e-12
    batch_size :          30000
    best_batch_loss : 1.7133436651776792
    Epoch 4, Loss: 1.7139512825714065, fit: 0.10943333333333333
    epoch : 5
      loss :              1.714558899965134
      loss_factor :              2.195452723681707e-08
      loss adapted learning_rate :              2.195452723681707e-12
    learning_rate : 2.195452723681707e-12
    batch_size :          30000
    best_batch_loss : 1.7109561674910538
    Epoch 5, Loss: 1.7139512825667753, fit: 0.11083333333333334
    epoch : 6
      loss :              1.7109561674910538
      loss_factor :              2.149754348677121e-08
      loss adapted learning_rate :              2.149754348677121e-12
    learning_rate : 2.149754348677121e-12
    batch_size :          30000
    best_batch_loss : 1.7138915242580341
    Epoch 6, Loss: 1.7139512825621512, fit: 0.1093
    epoch : 7
      loss :              1.714011040866268
      loss_factor :              2.1884475941514544e-08
      loss adapted learning_rate :              2.1884475941514545e-12
    learning_rate : 2.1884475941514545e-12
    batch_size :          30000
    best_batch_loss : 1.7124745464783067
    Epoch 7, Loss: 1.713951282557888, fit: 0.10786666666666667
    epoch : 8
      loss :              1.715428018637469
      loss_factor :              2.206607005876408e-08
      loss adapted learning_rate :              2.206607005876408e-12
    learning_rate : 2.206607005876408e-12
    batch_size :          30000
    best_batch_loss : 1.7132685883977028
    Epoch 8, Loss: 1.7139512825533718, fit: 0.10983333333333334
    Best fit: 0.11083333333333334
    Best loss: 1.7139512825533718
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7132685883977028_fit_0.10983333333333334_2024-01-03_191442
  self.fit : 0.10983333333333334
  self.loss : 1.7132685883977028
  current_accuracy : 0.1147
   Accuracy mean: 0.1148
   Accuracy mean: 0.1147
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.001
    learning_rate : 0.001
    batch_size :          30000
    best_batch_loss : 1.7578152555211102
    Epoch 0, Loss: 1.7579173671810289, fit: 0.0901
    Epoch 0, Loss: 1.7579173671810289
    Best fit: 0.0901
    Best loss: 1.7579173671810289
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7580194788409476_fit_0.0901_2024-01-03_191444
  self.fit : 0.0901
  self.loss : 1.7580194788409476
  current_accuracy : 0.098
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.001
    learning_rate : 0.001
    batch_size :          30000
    best_batch_loss : 1.748071487121616
    Epoch 0, Loss: 1.7544999722794572, fit: 0.09473333333333334
    Epoch 0, Loss: 1.7544999722794572
    epoch : 1
      loss :              1.748071487121616
      loss_factor :              2.664353777464384e-08
      loss adapted learning_rate :              2.664353777464384e-11
    learning_rate : 2.664353777464384e-11
    batch_size :          30000
    best_batch_loss : 1.7493458947963916
    Epoch 1, Loss: 1.7519671620754922, fit: 0.09403333333333333
    epoch : 2
      loss :              1.7493458947963916
      loss_factor :              2.6838417332807284e-08
      loss adapted learning_rate :              2.6838417332807284e-11
    learning_rate : 2.6838417332807284e-11
    batch_size :          30000
    best_batch_loss : 1.7506896254651598
    Epoch 2, Loss: 1.7519671619894663, fit: 0.0932
    epoch : 3
      loss :              1.7506896254651598
      loss_factor :              2.7045286184185326e-08
      loss adapted learning_rate :              2.7045286184185327e-11
    learning_rate : 2.7045286184185327e-11
    batch_size :          30000
    best_batch_loss : 1.7514343958912282
    Epoch 3, Loss: 1.751967161900227, fit: 0.09313333333333333
    epoch : 4
      loss :              1.7514343958912282
      loss_factor :              2.7160561519360422e-08
      loss adapted learning_rate :              2.716056151936042e-11
    learning_rate : 2.716056151936042e-11
    batch_size :          30000
    best_batch_loss : 1.7508542567729763
    Epoch 4, Loss: 1.751967161813328, fit: 0.09236666666666667
    epoch : 5
      loss :              1.7530800668536795
      loss_factor :              2.7416847531758457e-08
      loss adapted learning_rate :              2.741684753175846e-11
    learning_rate : 2.741684753175846e-11
    batch_size :          30000
    best_batch_loss : 1.7510786679059864
    Epoch 5, Loss: 1.751967161723115, fit: 0.0926
    epoch : 6
      loss :              1.7528556555402435
      loss_factor :              2.7381771508167987e-08
      loss adapted learning_rate :              2.7381771508167987e-11
    learning_rate : 2.7381771508167987e-11
    batch_size :          30000
    best_batch_loss : 1.751567502612853
    Epoch 6, Loss: 1.7519671616358583, fit: 0.0927
    epoch : 7
      loss :              1.7523668206588636
      loss_factor :              2.7305505218494095e-08
      loss adapted learning_rate :              2.7305505218494095e-11
    learning_rate : 2.7305505218494095e-11
    batch_size :          30000
    best_batch_loss : 1.7498253923712386
    Epoch 7, Loss: 1.7519671615474457, fit: 0.0918
    epoch : 8
      loss :              1.7541089307236528
      loss_factor :              2.7578179686602007e-08
      loss adapted learning_rate :              2.7578179686602006e-11
    learning_rate : 2.7578179686602006e-11
    batch_size :          30000
    best_batch_loss : 1.75054152195663
    Epoch 8, Loss: 1.7519671614565964, fit: 0.09386666666666667
    Best fit: 0.09473333333333334
    Best loss: 1.7519671614565964
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.75054152195663_fit_0.09386666666666667_2024-01-03_191505
  self.fit : 0.09386666666666667
  self.loss : 1.75054152195663
  current_accuracy : 0.1003
   Accuracy mean: 0.098
   Accuracy mean: 0.1003
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.02
    learning_rate : 0.02
    batch_size :          30000
    best_batch_loss : 1.7317998518445938
    Epoch 0, Loss: 1.7505556569580722, fit: 0.10343333333333334
    Epoch 0, Loss: 1.7505556569580722
    Best fit: 0.10343333333333334
    Best loss: 1.7505556569580722
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.7317998518445938_fit_0.10343333333333334_2024-01-03_191508
  self.fit : 0.10343333333333334
  self.loss : 1.7317998518445938
  current_accuracy : 0.1095
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.02
    learning_rate : 0.02
    batch_size :          30000
    best_batch_loss : 1.6773162912365547
    Epoch 0, Loss: 1.6942869152123174, fit: 0.1265
    Epoch 0, Loss: 1.6942869152123174
    epoch : 1
      loss :              1.6773162912365547
      loss_factor :              1.7625833025935747e-08
      loss adapted learning_rate :              3.5251666051871493e-10
    learning_rate : 3.5251666051871493e-10
    batch_size :          30000
    best_batch_loss : 1.64803133742245
    Epoch 1, Loss: 1.6540134700350257, fit: 0.1402
    epoch : 2
      loss :              1.64803133742245
      loss_factor :              1.4779326916586144e-08
      loss adapted learning_rate :              2.9558653833172286e-10
    learning_rate : 2.9558653833172286e-10
    batch_size :          30000
    best_batch_loss : 1.6513401711978797
    Epoch 2, Loss: 1.6540134692225312, fit: 0.1393
    epoch : 3
      loss :              1.6513401711978797
      loss_factor :              1.507875408868707e-08
      loss adapted learning_rate :              3.015750817737414e-10
    learning_rate : 3.015750817737414e-10
    batch_size :          30000
    best_batch_loss : 1.6535675881757885
    Epoch 3, Loss: 1.6540134685464816, fit: 0.1377
    epoch : 4
      loss :              1.6544593489171748
      loss_factor :              1.5366006318955268e-08
      loss adapted learning_rate :              3.0732012637910537e-10
    learning_rate : 3.0732012637910537e-10
    batch_size :          30000
    best_batch_loss : 1.6522925078148525
    Epoch 4, Loss: 1.6540134678217409, fit: 0.1391
    epoch : 5
      loss :              1.6522925078148525
      loss_factor :              1.516594008526612e-08
      loss adapted learning_rate :              3.033188017053224e-10
    learning_rate : 3.033188017053224e-10
    batch_size :          30000
    best_batch_loss : 1.646953918725018
    Epoch 5, Loss: 1.6540134670911968, fit: 0.13446666666666668
    epoch : 6
      loss :              1.6610730154573754
      loss_factor :              1.5991427397486282e-08
      loss adapted learning_rate :              3.1982854794972565e-10
    learning_rate : 3.1982854794972565e-10
    batch_size :          30000
    best_batch_loss : 1.6513203317198617
    Epoch 6, Loss: 1.654013466353316, fit: 0.1365
    epoch : 7
      loss :              1.6567066009867704
      loss_factor :              1.5576003156774166e-08
      loss adapted learning_rate :              3.115200631354833e-10
    learning_rate : 3.115200631354833e-10
    batch_size :          30000
    best_batch_loss : 1.6489346728712042
    Epoch 7, Loss: 1.6540134655994625, fit: 0.14083333333333334
    epoch : 8
      loss :              1.6489346728712042
      loss_factor :              1.4860536954292522e-08
      loss adapted learning_rate :              2.9721073908585043e-10
    learning_rate : 2.9721073908585043e-10
    batch_size :          30000
    best_batch_loss : 1.6494035720742466
    Epoch 8, Loss: 1.654013464872774, fit: 0.1404
    Best fit: 0.14083333333333334
    Best loss: 1.654013464872774
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.6494035720742466_fit_0.1404_2024-01-03_191528
  self.fit : 0.1404
  self.loss : 1.6494035720742466
  current_accuracy : 0.136
   Accuracy mean: 0.1095
   Accuracy mean: 0.136
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.025
    learning_rate : 0.025
    batch_size :          30000
    best_batch_loss : 1.7069873571733785
    Epoch 0, Loss: 1.708999719247893, fit: 0.1121
    Epoch 0, Loss: 1.708999719247893
    Best fit: 0.1121
    Best loss: 1.708999719247893
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.7110120813224075_fit_0.1121_2024-01-03_191531
  self.fit : 0.1121
  self.loss : 1.7110120813224075
  current_accuracy : 0.1182
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.025
    learning_rate : 0.025
    batch_size :          30000
    best_batch_loss : 1.6778917196784833
    Epoch 0, Loss: 1.6844719859971464, fit: 0.12626666666666667
    Epoch 0, Loss: 1.6844719859971464
    epoch : 1
      loss :              1.6778917196784833
      loss_factor :              1.7686394518062417e-08
      loss adapted learning_rate :              4.4215986295156044e-10
    learning_rate : 4.4215986295156044e-10
    batch_size :          30000
    best_batch_loss : 1.6422392943271793
    Epoch 1, Loss: 1.6475070253972652, fit: 0.13716666666666666
    epoch : 2
      loss :              1.6527747564673512
      loss_factor :              1.5210262549304482e-08
      loss adapted learning_rate :              3.8025656373261205e-10
    learning_rate : 3.8025656373261205e-10
    batch_size :          30000
    best_batch_loss : 1.643187057097831
    Epoch 2, Loss: 1.6475070245218548, fit: 0.1372
    epoch : 3
      loss :              1.6518269919458783
      loss_factor :              1.5123265791074028e-08
      loss adapted learning_rate :              3.780816447768507e-10
    learning_rate : 3.780816447768507e-10
    batch_size :          30000
    best_batch_loss : 1.646546457048956
    Epoch 3, Loss: 1.6475070237313765, fit: 0.1406
    epoch : 4
      loss :              1.646546457048956
      loss_factor :              1.464670342202318e-08
      loss adapted learning_rate :              3.661675855505795e-10
    learning_rate : 3.661675855505795e-10
    batch_size :          30000
    best_batch_loss : 1.6408002560635515
    Epoch 4, Loss: 1.6475070229480293, fit: 0.13623333333333335
    epoch : 5
      loss :              1.6542137898325073
      loss_factor :              1.5343214926234104e-08
      loss adapted learning_rate :              3.835803731558526e-10
    learning_rate : 3.835803731558526e-10
    batch_size :          30000
    best_batch_loss : 1.6442071454806566
    Epoch 5, Loss: 1.647507022187905, fit: 0.13763333333333333
    epoch : 6
      loss :              1.6508068988951534
      loss_factor :              1.5030130511166998e-08
      loss adapted learning_rate :              3.75753262779175e-10
    learning_rate : 3.75753262779175e-10
    batch_size :          30000
    best_batch_loss : 1.6444420115077476
    Epoch 6, Loss: 1.6475070213981702, fit: 0.1418
    epoch : 7
      loss :              1.6444420115077476
      loss_factor :              1.4460577395243643e-08
      loss adapted learning_rate :              3.6151443488109106e-10
    learning_rate : 3.6151443488109106e-10
    batch_size :          30000
    best_batch_loss : 1.6451978477282914
    Epoch 7, Loss: 1.6475070206159852, fit: 0.1413
    epoch : 8
      loss :              1.6451978477282914
      loss_factor :              1.4527180306174072e-08
      loss adapted learning_rate :              3.631795076543518e-10
    learning_rate : 3.631795076543518e-10
    batch_size :          30000
    best_batch_loss : 1.6452873206310694
    Epoch 8, Loss: 1.647507019868387, fit: 0.14083333333333334
    Best fit: 0.1418
    Best loss: 1.647507019868387
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.6452873206310694_fit_0.14083333333333334_2024-01-03_191552
  self.fit : 0.14083333333333334
  self.loss : 1.6452873206310694
  current_accuracy : 0.1422
   Accuracy mean: 0.1182
   Accuracy mean: 0.1422
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.03
    learning_rate : 0.03
    batch_size :          30000
    best_batch_loss : 1.6666042948844082
    Epoch 0, Loss: 1.6666751068594885, fit: 0.1334
    Epoch 0, Loss: 1.6666751068594885
    Best fit: 0.1334
    Best loss: 1.6666751068594885
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.6666042948844082_fit_0.1334_2024-01-03_191555
  self.fit : 0.1334
  self.loss : 1.6666042948844082
  current_accuracy : 0.1364
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.03
    learning_rate : 0.03
    batch_size :          30000
    best_batch_loss : 1.6250071041305685
    Epoch 0, Loss: 1.6357639172215763, fit: 0.15203333333333333
    Epoch 0, Loss: 1.6357639172215763
    epoch : 1
      loss :              1.6250071041305685
      loss_factor :              1.2839633862247735e-08
      loss adapted learning_rate :              3.8518901586743205e-10
    learning_rate : 3.8518901586743205e-10
    batch_size :          30000
    best_batch_loss : 1.5967614580907195
    Epoch 1, Loss: 1.59817592774465, fit: 0.16616666666666666
    epoch : 2
      loss :              1.5967614580907195
      loss_factor :              1.0774581552180088e-08
      loss adapted learning_rate :              3.232374465654026e-10
    learning_rate : 3.232374465654026e-10
    batch_size :          30000
    best_batch_loss : 1.593946986372904
    Epoch 2, Loss: 1.5981759270670162, fit: 0.1635
    epoch : 3
      loss :              1.6024048677611282
      loss_factor :              1.1161499799889719e-08
      loss adapted learning_rate :              3.3484499399669157e-10
    learning_rate : 3.3484499399669157e-10
    batch_size :          30000
    best_batch_loss : 1.5967636686230198
    Epoch 3, Loss: 1.5981759264520439, fit: 0.16623333333333334
    epoch : 4
      loss :              1.5967636686230198
      loss_factor :              1.077473071477917e-08
      loss adapted learning_rate :              3.232419214433751e-10
    learning_rate : 3.232419214433751e-10
    batch_size :          30000
    best_batch_loss : 1.5975477657915464
    Epoch 4, Loss: 1.5981759258284418, fit: 0.1658
    epoch : 5
      loss :              1.5975477657915464
      loss_factor :              1.0827757529895928e-08
      loss adapted learning_rate :              3.2483272589687786e-10
    learning_rate : 3.2483272589687786e-10
    batch_size :          30000
    best_batch_loss : 1.597405839006795
    Epoch 5, Loss: 1.59817592523087, fit: 0.16563333333333333
    epoch : 6
      loss :              1.597405839006795
      loss_factor :              1.0818141951404377e-08
      loss adapted learning_rate :              3.245442585421313e-10
    learning_rate : 3.245442585421313e-10
    batch_size :          30000
    best_batch_loss : 1.5931299257182039
    Epoch 6, Loss: 1.5981759246145093, fit: 0.16753333333333334
    epoch : 7
      loss :              1.5931299257182039
      loss_factor :              1.0532026808172397e-08
      loss adapted learning_rate :              3.159608042451719e-10
    learning_rate : 3.159608042451719e-10
    batch_size :          30000
    best_batch_loss : 1.596144908139747
    Epoch 7, Loss: 1.5981759240263664, fit: 0.16566666666666666
    epoch : 8
      loss :              1.596144908139747
      loss_factor :              1.0733050384090834e-08
      loss adapted learning_rate :              3.21991511522725e-10
    learning_rate : 3.21991511522725e-10
    batch_size :          30000
    best_batch_loss : 1.5940361467756763
    Epoch 8, Loss: 1.5981759234360302, fit: 0.1679
    Best fit: 0.1679
    Best loss: 1.5981759234360302
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.5940361467756763_fit_0.1679_2024-01-03_191616
  self.fit : 0.1679
  self.loss : 1.5940361467756763
  current_accuracy : 0.1603
   Accuracy mean: 0.1364
   Accuracy mean: 0.1603
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.1
    learning_rate : 0.1
    batch_size :          30000
    best_batch_loss : 1.6610119565034518
    Epoch 0, Loss: 1.7453170094366754, fit: 0.1374
    Epoch 0, Loss: 1.7453170094366754
    Best fit: 0.1374
    Best loss: 1.7453170094366754
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.6610119565034518_fit_0.1374_2024-01-03_191618
  self.fit : 0.1374
  self.loss : 1.6610119565034518
  current_accuracy : 0.1923
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.1
    learning_rate : 0.1
    batch_size :          30000
    best_batch_loss : 1.4301247361927625
    Epoch 0, Loss: 1.4941636563835994, fit: 0.2521333333333333
    Epoch 0, Loss: 1.4941636563835994
    epoch : 1
      loss :              1.4301247361927625
      loss_factor :              3.57881447290968e-09
      loss adapted learning_rate :              3.5788144729096804e-10
    learning_rate : 3.5788144729096804e-10
    batch_size :          30000
    best_batch_loss : 1.3126103782477383
    Epoch 1, Loss: 1.313411457060863, fit: 0.3107666666666667
    epoch : 2
      loss :              1.3142125358739878
      loss_factor :              1.5369369607485939e-09
      loss adapted learning_rate :              1.536936960748594e-10
    learning_rate : 1.536936960748594e-10
    batch_size :          30000
    best_batch_loss : 1.3112882400379273
    Epoch 2, Loss: 1.3134114564697223, fit: 0.31116666666666665
    epoch : 3
      loss :              1.3112882400379273
      loss_factor :              1.5030785032586047e-09
      loss adapted learning_rate :              1.5030785032586047e-10
    learning_rate : 1.5030785032586047e-10
    batch_size :          30000
    best_batch_loss : 1.3114464371222117
    Epoch 3, Loss: 1.3134114561714632, fit: 0.31203333333333333
    epoch : 4
      loss :              1.3114464371222117
      loss_factor :              1.5048928394285466e-09
      loss adapted learning_rate :              1.5048928394285468e-10
    learning_rate : 1.5048928394285468e-10
    batch_size :          30000
    best_batch_loss : 1.3085589978705967
    Epoch 4, Loss: 1.313411455888029, fit: 0.308
    epoch : 5
      loss :              1.3182639139054613
      loss_factor :              1.584979461243879e-09
      loss adapted learning_rate :              1.5849794612438792e-10
    learning_rate : 1.5849794612438792e-10
    batch_size :          30000
    best_batch_loss : 1.312168429359388
    Epoch 5, Loss: 1.3134114555923313, fit: 0.31093333333333334
    epoch : 6
      loss :              1.312168429359388
      loss_factor :              1.5131982997912215e-09
      loss adapted learning_rate :              1.5131982997912216e-10
    learning_rate : 1.5131982997912216e-10
    batch_size :          30000
    best_batch_loss : 1.3131438131663995
    Epoch 6, Loss: 1.313411455293557, fit: 0.3104
    epoch : 7
      loss :              1.3131438131663995
      loss_factor :              1.5244841679843255e-09
      loss adapted learning_rate :              1.5244841679843256e-10
    learning_rate : 1.5244841679843256e-10
    batch_size :          30000
    best_batch_loss : 1.3091110105612662
    Epoch 7, Loss: 1.3134114550022802, fit: 0.3083
    epoch : 8
      loss :              1.3177118994432941
      loss_factor :              1.5783549550155651e-09
      loss adapted learning_rate :              1.5783549550155653e-10
    learning_rate : 1.5783549550155653e-10
    batch_size :          30000
    best_batch_loss : 1.3084373550741584
    Epoch 8, Loss: 1.313411454702059, fit: 0.3127333333333333
    Best fit: 0.3127333333333333
    Best loss: 1.313411454702059
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.3084373550741584_fit_0.3127333333333333_2024-01-03_191640
  self.fit : 0.3127333333333333
  self.loss : 1.3084373550741584
  current_accuracy : 0.3138
   Accuracy mean: 0.1923
   Accuracy mean: 0.3138
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.9
    learning_rate : 0.9
    batch_size :          30000
    best_batch_loss : 1.2009025854423632
    Epoch 0, Loss: 1.4861631790500243, fit: 0.3847333333333333
    Epoch 0, Loss: 1.4861631790500243
    Best fit: 0.3847333333333333
    Best loss: 1.4861631790500243
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.2009025854423632_fit_0.3847333333333333_2024-01-03_191642
  self.fit : 0.3847333333333333
  self.loss : 1.2009025854423632
  current_accuracy : 0.3218
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.5
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.9
    learning_rate : 0.9
    batch_size :          30000
    best_batch_loss : 1.0778312390581988
    Epoch 0, Loss: 1.203109585682228, fit: 0.4414
    Epoch 0, Loss: 1.203109585682228
    epoch : 1
      loss :              1.0778312390581988
      loss_factor :              2.115961040422502e-10
      loss adapted learning_rate :              1.904364936380252e-10
    learning_rate : 1.904364936380252e-10
    batch_size :          30000
    best_batch_loss : 1.0715673887989667
    Epoch 1, Loss: 1.0739824686657715, fit: 0.4369
    epoch : 2
      loss :              1.076397548532576
      loss_factor :              2.0879831999605792e-10
      loss adapted learning_rate :              1.8791848799645213e-10
    learning_rate : 1.8791848799645213e-10
    batch_size :          30000
    best_batch_loss : 1.0732421494653794
    Epoch 2, Loss: 1.073982468740112, fit: 0.4386333333333333
    epoch : 3
      loss :              1.0747227880148444
      loss_factor :              2.0557229099991822e-10
      loss adapted learning_rate :              1.850150618999264e-10
    learning_rate : 1.850150618999264e-10
    batch_size :          30000
    best_batch_loss : 1.062480593084699
    Epoch 3, Loss: 1.0739824688194657, fit: 0.4328666666666667
    epoch : 4
      loss :              1.0854843445542324
      loss_factor :              2.271096782630955e-10
      loss adapted learning_rate :              2.0439871043678595e-10
    learning_rate : 2.0439871043678595e-10
    batch_size :          30000
    best_batch_loss : 1.0691333743692837
    Epoch 4, Loss: 1.0739824688951094, fit: 0.4415
    epoch : 5
      loss :              1.0691333743692837
      loss_factor :              1.9512767424967955e-10
      loss adapted learning_rate :              1.7561490682471158e-10
    learning_rate : 1.7561490682471158e-10
    batch_size :          30000
    best_batch_loss : 1.0667692486175326
    Epoch 5, Loss: 1.0739824689761694, fit: 0.4346333333333333
    epoch : 6
      loss :              1.0811956893348063
      loss_factor :              2.1829463147673228e-10
      loss adapted learning_rate :              1.9646516832905906e-10
    learning_rate : 1.9646516832905906e-10
    batch_size :          30000
    best_batch_loss : 1.0715175911952717
    Epoch 6, Loss: 1.0739824690517041, fit: 0.44033333333333335
    epoch : 7
      loss :              1.0715175911952717
      loss_factor :              1.995230398236796e-10
      loss adapted learning_rate :              1.7957073584131165e-10
    learning_rate : 1.7957073584131165e-10
    batch_size :          30000
    best_batch_loss : 1.0733187606982815
    Epoch 7, Loss: 1.0739824691312267, fit: 0.43896666666666667
    epoch : 8
      loss :              1.0733187606982815
      loss_factor :              2.0290240990324892e-10
      loss adapted learning_rate :              1.8261216891292402e-10
    learning_rate : 1.8261216891292402e-10
    batch_size :          30000
    best_batch_loss : 1.06925850751152
    Epoch 8, Loss: 1.0739824692041298, fit: 0.43666666666666665
    Best fit: 0.4415
    Best loss: 1.0739824686657715
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.0787064308967398_fit_0.43666666666666665_2024-01-03_191704
  self.fit : 0.43666666666666665
  self.loss : 1.0787064308967398
  current_accuracy : 0.4336
   Accuracy mean: 0.3218
   Accuracy mean: 0.4336
  Error saving file: doc/out/test_combinations_results/20240103191704.
  normalized_accuracies :      [0.0259704  0.0259704  0.         0.         0.04160849 0.04160849
   0.04663502 0.04663502 0.03490645 0.03490645 0.10974588 0.10946663
   0.06283161 0.0692544  0.09494555 0.16894722 0.11924044 0.18626082
   0.17006423 0.23680536 0.32616588 0.66545658 0.6877967  1.        ]
batch_rate :  0.2
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-64
    learning_rate : 1e-64
    batch_size :          12000
    best_batch_loss : 1.7013478691355097
    Epoch 0, Loss: 1.704169297400553, fit: 0.11683333333333333
    Epoch 0, Loss: 1.704169297400553
    Best fit: 0.11683333333333333
    Best loss: 1.704169297400553
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7071117431946423_fit_0.11683333333333333_2024-01-03_191707
  self.fit : 0.11683333333333333
  self.loss : 1.7071117431946423
  current_accuracy : 0.1171
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-64
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-64
    learning_rate : 1e-64
    batch_size :          12000
    best_batch_loss : 1.693801058779969
    Epoch 0, Loss: 1.7041692974005527, fit: 0.11558333333333333
    Epoch 0, Loss: 1.7041692974005527
    epoch : 1
      loss :              1.7091224105529046
      loss_factor :              2.1268246648668725e-08
      loss adapted learning_rate :              2.1268246648668725e-72
    learning_rate : 2.1268246648668725e-72
    batch_size :          12000
    best_batch_loss : 1.6972318096064876
    Epoch 1, Loss: 1.7041692974005527, fit: 0.11616666666666667
    epoch : 2
      loss :              1.7083176415424308
      loss_factor :              2.116831346630655e-08
      loss adapted learning_rate :              2.1168313466306547e-72
    learning_rate : 2.1168313466306547e-72
    batch_size :          12000
    best_batch_loss : 1.7024407195892326
    Epoch 2, Loss: 1.704169297400553, fit: 0.11883333333333333
    epoch : 3
      loss :              1.7038197824037367
      loss_factor :              2.061752660522794e-08
      loss adapted learning_rate :              2.061752660522794e-72
    learning_rate : 2.061752660522794e-72
    batch_size :          12000
    best_batch_loss : 1.6973556476813718
    Epoch 3, Loss: 1.7041692974005531, fit: 0.11941666666666667
    epoch : 4
      loss :              1.7006973859150387
      loss_factor :              2.0242793355161224e-08
      loss adapted learning_rate :              2.0242793355161224e-72
    learning_rate : 2.0242793355161224e-72
    batch_size :          12000
    best_batch_loss : 1.6989149376144823
    Epoch 4, Loss: 1.704169297400553, fit: 0.12166666666666667
    epoch : 5
      loss :              1.6990128001456757
      loss_factor :              2.0043174521182677e-08
      loss adapted learning_rate :              2.0043174521182675e-72
    learning_rate : 2.0043174521182675e-72
    batch_size :          12000
    best_batch_loss : 1.699255641980693
    Epoch 5, Loss: 1.704169297400553, fit: 0.12091666666666667
    epoch : 6
      loss :              1.699255641980693
      loss_factor :              2.0071840891950474e-08
      loss adapted learning_rate :              2.0071840891950472e-72
    learning_rate : 2.0071840891950472e-72
    batch_size :          12000
    best_batch_loss : 1.6975284211169908
    Epoch 6, Loss: 1.7041692974005531, fit: 0.11466666666666667
    epoch : 7
      loss :              1.7140107634875488
      loss_factor :              2.1884440525852993e-08
      loss adapted learning_rate :              2.1884440525852993e-72
    learning_rate : 2.1884440525852993e-72
    batch_size :          12000
    best_batch_loss : 1.6965691696332728
    Epoch 7, Loss: 1.7041692974005527, fit: 0.11358333333333333
    epoch : 8
      loss :              1.714279749463734
      loss_factor :              2.1918808842520787e-08
      loss adapted learning_rate :              2.1918808842520787e-72
    learning_rate : 2.1918808842520787e-72
    batch_size :          12000
    best_batch_loss : 1.6997138458125343
    Epoch 8, Loss: 1.7041692974005525, fit: 0.11666666666666667
    Best fit: 0.12166666666666667
    Best loss: 1.7041692974005525
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7064534289933202_fit_0.11666666666666667_2024-01-03_191728
  self.fit : 0.11666666666666667
  self.loss : 1.7064534289933202
  current_accuracy : 0.1171
   Accuracy mean: 0.1171
   Accuracy mean: 0.1171
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-33
    learning_rate : 1e-33
    batch_size :          12000
    best_batch_loss : 1.6873604603140269
    Epoch 0, Loss: 1.6953250021762427, fit: 0.12275
    Epoch 0, Loss: 1.6953250021762427
    Best fit: 0.12275
    Best loss: 1.6953250021762427
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7005373283514795_fit_0.12275_2024-01-03_191731
  self.fit : 0.12275
  self.loss : 1.7005373283514795
  current_accuracy : 0.1263
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-33
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-33
    learning_rate : 1e-33
    batch_size :          12000
    best_batch_loss : 1.689999865523668
    Epoch 0, Loss: 1.6953250021762427, fit: 0.12391666666666666
    Epoch 0, Loss: 1.6953250021762427
    epoch : 1
      loss :              1.6992472294588992
      loss_factor :              2.0070847215466222e-08
      loss adapted learning_rate :              2.0070847215466222e-41
    learning_rate : 2.0070847215466222e-41
    batch_size :          12000
    best_batch_loss : 1.6911601952362467
    Epoch 1, Loss: 1.695325002176243, fit: 0.12691666666666668
    epoch : 2
      loss :              1.693871336599392
      loss_factor :              1.9444831775127777e-08
      loss adapted learning_rate :              1.9444831775127777e-41
    learning_rate : 1.9444831775127777e-41
    batch_size :          12000
    best_batch_loss : 1.689291313056113
    Epoch 2, Loss: 1.695325002176243, fit: 0.12633333333333333
    epoch : 3
      loss :              1.6934640407909953
      loss_factor :              1.9398126725664508e-08
      loss adapted learning_rate :              1.939812672566451e-41
    learning_rate : 1.939812672566451e-41
    batch_size :          12000
    best_batch_loss : 1.688755444289026
    Epoch 3, Loss: 1.6953250021762427, fit: 0.1265
    epoch : 4
      loss :              1.6938459288454077
      loss_factor :              1.9441915283510364e-08
      loss adapted learning_rate :              1.9441915283510365e-41
    learning_rate : 1.9441915283510365e-41
    batch_size :          12000
    best_batch_loss : 1.6855469009884188
    Epoch 4, Loss: 1.695325002176243, fit: 0.1275
    epoch : 5
      loss :              1.6930326843930752
      loss_factor :              1.9348772731292162e-08
      loss adapted learning_rate :              1.9348772731292164e-41
    learning_rate : 1.9348772731292164e-41
    batch_size :          12000
    best_batch_loss : 1.6903809639934597
    Epoch 5, Loss: 1.6953250021762427, fit: 0.12533333333333332
    epoch : 6
      loss :              1.6948359448030497
      loss_factor :              1.95558483732045e-08
      loss adapted learning_rate :              1.9555848373204503e-41
    learning_rate : 1.9555848373204503e-41
    batch_size :          12000
    best_batch_loss : 1.6877585900807108
    Epoch 6, Loss: 1.695325002176243, fit: 0.121
    epoch : 7
      loss :              1.7042340031346424
      loss_factor :              2.0667705363255544e-08
      loss adapted learning_rate :              2.0667705363255546e-41
    learning_rate : 2.0667705363255546e-41
    batch_size :          12000
    best_batch_loss : 1.6891210583185132
    Epoch 7, Loss: 1.6953250021762427, fit: 0.12308333333333334
    epoch : 8
      loss :              1.6990959507463719
      loss_factor :              2.005298591924191e-08
      loss adapted learning_rate :              2.0052985919241912e-41
    learning_rate : 2.0052985919241912e-41
    batch_size :          12000
    best_batch_loss : 1.6900223781294879
    Epoch 8, Loss: 1.6953250021762427, fit: 0.12575
    Best fit: 0.1275
    Best loss: 1.6953250021762427
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.6943634440655582_fit_0.12575_2024-01-03_191752
  self.fit : 0.12575
  self.loss : 1.6943634440655582
  current_accuracy : 0.1263
   Accuracy mean: 0.1263
   Accuracy mean: 0.1263
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-17
    learning_rate : 1e-17
    batch_size :          12000
    best_batch_loss : 1.739906652176328
    Epoch 0, Loss: 1.7465293652573521, fit: 0.087
    Epoch 0, Loss: 1.7465293652573521
    Best fit: 0.087
    Best loss: 1.7465293652573521
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7567167539635102_fit_0.087_2024-01-03_191754
  self.fit : 0.087
  self.loss : 1.7567167539635102
  current_accuracy : 0.0942
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-17
    learning_rate : 1e-17
    batch_size :          12000
    best_batch_loss : 1.7424225889255733
    Epoch 0, Loss: 1.7465293652573521, fit: 0.09216666666666666
    Epoch 0, Loss: 1.7465293652573521
    epoch : 1
      loss :              1.7471718820939601
      loss_factor :              2.6506739996838427e-08
      loss adapted learning_rate :              2.6506739996838428e-25
    learning_rate : 2.6506739996838428e-25
    batch_size :          12000
    best_batch_loss : 1.7371914049323827
    Epoch 1, Loss: 1.7465293652573526, fit: 0.08966666666666667
    epoch : 2
      loss :              1.7525217087165341
      loss_factor :              2.7329649588257203e-08
      loss adapted learning_rate :              2.7329649588257203e-25
    learning_rate : 2.7329649588257203e-25
    batch_size :          12000
    best_batch_loss : 1.7429424752840539
    Epoch 2, Loss: 1.7465293652573521, fit: 0.093
    epoch : 3
      loss :              1.7458183438092034
      loss_factor :              2.630210604118823e-08
      loss adapted learning_rate :              2.6302106041188234e-25
    learning_rate : 2.6302106041188234e-25
    batch_size :          12000
    best_batch_loss : 1.742675245869686
    Epoch 3, Loss: 1.7465293652573521, fit: 0.09333333333333334
    epoch : 4
      loss :              1.746064272469077
      loss_factor :              2.6339180594888844e-08
      loss adapted learning_rate :              2.6339180594888845e-25
    learning_rate : 2.6339180594888845e-25
    batch_size :          12000
    best_batch_loss : 1.740573834015802
    Epoch 4, Loss: 1.7465293652573521, fit: 0.09216666666666666
    epoch : 5
      loss :              1.7502405638955003
      loss_factor :              2.697599354997388e-08
      loss adapted learning_rate :              2.6975993549973885e-25
    learning_rate : 2.6975993549973885e-25
    batch_size :          12000
    best_batch_loss : 1.7427068136613806
    Epoch 5, Loss: 1.7465293652573524, fit: 0.093
    epoch : 6
      loss :              1.7467561932389974
      loss_factor :              2.6443742377449266e-08
      loss adapted learning_rate :              2.6443742377449267e-25
    learning_rate : 2.6443742377449267e-25
    batch_size :          12000
    best_batch_loss : 1.7414601830162364
    Epoch 6, Loss: 1.7465293652573521, fit: 0.09116666666666666
    epoch : 7
      loss :              1.7489630871346968
      loss_factor :              2.6779744884780706e-08
      loss adapted learning_rate :              2.6779744884780707e-25
    learning_rate : 2.6779744884780707e-25
    batch_size :          12000
    best_batch_loss : 1.7408846959906386
    Epoch 7, Loss: 1.7465293652573521, fit: 0.09408333333333334
    epoch : 8
      loss :              1.742112863313505
      loss_factor :              2.5749148673693738e-08
      loss adapted learning_rate :              2.5749148673693738e-25
    learning_rate : 2.5749148673693738e-25
    batch_size :          12000
    best_batch_loss : 1.7337388904536046
    Epoch 8, Loss: 1.7465293652573521, fit: 0.08933333333333333
    Best fit: 0.09408333333333334
    Best loss: 1.7465293652573521
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7536491064507649_fit_0.08933333333333333_2024-01-03_191815
  self.fit : 0.08933333333333333
  self.loss : 1.7536491064507649
  current_accuracy : 0.0942
   Accuracy mean: 0.0942
   Accuracy mean: 0.0942
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-10
    learning_rate : 1e-10
    batch_size :          12000
    best_batch_loss : 1.7222549131575606
    Epoch 0, Loss: 1.7260496131636591, fit: 0.10591666666666667
    Epoch 0, Loss: 1.7260496131636591
    Best fit: 0.10591666666666667
    Best loss: 1.7260496131636591
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.728616980419962_fit_0.10591666666666667_2024-01-03_191818
  self.fit : 0.10591666666666667
  self.loss : 1.728616980419962
  current_accuracy : 0.1046
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-10
    learning_rate : 1e-10
    batch_size :          12000
    best_batch_loss : 1.7228971706508607
    Epoch 0, Loss: 1.7260496130877625, fit: 0.10716666666666666
    Epoch 0, Loss: 1.7260496130877625
    epoch : 1
      loss :              1.7228971706508607
      loss_factor :              2.304589495534687e-08
      loss adapted learning_rate :              2.3045894955346873e-18
    learning_rate : 2.3045894955346873e-18
    batch_size :          12000
    best_batch_loss : 1.7195833824346323
    Epoch 1, Loss: 1.7260496130245286, fit: 0.10283333333333333
    epoch : 2
      loss :              1.7298921048426388
      loss_factor :              2.3998835329283012e-08
      loss adapted learning_rate :              2.3998835329283012e-18
    learning_rate : 2.3998835329283012e-18
    batch_size :          12000
    best_batch_loss : 1.7209387063298498
    Epoch 2, Loss: 1.7260496130245284, fit: 0.10391666666666667
    epoch : 3
      loss :              1.7329632132753425
      loss_factor :              2.442831084342725e-08
      loss adapted learning_rate :              2.442831084342725e-18
    learning_rate : 2.442831084342725e-18
    batch_size :          12000
    best_batch_loss : 1.7219884139848485
    Epoch 3, Loss: 1.7260496130245286, fit: 0.10625
    epoch : 4
      loss :              1.7254228935970757
      loss_factor :              2.3385979320893097e-08
      loss adapted learning_rate :              2.33859793208931e-18
    learning_rate : 2.33859793208931e-18
    batch_size :          12000
    best_batch_loss : 1.724483765518989
    Epoch 4, Loss: 1.7260496130245284, fit: 0.10525
    epoch : 5
      loss :              1.727733665158876
      loss_factor :              2.3701070181137417e-08
      loss adapted learning_rate :              2.370107018113742e-18
    learning_rate : 2.370107018113742e-18
    batch_size :          12000
    best_batch_loss : 1.71881798270727
    Epoch 5, Loss: 1.7260496130245286, fit: 0.10991666666666666
    epoch : 6
      loss :              1.7205553969334402
      loss_factor :              2.2734562553129903e-08
      loss adapted learning_rate :              2.27345625531299e-18
    learning_rate : 2.27345625531299e-18
    batch_size :          12000
    best_batch_loss : 1.7208537293424324
    Epoch 6, Loss: 1.7260496130245286, fit: 0.10625
    epoch : 7
      loss :              1.7268244724256796
      loss_factor :              2.3576641955152754e-08
      loss adapted learning_rate :              2.3576641955152754e-18
    learning_rate : 2.3576641955152754e-18
    batch_size :          12000
    best_batch_loss : 1.7228309096967103
    Epoch 7, Loss: 1.7260496130245284, fit: 0.10558333333333333
    epoch : 8
      loss :              1.727254576789453
      loss_factor :              2.3635430731569903e-08
      loss adapted learning_rate :              2.3635430731569905e-18
    learning_rate : 2.3635430731569905e-18
    batch_size :          12000
    best_batch_loss : 1.7224383977059912
    Epoch 8, Loss: 1.7260496130245284, fit: 0.10275
    Best fit: 0.10991666666666666
    Best loss: 1.7260496130245284
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.734794266980654_fit_0.10275_2024-01-03_191838
  self.fit : 0.10275
  self.loss : 1.734794266980654
  current_accuracy : 0.1046
   Accuracy mean: 0.1046
   Accuracy mean: 0.1046
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-07
    learning_rate : 1e-07
    batch_size :          12000
    best_batch_loss : 1.794226221820799
    Epoch 0, Loss: 1.799357974132111, fit: 0.06991666666666667
    Epoch 0, Loss: 1.799357974132111
    Best fit: 0.06991666666666667
    Best loss: 1.799357974132111
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7993437836758062_fit_0.06991666666666667_2024-01-03_191841
  self.fit : 0.06991666666666667
  self.loss : 1.7993437836758062
  current_accuracy : 0.0667
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-07
    learning_rate : 1e-07
    batch_size :          12000
    best_batch_loss : 1.7939803884707022
    Epoch 0, Loss: 1.7993577971501882, fit: 0.06933333333333333
    Epoch 0, Loss: 1.7993577971501882
    epoch : 1
      loss :              1.8011325350696463
      loss_factor :              3.592995824410301e-08
      loss adapted learning_rate :              3.592995824410301e-15
    learning_rate : 3.592995824410301e-15
    batch_size :          12000
    best_batch_loss : 1.7952329319394815
    Epoch 1, Loss: 1.7993577096539637, fit: 0.07133333333333333
    epoch : 2
      loss :              1.7976650630733055
      loss_factor :              3.524421010603303e-08
      loss adapted learning_rate :              3.5244210106033027e-15
    learning_rate : 3.5244210106033027e-15
    batch_size :          12000
    best_batch_loss : 1.7951486764115738
    Epoch 2, Loss: 1.799357709653958, fit: 0.07258333333333333
    epoch : 3
      loss :              1.7951486764115738
      loss_factor :              3.4753954807740955e-08
      loss adapted learning_rate :              3.4753954807740955e-15
    learning_rate : 3.4753954807740955e-15
    batch_size :          12000
    best_batch_loss : 1.7939429539231302
    Epoch 3, Loss: 1.7993577096539537, fit: 0.07291666666666667
    epoch : 4
      loss :              1.7963326905102148
      loss_factor :              3.498386066593405e-08
      loss adapted learning_rate :              3.498386066593405e-15
    learning_rate : 3.498386066593405e-15
    batch_size :          12000
    best_batch_loss : 1.7937852676579762
    Epoch 4, Loss: 1.7993577096539481, fit: 0.07158333333333333
    epoch : 5
      loss :              1.7986263125065904
      loss_factor :              3.543312245657128e-08
      loss adapted learning_rate :              3.5433122456571282e-15
    learning_rate : 3.5433122456571282e-15
    batch_size :          12000
    best_batch_loss : 1.790087197233917
    Epoch 5, Loss: 1.7993577096539424, fit: 0.07633333333333334
    epoch : 6
      loss :              1.790087197233917
      loss_factor :              3.378639575639944e-08
      loss adapted learning_rate :              3.378639575639944e-15
    learning_rate : 3.378639575639944e-15
    batch_size :          12000
    best_batch_loss : 1.7939267377834374
    Epoch 6, Loss: 1.7993577096539375, fit: 0.07125
    epoch : 7
      loss :              1.800573983433292
      loss_factor :              3.58186907288881e-08
      loss adapted learning_rate :              3.58186907288881e-15
    learning_rate : 3.58186907288881e-15
    batch_size :          12000
    best_batch_loss : 1.7966299049528913
    Epoch 7, Loss: 1.7993577096539322, fit: 0.06925
    epoch : 8
      loss :              1.8033682975900984
      loss_factor :              3.63784597310723e-08
      loss adapted learning_rate :              3.63784597310723e-15
    learning_rate : 3.63784597310723e-15
    batch_size :          12000
    best_batch_loss : 1.7933723299741633
    Epoch 8, Loss: 1.7993577096539288, fit: 0.0705
    Best fit: 0.07633333333333334
    Best loss: 1.7993577096539288
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.8007724178633289_fit_0.0705_2024-01-03_191900
  self.fit : 0.0705
  self.loss : 1.8007724178633289
  current_accuracy : 0.0667
   Accuracy mean: 0.0667
   Accuracy mean: 0.0667
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    learning_rate : 0.0001
    batch_size :          12000
    best_batch_loss : 1.6925286458913864
    Epoch 0, Loss: 1.7007680337106172, fit: 0.12108333333333333
    Epoch 0, Loss: 1.7007680337106172
    Best fit: 0.12108333333333333
    Best loss: 1.7007680337106172
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.700326297785889_fit_0.12108333333333333_2024-01-03_191903
  self.fit : 0.12108333333333333
  self.loss : 1.700326297785889
  current_accuracy : 0.1218
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    learning_rate : 0.0001
    batch_size :          12000
    best_batch_loss : 1.6968796927379415
    Epoch 0, Loss: 1.7000926581181979, fit: 0.12325
    Epoch 0, Loss: 1.7000926581181979
    epoch : 1
      loss :              1.6975474530231383
      loss_factor :              1.987097757976676e-08
      loss adapted learning_rate :              1.987097757976676e-12
    learning_rate : 1.987097757976676e-12
    batch_size :          12000
    best_batch_loss : 1.6971498855937153
    Epoch 1, Loss: 1.69963467334221, fit: 0.12208333333333334
    epoch : 2
      loss :              1.6991214024057673
      loss_factor :              2.005598996484853e-08
      loss adapted learning_rate :              2.0055989964848533e-12
    learning_rate : 2.0055989964848533e-12
    batch_size :          12000
    best_batch_loss : 1.6885289812167175
    Epoch 2, Loss: 1.6996346733274554, fit: 0.12016666666666667
    epoch : 3
      loss :              1.7047576405657228
      loss_factor :              2.0731296161340418e-08
      loss adapted learning_rate :              2.0731296161340418e-12
    learning_rate : 2.0731296161340418e-12
    batch_size :          12000
    best_batch_loss : 1.6945072607803164
    Epoch 3, Loss: 1.6996346733140488, fit: 0.11983333333333333
    epoch : 4
      loss :              1.7013913411173234
      loss_factor :              2.032554419588178e-08
      loss adapted learning_rate :              2.032554419588178e-12
    learning_rate : 2.032554419588178e-12
    batch_size :          12000
    best_batch_loss : 1.6905596566809153
    Epoch 4, Loss: 1.6996346732993772, fit: 0.12633333333333333
    epoch : 5
      loss :              1.6905596566809153
      loss_factor :              1.9067994066046236e-08
      loss adapted learning_rate :              1.9067994066046236e-12
    learning_rate : 1.9067994066046236e-12
    batch_size :          12000
    best_batch_loss : 1.6885819048925916
    Epoch 5, Loss: 1.6996346732858, fit: 0.11925
    epoch : 6
      loss :              1.7041228241343118
      loss_factor :              2.0654226343509682e-08
      loss adapted learning_rate :              2.0654226343509684e-12
    learning_rate : 2.0654226343509684e-12
    batch_size :          12000
    best_batch_loss : 1.693635195670399
    Epoch 6, Loss: 1.699634673270604, fit: 0.12216666666666667
    epoch : 7
      loss :              1.6983007899484956
      loss_factor :              1.995933723550482e-08
      loss adapted learning_rate :              1.995933723550482e-12
    learning_rate : 1.995933723550482e-12
    batch_size :          12000
    best_batch_loss : 1.6917800533615341
    Epoch 7, Loss: 1.6996346732569034, fit: 0.12091666666666667
    epoch : 8
      loss :              1.7038807419108384
      loss_factor :              2.062490436104568e-08
      loss adapted learning_rate :              2.062490436104568e-12
    learning_rate : 2.062490436104568e-12
    batch_size :          12000
    best_batch_loss : 1.6956309195401105
    Epoch 8, Loss: 1.6996346732417682, fit: 0.12408333333333334
    Best fit: 0.12633333333333333
    Best loss: 1.6996346732417682
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6956309195401105_fit_0.12408333333333334_2024-01-03_191923
  self.fit : 0.12408333333333334
  self.loss : 1.6956309195401105
  current_accuracy : 0.1217
   Accuracy mean: 0.1218
   Accuracy mean: 0.1217
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.001
    learning_rate : 0.001
    batch_size :          12000
    best_batch_loss : 1.6529778230043757
    Epoch 0, Loss: 1.6584380026631869, fit: 0.142
    Epoch 0, Loss: 1.6584380026631869
    Best fit: 0.142
    Best loss: 1.6584380026631869
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.6529778230043757_fit_0.142_2024-01-03_191925
  self.fit : 0.142
  self.loss : 1.6529778230043757
  current_accuracy : 0.1374
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.001
    learning_rate : 0.001
    batch_size :          12000
    best_batch_loss : 1.6462419267881427
    Epoch 0, Loss: 1.6520993826228234, fit: 0.146
    Epoch 0, Loss: 1.6520993826228234
    epoch : 1
      loss :              1.6462419267881427
      loss_factor :              1.461963674585142e-08
      loss adapted learning_rate :              1.461963674585142e-11
    learning_rate : 1.461963674585142e-11
    batch_size :          12000
    best_batch_loss : 1.6446363886587991
    Epoch 1, Loss: 1.6482063638560258, fit: 0.14275
    epoch : 2
      loss :              1.6519967562637032
      loss_factor :              1.513881571615158e-08
      loss adapted learning_rate :              1.513881571615158e-11
    learning_rate : 1.513881571615158e-11
    batch_size :          12000
    best_batch_loss : 1.6462262142157593
    Epoch 2, Loss: 1.6482063637557316, fit: 0.14608333333333334
    epoch : 3
      loss :              1.6462262142157593
      loss_factor :              1.4618241433103639e-08
      loss adapted learning_rate :              1.461824143310364e-11
    learning_rate : 1.461824143310364e-11
    batch_size :          12000
    best_batch_loss : 1.6425243519713963
    Epoch 3, Loss: 1.6482063636610764, fit: 0.14291666666666666
    epoch : 4
      loss :              1.6510760417630173
      loss_factor :              1.5054653195150465e-08
      loss adapted learning_rate :              1.5054653195150466e-11
    learning_rate : 1.5054653195150466e-11
    batch_size :          12000
    best_batch_loss : 1.6425705833894437
    Epoch 4, Loss: 1.6482063635601034, fit: 0.1415
    epoch : 5
      loss :              1.6540495211051203
      loss_factor :              1.5327985429164673e-08
      loss adapted learning_rate :              1.5327985429164673e-11
    learning_rate : 1.5327985429164673e-11
    batch_size :          12000
    best_batch_loss : 1.638392890419847
    Epoch 5, Loss: 1.6482063634633826, fit: 0.13575
    epoch : 6
      loss :              1.665888280178996
      loss_factor :              1.6461095277552214e-08
      loss adapted learning_rate :              1.6461095277552214e-11
    learning_rate : 1.6461095277552214e-11
    batch_size :          12000
    best_batch_loss : 1.6337771494607198
    Epoch 6, Loss: 1.6482063633641997, fit: 0.14016666666666666
    epoch : 7
      loss :              1.6577333008957218
      loss_factor :              1.5672800937627482e-08
      loss adapted learning_rate :              1.567280093762748e-11
    learning_rate : 1.567280093762748e-11
    batch_size :          12000
    best_batch_loss : 1.6410351020871328
    Epoch 7, Loss: 1.6482063632598012, fit: 0.14816666666666667
    epoch : 8
      loss :              1.6410351020871328
      loss_factor :              1.4163764854535773e-08
      loss adapted learning_rate :              1.4163764854535774e-11
    learning_rate : 1.4163764854535774e-11
    batch_size :          12000
    best_batch_loss : 1.6371284340007697
    Epoch 8, Loss: 1.6482063631576764, fit: 0.14816666666666667
    Best fit: 0.14816666666666667
    Best loss: 1.6482063631576764
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.6414323371675892_fit_0.14816666666666667_2024-01-03_191947
  self.fit : 0.14816666666666667
  self.loss : 1.6414323371675892
  current_accuracy : 0.1404
   Accuracy mean: 0.1374
   Accuracy mean: 0.1404
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.02
    learning_rate : 0.02
    batch_size :          12000
    best_batch_loss : 1.6875199946840644
    Epoch 0, Loss: 1.7242333541170456, fit: 0.12025
    Epoch 0, Loss: 1.7242333541170456
    Best fit: 0.12025
    Best loss: 1.7242333541170456
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.6875199946840644_fit_0.12025_2024-01-03_191949
  self.fit : 0.12025
  self.loss : 1.6875199946840644
  current_accuracy : 0.1258
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.02
    learning_rate : 0.02
    batch_size :          12000
    best_batch_loss : 1.6028784146520865
    Epoch 0, Loss: 1.6347051102828503, fit: 0.1625
    Epoch 0, Loss: 1.6347051102828503
    epoch : 1
      loss :              1.6028784146520865
      loss_factor :              1.1194528456381908e-08
      loss adapted learning_rate :              2.2389056912763818e-10
    learning_rate : 2.2389056912763818e-10
    batch_size :          12000
    best_batch_loss : 1.563655882573436
    Epoch 1, Loss: 1.5754160746241372, fit: 0.17633333333333334
    epoch : 2
      loss :              1.5730379595055355
      loss_factor :              9.276668310089285e-09
      loss adapted learning_rate :              1.8553336620178572e-10
    learning_rate : 1.8553336620178572e-10
    batch_size :          12000
    best_batch_loss : 1.562900018472989
    Epoch 2, Loss: 1.5754160735757143, fit: 0.1805
    epoch : 3
      loss :              1.5672356991271421
      loss_factor :              8.940116050283858e-09
      loss adapted learning_rate :              1.7880232100567715e-10
    learning_rate : 1.7880232100567715e-10
    batch_size :          12000
    best_batch_loss : 1.5676713728200973
    Epoch 3, Loss: 1.5754160725996105, fit: 0.17483333333333334
    epoch : 4
      loss :              1.577067870180308
      loss_factor :              9.517082605399691e-09
      loss adapted learning_rate :              1.9034165210799381e-10
    learning_rate : 1.9034165210799381e-10
    batch_size :          12000
    best_batch_loss : 1.5645702018211125
    Epoch 4, Loss: 1.5754160716539298, fit: 0.18041666666666667
    epoch : 5
      loss :              1.5645702018211125
      loss_factor :              8.789224262293985e-09
      loss adapted learning_rate :              1.757844852458797e-10
    learning_rate : 1.757844852458797e-10
    batch_size :          12000
    best_batch_loss : 1.571015272739515
    Epoch 5, Loss: 1.5754160706884326, fit: 0.17825
    epoch : 6
      loss :              1.571015272739515
      loss_factor :              9.158072364123446e-09
      loss adapted learning_rate :              1.8316144728246892e-10
    learning_rate : 1.8316144728246892e-10
    batch_size :          12000
    best_batch_loss : 1.5662495199258013
    Epoch 6, Loss: 1.5754160697595765, fit: 0.1795
    epoch : 7
      loss :              1.5662495199258013
      loss_factor :              8.884019617103013e-09
      loss adapted learning_rate :              1.7768039234206027e-10
    learning_rate : 1.7768039234206027e-10
    batch_size :          12000
    best_batch_loss : 1.563323186541363
    Epoch 7, Loss: 1.5754160688343446, fit: 0.17858333333333334
    epoch : 8
      loss :              1.5692078405192957
      loss_factor :              9.05325376814254e-09
      loss adapted learning_rate :              1.810650753628508e-10
    learning_rate : 1.810650753628508e-10
    batch_size :          12000
    best_batch_loss : 1.5609035857084752
    Epoch 8, Loss: 1.5754160679004818, fit: 0.16716666666666666
    Best fit: 0.1805
    Best loss: 1.5754160679004818
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.5900765715960365_fit_0.16716666666666666_2024-01-03_192011
  self.fit : 0.16716666666666666
  self.loss : 1.5900765715960365
  current_accuracy : 0.1801
   Accuracy mean: 0.1258
   Accuracy mean: 0.1801
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.025
    learning_rate : 0.025
    batch_size :          12000
    best_batch_loss : 1.6477137233113723
    Epoch 0, Loss: 1.673754374954422, fit: 0.14141666666666666
    Epoch 0, Loss: 1.673754374954422
    Best fit: 0.14141666666666666
    Best loss: 1.673754374954422
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.6477137233113723_fit_0.14141666666666666_2024-01-03_192014
  self.fit : 0.14141666666666666
  self.loss : 1.6477137233113723
  current_accuracy : 0.1453
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.025
    learning_rate : 0.025
    batch_size :          12000
    best_batch_loss : 1.514539827719704
    Epoch 0, Loss: 1.5763905809287972, fit: 0.206
    Epoch 0, Loss: 1.5763905809287972
    epoch : 1
      loss :              1.514539827719704
      loss_factor :              6.3504863113040006e-09
      loss adapted learning_rate :              1.5876215778260002e-10
    learning_rate : 1.5876215778260002e-10
    batch_size :          12000
    best_batch_loss : 1.4715287271940352
    Epoch 1, Loss: 1.4848973420812879, fit: 0.22425
    epoch : 2
      loss :              1.4783954069363898
      loss_factor :              4.9877657210990095e-09
      loss adapted learning_rate :              1.2469414302747525e-10
    learning_rate : 1.2469414302747525e-10
    batch_size :          12000
    best_batch_loss : 1.4761030394961303
    Epoch 2, Loss: 1.4848973412539461, fit: 0.22341666666666668
    epoch : 3
      loss :              1.4811504969076663
      loss_factor :              5.081499486898223e-09
      loss adapted learning_rate :              1.270374871724556e-10
    learning_rate : 1.270374871724556e-10
    batch_size :          12000
    best_batch_loss : 1.4744574765750322
    Epoch 3, Loss: 1.4848973405439796, fit: 0.2235
    epoch : 4
      loss :              1.479691070439867
      loss_factor :              5.031651221981504e-09
      loss adapted learning_rate :              1.257912805495376e-10
    learning_rate : 1.257912805495376e-10
    batch_size :          12000
    best_batch_loss : 1.4721749340542156
    Epoch 4, Loss: 1.4848973398329055, fit: 0.21566666666666667
    epoch : 5
      loss :              1.4970054427585786
      loss_factor :              5.6524117873071175e-09
      loss adapted learning_rate :              1.4131029468267795e-10
    learning_rate : 1.4131029468267795e-10
    batch_size :          12000
    best_batch_loss : 1.4742123728231868
    Epoch 5, Loss: 1.4848973391035565, fit: 0.217
    epoch : 6
      loss :              1.498313416008184
      loss_factor :              5.701993033980179e-09
      loss adapted learning_rate :              1.425498258495045e-10
    learning_rate : 1.425498258495045e-10
    batch_size :          12000
    best_batch_loss : 1.4799545866694668
    Epoch 6, Loss: 1.4848973383077033, fit: 0.21816666666666668
    epoch : 7
      loss :              1.4935140916242047
      loss_factor :              5.521959870764678e-09
      loss adapted learning_rate :              1.3804899676911695e-10
    learning_rate : 1.3804899676911695e-10
    batch_size :          12000
    best_batch_loss : 1.468383078716133
    Epoch 7, Loss: 1.4848973375035017, fit: 0.21766666666666667
    epoch : 8
      loss :              1.4928262352371173
      loss_factor :              5.496580445668861e-09
      loss adapted learning_rate :              1.3741451114172152e-10
    learning_rate : 1.3741451114172152e-10
    batch_size :          12000
    best_batch_loss : 1.475011160417005
    Epoch 8, Loss: 1.4848973367414104, fit: 0.22258333333333333
    Best fit: 0.22425
    Best loss: 1.4848973367414104
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.48555729883957_fit_0.22258333333333333_2024-01-03_192034
  self.fit : 0.22258333333333333
  self.loss : 1.48555729883957
  current_accuracy : 0.2241
   Accuracy mean: 0.1453
   Accuracy mean: 0.2241
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.03
    learning_rate : 0.03
    batch_size :          12000
    best_batch_loss : 1.5513706771413849
    Epoch 0, Loss: 1.6293146224811148, fit: 0.18983333333333333
    Epoch 0, Loss: 1.6293146224811148
    Best fit: 0.18983333333333333
    Best loss: 1.6293146224811148
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.5513706771413849_fit_0.18983333333333333_2024-01-03_192037
  self.fit : 0.18983333333333333
  self.loss : 1.5513706771413849
  current_accuracy : 0.2084
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.03
    learning_rate : 0.03
    batch_size :          12000
    best_batch_loss : 1.392112298528873
    Epoch 0, Loss: 1.45979991427878, fit: 0.27016666666666667
    Epoch 0, Loss: 1.45979991427878
    epoch : 1
      loss :              1.392112298528873
      loss_factor :              2.733648696987808e-09
      loss adapted learning_rate :              8.200946090963424e-11
    learning_rate : 8.200946090963424e-11
    batch_size :          12000
    best_batch_loss : 1.3555723284561902
    Epoch 1, Loss: 1.37048816518142, fit: 0.28175
    epoch : 2
      loss :              1.3681048167681962
      loss_factor :              2.297173086381731e-09
      loss adapted learning_rate :              6.891519259145193e-11
    learning_rate : 6.891519259145193e-11
    batch_size :          12000
    best_batch_loss : 1.3617241565096858
    Epoch 2, Loss: 1.3704881648274958, fit: 0.2791666666666667
    epoch : 3
      loss :              1.3725523996534714
      loss_factor :              2.372954080091928e-09
      loss adapted learning_rate :              7.118862240275784e-11
    learning_rate : 7.118862240275784e-11
    batch_size :          12000
    best_batch_loss : 1.3655199793420076
    Epoch 3, Loss: 1.370488164500931, fit: 0.28491666666666665
    epoch : 4
      loss :              1.3655199793420076
      loss_factor :              2.254138455852451e-09
      loss adapted learning_rate :              6.762415367557352e-11
    learning_rate : 6.762415367557352e-11
    batch_size :          12000
    best_batch_loss : 1.363145505041365
    Epoch 4, Loss: 1.3704881641801576, fit: 0.28441666666666665
    epoch : 5
      loss :              1.364810272843994
      loss_factor :              2.24245030514403e-09
      loss adapted learning_rate :              6.72735091543209e-11
    learning_rate : 6.72735091543209e-11
    batch_size :          12000
    best_batch_loss : 1.3633109526997065
    Epoch 5, Loss: 1.3704881638707915, fit: 0.2788333333333333
    epoch : 6
      loss :              1.3746204762296184
      loss_factor :              2.4089516775953968e-09
      loss adapted learning_rate :              7.22685503278619e-11
    learning_rate : 7.22685503278619e-11
    batch_size :          12000
    best_batch_loss : 1.3556539698755332
    Epoch 6, Loss: 1.3704881635464696, fit: 0.28258333333333335
    epoch : 7
      loss :              1.3670997024919247
      loss_factor :              2.2803519848367877e-09
      loss adapted learning_rate :              6.841055954510363e-11
    learning_rate : 6.841055954510363e-11
    batch_size :          12000
    best_batch_loss : 1.3600236451731698
    Epoch 7, Loss: 1.3704881632233863, fit: 0.27858333333333335
    epoch : 8
      loss :              1.3766053135268217
      loss_factor :              2.4439618112334867e-09
      loss adapted learning_rate :              7.33188543370046e-11
    learning_rate : 7.33188543370046e-11
    batch_size :          12000
    best_batch_loss : 1.3621680260054074
    Epoch 8, Loss: 1.37048816289425, fit: 0.2755
    Best fit: 0.28491666666666665
    Best loss: 1.37048816289425
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.3818835378816572_fit_0.2755_2024-01-03_192057
  self.fit : 0.2755
  self.loss : 1.3818835378816572
  current_accuracy : 0.2871
   Accuracy mean: 0.2084
   Accuracy mean: 0.2871
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.1
    learning_rate : 0.1
    batch_size :          12000
    best_batch_loss : 1.2862738694100588
    Epoch 0, Loss: 1.5408379896844542, fit: 0.3234166666666667
    Epoch 0, Loss: 1.5408379896844542
    Best fit: 0.3234166666666667
    Best loss: 1.5408379896844542
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.2862738694100588_fit_0.3234166666666667_2024-01-03_192100
  self.fit : 0.3234166666666667
  self.loss : 1.2862738694100588
  current_accuracy : 0.3689
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.1
    learning_rate : 0.1
    batch_size :          12000
    best_batch_loss : 0.9762900159977025
    Epoch 0, Loss: 1.0715021930510136, fit: 0.48383333333333334
    Epoch 0, Loss: 1.0715021930510136
    epoch : 1
      loss :              0.9762900159977025
      loss_factor :              7.866625586494499e-11
      loss adapted learning_rate :              7.8666255864945e-12
    learning_rate : 7.8666255864945e-12
    batch_size :          12000
    best_batch_loss : 0.9118086159383109
    Epoch 1, Loss: 0.9220795148208897, fit: 0.5145
    epoch : 2
      loss :              0.9151995085472763
      loss_factor :              4.122472643513368e-11
      loss adapted learning_rate :              4.122472643513368e-12
    learning_rate : 4.122472643513368e-12
    batch_size :          12000
    best_batch_loss : 0.9114572679228911
    Epoch 2, Loss: 0.9220795148112485, fit: 0.5173333333333333
    epoch : 3
      loss :              0.9114572679228911
      loss_factor :              3.956973331756539e-11
      loss adapted learning_rate :              3.956973331756539e-12
    learning_rate : 3.956973331756539e-12
    batch_size :          12000
    best_batch_loss : 0.9142651571498878
    Epoch 3, Loss: 0.9220795148053801, fit: 0.50475
    epoch : 4
      loss :              0.9386704098912564
      loss_factor :              5.3104494151518317e-11
      loss adapted learning_rate :              5.310449415151832e-12
    learning_rate : 5.310449415151832e-12
    batch_size :          12000
    best_batch_loss : 0.9150847536344496
    Epoch 4, Loss: 0.9220795147986982, fit: 0.5109166666666667
    epoch : 5
      loss :              0.9230442115028107
      loss_factor :              4.4897796938523054e-11
      loss adapted learning_rate :              4.489779693852306e-12
    learning_rate : 4.489779693852306e-12
    batch_size :          12000
    best_batch_loss : 0.9001367902203264
    Epoch 5, Loss: 0.9220795147911924, fit: 0.5098333333333334
    epoch : 6
      loss :              0.9283703673596674
      loss_factor :              4.7556808050445966e-11
      loss adapted learning_rate :              4.755680805044597e-12
    learning_rate : 4.755680805044597e-12
    batch_size :          12000
    best_batch_loss : 0.9133814361549507
    Epoch 6, Loss: 0.9220795147843324, fit: 0.50975
    epoch : 7
      loss :              0.9245648559139337
      loss_factor :              4.5642961176208644e-11
      loss adapted learning_rate :              4.564296117620865e-12
    learning_rate : 4.564296117620865e-12
    batch_size :          12000
    best_batch_loss : 0.9158225515017278
    Epoch 7, Loss: 0.9220795147772339, fit: 0.5155
    epoch : 8
      loss :              0.9158225515017278
      loss_factor :              4.150623448736619e-11
      loss adapted learning_rate :              4.150623448736619e-12
    learning_rate : 4.150623448736619e-12
    batch_size :          12000
    best_batch_loss : 0.9184201904391738
    Epoch 8, Loss: 0.9220795147707811, fit: 0.5136666666666667
    Best fit: 0.5173333333333333
    Best loss: 0.9220795147707811
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_0.918972429250587_fit_0.5136666666666667_2024-01-03_192121
  self.fit : 0.5136666666666667
  self.loss : 0.918972429250587
  current_accuracy : 0.5222
   Accuracy mean: 0.3689
   Accuracy mean: 0.5222
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.9
    learning_rate : 0.9
    batch_size :          12000
    best_batch_loss : 1.055682109971251
    Epoch 0, Loss: 1.3090309537359974, fit: 0.43983333333333335
    Epoch 0, Loss: 1.3090309537359974
    Best fit: 0.43983333333333335
    Best loss: 1.3090309537359974
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.078399606770739_fit_0.43983333333333335_2024-01-03_192124
  self.fit : 0.43983333333333335
  self.loss : 1.078399606770739
  current_accuracy : 0.3977
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.2
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.9
    learning_rate : 0.9
    batch_size :          12000
    best_batch_loss : 1.169492446421338
    Epoch 0, Loss: 1.6358754218994067, fit: 0.09841666666666667
    Epoch 0, Loss: 1.6358754218994067
    epoch : 1
      loss :              1.8031666666666666
      loss_factor :              3.6337806177241216e-08
      loss adapted learning_rate :              3.2704025559517094e-08
    learning_rate : 3.2704025559517094e-08
    batch_size :          12000
    best_batch_loss : 1.7969997496965888
    Epoch 1, Loss: 1.8025191892167738, fit: 0.09666666666666666
    epoch : 2
      loss :              1.806630347892998
      loss_factor :              3.704187933205388e-08
      loss adapted learning_rate :              3.333769139884849e-08
    learning_rate : 3.333769139884849e-08
    batch_size :          12000
    best_batch_loss : 1.795166629190102
    Epoch 2, Loss: 1.80251918908766, fit: 0.09841666666666667
    epoch : 3
      loss :              1.8031667514903353
      loss_factor :              3.6337823271094765e-08
      loss adapted learning_rate :              3.270404094398529e-08
    learning_rate : 3.270404094398529e-08
    batch_size :          12000
    best_batch_loss : 1.7948333806590957
    Epoch 3, Loss: 1.8025191883453617, fit: 0.10258333333333333
    epoch : 4
      loss :              1.7948333806590957
      loss_factor :              3.469296198948405e-08
      loss adapted learning_rate :              3.122366579053565e-08
    learning_rate : 3.122366579053565e-08
    batch_size :          12000
    best_batch_loss : 1.799333328043999
    Epoch 4, Loss: 1.802519188021766, fit: 0.09975
    epoch : 5
      loss :              1.8004995912553876
      loss_factor :              3.580389469672882e-08
      loss adapted learning_rate :              3.222350522705594e-08
    learning_rate : 3.222350522705594e-08
    batch_size :          12000
    best_batch_loss : 1.8011664871553315
    Epoch 5, Loss: 1.8025191873260231, fit: 0.09858333333333333
    epoch : 6
      loss :              1.8028333333333322
      loss_factor :              3.627068797269697e-08
      loss adapted learning_rate :              3.264361917542728e-08
    learning_rate : 3.264361917542728e-08
    batch_size :          12000
    best_batch_loss : 1.7967993386833445
    Epoch 6, Loss: 1.8025191868975319, fit: 0.1
    epoch : 7
      loss :              1.7999995923712402
      loss_factor :              3.570459140937089e-08
      loss adapted learning_rate :              3.2134132268433804e-08
    learning_rate : 3.2134132268433804e-08
    batch_size :          12000
    best_batch_loss : 1.7943332959487248
    Epoch 7, Loss: 1.8025191860836771, fit: 0.10158333333333333
    epoch : 8
      loss :              1.7967970008987701
      loss_factor :              3.507439109195312e-08
      loss adapted learning_rate :              3.156695198275781e-08
    learning_rate : 3.156695198275781e-08
    batch_size :          12000
    best_batch_loss : 1.7976666614471153
    Epoch 8, Loss: 1.8025191858508818, fit: 0.09775
    Best fit: 0.10258333333333333
    Best loss: 1.6358754218994067
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.8044999989024524_fit_0.09775_2024-01-03_192146
  self.fit : 0.09775
  self.loss : 1.8044999989024524
  current_accuracy : 0.098
   Accuracy mean: 0.3977
   Accuracy mean: 0.098
  Error saving file: doc/out/test_combinations_results/20240103192146.
  normalized_accuracies :      [0.11064764 0.11064764 0.13084523 0.13084523 0.06037322 0.06037322
   0.08320527 0.08320527 0.         0.         0.12096597 0.12074643
   0.15521405 0.16180022 0.12974753 0.24895719 0.17255763 0.34555434
   0.31108672 0.48386389 0.66344676 1.         0.72667398 0.0687157 ]
batch_rate :  0.1
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-64
    learning_rate : 1e-64
    batch_size :          6000
    best_batch_loss : 1.7709700971428068
    Epoch 0, Loss: 1.7806222747736176, fit: 0.08216666666666667
    Epoch 0, Loss: 1.7806222747736176
    Best fit: 0.08216666666666667
    Best loss: 1.7806222747736176
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7858105391971382_fit_0.08216666666666667_2024-01-03_192149
  self.fit : 0.08216666666666667
  self.loss : 1.7858105391971382
  current_accuracy : 0.0783
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-64
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-64
    learning_rate : 1e-64
    batch_size :          6000
    best_batch_loss : 1.7736344320878057
    Epoch 0, Loss: 1.7806222747736176, fit: 0.09033333333333333
    Epoch 0, Loss: 1.7806222747736176
    epoch : 1
      loss :              1.775718178378903
      loss_factor :              3.117025869832841e-08
      loss adapted learning_rate :              3.117025869832841e-72
    learning_rate : 3.117025869832841e-72
    batch_size :          6000
    best_batch_loss : 1.7749200511365022
    Epoch 1, Loss: 1.7806222747736173, fit: 0.08516666666666667
    epoch : 2
      loss :              1.7828359333355672
      loss_factor :              3.244246074025412e-08
      loss adapted learning_rate :              3.244246074025412e-72
    learning_rate : 3.244246074025412e-72
    batch_size :          6000
    best_batch_loss : 1.7722724009191266
    Epoch 2, Loss: 1.7806222747736173, fit: 0.082
    epoch : 3
      loss :              1.7904932860801133
      loss_factor :              3.386311991392459e-08
      loss adapted learning_rate :              3.3863119913924584e-72
    learning_rate : 3.3863119913924584e-72
    batch_size :          6000
    best_batch_loss : 1.7738000788370163
    Epoch 3, Loss: 1.7806222747736173, fit: 0.0895
    epoch : 4
      loss :              1.7738000788370163
      loss_factor :              3.083519495852128e-08
      loss adapted learning_rate :              3.0835194958521275e-72
    learning_rate : 3.0835194958521275e-72
    batch_size :          6000
    best_batch_loss : 1.7666023032934928
    Epoch 4, Loss: 1.7806222747736176, fit: 0.09266666666666666
    epoch : 5
      loss :              1.7666023032934928
      loss_factor :              2.9606558220395518e-08
      loss adapted learning_rate :              2.960655822039552e-72
    learning_rate : 2.960655822039552e-72
    batch_size :          6000
    best_batch_loss : 1.7706090210702101
    Epoch 5, Loss: 1.7806222747736173, fit: 0.089
    epoch : 6
      loss :              1.778508968889857
      loss_factor :              3.166362230141946e-08
      loss adapted learning_rate :              3.1663622301419455e-72
    learning_rate : 3.1663622301419455e-72
    batch_size :          6000
    best_batch_loss : 1.7712215528162705
    Epoch 6, Loss: 1.7806222747736178, fit: 0.08766666666666667
    epoch : 7
      loss :              1.77856823558605
      loss_factor :              3.167417540939064e-08
      loss adapted learning_rate :              3.167417540939064e-72
    learning_rate : 3.167417540939064e-72
    batch_size :          6000
    best_batch_loss : 1.7667954097679273
    Epoch 7, Loss: 1.7806222747736173, fit: 0.08283333333333333
    epoch : 8
      loss :              1.7861275113562056
      loss_factor :              3.304643373814318e-08
      loss adapted learning_rate :              3.304643373814318e-72
    learning_rate : 3.304643373814318e-72
    batch_size :          6000
    best_batch_loss : 1.7735096855584922
    Epoch 8, Loss: 1.7806222747736176, fit: 0.084
    Best fit: 0.09266666666666666
    Best loss: 1.7806222747736173
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7833394395657145_fit_0.084_2024-01-03_192210
  self.fit : 0.084
  self.loss : 1.7833394395657145
  current_accuracy : 0.0783
   Accuracy mean: 0.0783
   Accuracy mean: 0.0783
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-33
    learning_rate : 1e-33
    batch_size :          6000
    best_batch_loss : 1.752820232762082
    Epoch 0, Loss: 1.7633992287623075, fit: 0.09233333333333334
    Epoch 0, Loss: 1.7633992287623075
    Best fit: 0.09233333333333334
    Best loss: 1.7633992287623075
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.752820232762082_fit_0.09233333333333334_2024-01-03_192212
  self.fit : 0.09233333333333334
  self.loss : 1.752820232762082
  current_accuracy : 0.0862
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-33
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-33
    learning_rate : 1e-33
    batch_size :          6000
    best_batch_loss : 1.7441539300220277
    Epoch 0, Loss: 1.7633992287623075, fit: 0.09733333333333333
    Epoch 0, Loss: 1.7633992287623075
    epoch : 1
      loss :              1.7441539300220277
      loss_factor :              2.6052422267618656e-08
      loss adapted learning_rate :              2.605242226761866e-41
    learning_rate : 2.605242226761866e-41
    batch_size :          6000
    best_batch_loss : 1.746568182205957
    Epoch 1, Loss: 1.7633992287623075, fit: 0.09133333333333334
    epoch : 2
      loss :              1.754475592482957
      loss_factor :              2.7635880668516583e-08
      loss adapted learning_rate :              2.7635880668516584e-41
    learning_rate : 2.7635880668516584e-41
    batch_size :          6000
    best_batch_loss : 1.7546911620113337
    Epoch 2, Loss: 1.763399228762307, fit: 0.09233333333333334
    epoch : 3
      loss :              1.7546911620113337
      loss_factor :              2.7669855200783842e-08
      loss adapted learning_rate :              2.766985520078384e-41
    learning_rate : 2.766985520078384e-41
    batch_size :          6000
    best_batch_loss : 1.7538575550637479
    Epoch 3, Loss: 1.7633992287623075, fit: 0.08733333333333333
    epoch : 4
      loss :              1.7607552115195215
      loss_factor :              2.8641108664315824e-08
      loss adapted learning_rate :              2.8641108664315824e-41
    learning_rate : 2.8641108664315824e-41
    batch_size :          6000
    best_batch_loss : 1.7529137951225193
    Epoch 4, Loss: 1.7633992287623073, fit: 0.09
    epoch : 5
      loss :              1.7553060317772131
      loss_factor :              2.7766967505433648e-08
      loss adapted learning_rate :              2.776696750543365e-41
    learning_rate : 2.776696750543365e-41
    batch_size :          6000
    best_batch_loss : 1.7541071248118945
    Epoch 5, Loss: 1.7633992287623073, fit: 0.08916666666666667
    epoch : 6
      loss :              1.7564953848149736
      loss_factor :              2.7955684481994385e-08
      loss adapted learning_rate :              2.795568448199439e-41
    learning_rate : 2.795568448199439e-41
    batch_size :          6000
    best_batch_loss : 1.753080675806439
    Epoch 6, Loss: 1.7633992287623073, fit: 0.08483333333333333
    epoch : 7
      loss :              1.763699896908257
      loss_factor :              2.912372323290167e-08
      loss adapted learning_rate :              2.912372323290167e-41
    learning_rate : 2.912372323290167e-41
    batch_size :          6000
    best_batch_loss : 1.7511294524207093
    Epoch 7, Loss: 1.7633992287623075, fit: 0.08683333333333333
    epoch : 8
      loss :              1.7619944800720733
      loss_factor :              2.884333242531473e-08
      loss adapted learning_rate :              2.884333242531473e-41
    learning_rate : 2.884333242531473e-41
    batch_size :          6000
    best_batch_loss : 1.7555441034513954
    Epoch 8, Loss: 1.7633992287623075, fit: 0.0855
    Best fit: 0.09733333333333333
    Best loss: 1.763399228762307
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7664990171191657_fit_0.0855_2024-01-03_192233
  self.fit : 0.0855
  self.loss : 1.7664990171191657
  current_accuracy : 0.0862
   Accuracy mean: 0.0862
   Accuracy mean: 0.0862
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-17
    learning_rate : 1e-17
    batch_size :          6000
    best_batch_loss : 1.726562180802746
    Epoch 0, Loss: 1.7435269393266253, fit: 0.09533333333333334
    Epoch 0, Loss: 1.7435269393266253
    Best fit: 0.09533333333333334
    Best loss: 1.7435269393266253
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7530099615869081_fit_0.09533333333333334_2024-01-03_192235
  self.fit : 0.09533333333333334
  self.loss : 1.7530099615869081
  current_accuracy : 0.0993
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-17
    learning_rate : 1e-17
    batch_size :          6000
    best_batch_loss : 1.7275040829023796
    Epoch 0, Loss: 1.7435269393266253, fit: 0.09483333333333334
    Epoch 0, Loss: 1.7435269393266253
    epoch : 1
      loss :              1.7492021759956051
      loss_factor :              2.6816376183178605e-08
      loss adapted learning_rate :              2.681637618317861e-25
    learning_rate : 2.681637618317861e-25
    batch_size :          6000
    best_batch_loss : 1.7326949554866036
    Epoch 1, Loss: 1.743526939326625, fit: 0.09433333333333334
    epoch : 2
      loss :              1.7528512402737064
      loss_factor :              2.7381081796789234e-08
      loss adapted learning_rate :              2.7381081796789237e-25
    learning_rate : 2.7381081796789237e-25
    batch_size :          6000
    best_batch_loss : 1.7338295750194062
    Epoch 2, Loss: 1.7435269393266253, fit: 0.101
    epoch : 3
      loss :              1.739301043784644
      loss_factor :              2.5336555707273673e-08
      loss adapted learning_rate :              2.5336555707273676e-25
    learning_rate : 2.5336555707273676e-25
    batch_size :          6000
    best_batch_loss : 1.7353943185237453
    Epoch 3, Loss: 1.743526939326625, fit: 0.09766666666666667
    epoch : 4
      loss :              1.7474264595552123
      loss_factor :              2.6545387853720767e-08
      loss adapted learning_rate :              2.654538785372077e-25
    learning_rate : 2.654538785372077e-25
    batch_size :          6000
    best_batch_loss : 1.7340024183008338
    Epoch 4, Loss: 1.743526939326625, fit: 0.10333333333333333
    epoch : 5
      loss :              1.7340024183008338
      loss_factor :              2.4575195886092763e-08
      loss adapted learning_rate :              2.457519588609276e-25
    learning_rate : 2.457519588609276e-25
    batch_size :          6000
    best_batch_loss : 1.7254925345937937
    Epoch 5, Loss: 1.743526939326625, fit: 0.10466666666666667
    epoch : 6
      loss :              1.7314558962513313
      loss_factor :              2.4216665138695137e-08
      loss adapted learning_rate :              2.4216665138695137e-25
    learning_rate : 2.4216665138695137e-25
    batch_size :          6000
    best_batch_loss : 1.73538095089909
    Epoch 6, Loss: 1.7435269393266253, fit: 0.1
    epoch : 7
      loss :              1.740142446826442
      loss_factor :              2.5459390814532765e-08
      loss adapted learning_rate :              2.5459390814532767e-25
    learning_rate : 2.5459390814532767e-25
    batch_size :          6000
    best_batch_loss : 1.734684589537178
    Epoch 7, Loss: 1.743526939326625, fit: 0.10216666666666667
    epoch : 8
      loss :              1.734684589537178
      loss_factor :              2.467204812551069e-08
      loss adapted learning_rate :              2.467204812551069e-25
    learning_rate : 2.467204812551069e-25
    batch_size :          6000
    best_batch_loss : 1.736763162290678
    Epoch 8, Loss: 1.7435269393266248, fit: 0.1025
    Best fit: 0.10466666666666667
    Best loss: 1.7435269393266248
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7391482872202417_fit_0.1025_2024-01-03_192256
  self.fit : 0.1025
  self.loss : 1.7391482872202417
  current_accuracy : 0.0993
   Accuracy mean: 0.0993
   Accuracy mean: 0.0993
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-10
    learning_rate : 1e-10
    batch_size :          6000
    best_batch_loss : 1.8009285061915135
    Epoch 0, Loss: 1.8118422444725388, fit: 0.06566666666666666
    Epoch 0, Loss: 1.8118422444725388
    Best fit: 0.06566666666666666
    Best loss: 1.8118422444725388
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.810555928357322_fit_0.06566666666666666_2024-01-03_192259
  self.fit : 0.06566666666666666
  self.loss : 1.810555928357322
  current_accuracy : 0.0629
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-10
    learning_rate : 1e-10
    batch_size :          6000
    best_batch_loss : 1.8034397366385435
    Epoch 0, Loss: 1.811842243393533, fit: 0.061
    Epoch 0, Loss: 1.811842243393533
    epoch : 1
      loss :              1.8152685093020184
      loss_factor :              3.885158464748633e-08
      loss adapted learning_rate :              3.885158464748633e-18
    learning_rate : 3.885158464748633e-18
    batch_size :          6000
    best_batch_loss : 1.8035814724882377
    Epoch 1, Loss: 1.811842242792407, fit: 0.06266666666666666
    epoch : 2
      loss :              1.8129553635892417
      loss_factor :              3.835933905339443e-08
      loss adapted learning_rate :              3.835933905339443e-18
    learning_rate : 3.835933905339443e-18
    batch_size :          6000
    best_batch_loss : 1.8038046614809962
    Epoch 2, Loss: 1.811842242792407, fit: 0.06366666666666666
    epoch : 3
      loss :              1.8157705174417773
      loss_factor :              3.8959161578054134e-08
      loss adapted learning_rate :              3.8959161578054134e-18
    learning_rate : 3.8959161578054134e-18
    batch_size :          6000
    best_batch_loss : 1.8056543335883262
    Epoch 3, Loss: 1.8118422427924066, fit: 0.063
    epoch : 4
      loss :              1.8111488889175071
      loss_factor :              3.7978826182676766e-08
      loss adapted learning_rate :              3.797882618267677e-18
    learning_rate : 3.797882618267677e-18
    batch_size :          6000
    best_batch_loss : 1.7981930166420876
    Epoch 4, Loss: 1.8118422427924064, fit: 0.06916666666666667
    epoch : 5
      loss :              1.7981930166420876
      loss_factor :              3.5347855204002337e-08
      loss adapted learning_rate :              3.5347855204002334e-18
    learning_rate : 3.5347855204002334e-18
    batch_size :          6000
    best_batch_loss : 1.7991078784406285
    Epoch 5, Loss: 1.811842242792407, fit: 0.0635
    epoch : 6
      loss :              1.8106449449869044
      loss_factor :              3.787328404553711e-08
      loss adapted learning_rate :              3.787328404553711e-18
    learning_rate : 3.787328404553711e-18
    batch_size :          6000
    best_batch_loss : 1.8014873005509722
    Epoch 6, Loss: 1.8118422427924066, fit: 0.06216666666666667
    epoch : 7
      loss :              1.8111570752492745
      loss_factor :              3.798054284770114e-08
      loss adapted learning_rate :              3.798054284770114e-18
    learning_rate : 3.798054284770114e-18
    batch_size :          6000
    best_batch_loss : 1.8016518164845132
    Epoch 7, Loss: 1.8118422427924068, fit: 0.06433333333333334
    epoch : 8
      loss :              1.8109215235486436
      loss_factor :              3.7931175802115316e-08
      loss adapted learning_rate :              3.793117580211532e-18
    learning_rate : 3.793117580211532e-18
    batch_size :          6000
    best_batch_loss : 1.7928658723667237
    Epoch 8, Loss: 1.811842242792407, fit: 0.07416666666666667
    Best fit: 0.07416666666666667
    Best loss: 1.8118422427924064
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7928658723667237_fit_0.07416666666666667_2024-01-03_192322
  self.fit : 0.07416666666666667
  self.loss : 1.7928658723667237
  current_accuracy : 0.0629
   Accuracy mean: 0.0629
   Accuracy mean: 0.0629
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-07
    learning_rate : 1e-07
    batch_size :          6000
    best_batch_loss : 1.6826302509019953
    Epoch 0, Loss: 1.6961093921120804, fit: 0.13066666666666665
    Epoch 0, Loss: 1.6961093921120804
    Best fit: 0.13066666666666665
    Best loss: 1.6961093921120804
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.6826302509019953_fit_0.13066666666666665_2024-01-03_192324
  self.fit : 0.13066666666666665
  self.loss : 1.6826302509019953
  current_accuracy : 0.1162
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-07
    learning_rate : 1e-07
    batch_size :          6000
    best_batch_loss : 1.6754520327828388
    Epoch 0, Loss: 1.6961073666346334, fit: 0.1135
    Epoch 0, Loss: 1.6961073666346334
    epoch : 1
      loss :              1.709956545370419
      loss_factor :              2.1372274293991275e-08
      loss adapted learning_rate :              2.1372274293991273e-15
    learning_rate : 2.1372274293991273e-15
    batch_size :          6000
    best_batch_loss : 1.6860280503940117
    Epoch 1, Loss: 1.6961063134079493, fit: 0.11516666666666667
    epoch : 2
      loss :              1.7077577426992478
      loss_factor :              2.1099036834801862e-08
      loss adapted learning_rate :              2.109903683480186e-15
    learning_rate : 2.109903683480186e-15
    batch_size :          6000
    best_batch_loss : 1.676590116471932
    Epoch 2, Loss: 1.696106313407907, fit: 0.12066666666666667
    epoch : 3
      loss :              1.7016591908915686
      loss_factor :              2.03575653468355e-08
      loss adapted learning_rate :              2.03575653468355e-15
    learning_rate : 2.03575653468355e-15
    batch_size :          6000
    best_batch_loss : 1.6863110770272742
    Epoch 3, Loss: 1.6961063134078664, fit: 0.11933333333333333
    epoch : 4
      loss :              1.703981582823025
      loss_factor :              2.0637114066309995e-08
      loss adapted learning_rate :              2.0637114066309994e-15
    learning_rate : 2.0637114066309994e-15
    batch_size :          6000
    best_batch_loss : 1.6857710800118655
    Epoch 4, Loss: 1.6961063134078276, fit: 0.12616666666666668
    epoch : 5
      loss :              1.690588885519286
      loss_factor :              1.907129107291902e-08
      loss adapted learning_rate :              1.907129107291902e-15
    learning_rate : 1.907129107291902e-15
    batch_size :          6000
    best_batch_loss : 1.687584182701796
    Epoch 5, Loss: 1.6961063134077887, fit: 0.12516666666666668
    epoch : 6
      loss :              1.687584182701796
      loss_factor :              1.8735032981463947e-08
      loss adapted learning_rate :              1.8735032981463946e-15
    learning_rate : 1.8735032981463946e-15
    batch_size :          6000
    best_batch_loss : 1.6802377690318233
    Epoch 6, Loss: 1.6961063134077516, fit: 0.12083333333333333
    epoch : 7
      loss :              1.6979689258929074
      loss_factor :              1.992036907923181e-08
      loss adapted learning_rate :              1.992036907923181e-15
    learning_rate : 1.992036907923181e-15
    batch_size :          6000
    best_batch_loss : 1.6908789419099282
    Epoch 7, Loss: 1.696106313407715, fit: 0.11766666666666667
    epoch : 8
      loss :              1.7057601499192214
      loss_factor :              2.0853532911888764e-08
      loss adapted learning_rate :              2.085353291188876e-15
    learning_rate : 2.085353291188876e-15
    batch_size :          6000
    best_batch_loss : 1.683866921706051
    Epoch 8, Loss: 1.6961063134076755, fit: 0.12183333333333334
    Best fit: 0.12616666666666668
    Best loss: 1.6961063134076755
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.698494327999219_fit_0.12183333333333334_2024-01-03_192345
  self.fit : 0.12183333333333334
  self.loss : 1.698494327999219
  current_accuracy : 0.1162
   Accuracy mean: 0.1162
   Accuracy mean: 0.1162
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    learning_rate : 0.0001
    batch_size :          6000
    best_batch_loss : 1.7076460642588989
    Epoch 0, Loss: 1.7198321649478698, fit: 0.1095
    Epoch 0, Loss: 1.7198321649478698
    Best fit: 0.1095
    Best loss: 1.7198321649478698
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.715984381672672_fit_0.1095_2024-01-03_192348
  self.fit : 0.1095
  self.loss : 1.715984381672672
  current_accuracy : 0.1069
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    learning_rate : 0.0001
    batch_size :          6000
    best_batch_loss : 1.7064943658536134
    Epoch 0, Loss: 1.7190712074720655, fit: 0.11316666666666667
    Epoch 0, Loss: 1.7190712074720655
    epoch : 1
      loss :              1.7064943658536134
      loss_factor :              2.0943467518752667e-08
      loss adapted learning_rate :              2.094346751875267e-12
    learning_rate : 2.094346751875267e-12
    batch_size :          6000
    best_batch_loss : 1.7061531098020777
    Epoch 1, Loss: 1.7186652049341669, fit: 0.106
    epoch : 2
      loss :              1.7284331261214338
      loss_factor :              2.379719730983434e-08
      loss adapted learning_rate :              2.379719730983434e-12
    learning_rate : 2.379719730983434e-12
    batch_size :          6000
    best_batch_loss : 1.7101537327579865
    Epoch 2, Loss: 1.7186652049175466, fit: 0.11266666666666666
    epoch : 3
      loss :              1.7118838956158877
      loss_factor :              2.1614393994591212e-08
      loss adapted learning_rate :              2.1614393994591214e-12
    learning_rate : 2.1614393994591214e-12
    batch_size :          6000
    best_batch_loss : 1.7065446472526822
    Epoch 3, Loss: 1.7186652048993454, fit: 0.10816666666666666
    epoch : 4
      loss :              1.7182821872262468
      loss_factor :              2.2435971493517944e-08
      loss adapted learning_rate :              2.2435971493517944e-12
    learning_rate : 2.2435971493517944e-12
    batch_size :          6000
    best_batch_loss : 1.7069405996606255
    Epoch 4, Loss: 1.7186652048835636, fit: 0.1065
    epoch : 5
      loss :              1.725211175112172
      loss_factor :              2.3357299331334154e-08
      loss adapted learning_rate :              2.3357299331334155e-12
    learning_rate : 2.3357299331334155e-12
    batch_size :          6000
    best_batch_loss : 1.7013771164256202
    Epoch 5, Loss: 1.71866520486606, fit: 0.105
    epoch : 6
      loss :              1.7265043538998612
      loss_factor :              2.3532972045139356e-08
      loss adapted learning_rate :              2.3532972045139356e-12
    learning_rate : 2.3532972045139356e-12
    batch_size :          6000
    best_batch_loss : 1.709081209409374
    Epoch 6, Loss: 1.718665204847674, fit: 0.09966666666666667
    epoch : 7
      loss :              1.737029459547626
      loss_factor :              2.500758998813231e-08
      loss adapted learning_rate :              2.500758998813231e-12
    learning_rate : 2.500758998813231e-12
    batch_size :          6000
    best_batch_loss : 1.704619688736838
    Epoch 7, Loss: 1.7186652048292095, fit: 0.10833333333333334
    epoch : 8
      loss :              1.7182320266941011
      loss_factor :              2.242942278777671e-08
      loss adapted learning_rate :              2.242942278777671e-12
    learning_rate : 2.242942278777671e-12
    batch_size :          6000
    best_batch_loss : 1.7101324279733638
    Epoch 8, Loss: 1.7186652048109616, fit: 0.1135
    Best fit: 0.1135
    Best loss: 1.7186652048109616
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.7101324279733638_fit_0.1135_2024-01-03_192411
  self.fit : 0.1135
  self.loss : 1.7101324279733638
  current_accuracy : 0.1069
   Accuracy mean: 0.1069
   Accuracy mean: 0.1069
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.001
    learning_rate : 0.001
    batch_size :          6000
    best_batch_loss : 1.69977185410116
    Epoch 0, Loss: 1.7134979030778792, fit: 0.11416666666666667
    Epoch 0, Loss: 1.7134979030778792
    Best fit: 0.11416666666666667
    Best loss: 1.7134979030778792
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.7165224573892894_fit_0.11416666666666667_2024-01-03_192413
  self.fit : 0.11416666666666667
  self.loss : 1.7165224573892894
  current_accuracy : 0.1253
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.001
    learning_rate : 0.001
    batch_size :          6000
    best_batch_loss : 1.6786689051067676
    Epoch 0, Loss: 1.694637798477894, fit: 0.12683333333333333
    Epoch 0, Loss: 1.694637798477894
    epoch : 1
      loss :              1.6891978381082948
      loss_factor :              1.8914948766376456e-08
      loss adapted learning_rate :              1.8914948766376455e-11
    learning_rate : 1.8914948766376455e-11
    batch_size :          6000
    best_batch_loss : 1.6758496783318093
    Epoch 1, Loss: 1.6870938570661924, fit: 0.13083333333333333
    epoch : 2
      loss :              1.6827541065708498
      loss_factor :              1.8205666831557828e-08
      loss adapted learning_rate :              1.820566683155783e-11
    learning_rate : 1.820566683155783e-11
    batch_size :          6000
    best_batch_loss : 1.6748034947117372
    Epoch 2, Loss: 1.6870938567868292, fit: 0.1325
    epoch : 3
      loss :              1.6801838560652649
      loss_factor :              1.792949572588251e-08
      loss adapted learning_rate :              1.792949572588251e-11
    learning_rate : 1.792949572588251e-11
    batch_size :          6000
    best_batch_loss : 1.6793639633699893
    Epoch 3, Loss: 1.6870938565029914, fit: 0.12783333333333333
    epoch : 4
      loss :              1.683801081040434
      loss_factor :              1.8319256355266465e-08
      loss adapted learning_rate :              1.8319256355266465e-11
    learning_rate : 1.8319256355266465e-11
    batch_size :          6000
    best_batch_loss : 1.6809102692196778
    Epoch 4, Loss: 1.6870938562497013, fit: 0.12266666666666666
    epoch : 5
      loss :              1.6977161924560669
      loss_factor :              1.9890738545585504e-08
      loss adapted learning_rate :              1.9890738545585505e-11
    learning_rate : 1.9890738545585505e-11
    batch_size :          6000
    best_batch_loss : 1.6766420029757705
    Epoch 5, Loss: 1.6870938559567368, fit: 0.127
    epoch : 6
      loss :              1.6908621044610421
      loss_factor :              1.9102134935039208e-08
      loss adapted learning_rate :              1.9102134935039208e-11
    learning_rate : 1.9102134935039208e-11
    batch_size :          6000
    best_batch_loss : 1.6720244944050082
    Epoch 6, Loss: 1.6870938556998605, fit: 0.126
    epoch : 7
      loss :              1.6924333553763993
      loss_factor :              1.9280387603622666e-08
      loss adapted learning_rate :              1.9280387603622668e-11
    learning_rate : 1.9280387603622668e-11
    batch_size :          6000
    best_batch_loss : 1.6683812095330242
    Epoch 7, Loss: 1.687093855427444, fit: 0.137
    epoch : 8
      loss :              1.6683812095330242
      loss_factor :              1.670909386633092e-08
      loss adapted learning_rate :              1.670909386633092e-11
    learning_rate : 1.670909386633092e-11
    batch_size :          6000
    best_batch_loss : 1.6726156613950414
    Epoch 8, Loss: 1.6870938551535481, fit: 0.131
    Best fit: 0.137
    Best loss: 1.6870938551535481
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.684576259401425_fit_0.131_2024-01-03_192433
  self.fit : 0.131
  self.loss : 1.684576259401425
  current_accuracy : 0.1301
   Accuracy mean: 0.1253
   Accuracy mean: 0.1301
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.02
    learning_rate : 0.02
    batch_size :          6000
    best_batch_loss : 1.5911361418630752
    Epoch 0, Loss: 1.6913005063347046, fit: 0.16816666666666666
    Epoch 0, Loss: 1.6913005063347046
    Best fit: 0.16816666666666666
    Best loss: 1.6913005063347046
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.5911361418630752_fit_0.16816666666666666_2024-01-03_192436
  self.fit : 0.16816666666666666
  self.loss : 1.5911361418630752
  current_accuracy : 0.1796
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.02
    learning_rate : 0.02
    batch_size :          6000
    best_batch_loss : 1.364741727534911
    Epoch 0, Loss: 1.4693365850964912, fit: 0.2821666666666667
    Epoch 0, Loss: 1.4693365850964912
    epoch : 1
      loss :              1.364741727534911
      loss_factor :              2.2413243265528583e-09
      loss adapted learning_rate :              4.482648653105717e-11
    learning_rate : 4.482648653105717e-11
    batch_size :          6000
    best_batch_loss : 1.3176785258642851
    Epoch 1, Loss: 1.3516425138443682, fit: 0.2935
    epoch : 2
      loss :              1.3445912287295718
      loss_factor :              1.9315356961692873e-09
      loss adapted learning_rate :              3.863071392338575e-11
    learning_rate : 3.863071392338575e-11
    batch_size :          6000
    best_batch_loss : 1.337100104478396
    Epoch 2, Loss: 1.3516425134515984, fit: 0.2816666666666667
    epoch : 3
      loss :              1.3656763924324198
      loss_factor :              2.256721783515687e-09
      loss adapted learning_rate :              4.5134435670313737e-11
    learning_rate : 4.5134435670313737e-11
    batch_size :          6000
    best_batch_loss : 1.328730588916269
    Epoch 3, Loss: 1.3516425130650949, fit: 0.2921666666666667
    epoch : 4
      loss :              1.346805510979811
      loss_factor :              1.9635811217150113e-09
      loss adapted learning_rate :              3.927162243430023e-11
    learning_rate : 3.927162243430023e-11
    batch_size :          6000
    best_batch_loss : 1.3295634496703033
    Epoch 4, Loss: 1.3516425126696239, fit: 0.2905
    epoch : 5
      loss :              1.354611229983185
      loss_factor :              2.080399384846549e-09
      loss adapted learning_rate :              4.1607987696930976e-11
    learning_rate : 4.1607987696930976e-11
    batch_size :          6000
    best_batch_loss : 1.3294229101242923
    Epoch 5, Loss: 1.3516425122987212, fit: 0.2996666666666667
    epoch : 6
      loss :              1.3294229101242923
      loss_factor :              1.7243744828660102e-09
      loss adapted learning_rate :              3.4487489657320205e-11
    learning_rate : 3.4487489657320205e-11
    batch_size :          6000
    best_batch_loss : 1.3409338253754626
    Epoch 6, Loss: 1.351642511939321, fit: 0.2961666666666667
    epoch : 7
      loss :              1.3409338253754626
      loss_factor :              1.879634730366589e-09
      loss adapted learning_rate :              3.759269460733178e-11
    learning_rate : 3.759269460733178e-11
    batch_size :          6000
    best_batch_loss : 1.3337427333471308
    Epoch 7, Loss: 1.3516425116037762, fit: 0.28783333333333333
    epoch : 8
      loss :              1.357992466679068
      loss_factor :              2.132915289764931e-09
      loss adapted learning_rate :              4.265830579529862e-11
    learning_rate : 4.265830579529862e-11
    batch_size :          6000
    best_batch_loss : 1.3163133964346685
    Epoch 8, Loss: 1.35164251123716, fit: 0.29283333333333333
    Best fit: 0.2996666666666667
    Best loss: 1.35164251123716
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_1.3510002081816166_fit_0.29283333333333333_2024-01-03_192456
  self.fit : 0.29283333333333333
  self.loss : 1.3510002081816166
  current_accuracy : 0.2896
   Accuracy mean: 0.1796
   Accuracy mean: 0.2896
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.025
    learning_rate : 0.025
    batch_size :          6000
    best_batch_loss : 1.542238329529616
    Epoch 0, Loss: 1.6867360241383094, fit: 0.19
    Epoch 0, Loss: 1.6867360241383094
    Best fit: 0.19
    Best loss: 1.6867360241383094
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.542238329529616_fit_0.19_2024-01-03_192459
  self.fit : 0.19
  self.loss : 1.542238329529616
  current_accuracy : 0.194
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.025
    learning_rate : 0.025
    batch_size :          6000
    best_batch_loss : 1.2865748666302872
    Epoch 0, Loss: 1.40613381027754, fit: 0.32183333333333336
    Epoch 0, Loss: 1.40613381027754
    epoch : 1
      loss :              1.2865748666302872
      loss_factor :              1.2426551716783473e-09
      loss adapted learning_rate :              3.106637929195868e-11
    learning_rate : 3.106637929195868e-11
    batch_size :          6000
    best_batch_loss : 1.237968971156121
    Epoch 1, Loss: 1.2639519156211052, fit: 0.3393333333333333
    epoch : 2
      loss :              1.254485558370014
      loss_factor :              9.652874522447542e-10
      loss adapted learning_rate :              2.413218630611886e-11
    learning_rate : 2.413218630611886e-11
    batch_size :          6000
    best_batch_loss : 1.241687955705065
    Epoch 2, Loss: 1.2639519153829408, fit: 0.3338333333333333
    epoch : 3
      loss :              1.2652508360555466
      loss_factor :              1.0513960220881144e-09
      loss adapted learning_rate :              2.628490055220286e-11
    learning_rate : 2.628490055220286e-11
    batch_size :          6000
    best_batch_loss : 1.2474997172114066
    Epoch 3, Loss: 1.2639519151691703, fit: 0.33116666666666666
    epoch : 4
      loss :              1.2702352960271064
      loss_factor :              1.093557851606938e-09
      loss adapted learning_rate :              2.733894629017345e-11
    learning_rate : 2.733894629017345e-11
    batch_size :          6000
    best_batch_loss : 1.2514536307196107
    Epoch 4, Loss: 1.263951914938056, fit: 0.34
    epoch : 5
      loss :              1.2514536307196107
      loss_factor :              9.422098197759075e-10
      loss adapted learning_rate :              2.355524549439769e-11
    learning_rate : 2.355524549439769e-11
    batch_size :          6000
    best_batch_loss : 1.2458144244242084
    Epoch 5, Loss: 1.2639519147162999, fit: 0.3368333333333333
    epoch : 6
      loss :              1.257141734549551
      loss_factor :              9.859217402930938e-10
      loss adapted learning_rate :              2.4648043507327345e-11
    learning_rate : 2.4648043507327345e-11
    batch_size :          6000
    best_batch_loss : 1.2528215984977853
    Epoch 6, Loss: 1.2639519145144882, fit: 0.3373333333333333
    epoch : 7
      loss :              1.256734995524126
      loss_factor :              9.827365027917186e-10
      loss adapted learning_rate :              2.4568412569792967e-11
    learning_rate : 2.4568412569792967e-11
    batch_size :          6000
    best_batch_loss : 1.2551416729217804
    Epoch 7, Loss: 1.2639519143023508, fit: 0.33416666666666667
    epoch : 8
      loss :              1.2649523024288996
      loss_factor :              1.0489179046750595e-09
      loss adapted learning_rate :              2.6222947616876488e-11
    learning_rate : 2.6222947616876488e-11
    batch_size :          6000
    best_batch_loss : 1.2500924483726412
    Epoch 8, Loss: 1.263951914088358, fit: 0.3415
    Best fit: 0.3415
    Best loss: 1.263951914088358
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_1.2500924483726412_fit_0.3415_2024-01-03_192519
  self.fit : 0.3415
  self.loss : 1.2500924483726412
  current_accuracy : 0.3324
   Accuracy mean: 0.194
   Accuracy mean: 0.3324
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.03
    learning_rate : 0.03
    batch_size :          6000
    best_batch_loss : 1.5137075409829315
    Epoch 0, Loss: 1.637899818415783, fit: 0.20916666666666667
    Epoch 0, Loss: 1.637899818415783
    Best fit: 0.20916666666666667
    Best loss: 1.637899818415783
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.5137075409829315_fit_0.20916666666666667_2024-01-03_192522
  self.fit : 0.20916666666666667
  self.loss : 1.5137075409829315
  current_accuracy : 0.2238
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.03
    learning_rate : 0.03
    batch_size :          6000
    best_batch_loss : 1.248329249901005
    Epoch 0, Loss: 1.3610535616468806, fit: 0.344
    Epoch 0, Loss: 1.3610535616468806
    epoch : 1
      loss :              1.248329249901005
      loss_factor :              9.18949121447873e-10
      loss adapted learning_rate :              2.7568473643436187e-11
    learning_rate : 2.7568473643436187e-11
    batch_size :          6000
    best_batch_loss : 1.2079534043463078
    Epoch 1, Loss: 1.2242957644683075, fit: 0.3645
    epoch : 2
      loss :              1.2079534043463078
      loss_factor :              6.614573111507697e-10
      loss adapted learning_rate :              1.9843719334523088e-11
    learning_rate : 1.9843719334523088e-11
    batch_size :          6000
    best_batch_loss : 1.2070774891644056
    Epoch 2, Loss: 1.2242957642973904, fit: 0.3566666666666667
    epoch : 3
      loss :              1.2228965241332008
      loss_factor :              7.479922634437294e-10
      loss adapted learning_rate :              2.2439767903311883e-11
    learning_rate : 2.2439767903311883e-11
    batch_size :          6000
    best_batch_loss : 1.208937576630398
    Epoch 3, Loss: 1.2242957641475916, fit: 0.3471666666666667
    epoch : 4
      loss :              1.2439560711018582
      loss_factor :              8.872590546147467e-10
      loss adapted learning_rate :              2.66177716384424e-11
    learning_rate : 2.66177716384424e-11
    batch_size :          6000
    best_batch_loss : 1.2123236076797617
    Epoch 4, Loss: 1.2242957639725194, fit: 0.355
    epoch : 5
      loss :              1.226680680807322
      loss_factor :              7.714632731317755e-10
      loss adapted learning_rate :              2.3143898193953266e-11
    learning_rate : 2.3143898193953266e-11
    batch_size :          6000
    best_batch_loss : 1.2077610397490486
    Epoch 5, Loss: 1.2242957637916985, fit: 0.3566666666666667
    epoch : 6
      loss :              1.2235015403561478
      loss_factor :              7.517011324779151e-10
      loss adapted learning_rate :              2.2551033974337453e-11
    learning_rate : 2.2551033974337453e-11
    batch_size :          6000
    best_batch_loss : 1.1996115213172671
    Epoch 6, Loss: 1.2242957636314362, fit: 0.35283333333333333
    epoch : 7
      loss :              1.2271593639028278
      loss_factor :              7.744790178360966e-10
      loss adapted learning_rate :              2.3234370535082897e-11
    learning_rate : 2.3234370535082897e-11
    batch_size :          6000
    best_batch_loss : 1.1966614251823922
    Epoch 7, Loss: 1.2242957634657579, fit: 0.353
    epoch : 8
      loss :              1.2302196809825596
      loss_factor :              7.940113410668705e-10
      loss adapted learning_rate :              2.3820340232006113e-11
    learning_rate : 2.3820340232006113e-11
    batch_size :          6000
    best_batch_loss : 1.2044999904334015
    Epoch 8, Loss: 1.2242957633023097, fit: 0.3465
    Best fit: 0.3645
    Best loss: 1.2242957633023097
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_1.2417429835645581_fit_0.3465_2024-01-03_192542
  self.fit : 0.3465
  self.loss : 1.2417429835645581
  current_accuracy : 0.3632
   Accuracy mean: 0.2238
   Accuracy mean: 0.3632
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.1
    learning_rate : 0.1
    batch_size :          6000
    best_batch_loss : 0.973574382315728
    Epoch 0, Loss: 1.2976783176209736, fit: 0.48583333333333334
    Epoch 0, Loss: 1.2976783176209736
    Best fit: 0.48583333333333334
    Best loss: 1.2976783176209736
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_0.973574382315728_fit_0.48583333333333334_2024-01-03_192545
  self.fit : 0.48583333333333334
  self.loss : 0.973574382315728
  current_accuracy : 0.5046
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.1
    learning_rate : 0.1
    batch_size :          6000
    best_batch_loss : 0.7392470013015733
    Epoch 0, Loss: 0.8322087388045007, fit: 0.6068333333333333
    Epoch 0, Loss: 0.8322087388045007
    epoch : 1
      loss :              0.7392470013015733
      loss_factor :              4.8741143655764776e-12
      loss adapted learning_rate :              4.874114365576478e-13
    learning_rate : 4.874114365576478e-13
    batch_size :          6000
    best_batch_loss : 0.7029245417404628
    Epoch 1, Loss: 0.7235252625980144, fit: 0.6156666666666667
    epoch : 2
      loss :              0.7296800029596767
      loss_factor :              4.2788242137857704e-12
      loss adapted learning_rate :              4.2788242137857705e-13
    learning_rate : 4.2788242137857705e-13
    batch_size :          6000
    best_batch_loss : 0.7068108324479
    Epoch 2, Loss: 0.7235252625973638, fit: 0.6261666666666666
    epoch : 3
      loss :              0.7068108324479
      loss_factor :              3.1119453959937927e-12
      loss adapted learning_rate :              3.111945395993793e-13
    learning_rate : 3.111945395993793e-13
    batch_size :          6000
    best_batch_loss : 0.7159309082869569
    Epoch 3, Loss: 0.723525262596823, fit: 0.6158333333333333
    epoch : 4
      loss :              0.7252079463029528
      loss_factor :              4.0236995211409965e-12
      loss adapted learning_rate :              4.0236995211409967e-13
    learning_rate : 4.0236995211409967e-13
    batch_size :          6000
    best_batch_loss : 0.7084171807888815
    Epoch 4, Loss: 0.7235252625963222, fit: 0.6071666666666666
    epoch : 5
      loss :              0.7421247890118957
      loss_factor :              5.067215618380852e-12
      loss adapted learning_rate :              5.067215618380851e-13
    learning_rate : 5.067215618380851e-13
    batch_size :          6000
    best_batch_loss : 0.7073704807458789
    Epoch 5, Loss: 0.7235252625956272, fit: 0.6148333333333333
    epoch : 6
      loss :              0.7232099535408629
      loss_factor :              3.9142084219434194e-12
      loss adapted learning_rate :              3.9142084219434195e-13
    learning_rate : 3.9142084219434195e-13
    batch_size :          6000
    best_batch_loss : 0.7088731669779796
    Epoch 6, Loss: 0.7235252625949891, fit: 0.619
    epoch : 7
      loss :              0.7152445420843815
      loss_factor :              3.5038501521931514e-12
      loss adapted learning_rate :              3.5038501521931515e-13
    learning_rate : 3.5038501521931515e-13
    batch_size :          6000
    best_batch_loss : 0.7044338897087201
    Epoch 7, Loss: 0.7235252625944535, fit: 0.6146666666666667
    epoch : 8
      loss :              0.7240405427008927
      loss_factor :              3.959395200286418e-12
      loss adapted learning_rate :              3.959395200286418e-13
    learning_rate : 3.959395200286418e-13
    batch_size :          6000
    best_batch_loss : 0.705415784259693
    Epoch 8, Loss: 0.7235252625938835, fit: 0.616
    Best fit: 0.6261666666666666
    Best loss: 0.7235252625938835
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_0.7200132429039587_fit_0.616_2024-01-03_192606
  self.fit : 0.616
  self.loss : 0.7200132429039587
  current_accuracy : 0.6241
   Accuracy mean: 0.5046
   Accuracy mean: 0.6241
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.9
    learning_rate : 0.9
    batch_size :          6000
    best_batch_loss : 1.4720823954274311
    Epoch 0, Loss: 1.7149142003345623, fit: 0.094
    Epoch 0, Loss: 1.7149142003345623
    Best fit: 0.094
    Best loss: 1.7149142003345623
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.812_fit_0.094_2024-01-03_192609
  self.fit : 0.094
  self.loss : 1.812
  current_accuracy : 0.0892
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.1
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.9
    learning_rate : 0.9
    batch_size :          6000
    best_batch_loss : 1.8066666666666666
    Epoch 0, Loss: 1.8193, fit: 0.08866666666666667
    Epoch 0, Loss: 1.8193
    epoch : 1
      loss :              1.8226666666666667
      loss_factor :              4.04643447148621e-08
      loss adapted learning_rate :              3.641791024337589e-08
    learning_rate : 3.641791024337589e-08
    batch_size :          6000
    best_batch_loss : 1.81
    Epoch 1, Loss: 1.8193, fit: 0.08716666666666667
    epoch : 2
      loss :              1.8256666666666668
      loss_factor :              4.11353182923357e-08
      loss adapted learning_rate :              3.702178646310213e-08
    learning_rate : 3.702178646310213e-08
    batch_size :          6000
    best_batch_loss : 1.8093333333333332
    Epoch 2, Loss: 1.8193, fit: 0.09533333333333334
    epoch : 3
      loss :              1.8093333333333332
      loss_factor :              3.759982665143646e-08
      loss adapted learning_rate :              3.383984398629282e-08
    learning_rate : 3.383984398629282e-08
    batch_size :          6000
    best_batch_loss : 1.8093333333333332
    Epoch 3, Loss: 1.8193000000000001, fit: 0.08966666666666667
    epoch : 4
      loss :              1.8206666666666667
      loss_factor :              4.002251820503379e-08
      loss adapted learning_rate :              3.602026638453041e-08
    learning_rate : 3.602026638453041e-08
    batch_size :          6000
    best_batch_loss : 1.8036666666666668
    Epoch 4, Loss: 1.8193000000000004, fit: 0.08533333333333333
    epoch : 5
      loss :              1.8293333333333333
      loss_factor :              4.1968986319863555e-08
      loss adapted learning_rate :              3.7772087687877204e-08
    learning_rate : 3.7772087687877204e-08
    batch_size :          6000
    best_batch_loss : 1.8003333333333333
    Epoch 5, Loss: 1.8193000000000001, fit: 0.08683333333333333
    epoch : 6
      loss :              1.8263333333333334
      loss_factor :              4.128577649633572e-08
      loss adapted learning_rate :              3.715719884670215e-08
    learning_rate : 3.715719884670215e-08
    batch_size :          6000
    best_batch_loss : 1.8066666666666666
    Epoch 6, Loss: 1.8193000000000001, fit: 0.09366666666666666
    epoch : 7
      loss :              1.8126666666666666
      loss_factor :              3.8298298990806466e-08
      loss adapted learning_rate :              3.446846909172582e-08
    learning_rate : 3.446846909172582e-08
    batch_size :          6000
    best_batch_loss : 1.8036666666666668
    Epoch 7, Loss: 1.8192999999999997, fit: 0.088
    epoch : 8
      loss :              1.824
      loss_factor :              4.076132941612388e-08
      loss adapted learning_rate :              3.6685196474511494e-08
    learning_rate : 3.6685196474511494e-08
    batch_size :          6000
    best_batch_loss : 1.808
    Epoch 8, Loss: 1.8193000000000001, fit: 0.09416666666666666
    Best fit: 0.09533333333333334
    Best loss: 1.8192999999999997
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.8116666666666668_fit_0.09416666666666666_2024-01-03_192630
  self.fit : 0.09416666666666666
  self.loss : 1.8116666666666668
  current_accuracy : 0.0892
   Accuracy mean: 0.0892
   Accuracy mean: 0.0892
  Error saving file: doc/out/test_combinations_results/20240103192630.
  normalized_accuracies :      [0.0274412  0.0274412  0.04151818 0.04151818 0.06486101 0.06486101
   0.         0.         0.09497505 0.09497505 0.07840342 0.07840342
   0.11119031 0.11974341 0.20794726 0.40395581 0.23360656 0.48022096
   0.28670706 0.53510335 0.78706344 1.         0.04686386 0.04686386]
batch_rate :  0.01
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-64
    learning_rate : 1e-64
    batch_size :          600
    best_batch_loss : 1.6756178562511537
    Epoch 0, Loss: 1.7316195149195936, fit: 0.07333333333333333
    Epoch 0, Loss: 1.7316195149195936
    Best fit: 0.07333333333333333
    Best loss: 1.7316195149195936
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7959573580816421_fit_0.07333333333333333_2024-01-03_192634
  self.fit : 0.07333333333333333
  self.loss : 1.7959573580816421
  current_accuracy : 0.0963
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-64
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-64
    learning_rate : 1e-64
    batch_size :          600
    best_batch_loss : 1.6777188460222923
    Epoch 0, Loss: 1.731619514919593, fit: 0.08833333333333333
    Epoch 0, Loss: 1.731619514919593
    epoch : 1
      loss :              1.7533345288110664
      loss_factor :              2.7456669463066165e-08
      loss adapted learning_rate :              2.745666946306616e-72
    learning_rate : 2.745666946306616e-72
    batch_size :          600
    best_batch_loss : 1.6667244324606878
    Epoch 1, Loss: 1.731619514919594, fit: 0.09833333333333333
    epoch : 2
      loss :              1.749555543771296
      loss_factor :              2.6870598973460397e-08
      loss adapted learning_rate :              2.6870598973460396e-72
    learning_rate : 2.6870598973460396e-72
    batch_size :          600
    best_batch_loss : 1.6726036709181942
    Epoch 2, Loss: 1.7316195149195939, fit: 0.09166666666666666
    epoch : 3
      loss :              1.7440486343299146
      loss_factor :              2.603669852520598e-08
      loss adapted learning_rate :              2.603669852520598e-72
    learning_rate : 2.603669852520598e-72
    batch_size :          600
    best_batch_loss : 1.6793811118302475
    Epoch 3, Loss: 1.7316195149195939, fit: 0.11333333333333333
    epoch : 4
      loss :              1.702872742844924
      loss_factor :              2.0503213768894816e-08
      loss adapted learning_rate :              2.0503213768894815e-72
    learning_rate : 2.0503213768894815e-72
    batch_size :          600
    best_batch_loss : 1.6664074745517128
    Epoch 4, Loss: 1.7316195149195936, fit: 0.09333333333333334
    epoch : 5
      loss :              1.7572178747127063
      loss_factor :              2.807088618413766e-08
      loss adapted learning_rate :              2.807088618413766e-72
    learning_rate : 2.807088618413766e-72
    batch_size :          600
    best_batch_loss : 1.6728493857590583
    Epoch 5, Loss: 1.731619514919594, fit: 0.115
    epoch : 6
      loss :              1.703694423865503
      loss_factor :              2.060236228169637e-08
      loss adapted learning_rate :              2.0602362281696367e-72
    learning_rate : 2.0602362281696367e-72
    batch_size :          600
    best_batch_loss : 1.6685278837897712
    Epoch 6, Loss: 1.731619514919593, fit: 0.115
    epoch : 7
      loss :              1.7075675086498225
      loss_factor :              2.107554554136312e-08
      loss adapted learning_rate :              2.1075545541363119e-72
    learning_rate : 2.1075545541363119e-72
    batch_size :          600
    best_batch_loss : 1.6793181834816948
    Epoch 7, Loss: 1.7316195149195939, fit: 0.11166666666666666
    epoch : 8
      loss :              1.7271205195954837
      loss_factor :              2.361709299971448e-08
      loss adapted learning_rate :              2.3617092999714477e-72
    learning_rate : 2.3617092999714477e-72
    batch_size :          600
    best_batch_loss : 1.6643064965455876
    Epoch 8, Loss: 1.7316195149195932, fit: 0.09833333333333333
    Best fit: 0.115
    Best loss: 1.731619514919593
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7458569612584909_fit_0.09833333333333333_2024-01-03_192658
  self.fit : 0.09833333333333333
  self.loss : 1.7458569612584909
  current_accuracy : 0.0963
   Accuracy mean: 0.0963
   Accuracy mean: 0.0963
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-33
    learning_rate : 1e-33
    batch_size :          600
    best_batch_loss : 1.6411404818270408
    Epoch 0, Loss: 1.733727268018859, fit: 0.095
    Epoch 0, Loss: 1.733727268018859
    Best fit: 0.095
    Best loss: 1.733727268018859
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.754568504763477_fit_0.095_2024-01-03_192700
  self.fit : 0.095
  self.loss : 1.754568504763477
  current_accuracy : 0.0991
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-33
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-33
    learning_rate : 1e-33
    batch_size :          600
    best_batch_loss : 1.6548172248959747
    Epoch 0, Loss: 1.7337272680188585, fit: 0.11166666666666666
    Epoch 0, Loss: 1.7337272680188585
    epoch : 1
      loss :              1.7290470130035562
      loss_factor :              2.3881852931464713e-08
      loss adapted learning_rate :              2.3881852931464716e-41
    learning_rate : 2.3881852931464716e-41
    batch_size :          600
    best_batch_loss : 1.6684322515413597
    Epoch 1, Loss: 1.7337272680188591, fit: 0.11333333333333333
    epoch : 2
      loss :              1.7187824599903736
      loss_factor :              2.2501378807718633e-08
      loss adapted learning_rate :              2.2501378807718635e-41
    learning_rate : 2.2501378807718635e-41
    batch_size :          600
    best_batch_loss : 1.6842925969807403
    Epoch 2, Loss: 1.7337272680188596, fit: 0.09666666666666666
    epoch : 3
      loss :              1.7384154013655362
      loss_factor :              2.520783858918347e-08
      loss adapted learning_rate :              2.520783858918347e-41
    learning_rate : 2.520783858918347e-41
    batch_size :          600
    best_batch_loss : 1.6813284538345845
    Epoch 3, Loss: 1.7337272680188598, fit: 0.09166666666666666
    epoch : 4
      loss :              1.762722464534357
      loss_factor :              2.8962723150038358e-08
      loss adapted learning_rate :              2.896272315003836e-41
    learning_rate : 2.896272315003836e-41
    batch_size :          600
    best_batch_loss : 1.6733987966758643
    Epoch 4, Loss: 1.7337272680188596, fit: 0.11
    epoch : 5
      loss :              1.722924346144925
      loss_factor :              2.3049530274083047e-08
      loss adapted learning_rate :              2.3049530274083047e-41
    learning_rate : 2.3049530274083047e-41
    batch_size :          600
    best_batch_loss : 1.6758901420441252
    Epoch 5, Loss: 1.7337272680188596, fit: 0.085
    epoch : 6
      loss :              1.7792088952265286
      loss_factor :              3.1788454355393543e-08
      loss adapted learning_rate :              3.1788454355393546e-41
    learning_rate : 3.1788454355393546e-41
    batch_size :          600
    best_batch_loss : 1.6646299962719224
    Epoch 6, Loss: 1.7337272680188598, fit: 0.10166666666666667
    epoch : 7
      loss :              1.7268680894412842
      loss_factor :              2.3582597740869193e-08
      loss adapted learning_rate :              2.3582597740869194e-41
    learning_rate : 2.3582597740869194e-41
    batch_size :          600
    best_batch_loss : 1.6790009926179843
    Epoch 7, Loss: 1.7337272680188587, fit: 0.10166666666666667
    epoch : 8
      loss :              1.7344122860356552
      loss_factor :              2.4633346327016396e-08
      loss adapted learning_rate :              2.4633346327016397e-41
    learning_rate : 2.4633346327016397e-41
    batch_size :          600
    best_batch_loss : 1.6695734907519184
    Epoch 8, Loss: 1.7337272680188593, fit: 0.10666666666666667
    Best fit: 0.11333333333333333
    Best loss: 1.7337272680188585
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7330593358177497_fit_0.10666666666666667_2024-01-03_192723
  self.fit : 0.10666666666666667
  self.loss : 1.7330593358177497
  current_accuracy : 0.0991
   Accuracy mean: 0.0991
   Accuracy mean: 0.0991
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-17
    learning_rate : 1e-17
    batch_size :          600
    best_batch_loss : 1.7180806559303692
    Epoch 0, Loss: 1.7651329454101827, fit: 0.095
    Epoch 0, Loss: 1.7651329454101827
    Best fit: 0.095
    Best loss: 1.7651329454101827
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7577407562785412_fit_0.095_2024-01-03_192726
  self.fit : 0.095
  self.loss : 1.7577407562785412
  current_accuracy : 0.0909
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-17
    learning_rate : 1e-17
    batch_size :          600
    best_batch_loss : 1.7115356905432195
    Epoch 0, Loss: 1.7651329454101823, fit: 0.1
    Epoch 0, Loss: 1.7651329454101823
    epoch : 1
      loss :              1.7434256834612198
      loss_factor :              2.5943848256710512e-08
      loss adapted learning_rate :              2.5943848256710515e-25
    learning_rate : 2.5943848256710515e-25
    batch_size :          600
    best_batch_loss : 1.7175259152180427
    Epoch 1, Loss: 1.7651329454101818, fit: 0.1
    epoch : 2
      loss :              1.7483357927541412
      loss_factor :              2.6683849801012513e-08
      loss adapted learning_rate :              2.6683849801012515e-25
    learning_rate : 2.6683849801012515e-25
    batch_size :          600
    best_batch_loss : 1.706956125593677
    Epoch 2, Loss: 1.7651329454101814, fit: 0.10666666666666667
    epoch : 3
      loss :              1.7387461525427037
      loss_factor :              2.525584014419566e-08
      loss adapted learning_rate :              2.525584014419566e-25
    learning_rate : 2.525584014419566e-25
    batch_size :          600
    best_batch_loss : 1.7165818302537321
    Epoch 3, Loss: 1.7651329454101818, fit: 0.08166666666666667
    epoch : 4
      loss :              1.7860713280393623
      loss_factor :              3.3036040328607774e-08
      loss adapted learning_rate :              3.3036040328607775e-25
    learning_rate : 3.3036040328607775e-25
    batch_size :          600
    best_batch_loss : 1.6891080263704799
    Epoch 4, Loss: 1.765132945410182, fit: 0.09833333333333333
    epoch : 5
      loss :              1.7502192834541361
      loss_factor :              2.6972713831443237e-08
      loss adapted learning_rate :              2.697271383144324e-25
    learning_rate : 2.697271383144324e-25
    batch_size :          600
    best_batch_loss : 1.71784083862012
    Epoch 5, Loss: 1.7651329454101818, fit: 0.09666666666666666
    epoch : 6
      loss :              1.7480351964598269
      loss_factor :              2.663800698657394e-08
      loss adapted learning_rate :              2.6638006986573942e-25
    learning_rate : 2.6638006986573942e-25
    batch_size :          600
    best_batch_loss : 1.7037300080837323
    Epoch 6, Loss: 1.7651329454101823, fit: 0.08
    epoch : 7
      loss :              1.7786155817874647
      loss_factor :              3.1682608212346045e-08
      loss adapted learning_rate :              3.168260821234605e-25
    learning_rate : 3.168260821234605e-25
    batch_size :          600
    best_batch_loss : 1.7024048699059482
    Epoch 7, Loss: 1.7651329454101814, fit: 0.09666666666666666
    epoch : 8
      loss :              1.751343250049185
      loss_factor :              2.714643028727325e-08
      loss adapted learning_rate :              2.714643028727325e-25
    learning_rate : 2.714643028727325e-25
    batch_size :          600
    best_batch_loss : 1.7108081962747164
    Epoch 8, Loss: 1.7651329454101823, fit: 0.06666666666666667
    Best fit: 0.10666666666666667
    Best loss: 1.7651329454101814
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.8038429204499933_fit_0.06666666666666667_2024-01-03_192750
  self.fit : 0.06666666666666667
  self.loss : 1.8038429204499933
  current_accuracy : 0.0909
   Accuracy mean: 0.0909
   Accuracy mean: 0.0909
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-10
    learning_rate : 1e-10
    batch_size :          600
    best_batch_loss : 1.7094218634207892
    Epoch 0, Loss: 1.7629774293363791, fit: 0.07833333333333334
    Epoch 0, Loss: 1.7629774293363791
    Best fit: 0.07833333333333334
    Best loss: 1.7629774293363791
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7607382944836976_fit_0.07833333333333334_2024-01-03_192752
  self.fit : 0.07833333333333334
  self.loss : 1.7607382944836976
  current_accuracy : 0.0893
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-10
    learning_rate : 1e-10
    batch_size :          600
    best_batch_loss : 1.6848402207812248
    Epoch 0, Loss: 1.76297742026112, fit: 0.10166666666666667
    Epoch 0, Loss: 1.76297742026112
    epoch : 1
      loss :              1.73644925178901
      loss_factor :              2.4924184334619844e-08
      loss adapted learning_rate :              2.4924184334619844e-18
    learning_rate : 2.4924184334619844e-18
    batch_size :          600
    best_batch_loss : 1.7162612592484243
    Epoch 1, Loss: 1.7629774145011312, fit: 0.08666666666666667
    epoch : 2
      loss :              1.775178135317415
      loss_factor :              3.107559129391436e-08
      loss adapted learning_rate :              3.1075591293914366e-18
    learning_rate : 3.1075591293914366e-18
    batch_size :          600
    best_batch_loss : 1.7213016456729475
    Epoch 2, Loss: 1.7629774145011308, fit: 0.075
    epoch : 3
      loss :              1.7880991984800123
      loss_factor :              3.3413047268047707e-08
      loss adapted learning_rate :              3.341304726804771e-18
    learning_rate : 3.341304726804771e-18
    batch_size :          600
    best_batch_loss : 1.7199205109224271
    Epoch 3, Loss: 1.7629774145011308, fit: 0.09666666666666666
    epoch : 4
      loss :              1.7499565375651094
      loss_factor :              2.6932249278958147e-08
      loss adapted learning_rate :              2.693224927895815e-18
    learning_rate : 2.693224927895815e-18
    batch_size :          600
    best_batch_loss : 1.6974449338678967
    Epoch 4, Loss: 1.7629774145011308, fit: 0.09333333333333334
    epoch : 5
      loss :              1.7469378611163662
      loss_factor :              2.6471257536785136e-08
      loss adapted learning_rate :              2.6471257536785136e-18
    learning_rate : 2.6471257536785136e-18
    batch_size :          600
    best_batch_loss : 1.7075029470829377
    Epoch 5, Loss: 1.762977414501131, fit: 0.085
    epoch : 6
      loss :              1.7721432205067753
      loss_factor :              3.0548379447778695e-08
      loss adapted learning_rate :              3.0548379447778697e-18
    learning_rate : 3.0548379447778697e-18
    batch_size :          600
    best_batch_loss : 1.70541147874934
    Epoch 6, Loss: 1.7629774145011312, fit: 0.07166666666666667
    epoch : 7
      loss :              1.7826106482097195
      loss_factor :              3.240148866272344e-08
      loss adapted learning_rate :              3.2401488662723438e-18
    learning_rate : 3.2401488662723438e-18
    batch_size :          600
    best_batch_loss : 1.7118179214906077
    Epoch 7, Loss: 1.7629774145011308, fit: 0.07333333333333333
    epoch : 8
      loss :              1.7902308636017012
      loss_factor :              3.3813521374502784e-08
      loss adapted learning_rate :              3.3813521374502786e-18
    learning_rate : 3.3813521374502786e-18
    batch_size :          600
    best_batch_loss : 1.69651283298668
    Epoch 8, Loss: 1.762977414501131, fit: 0.08166666666666667
    Best fit: 0.10166666666666667
    Best loss: 1.7629774145011308
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.7773332973632243_fit_0.08166666666666667_2024-01-03_192816
  self.fit : 0.08166666666666667
  self.loss : 1.7773332973632243
  current_accuracy : 0.0893
   Accuracy mean: 0.0893
   Accuracy mean: 0.0893
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-07
    learning_rate : 1e-07
    batch_size :          600
    best_batch_loss : 1.5759826707235034
    Epoch 0, Loss: 1.666326975075237, fit: 0.11
    Epoch 0, Loss: 1.666326975075237
    Best fit: 0.11
    Best loss: 1.666326975075237
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7165698099441253_fit_0.11_2024-01-03_192819
  self.fit : 0.11
  self.loss : 1.7165698099441253
  current_accuracy : 0.1356
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-07
    learning_rate : 1e-07
    batch_size :          600
    best_batch_loss : 1.5691754769566804
    Epoch 0, Loss: 1.6663163847283164, fit: 0.13833333333333334
    Epoch 0, Loss: 1.6663163847283164
    epoch : 1
      loss :              1.6495664352268504
      loss_factor :              1.491757093486814e-08
      loss adapted learning_rate :              1.4917570934868139e-15
    learning_rate : 1.4917570934868139e-15
    batch_size :          600
    best_batch_loss : 1.6081311280164596
    Epoch 1, Loss: 1.6663114300468345, fit: 0.135
    epoch : 2
      loss :              1.6593428222649598
      loss_factor :              1.5825637393034136e-08
      loss adapted learning_rate :              1.5825637393034135e-15
    learning_rate : 1.5825637393034135e-15
    batch_size :          600
    best_batch_loss : 1.596669520560586
    Epoch 2, Loss: 1.6663114300466795, fit: 0.13333333333333333
    epoch : 3
      loss :              1.6639393399772167
      loss_factor :              1.6269525894459007e-08
      loss adapted learning_rate :              1.6269525894459006e-15
    learning_rate : 1.6269525894459006e-15
    batch_size :          600
    best_batch_loss : 1.6150573768980023
    Epoch 3, Loss: 1.6663114300465258, fit: 0.12
    epoch : 4
      loss :              1.6972441405785068
      loss_factor :              1.983550127536174e-08
      loss adapted learning_rate :              1.9835501275361736e-15
    learning_rate : 1.9835501275361736e-15
    batch_size :          600
    best_batch_loss : 1.6061431273155933
    Epoch 4, Loss: 1.666311430046336, fit: 0.145
    epoch : 5
      loss :              1.6422255921185966
      loss_factor :              1.426685206672714e-08
      loss adapted learning_rate :              1.4266852066727139e-15
    learning_rate : 1.4266852066727139e-15
    batch_size :          600
    best_batch_loss : 1.6051903401481127
    Epoch 5, Loss: 1.6663114300461648, fit: 0.14333333333333334
    epoch : 6
      loss :              1.6554535327363111
      loss_factor :              1.5458592520632462e-08
      loss adapted learning_rate :              1.5458592520632461e-15
    learning_rate : 1.5458592520632461e-15
    batch_size :          600
    best_batch_loss : 1.5880148830063856
    Epoch 6, Loss: 1.6663114300460227, fit: 0.12666666666666668
    epoch : 7
      loss :              1.6738639773403012
      loss_factor :              1.726639339578427e-08
      loss adapted learning_rate :              1.726639339578427e-15
    learning_rate : 1.726639339578427e-15
    batch_size :          600
    best_batch_loss : 1.5709247151652639
    Epoch 7, Loss: 1.6663114300458584, fit: 0.12333333333333334
    epoch : 8
      loss :              1.685771045262024
      loss_factor :              1.8534714527598018e-08
      loss adapted learning_rate :              1.8534714527598017e-15
    learning_rate : 1.8534714527598017e-15
    batch_size :          600
    best_batch_loss : 1.5730885113514717
    Epoch 8, Loss: 1.6663114300456807, fit: 0.125
    Best fit: 0.145
    Best loss: 1.6663114300456807
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.6878731520429187_fit_0.125_2024-01-03_192842
  self.fit : 0.125
  self.loss : 1.6878731520429187
  current_accuracy : 0.1356
   Accuracy mean: 0.1356
   Accuracy mean: 0.1356
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    learning_rate : 0.0001
    batch_size :          600
    best_batch_loss : 1.5993455685448899
    Epoch 0, Loss: 1.6704514077539372, fit: 0.13333333333333333
    Epoch 0, Loss: 1.6704514077539372
    Best fit: 0.13333333333333333
    Best loss: 1.6704514077539372
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6670374045596084_fit_0.13333333333333333_2024-01-03_192845
  self.fit : 0.13333333333333333
  self.loss : 1.6670374045596084
  current_accuracy : 0.1243
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    learning_rate : 0.0001
    batch_size :          600
    best_batch_loss : 1.5872937625930457
    Epoch 0, Loss: 1.661236576554602, fit: 0.12333333333333334
    Epoch 0, Loss: 1.661236576554602
    epoch : 1
      loss :              1.6808024831951744
      loss_factor :              1.7995619845553993e-08
      loss adapted learning_rate :              1.7995619845553994e-12
    learning_rate : 1.7995619845553994e-12
    batch_size :          600
    best_batch_loss : 1.580629844204932
    Epoch 1, Loss: 1.6563721691021855, fit: 0.13
    epoch : 2
      loss :              1.6611668889210833
      loss_factor :              1.6000467051303582e-08
      loss adapted learning_rate :              1.6000467051303583e-12
    learning_rate : 1.6000467051303583e-12
    batch_size :          600
    best_batch_loss : 1.5752361878829622
    Epoch 2, Loss: 1.6563721689424649, fit: 0.15666666666666668
    epoch : 3
      loss :              1.615832626633545
      loss_factor :              1.2132875212828154e-08
      loss adapted learning_rate :              1.2132875212828155e-12
    learning_rate : 1.2132875212828155e-12
    batch_size :          600
    best_batch_loss : 1.5770833121091425
    Epoch 3, Loss: 1.6563721688082653, fit: 0.09833333333333333
    epoch : 4
      loss :              1.7335820031697315
      loss_factor :              2.4515677404606846e-08
      loss adapted learning_rate :              2.4515677404606847e-12
    learning_rate : 2.4515677404606847e-12
    batch_size :          600
    best_batch_loss : 1.5847519718960552
    Epoch 4, Loss: 1.6563721686414214, fit: 0.12666666666666668
    epoch : 5
      loss :              1.689405909308843
      loss_factor :              1.8938260650015113e-08
      loss adapted learning_rate :              1.8938260650015113e-12
    learning_rate : 1.8938260650015113e-12
    batch_size :          600
    best_batch_loss : 1.5766013551508233
    Epoch 5, Loss: 1.6563721684309507, fit: 0.13833333333333334
    epoch : 6
      loss :              1.6665457212136419
      loss_factor :              1.6526174306217135e-08
      loss adapted learning_rate :              1.6526174306217135e-12
    learning_rate : 1.6526174306217135e-12
    batch_size :          600
    best_batch_loss : 1.5651887454736824
    Epoch 6, Loss: 1.6563721682617956, fit: 0.135
    epoch : 7
      loss :              1.6789947693200578
      loss_factor :              1.7803009826285008e-08
      loss adapted learning_rate :              1.7803009826285009e-12
    learning_rate : 1.7803009826285009e-12
    batch_size :          600
    best_batch_loss : 1.5850712057073382
    Epoch 7, Loss: 1.6563721681037833, fit: 0.125
    epoch : 8
      loss :              1.688212989618157
      loss_factor :              1.8804958329296825e-08
      loss adapted learning_rate :              1.8804958329296825e-12
    learning_rate : 1.8804958329296825e-12
    batch_size :          600
    best_batch_loss : 1.5846179957361353
    Epoch 8, Loss: 1.6563721679289427, fit: 0.13833333333333334
    Best fit: 0.15666666666666668
    Best loss: 1.6563721679289427
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.6627048524913532_fit_0.13833333333333334_2024-01-03_192909
  self.fit : 0.13833333333333334
  self.loss : 1.6627048524913532
  current_accuracy : 0.1252
   Accuracy mean: 0.1243
   Accuracy mean: 0.1252
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.001
    learning_rate : 0.001
    batch_size :          600
    best_batch_loss : 1.6242994506888564
    Epoch 0, Loss: 1.710172146029715, fit: 0.13666666666666666
    Epoch 0, Loss: 1.710172146029715
    Best fit: 0.13666666666666666
    Best loss: 1.710172146029715
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.6584194781640973_fit_0.13666666666666666_2024-01-03_192912
  self.fit : 0.13666666666666666
  self.loss : 1.6584194781640973
  current_accuracy : 0.132
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.001
    learning_rate : 0.001
    batch_size :          600
    best_batch_loss : 1.5452861017968371
    Epoch 0, Loss: 1.633236904913857, fit: 0.175
    Epoch 0, Loss: 1.633236904913857
    epoch : 1
      loss :              1.5677359261205506
      loss_factor :              8.96869194417579e-09
      loss adapted learning_rate :              8.968691944175789e-12
    learning_rate : 8.968691944175789e-12
    batch_size :          600
    best_batch_loss : 1.4821389963982559
    Epoch 1, Loss: 1.5877391532521867, fit: 0.15
    epoch : 2
      loss :              1.6086994210210919
      loss_factor :              1.1607776972024042e-08
      loss adapted learning_rate :              1.1607776972024043e-11
    learning_rate : 1.1607776972024043e-11
    batch_size :          600
    best_batch_loss : 1.4958165930729999
    Epoch 2, Loss: 1.5877391522628461, fit: 0.16666666666666666
    epoch : 3
      loss :              1.6068483134069076
      loss_factor :              1.1474897442281323e-08
      loss adapted learning_rate :              1.1474897442281322e-11
    learning_rate : 1.1474897442281322e-11
    batch_size :          600
    best_batch_loss : 1.5213614818532495
    Epoch 3, Loss: 1.587739151168726, fit: 0.16
    epoch : 4
      loss :              1.6076862844660593
      loss_factor :              1.1534879637040486e-08
      loss adapted learning_rate :              1.1534879637040486e-11
    learning_rate : 1.1534879637040486e-11
    batch_size :          600
    best_batch_loss : 1.5071173419721413
    Epoch 4, Loss: 1.5877391500634972, fit: 0.17
    epoch : 5
      loss :              1.5844479751984768
      loss_factor :              9.971944173713957e-09
      loss adapted learning_rate :              9.971944173713958e-12
    learning_rate : 9.971944173713958e-12
    batch_size :          600
    best_batch_loss : 1.5187401906589644
    Epoch 5, Loss: 1.5877391490308523, fit: 0.165
    epoch : 6
      loss :              1.611392094653489
      loss_factor :              1.1803540296780512e-08
      loss adapted learning_rate :              1.1803540296780512e-11
    learning_rate : 1.1803540296780512e-11
    batch_size :          600
    best_batch_loss : 1.5321614169010862
    Epoch 6, Loss: 1.587739147980793, fit: 0.16833333333333333
    epoch : 7
      loss :              1.5891199951635178
      loss_factor :              1.0269916727137545e-08
      loss adapted learning_rate :              1.0269916727137545e-11
    learning_rate : 1.0269916727137545e-11
    batch_size :          600
    best_batch_loss : 1.4784246662399765
    Epoch 7, Loss: 1.5877391469235038, fit: 0.17833333333333334
    epoch : 8
      loss :              1.5799739441583405
      loss_factor :              9.693915936143591e-09
      loss adapted learning_rate :              9.693915936143592e-12
    learning_rate : 9.693915936143592e-12
    batch_size :          600
    best_batch_loss : 1.5185380826913635
    Epoch 8, Loss: 1.5877391459907875, fit: 0.16
    Best fit: 0.17833333333333334
    Best loss: 1.5877391459907875
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_1.6127313850351812_fit_0.16_2024-01-03_192937
  self.fit : 0.16
  self.loss : 1.6127313850351812
  current_accuracy : 0.1754
   Accuracy mean: 0.132
   Accuracy mean: 0.1754
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.02
    learning_rate : 0.02
    batch_size :          600
    best_batch_loss : 0.6997877378656673
    Epoch 0, Loss: 1.0672574769460477, fit: 0.61
    Epoch 0, Loss: 1.0672574769460477
    Best fit: 0.61
    Best loss: 1.0672574769460477
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_0.7432052049790542_fit_0.61_2024-01-03_192940
  self.fit : 0.61
  self.loss : 0.7432052049790542
  current_accuracy : 0.6249
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.02
    learning_rate : 0.02
    batch_size :          600
    best_batch_loss : 0.4834739494238674
    Epoch 0, Loss: 0.6107208558542061, fit: 0.745
    Epoch 0, Loss: 0.6107208558542061
    epoch : 1
      loss :              0.4834739494238674
      loss_factor :              6.977997084322674e-14
      loss adapted learning_rate :              1.3955994168645348e-15
    learning_rate : 1.3955994168645348e-15
    batch_size :          600
    best_batch_loss : 0.4717529514425085
    Epoch 1, Loss: 0.5353081348724968, fit: 0.74
    epoch : 2
      loss :              0.4827821317260246
      loss_factor :              6.878787292131436e-14
      loss adapted learning_rate :              1.3757574584262871e-15
    learning_rate : 1.3757574584262871e-15
    batch_size :          600
    best_batch_loss : 0.4606935160022464
    Epoch 2, Loss: 0.5353081348724924, fit: 0.7116666666666667
    epoch : 3
      loss :              0.5492920776044643
      loss_factor :              2.5005373892874127e-13
      loss adapted learning_rate :              5.0010747785748255e-15
    learning_rate : 5.0010747785748255e-15
    batch_size :          600
    best_batch_loss : 0.46811507164677685
    Epoch 3, Loss: 0.5353081348724773, fit: 0.7216666666666667
    epoch : 4
      loss :              0.5269543070215184
      loss_factor :              1.6509335198717942e-13
      loss adapted learning_rate :              3.3018670397435885e-15
    learning_rate : 3.3018670397435885e-15
    batch_size :          600
    best_batch_loss : 0.44310855949286176
    Epoch 4, Loss: 0.5353081348724531, fit: 0.715
    epoch : 5
      loss :              0.5315540304679678
      loss_factor :              1.8008359630962438e-13
      loss adapted learning_rate :              3.6016719261924876e-15
    learning_rate : 3.6016719261924876e-15
    batch_size :          600
    best_batch_loss : 0.45430609889133877
    Epoch 5, Loss: 0.535308134872435, fit: 0.73
    epoch : 6
      loss :              0.5049781512328382
      loss_factor :              1.0782659228017317e-13
      loss adapted learning_rate :              2.1565318456034632e-15
    learning_rate : 2.1565318456034632e-15
    batch_size :          600
    best_batch_loss : 0.44057998668701276
    Epoch 6, Loss: 0.5353081348724208, fit: 0.6966666666666667
    epoch : 7
      loss :              0.568059804276953
      loss_factor :              3.4989730569688634e-13
      loss adapted learning_rate :              6.997946113937727e-15
    learning_rate : 6.997946113937727e-15
    batch_size :          600
    best_batch_loss : 0.44971219499645104
    Epoch 7, Loss: 0.5353081348723959, fit: 0.7083333333333334
    epoch : 8
      loss :              0.544998322128693
      loss_factor :              2.3118074145053943e-13
      loss adapted learning_rate :              4.623614829010789e-15
    learning_rate : 4.623614829010789e-15
    batch_size :          600
    best_batch_loss : 0.4452470144545074
    Epoch 8, Loss: 0.5353081348723612, fit: 0.6933333333333334
    Best fit: 0.745
    Best loss: 0.5353081348723612
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_0.5718319120907861_fit_0.6933333333333334_2024-01-03_193004
  self.fit : 0.6933333333333334
  self.loss : 0.5718319120907861
  current_accuracy : 0.7274
   Accuracy mean: 0.6249
   Accuracy mean: 0.7274
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.025
    learning_rate : 0.025
    batch_size :          600
    best_batch_loss : 0.6093483941795311
    Epoch 0, Loss: 0.9866531800421435, fit: 0.6283333333333333
    Epoch 0, Loss: 0.9866531800421435
    Best fit: 0.6283333333333333
    Best loss: 0.9866531800421435
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_0.7031422538530772_fit_0.6283333333333333_2024-01-03_193007
  self.fit : 0.6283333333333333
  self.loss : 0.7031422538530772
  current_accuracy : 0.6546
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.025
    learning_rate : 0.025
    batch_size :          600
    best_batch_loss : 0.4581025430384748
    Epoch 0, Loss: 0.5750868745238357, fit: 0.7483333333333333
    Epoch 0, Loss: 0.5750868745238357
    epoch : 1
      loss :              0.46447269307432215
      loss_factor :              4.673065449816645e-14
      loss adapted learning_rate :              1.1682663624541615e-15
    learning_rate : 1.1682663624541615e-15
    batch_size :          600
    best_batch_loss : 0.4194350684467737
    Epoch 1, Loss: 0.5038110111564204, fit: 0.7233333333333334
    epoch : 2
      loss :              0.5304636468850737
      loss_factor :              1.7642343228846867e-13
      loss adapted learning_rate :              4.410585807211717e-15
    learning_rate : 4.410585807211717e-15
    batch_size :          600
    best_batch_loss : 0.42403664895863724
    Epoch 2, Loss: 0.5038110111564144, fit: 0.7416666666666667
    epoch : 3
      loss :              0.49388138246569613
      loss_factor :              8.634288414201786e-14
      loss adapted learning_rate :              2.1585721035504467e-15
    learning_rate : 2.1585721035504467e-15
    batch_size :          600
    best_batch_loss : 0.42434830353059955
    Epoch 3, Loss: 0.5038110111564051, fit: 0.7683333333333333
    epoch : 4
      loss :              0.42947181842332083
      loss_factor :              2.134748468525243e-14
      loss adapted learning_rate :              5.336871171313108e-16
    learning_rate : 5.336871171313108e-16
    batch_size :          600
    best_batch_loss : 0.40550578071173915
    Epoch 4, Loss: 0.5038110111564037, fit: 0.7383333333333333
    epoch : 5
      loss :              0.4965445220577552
      loss_factor :              9.111333653252246e-14
      loss adapted learning_rate :              2.2778334133130615e-15
    learning_rate : 2.2778334133130615e-15
    batch_size :          600
    best_batch_loss : 0.4158344392659801
    Epoch 5, Loss: 0.503811011156401, fit: 0.7016666666666667
    epoch : 6
      loss :              0.5522899275627104
      loss_factor :              2.640409144149306e-13
      loss adapted learning_rate :              6.601022860373266e-15
    learning_rate : 6.601022860373266e-15
    batch_size :          600
    best_batch_loss : 0.41524090654808865
    Epoch 6, Loss: 0.5038110111563895, fit: 0.7416666666666667
    epoch : 7
      loss :              0.49637906299280404
      loss_factor :              9.081018261171354e-14
      loss adapted learning_rate :              2.2702545652928387e-15
    learning_rate : 2.2702545652928387e-15
    batch_size :          600
    best_batch_loss : 0.4207104587172543
    Epoch 7, Loss: 0.5038110111563773, fit: 0.7383333333333333
    epoch : 8
      loss :              0.49705774035795325
      loss_factor :              9.205945759060607e-14
      loss adapted learning_rate :              2.301486439765152e-15
    learning_rate : 2.301486439765152e-15
    batch_size :          600
    best_batch_loss : 0.42949475715587077
    Epoch 8, Loss: 0.5038110111563721, fit: 0.6883333333333334
    Best fit: 0.7683333333333333
    Best loss: 0.5038110111563721
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_0.5933278093711521_fit_0.6883333333333334_2024-01-03_193030
  self.fit : 0.6883333333333334
  self.loss : 0.5933278093711521
  current_accuracy : 0.7391
   Accuracy mean: 0.6546
   Accuracy mean: 0.7391
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.03
    learning_rate : 0.03
    batch_size :          600
    best_batch_loss : 0.5814417877583321
    Epoch 0, Loss: 0.9249632900262909, fit: 0.6716666666666666
    Epoch 0, Loss: 0.9249632900262909
    Best fit: 0.6716666666666666
    Best loss: 0.9249632900262909
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_0.630187747049936_fit_0.6716666666666666_2024-01-03_193032
  self.fit : 0.6716666666666666
  self.loss : 0.630187747049936
  current_accuracy : 0.6874
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.03
    learning_rate : 0.03
    batch_size :          600
    best_batch_loss : 0.4099251804428048
    Epoch 0, Loss: 0.5264697457665822, fit: 0.7383333333333333
    Epoch 0, Loss: 0.5264697457665822
    epoch : 1
      loss :              0.4931620026249046
      loss_factor :              8.509343883073467e-14
      loss adapted learning_rate :              2.55280316492204e-15
    learning_rate : 2.55280316492204e-15
    batch_size :          600
    best_batch_loss : 0.38424067735005957
    Epoch 1, Loss: 0.46487392574909947, fit: 0.7616666666666667
    epoch : 2
      loss :              0.44598409889410884
      loss_factor :              3.113104839830133e-14
      loss adapted learning_rate :              9.3393145194904e-16
    learning_rate : 9.3393145194904e-16
    batch_size :          600
    best_batch_loss : 0.38216494610611523
    Epoch 2, Loss: 0.464873925749093, fit: 0.7633333333333333
    epoch : 3
      loss :              0.44241804145453195
      loss_factor :              2.8729514598410584e-14
      loss adapted learning_rate :              8.618854379523175e-16
    learning_rate : 8.618854379523175e-16
    batch_size :          600
    best_batch_loss : 0.4047452954335796
    Epoch 3, Loss: 0.4648739257490915, fit: 0.755
    epoch : 4
      loss :              0.45716843351678127
      loss_factor :              3.988066181047694e-14
      loss adapted learning_rate :              1.1964198543143081e-15
    learning_rate : 1.1964198543143081e-15
    batch_size :          600
    best_batch_loss : 0.3799201709075151
    Epoch 4, Loss: 0.4648739257490892, fit: 0.7883333333333333
    epoch : 5
      loss :              0.402439771095142
      loss_factor :              1.1143174538585404e-14
      loss adapted learning_rate :              3.342952361575621e-16
    learning_rate : 3.342952361575621e-16
    batch_size :          600
    best_batch_loss : 0.37743684068917155
    Epoch 5, Loss: 0.4648739257490878, fit: 0.7416666666666667
    epoch : 6
      loss :              0.48712590321458926
      loss_factor :              7.52336687473516e-14
      loss adapted learning_rate :              2.257010062420548e-15
    learning_rate : 2.257010062420548e-15
    batch_size :          600
    best_batch_loss : 0.39223030783667734
    Epoch 6, Loss: 0.4648739257490836, fit: 0.735
    epoch : 7
      loss :              0.4991966456472911
      loss_factor :              9.609849459890544e-14
      loss adapted learning_rate :              2.882954837967163e-15
    learning_rate : 2.882954837967163e-15
    batch_size :          600
    best_batch_loss : 0.35547623736442047
    Epoch 7, Loss: 0.46487392574907344, fit: 0.73
    epoch : 8
      loss :              0.5041581080178904
      loss_factor :              1.0608831713767978e-13
      loss adapted learning_rate :              3.182649514130393e-15
    learning_rate : 3.182649514130393e-15
    batch_size :          600
    best_batch_loss : 0.3833024082463093
    Epoch 8, Loss: 0.46487392574906183, fit: 0.75
    Best fit: 0.7883333333333333
    Best loss: 0.46487392574906183
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_0.4679603526907927_fit_0.75_2024-01-03_193055
  self.fit : 0.75
  self.loss : 0.4679603526907927
  current_accuracy : 0.7624
   Accuracy mean: 0.6874
   Accuracy mean: 0.7624
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.1
    learning_rate : 0.1
    batch_size :          600
    best_batch_loss : 0.35851300544576103
    Epoch 0, Loss: 0.6148994247307128, fit: 0.7833333333333333
    Epoch 0, Loss: 0.6148994247307128
    Best fit: 0.7833333333333333
    Best loss: 0.6148994247307128
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_0.4084562018031236_fit_0.7833333333333333_2024-01-03_193058
  self.fit : 0.7833333333333333
  self.loss : 0.4084562018031236
  current_accuracy : 0.8037
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.1
    learning_rate : 0.1
    batch_size :          600
    best_batch_loss : 0.23697772420333366
    Epoch 0, Loss: 0.3262639469474914, fit: 0.8333333333333334
    Epoch 0, Loss: 0.3262639469474914
    epoch : 1
      loss :              0.3033018987408884
      loss_factor :              6.587965118673277e-16
      loss adapted learning_rate :              6.587965118673277e-17
    learning_rate : 6.587965118673277e-17
    batch_size :          600
    best_batch_loss : 0.21190083500538412
    Epoch 1, Loss: 0.2826858371907472, fit: 0.8383333333333334
    epoch : 2
      loss :              0.303816461604317
      loss_factor :              6.700589527507733e-16
      loss adapted learning_rate :              6.700589527507734e-17
    learning_rate : 6.700589527507734e-17
    batch_size :          600
    best_batch_loss : 0.2206544429463303
    Epoch 2, Loss: 0.2826858371907472, fit: 0.8483333333333334
    epoch : 3
      loss :              0.28948137655878875
      loss_factor :              4.1324374761927197e-16
      loss adapted learning_rate :              4.13243747619272e-17
    learning_rate : 4.13243747619272e-17
    batch_size :          600
    best_batch_loss : 0.21719350698146717
    Epoch 3, Loss: 0.28268583719074714, fit: 0.8416666666666667
    epoch : 4
      loss :              0.29998540560533227
      loss_factor :              5.902028014077069e-16
      loss adapted learning_rate :              5.90202801407707e-17
    learning_rate : 5.90202801407707e-17
    batch_size :          600
    best_batch_loss : 0.22352658886527374
    Epoch 4, Loss: 0.28268583719074714, fit: 0.865
    epoch : 5
      loss :              0.24798118083682114
      loss_factor :              8.794015706186365e-17
      loss adapted learning_rate :              8.794015706186366e-18
    learning_rate : 8.794015706186366e-18
    batch_size :          600
    best_batch_loss : 0.22034179689421796
    Epoch 5, Loss: 0.2826858371907471, fit: 0.83
    epoch : 6
      loss :              0.3101438465692427
      loss_factor :              8.234394873019243e-16
      loss adapted learning_rate :              8.234394873019244e-17
    learning_rate : 8.234394873019244e-17
    batch_size :          600
    best_batch_loss : 0.2289071296603278
    Epoch 6, Loss: 0.28268583719074725, fit: 0.835
    epoch : 7
      loss :              0.3054247560089445
      loss_factor :              7.063864437236497e-16
      loss adapted learning_rate :              7.063864437236497e-17
    learning_rate : 7.063864437236497e-17
    batch_size :          600
    best_batch_loss : 0.22010859412972159
    Epoch 7, Loss: 0.28268583719074725, fit: 0.85
    epoch : 8
      loss :              0.2775956841706014
      loss_factor :              2.7172343607521784e-16
      loss adapted learning_rate :              2.7172343607521785e-17
    learning_rate : 2.7172343607521785e-17
    batch_size :          600
    best_batch_loss : 0.2004032393575723
    Epoch 8, Loss: 0.2826858371907473, fit: 0.8483333333333334
    Best fit: 0.865
    Best loss: 0.2826858371907471
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_0.28540765937089363_fit_0.8483333333333334_2024-01-03_193120
  self.fit : 0.8483333333333334
  self.loss : 0.28540765937089363
  current_accuracy : 0.8546
   Accuracy mean: 0.8037
   Accuracy mean: 0.8546
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.9
    learning_rate : 0.9
    batch_size :          600
    best_batch_loss : 0.9882667790206092
    Epoch 0, Loss: 1.7726374527698834, fit: 0.085
    Epoch 0, Loss: 1.7726374527698834
    Best fit: 0.085
    Best loss: 1.7726374527698834
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.83_fit_0.085_2024-01-03_193123
  self.fit : 0.085
  self.loss : 1.83
  current_accuracy : 0.0958
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.01
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.9
    learning_rate : 0.9
    batch_size :          600
    best_batch_loss : 1.7333333333333334
    Epoch 0, Loss: 1.8027333333333342, fit: 0.11666666666666667
    Epoch 0, Loss: 1.8027333333333342
    epoch : 1
      loss :              1.7666666666666666
      loss_factor :              2.961734667234213e-08
      loss adapted learning_rate :              2.665561200510792e-08
    learning_rate : 2.665561200510792e-08
    batch_size :          600
    best_batch_loss : 1.7666666666666666
    Epoch 1, Loss: 1.8027333333333346, fit: 0.115
    epoch : 2
      loss :              1.77
      loss_factor :              3.018093316564956e-08
      loss adapted learning_rate :              2.7162839849084605e-08
    learning_rate : 2.7162839849084605e-08
    batch_size :          600
    best_batch_loss : 1.75
    Epoch 2, Loss: 1.8027333333333342, fit: 0.09
    epoch : 3
      loss :              1.82
      loss_factor :              3.987621049529419e-08
      loss adapted learning_rate :              3.5888589445764776e-08
    learning_rate : 3.5888589445764776e-08
    batch_size :          600
    best_batch_loss : 1.74
    Epoch 3, Loss: 1.8027333333333342, fit: 0.09666666666666666
    epoch : 4
      loss :              1.8066666666666666
      loss_factor :              3.7049326551913005e-08
      loss adapted learning_rate :              3.334439389672171e-08
    learning_rate : 3.334439389672171e-08
    batch_size :          600
    best_batch_loss : 1.7433333333333334
    Epoch 4, Loss: 1.8027333333333344, fit: 0.08833333333333333
    epoch : 5
      loss :              1.8233333333333333
      loss_factor :              4.061259274559433e-08
      loss adapted learning_rate :              3.65513334710349e-08
    learning_rate : 3.65513334710349e-08
    batch_size :          600
    best_batch_loss : 1.7333333333333334
    Epoch 5, Loss: 1.802733333333334, fit: 0.09333333333333334
    epoch : 6
      loss :              1.8133333333333332
      loss_factor :              3.8439386690897824e-08
      loss adapted learning_rate :              3.4595448021808045e-08
    learning_rate : 3.4595448021808045e-08
    batch_size :          600
    best_batch_loss : 1.74
    Epoch 6, Loss: 1.8027333333333337, fit: 0.11
    epoch : 7
      loss :              1.78
      loss_factor :              3.193008120828536e-08
      loss adapted learning_rate :              2.8737073087456825e-08
    learning_rate : 2.8737073087456825e-08
    batch_size :          600
    best_batch_loss : 1.73
    Epoch 7, Loss: 1.802733333333334, fit: 0.10166666666666667
    epoch : 8
      loss :              1.7966666666666666
      loss_factor :              3.504895749017299e-08
      loss adapted learning_rate :              3.154406174115569e-08
    learning_rate : 3.154406174115569e-08
    batch_size :          600
    best_batch_loss : 1.73
    Epoch 8, Loss: 1.8027333333333346, fit: 0.1
    Best fit: 0.11666666666666667
    Best loss: 1.8027333333333337
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.8_fit_0.1_2024-01-03_193145
  self.fit : 0.1
  self.loss : 1.8
  current_accuracy : 0.0958
   Accuracy mean: 0.0958
   Accuracy mean: 0.0958
  Error saving file: doc/out/test_combinations_results/20240103193145.
  normalized_accuracies :      [0.00914674 0.00914674 0.01280544 0.01280544 0.00209068 0.00209068
   0.         0.         0.06049915 0.06049915 0.0457337  0.04690971
   0.05579511 0.1125049  0.69985627 0.83379067 0.73866458 0.84907879
   0.78152359 0.87952437 0.93349013 1.         0.0084934  0.0084934 ]
batch_rate :  0.001
  LR: 1e-64
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-64
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-64
    learning_rate : 1e-64
    batch_size :          60
    best_batch_loss : 1.5262014702006763
    Epoch 0, Loss: 1.771210551857522, fit: 0.1
    Epoch 0, Loss: 1.771210551857522
    Best fit: 0.1
    Best loss: 1.771210551857522
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.7328608949440052_fit_0.1_2024-01-03_193154
  self.fit : 0.1
  self.loss : 1.7328608949440052
  current_accuracy : 0.0797
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-64
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-64
    learning_rate : 1e-64
    batch_size :          60
    best_batch_loss : 1.5050426218233866
    Epoch 0, Loss: 1.771210551857522, fit: 0.05
    Epoch 0, Loss: 1.771210551857522
    epoch : 1
      loss :              1.8251234151586584
      loss_factor :              4.101307842975748e-08
      loss adapted learning_rate :              4.101307842975748e-72
    learning_rate : 4.101307842975748e-72
    batch_size :          60
    best_batch_loss : 1.5288908115746123
    Epoch 1, Loss: 1.7712105518575216, fit: 0.11666666666666667
    epoch : 2
      loss :              1.7238820467396536
      loss_factor :              2.3177973796303948e-08
      loss adapted learning_rate :              2.3177973796303946e-72
    learning_rate : 2.3177973796303946e-72
    batch_size :          60
    best_batch_loss : 1.5509122276623928
    Epoch 2, Loss: 1.771210551857522, fit: 0.11666666666666667
    epoch : 3
      loss :              1.7350856236903562
      loss_factor :              2.4729145731384544e-08
      loss adapted learning_rate :              2.472914573138454e-72
    learning_rate : 2.472914573138454e-72
    batch_size :          60
    best_batch_loss : 1.507642643775612
    Epoch 3, Loss: 1.7712105518575219, fit: 0.016666666666666666
    epoch : 4
      loss :              1.9065151782187586
      loss_factor :              6.344576984608489e-08
      loss adapted learning_rate :              6.344576984608488e-72
    learning_rate : 6.344576984608488e-72
    batch_size :          60
    best_batch_loss : 1.556818916115859
    Epoch 4, Loss: 1.771210551857518, fit: 0.15
    epoch : 5
      loss :              1.6516562811348692
      loss_factor :              1.5107643666819998e-08
      loss adapted learning_rate :              1.5107643666819997e-72
    learning_rate : 1.5107643666819997e-72
    batch_size :          60
    best_batch_loss : 1.4978921398229987
    Epoch 5, Loss: 1.7712105518575203, fit: 0.05
    epoch : 6
      loss :              1.8465968461082134
      loss_factor :              4.6102119777430364e-08
      loss adapted learning_rate :              4.610211977743036e-72
    learning_rate : 4.610211977743036e-72
    batch_size :          60
    best_batch_loss : 1.4950192971503857
    Epoch 6, Loss: 1.7712105518575207, fit: 0.15
    epoch : 7
      loss :              1.6467137134885363
      loss_factor :              1.466158836844087e-08
      loss adapted learning_rate :              1.466158836844087e-72
    learning_rate : 1.466158836844087e-72
    batch_size :          60
    best_batch_loss : 1.5248986924529413
    Epoch 7, Loss: 1.7712105518575192, fit: 0.11666666666666667
    epoch : 8
      loss :              1.694469795823762
      loss_factor :              1.9513641356867507e-08
      loss adapted learning_rate :              1.9513641356867507e-72
    learning_rate : 1.9513641356867507e-72
    batch_size :          60
    best_batch_loss : 1.293920784357163
    Epoch 8, Loss: 1.7712105518575219, fit: 0.06666666666666667
    Best fit: 0.15
    Best loss: 1.771210551857518
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-64_hidden_size_784_loss_1.8086972780692794_fit_0.06666666666666667_2024-01-03_193256
  self.fit : 0.06666666666666667
  self.loss : 1.8086972780692794
  current_accuracy : 0.0797
   Accuracy mean: 0.0797
   Accuracy mean: 0.0797
  LR: 1e-33
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-33
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-33
    learning_rate : 1e-33
    batch_size :          60
    best_batch_loss : 1.4948957198045776
    Epoch 0, Loss: 1.792595442104774, fit: 0.1
    Epoch 0, Loss: 1.792595442104774
    Best fit: 0.1
    Best loss: 1.792595442104774
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.7193403074617764_fit_0.1_2024-01-03_193304
  self.fit : 0.1
  self.loss : 1.7193403074617764
  current_accuracy : 0.0782
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-33
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-33
    learning_rate : 1e-33
    batch_size :          60
    best_batch_loss : 1.5600518634031642
    Epoch 0, Loss: 1.7925954421047725, fit: 0.016666666666666666
    Epoch 0, Loss: 1.7925954421047725
    epoch : 1
      loss :              1.9195887450813918
      loss_factor :              6.79331806975237e-08
      loss adapted learning_rate :              6.79331806975237e-41
    learning_rate : 6.79331806975237e-41
    batch_size :          60
    best_batch_loss : 1.5711596825399672
    Epoch 1, Loss: 1.7925954421047743, fit: 0.08333333333333333
    epoch : 2
      loss :              1.793444495490477
      loss_factor :              3.442543243463013e-08
      loss adapted learning_rate :              3.442543243463013e-41
    learning_rate : 3.442543243463013e-41
    batch_size :          60
    best_batch_loss : 1.577981432302319
    Epoch 2, Loss: 1.7925954421047765, fit: 0.11666666666666667
    epoch : 3
      loss :              1.71865834136544
      loss_factor :              2.248513514176362e-08
      loss adapted learning_rate :              2.248513514176362e-41
    learning_rate : 2.248513514176362e-41
    batch_size :          60
    best_batch_loss : 1.5664059639517514
    Epoch 3, Loss: 1.792595442104774, fit: 0.03333333333333333
    epoch : 4
      loss :              1.8580376432644832
      loss_factor :              4.9039392802005886e-08
      loss adapted learning_rate :              4.903939280200589e-41
    learning_rate : 4.903939280200589e-41
    batch_size :          60
    best_batch_loss : 1.5621921832768622
    Epoch 4, Loss: 1.792595442104776, fit: 0.03333333333333333
    epoch : 5
      loss :              1.8100358066481002
      loss_factor :              3.7746063216890975e-08
      loss adapted learning_rate :              3.7746063216890975e-41
    learning_rate : 3.7746063216890975e-41
    batch_size :          60
    best_batch_loss : 1.5056925134600623
    Epoch 5, Loss: 1.7925954421047703, fit: 0.06666666666666667
    epoch : 6
      loss :              1.81498181300241
      loss_factor :              3.879026758895535e-08
      loss adapted learning_rate :              3.879026758895535e-41
    learning_rate : 3.879026758895535e-41
    batch_size :          60
    best_batch_loss : 1.557587362580314
    Epoch 6, Loss: 1.792595442104776, fit: 0.016666666666666666
    epoch : 7
      loss :              1.9109413062563056
      loss_factor :              6.493419799442113e-08
      loss adapted learning_rate :              6.493419799442113e-41
    learning_rate : 6.493419799442113e-41
    batch_size :          60
    best_batch_loss : 1.589827796321957
    Epoch 7, Loss: 1.7925954421047727, fit: 0.05
    epoch : 8
      loss :              1.854481695430871
      loss_factor :              4.810890920256355e-08
      loss adapted learning_rate :              4.810890920256355e-41
    learning_rate : 4.810890920256355e-41
    batch_size :          60
    best_batch_loss : 1.5538265361655046
    Epoch 8, Loss: 1.7925954421047776, fit: 0.05
    Best fit: 0.11666666666666667
    Best loss: 1.7925954421047703
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-33_hidden_size_784_loss_1.8272338114148592_fit_0.05_2024-01-03_193406
  self.fit : 0.05
  self.loss : 1.8272338114148592
  current_accuracy : 0.0782
   Accuracy mean: 0.0782
   Accuracy mean: 0.0782
  LR: 1e-17
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-17
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-17
    learning_rate : 1e-17
    batch_size :          60
    best_batch_loss : 1.4869667700673785
    Epoch 0, Loss: 1.7441467987942694, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7441467987942694
    Best fit: 0.08333333333333333
    Best loss: 1.7441467987942694
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7680417746680912_fit_0.08333333333333333_2024-01-03_193413
  self.fit : 0.08333333333333333
  self.loss : 1.7680417746680912
  current_accuracy : 0.093
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-17
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-17
    learning_rate : 1e-17
    batch_size :          60
    best_batch_loss : 1.502452697908851
    Epoch 0, Loss: 1.7441467987942685, fit: 0.08333333333333333
    Epoch 0, Loss: 1.7441467987942685
    epoch : 1
      loss :              1.7655382211621065
      loss_factor :              2.9428710860193872e-08
      loss adapted learning_rate :              2.9428710860193874e-25
    learning_rate : 2.9428710860193874e-25
    batch_size :          60
    best_batch_loss : 1.5095394273432505
    Epoch 1, Loss: 1.7441467987942672, fit: 0.13333333333333333
    epoch : 2
      loss :              1.6791924621730974
      loss_factor :              1.7823983049562494e-08
      loss adapted learning_rate :              1.7823983049562495e-25
    learning_rate : 1.7823983049562495e-25
    batch_size :          60
    best_batch_loss : 1.361682216127295
    Epoch 2, Loss: 1.7441467987942683, fit: 0.08333333333333333
    epoch : 3
      loss :              1.777578408815246
      loss_factor :              3.1498339896364153e-08
      loss adapted learning_rate :              3.1498339896364154e-25
    learning_rate : 3.1498339896364154e-25
    batch_size :          60
    best_batch_loss : 1.4496934970322497
    Epoch 3, Loss: 1.7441467987942696, fit: 0.11666666666666667
    epoch : 4
      loss :              1.6841029723432612
      loss_factor :              1.8352127745935156e-08
      loss adapted learning_rate :              1.8352127745935159e-25
    learning_rate : 1.8352127745935159e-25
    batch_size :          60
    best_batch_loss : 1.4799195573734962
    Epoch 4, Loss: 1.7441467987942672, fit: 0.11666666666666667
    epoch : 5
      loss :              1.6759230861975838
      loss_factor :              1.7479976121372434e-08
      loss adapted learning_rate :              1.7479976121372435e-25
    learning_rate : 1.7479976121372435e-25
    batch_size :          60
    best_batch_loss : 1.4928645534970073
    Epoch 5, Loss: 1.7441467987942698, fit: 0.11666666666666667
    epoch : 6
      loss :              1.7221465937701652
      loss_factor :              2.2945692546981707e-08
      loss adapted learning_rate :              2.294569254698171e-25
    learning_rate : 2.294569254698171e-25
    batch_size :          60
    best_batch_loss : 1.486635453097635
    Epoch 6, Loss: 1.7441467987942691, fit: 0.11666666666666667
    epoch : 7
      loss :              1.6628059383959004
      loss_factor :              1.6159044181119497e-08
      loss adapted learning_rate :              1.61590441811195e-25
    learning_rate : 1.61590441811195e-25
    batch_size :          60
    best_batch_loss : 1.4495901603581798
    Epoch 7, Loss: 1.7441467987942687, fit: 0.06666666666666667
    epoch : 8
      loss :              1.7781807889966343
      loss_factor :              3.160524340089628e-08
      loss adapted learning_rate :              3.1605243400896282e-25
    learning_rate : 3.1605243400896282e-25
    batch_size :          60
    best_batch_loss : 1.4784793249023664
    Epoch 8, Loss: 1.7441467987942687, fit: 0.06666666666666667
    Best fit: 0.13333333333333333
    Best loss: 1.7441467987942672
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-17_hidden_size_784_loss_1.7684747787465764_fit_0.06666666666666667_2024-01-03_193515
  self.fit : 0.06666666666666667
  self.loss : 1.7684747787465764
  current_accuracy : 0.093
   Accuracy mean: 0.093
   Accuracy mean: 0.093
  LR: 1e-10
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-10
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-10
    learning_rate : 1e-10
    batch_size :          60
    best_batch_loss : 1.50752560219005
    Epoch 0, Loss: 1.8032898304057896, fit: 0.03333333333333333
    Epoch 0, Loss: 1.8032898304057896
    Best fit: 0.03333333333333333
    Best loss: 1.8032898304057896
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.8870542637721759_fit_0.03333333333333333_2024-01-03_193522
  self.fit : 0.03333333333333333
  self.loss : 1.8870542637721759
  current_accuracy : 0.0683
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-10
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-10
    learning_rate : 1e-10
    batch_size :          60
    best_batch_loss : 1.5171328464107376
    Epoch 0, Loss: 1.8032897287037732, fit: 0.016666666666666666
    Epoch 0, Loss: 1.8032897287037732
    epoch : 1
      loss :              1.9189413083921372
      loss_factor :              6.770440388703837e-08
      loss adapted learning_rate :              6.770440388703837e-18
    learning_rate : 6.770440388703837e-18
    batch_size :          60
    best_batch_loss : 1.5856522689644488
    Epoch 1, Loss: 1.8032896788648922, fit: 0.05
    epoch : 2
      loss :              1.8197427209991792
      loss_factor :              3.9819876496378786e-08
      loss adapted learning_rate :              3.981987649637879e-18
    learning_rate : 3.981987649637879e-18
    batch_size :          60
    best_batch_loss : 1.585416544767695
    Epoch 2, Loss: 1.8032896788648909, fit: 0.1
    epoch : 3
      loss :              1.7253126997351331
      loss_factor :              2.337104819353771e-08
      loss adapted learning_rate :              2.3371048193537712e-18
    learning_rate : 2.3371048193537712e-18
    batch_size :          60
    best_batch_loss : 1.5445231032324194
    Epoch 3, Loss: 1.803289678864894, fit: 0.11666666666666667
    epoch : 4
      loss :              1.69693095439244
      loss_factor :              1.9798929936190465e-08
      loss adapted learning_rate :              1.9798929936190468e-18
    learning_rate : 1.9798929936190468e-18
    batch_size :          60
    best_batch_loss : 1.5344070441229836
    Epoch 4, Loss: 1.8032896788648927, fit: 0.05
    epoch : 5
      loss :              1.8770860027104597
      loss_factor :              5.430523683574794e-08
      loss adapted learning_rate :              5.430523683574794e-18
    learning_rate : 5.430523683574794e-18
    batch_size :          60
    best_batch_loss : 1.552210163249399
    Epoch 5, Loss: 1.8032896788648933, fit: 0.05
    epoch : 6
      loss :              1.8397581153421447
      loss_factor :              4.442293749408018e-08
      loss adapted learning_rate :              4.442293749408018e-18
    learning_rate : 4.442293749408018e-18
    batch_size :          60
    best_batch_loss : 1.527975836885466
    Epoch 6, Loss: 1.8032896788648909, fit: 0.03333333333333333
    epoch : 7
      loss :              1.9105093008118896
      loss_factor :              6.478755085988385e-08
      loss adapted learning_rate :              6.478755085988385e-18
    learning_rate : 6.478755085988385e-18
    batch_size :          60
    best_batch_loss : 1.550846030970163
    Epoch 7, Loss: 1.8032896788648938, fit: 0.05
    epoch : 8
      loss :              1.8002947909752967
      loss_factor :              3.5763189908140986e-08
      loss adapted learning_rate :              3.576318990814098e-18
    learning_rate : 3.576318990814098e-18
    batch_size :          60
    best_batch_loss : 1.580587685301794
    Epoch 8, Loss: 1.8032896788648927, fit: 0.08333333333333333
    Best fit: 0.11666666666666667
    Best loss: 1.8032896788648909
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-10_hidden_size_784_loss_1.791991362480339_fit_0.08333333333333333_2024-01-03_193625
  self.fit : 0.08333333333333333
  self.loss : 1.791991362480339
  current_accuracy : 0.0683
   Accuracy mean: 0.0683
   Accuracy mean: 0.0683
  LR: 1e-07
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          1e-07
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-07
    learning_rate : 1e-07
    batch_size :          60
    best_batch_loss : 1.4404397775500362
    Epoch 0, Loss: 1.743396239488223, fit: 0.06666666666666667
    Epoch 0, Loss: 1.743396239488223
    Best fit: 0.06666666666666667
    Best loss: 1.743396239488223
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.7811669972015869_fit_0.06666666666666667_2024-01-03_193632
  self.fit : 0.06666666666666667
  self.loss : 1.7811669972015869
  current_accuracy : 0.1006
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          1e-07
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              1e-07
    learning_rate : 1e-07
    batch_size :          60
    best_batch_loss : 1.4855091118656654
    Epoch 0, Loss: 1.743263340281882, fit: 0.21666666666666667
    Epoch 0, Loss: 1.743263340281882
    epoch : 1
      loss :              1.5230200826703333
      loss_factor :              6.715158998763822e-09
      loss adapted learning_rate :              6.715158998763822e-16
    learning_rate : 6.715158998763822e-16
    batch_size :          60
    best_batch_loss : 1.4671986887257433
    Epoch 1, Loss: 1.7431863793917983, fit: 0.08333333333333333
    epoch : 2
      loss :              1.764955792879669
      loss_factor :              2.9331773329612655e-08
      loss adapted learning_rate :              2.9331773329612653e-15
    learning_rate : 2.9331773329612653e-15
    batch_size :          60
    best_batch_loss : 1.4660763007793556
    Epoch 2, Loss: 1.7431863793893174, fit: 0.03333333333333333
    epoch : 3
      loss :              1.8704296747614086
      loss_factor :              5.2409961474306834e-08
      loss adapted learning_rate :              5.240996147430683e-15
    learning_rate : 5.240996147430683e-15
    batch_size :          60
    best_batch_loss : 1.4909223387463493
    Epoch 3, Loss: 1.743186379383811, fit: 0.08333333333333333
    epoch : 4
      loss :              1.7844850017614344
      loss_factor :              3.274379566679102e-08
      loss adapted learning_rate :              3.274379566679102e-15
    learning_rate : 3.274379566679102e-15
    batch_size :          60
    best_batch_loss : 1.4433873705294125
    Epoch 4, Loss: 1.743186379377367, fit: 0.15
    epoch : 5
      loss :              1.6381488200439331
      loss_factor :              1.3916612457073805e-08
      loss adapted learning_rate :              1.3916612457073804e-15
    learning_rate : 1.3916612457073804e-15
    batch_size :          60
    best_batch_loss : 1.4724204716311227
    Epoch 5, Loss: 1.743186379374126, fit: 0.05
    epoch : 6
      loss :              1.8482180637791548
      loss_factor :              4.6508475666959564e-08
      loss adapted learning_rate :              4.650847566695956e-15
    learning_rate : 4.650847566695956e-15
    batch_size :          60
    best_batch_loss : 1.42037824994515
    Epoch 6, Loss: 1.743186379369687, fit: 0.06666666666666667
    epoch : 7
      loss :              1.7975292965738054
      loss_factor :              3.521760138314591e-08
      loss adapted learning_rate :              3.521760138314591e-15
    learning_rate : 3.521760138314591e-15
    batch_size :          60
    best_batch_loss : 1.484082277127263
    Epoch 7, Loss: 1.7431863793642666, fit: 0.15
    epoch : 8
      loss :              1.6493269992283466
      loss_factor :              1.489593208909664e-08
      loss adapted learning_rate :              1.4895932089096639e-15
    learning_rate : 1.4895932089096639e-15
    batch_size :          60
    best_batch_loss : 1.348734455368637
    Epoch 8, Loss: 1.7431863793604665, fit: 0.08333333333333333
    Best fit: 0.21666666666666667
    Best loss: 1.7431863793604665
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_1e-07_hidden_size_784_loss_1.770312120603922_fit_0.08333333333333333_2024-01-03_193733
  self.fit : 0.08333333333333333
  self.loss : 1.770312120603922
  current_accuracy : 0.1005
   Accuracy mean: 0.1006
   Accuracy mean: 0.1005
  LR: 0.0001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.0001
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    learning_rate : 0.0001
    batch_size :          60
    best_batch_loss : 1.2715884647513818
    Epoch 0, Loss: 1.6229940563210128, fit: 0.23333333333333334
    Epoch 0, Loss: 1.6229940563210128
    Best fit: 0.23333333333333334
    Best loss: 1.6229940563210128
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.4719312490653311_fit_0.23333333333333334_2024-01-03_193740
  self.fit : 0.23333333333333334
  self.loss : 1.4719312490653311
  current_accuracy : 0.1865
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.0001
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.0001
    learning_rate : 0.0001
    batch_size :          60
    best_batch_loss : 1.1808005825446561
    Epoch 0, Loss: 1.5004271651590353, fit: 0.3333333333333333
    Epoch 0, Loss: 1.5004271651590353
    epoch : 1
      loss :              1.2819164423054417
      loss_factor :              1.1983872501461686e-09
      loss adapted learning_rate :              1.1983872501461687e-13
    learning_rate : 1.1983872501461687e-13
    batch_size :          60
    best_batch_loss : 1.0484187431166192
    Epoch 1, Loss: 1.4446623937218912, fit: 0.35
    epoch : 2
      loss :              1.2403495792608095
      loss_factor :              8.618685565439307e-10
      loss adapted learning_rate :              8.618685565439307e-14
    learning_rate : 8.618685565439307e-14
    batch_size :          60
    best_batch_loss : 1.1377157473519277
    Epoch 2, Loss: 1.4446623936113439, fit: 0.26666666666666666
    epoch : 3
      loss :              1.3874823843301485
      loss_factor :              2.64408114775317e-09
      loss adapted learning_rate :              2.6440811477531704e-13
    learning_rate : 2.6440811477531704e-13
    batch_size :          60
    best_batch_loss : 1.1020829230078164
    Epoch 3, Loss: 1.4446623934287441, fit: 0.25
    epoch : 4
      loss :              1.4346200046160968
      loss_factor :              3.692910856210055e-09
      loss adapted learning_rate :              3.692910856210055e-13
    learning_rate : 3.692910856210055e-13
    batch_size :          60
    best_batch_loss : 1.0975631722345787
    Epoch 4, Loss: 1.4446623930879339, fit: 0.2
    epoch : 5
      loss :              1.5178792507711314
      loss_factor :              6.491906295042993e-09
      loss adapted learning_rate :              6.491906295042994e-13
    learning_rate : 6.491906295042994e-13
    batch_size :          60
    best_batch_loss : 1.1222453434562931
    Epoch 5, Loss: 1.4446623925398763, fit: 0.3
    epoch : 6
      loss :              1.3394029752783783
      loss_factor :              1.8582861634066762e-09
      loss adapted learning_rate :              1.8582861634066762e-13
    learning_rate : 1.8582861634066762e-13
    batch_size :          60
    best_batch_loss : 1.1121340950861744
    Epoch 6, Loss: 1.444662392100013, fit: 0.31666666666666665
    epoch : 7
      loss :              1.3034533997673763
      loss_factor :              1.4156473868916305e-09
      loss adapted learning_rate :              1.4156473868916307e-13
    learning_rate : 1.4156473868916307e-13
    batch_size :          60
    best_batch_loss : 1.0460500555348335
    Epoch 7, Loss: 1.4446623919269441, fit: 0.23333333333333334
    epoch : 8
      loss :              1.4654377726256764
      loss_factor :              4.5674501464411076e-09
      loss adapted learning_rate :              4.567450146441108e-13
    learning_rate : 4.567450146441108e-13
    batch_size :          60
    best_batch_loss : 1.0980077276737268
    Epoch 8, Loss: 1.4446623916055477, fit: 0.21666666666666667
    Best fit: 0.35
    Best loss: 1.4446623916055477
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.0001_hidden_size_784_loss_1.526371312455126_fit_0.21666666666666667_2024-01-03_193842
  self.fit : 0.21666666666666667
  self.loss : 1.526371312455126
  current_accuracy : 0.2487
   Accuracy mean: 0.1865
   Accuracy mean: 0.2487
  LR: 0.001
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.001
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.001
    learning_rate : 0.001
    batch_size :          60
    best_batch_loss : 0.7106979663225359
    Epoch 0, Loss: 1.280792925423385, fit: 0.5166666666666667
    Epoch 0, Loss: 1.280792925423385
    Best fit: 0.5166666666666667
    Best loss: 1.280792925423385
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_0.8886383070271896_fit_0.5166666666666667_2024-01-03_193849
  self.fit : 0.5166666666666667
  self.loss : 0.8886383070271896
  current_accuracy : 0.5155
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.001
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.001
    learning_rate : 0.001
    batch_size :          60
    best_batch_loss : 0.3301198953151611
    Epoch 0, Loss: 0.8045164766821553, fit: 0.7
    Epoch 0, Loss: 0.8045164766821553
    epoch : 1
      loss :              0.5325388743693008
      loss_factor :              1.834480753703505e-13
      loss adapted learning_rate :              1.834480753703505e-16
    learning_rate : 1.834480753703505e-16
    batch_size :          60
    best_batch_loss : 0.33308725422929586
    Epoch 1, Loss: 0.7064287186955764, fit: 0.6666666666666666
    epoch : 2
      loss :              0.5944247714459145
      loss_factor :              5.507683743671289e-13
      loss adapted learning_rate :              5.507683743671289e-16
    learning_rate : 5.507683743671289e-16
    batch_size :          60
    best_batch_loss : 0.3902877887485335
    Epoch 2, Loss: 0.7064287186955475, fit: 0.6666666666666666
    epoch : 3
      loss :              0.5796539148342603
      loss_factor :              4.282404958584481e-13
      loss adapted learning_rate :              4.2824049585844814e-16
    learning_rate : 4.2824049585844814e-16
    batch_size :          60
    best_batch_loss : 0.30347437059213206
    Epoch 3, Loss: 0.7064287186955027, fit: 0.65
    epoch : 4
      loss :              0.6455590778583197
      loss_factor :              1.2570713416218305e-12
      loss adapted learning_rate :              1.2570713416218305e-15
    learning_rate : 1.2570713416218305e-15
    batch_size :          60
    best_batch_loss : 0.3128463150936844
    Epoch 4, Loss: 0.7064287186954037, fit: 0.6
    epoch : 5
      loss :              0.7246580822060615
      loss_factor :              3.9932950804117835e-12
      loss adapted learning_rate :              3.9932950804117835e-15
    learning_rate : 3.9932950804117835e-15
    batch_size :          60
    best_batch_loss : 0.3296821259809806
    Epoch 5, Loss: 0.706428718695044, fit: 0.6
    epoch : 6
      loss :              0.7586724374053448
      loss_factor :              6.317468195235429e-12
      loss adapted learning_rate :              6.3174681952354284e-15
    learning_rate : 6.3174681952354284e-15
    batch_size :          60
    best_batch_loss : 0.34088841679447685
    Epoch 6, Loss: 0.706428718694324, fit: 0.4666666666666667
    epoch : 7
      loss :              0.9611949183773242
      loss_factor :              6.731543571348105e-11
      loss adapted learning_rate :              6.731543571348105e-14
    learning_rate : 6.731543571348105e-14
    batch_size :          60
    best_batch_loss : 0.3761494677255191
    Epoch 7, Loss: 0.706428718689073, fit: 0.5166666666666667
    epoch : 8
      loss :              0.9456531435375105
      loss_factor :              5.718981685567522e-11
      loss adapted learning_rate :              5.718981685567523e-14
    learning_rate : 5.718981685567523e-14
    batch_size :          60
    best_batch_loss : 0.29373267857551716
    Epoch 8, Loss: 0.7064287186804019, fit: 0.5333333333333333
    Best fit: 0.7
    Best loss: 0.7064287186804019
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.001_hidden_size_784_loss_0.8951768908687315_fit_0.5333333333333333_2024-01-03_193950
  self.fit : 0.5333333333333333
  self.loss : 0.8951768908687315
  current_accuracy : 0.6337
   Accuracy mean: 0.5155
   Accuracy mean: 0.6337
  LR: 0.02
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.02
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.02
    learning_rate : 0.02
    batch_size :          60
    best_batch_loss : 0.034711863746524844
    Epoch 0, Loss: 0.47156924484714685, fit: 0.8833333333333333
    Epoch 0, Loss: 0.47156924484714685
    Best fit: 0.8833333333333333
    Best loss: 0.47156924484714685
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_0.19935430775222088_fit_0.8833333333333333_2024-01-03_193958
  self.fit : 0.8833333333333333
  self.loss : 0.19935430775222088
  current_accuracy : 0.8559
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.02
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.02
    learning_rate : 0.02
    batch_size :          60
    best_batch_loss : 0.006721760066619979
    Epoch 0, Loss: 0.23412468566577063, fit: 0.8666666666666667
    Epoch 0, Loss: 0.23412468566577063
    epoch : 1
      loss :              0.2304217479935704
      loss_factor :              4.219244266064849e-17
      loss adapted learning_rate :              8.438488532129698e-19
    learning_rate : 8.438488532129698e-19
    batch_size :          60
    best_batch_loss : 0.03384228159721241
    Epoch 1, Loss: 0.20094059283849683, fit: 0.9333333333333333
    epoch : 2
      loss :              0.1290308570441767
      loss_factor :              1.279192249466403e-19
      loss adapted learning_rate :              2.558384498932806e-21
    learning_rate : 2.558384498932806e-21
    batch_size :          60
    best_batch_loss : 0.02858003663077447
    Epoch 2, Loss: 0.20094059283849683, fit: 0.8666666666666667
    epoch : 3
      loss :              0.24549039143763285
      loss_factor :              7.949594637353863e-17
      loss adapted learning_rate :              1.5899189274707726e-18
    learning_rate : 1.5899189274707726e-18
    batch_size :          60
    best_batch_loss : 0.023478428077723792
    Epoch 3, Loss: 0.20094059283849675, fit: 0.8166666666666667
    epoch : 4
      loss :              0.3421759715492083
      loss_factor :              2.2003668345130366e-15
      loss adapted learning_rate :              4.4007336690260736e-17
    learning_rate : 4.4007336690260736e-17
    batch_size :          60
    best_batch_loss : 0.004620728569197358
    Epoch 4, Loss: 0.20094059283849708, fit: 0.9166666666666666
    epoch : 5
      loss :              0.1368255082127249
      loss_factor :              2.2996974265816064e-19
      loss adapted learning_rate :              4.599394853163213e-21
    learning_rate : 4.599394853163213e-21
    batch_size :          60
    best_batch_loss : 3.3632947838504476e-06
    Epoch 5, Loss: 0.20094059283849686, fit: 0.8
    epoch : 6
      loss :              0.34584852165096425
      loss_factor :              2.448269577564951e-15
      loss adapted learning_rate :              4.896539155129903e-17
    learning_rate : 4.896539155129903e-17
    batch_size :          60
    best_batch_loss : 0.008779573868791904
    Epoch 6, Loss: 0.20094059283849688, fit: 0.95
    epoch : 7
      loss :              0.10754087848612856
      loss_factor :              2.0688823684480887e-20
      loss adapted learning_rate :              4.1377647368961774e-22
    learning_rate : 4.1377647368961774e-22
    batch_size :          60
    best_batch_loss : 0.00679865428230269
    Epoch 7, Loss: 0.20094059283849663, fit: 0.9666666666666667
    epoch : 8
      loss :              0.06729742100332747
      loss_factor :              1.905391456139968e-22
      loss adapted learning_rate :              3.810782912279936e-24
    learning_rate : 3.810782912279936e-24
    batch_size :          60
    best_batch_loss : 0.0004384035733077663
    Epoch 8, Loss: 0.20094059283849683, fit: 0.9333333333333333
    Best fit: 0.9666666666666667
    Best loss: 0.20094059283849663
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.02_hidden_size_784_loss_0.13204487388667865_fit_0.9333333333333333_2024-01-03_194059
  self.fit : 0.9333333333333333
  self.loss : 0.13204487388667865
  current_accuracy : 0.8937
   Accuracy mean: 0.8559
   Accuracy mean: 0.8937
  LR: 0.025
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.025
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.025
    learning_rate : 0.025
    batch_size :          60
    best_batch_loss : 0.03222833972057608
    Epoch 0, Loss: 0.4250942425138724, fit: 0.8333333333333334
    Epoch 0, Loss: 0.4250942425138724
    Best fit: 0.8333333333333334
    Best loss: 0.4250942425138724
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_0.2902943227205453_fit_0.8333333333333334_2024-01-03_194106
  self.fit : 0.8333333333333334
  self.loss : 0.2902943227205453
  current_accuracy : 0.8767
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.025
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.025
    learning_rate : 0.025
    batch_size :          60
    best_batch_loss : 0.032884969836851914
    Epoch 0, Loss: 0.21441042055844498, fit: 0.9
    Epoch 0, Loss: 0.21441042055844498
    epoch : 1
      loss :              0.16899806827194983
      loss_factor :              1.900279155397465e-18
      loss adapted learning_rate :              4.7506978884936634e-20
    learning_rate : 4.7506978884936634e-20
    batch_size :          60
    best_batch_loss : 0.00019229629361340665
    Epoch 1, Loss: 0.18215875030555603, fit: 0.95
    epoch : 2
      loss :              0.10912096503292297
      loss_factor :              2.3937675809628588e-20
      loss adapted learning_rate :              5.984418952407148e-22
    learning_rate : 5.984418952407148e-22
    batch_size :          60
    best_batch_loss : 0.003314524342210652
    Epoch 2, Loss: 0.182158750305556, fit: 0.9166666666666666
    epoch : 3
      loss :              0.1636869182024459
      loss_factor :              1.3808284340196182e-18
      loss adapted learning_rate :              3.452071085049046e-20
    learning_rate : 3.452071085049046e-20
    batch_size :          60
    best_batch_loss : 0.0022633430637773433
    Epoch 3, Loss: 0.18215875030555603, fit: 0.8833333333333333
    epoch : 4
      loss :              0.23279684587266708
      loss_factor :              4.674884853746434e-17
      loss adapted learning_rate :              1.1687212134366087e-18
    learning_rate : 1.1687212134366087e-18
    batch_size :          60
    best_batch_loss : 1.1653042545699307e-07
    Epoch 4, Loss: 0.18215875030555578, fit: 0.85
    epoch : 5
      loss :              0.28246447936954927
      loss_factor :              3.233243547053072e-16
      loss adapted learning_rate :              8.08310886763268e-18
    learning_rate : 8.08310886763268e-18
    batch_size :          60
    best_batch_loss : 0.0003222148668064156
    Epoch 5, Loss: 0.1821587503055561, fit: 0.9166666666666666
    epoch : 6
      loss :              0.15755720388258404
      loss_factor :              9.42719844519075e-19
      loss adapted learning_rate :              2.3567996112976876e-20
    learning_rate : 2.3567996112976876e-20
    batch_size :          60
    best_batch_loss : 0.0021803709013812093
    Epoch 6, Loss: 0.18215875030555576, fit: 0.9
    epoch : 7
      loss :              0.1994446426043653
      loss_factor :              9.959183851141781e-18
      loss adapted learning_rate :              2.4897959627854454e-19
    learning_rate : 2.4897959627854454e-19
    batch_size :          60
    best_batch_loss : 4.9993298592520935e-05
    Epoch 7, Loss: 0.18215875030555587, fit: 0.8333333333333334
    epoch : 8
      loss :              0.3102001222669163
      loss_factor :              8.249348414093761e-16
      loss adapted learning_rate :              2.0623371035234406e-17
    learning_rate : 2.0623371035234406e-17
    batch_size :          60
    best_batch_loss : 1.0703382743116845e-06
    Epoch 8, Loss: 0.1821587503055557, fit: 0.9
    Best fit: 0.95
    Best loss: 0.1821587503055557
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.025_hidden_size_784_loss_0.20188037418171187_fit_0.9_2024-01-03_194208
  self.fit : 0.9
  self.loss : 0.20188037418171187
  current_accuracy : 0.8997
   Accuracy mean: 0.8767
   Accuracy mean: 0.8997
  LR: 0.03
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.03
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.03
    learning_rate : 0.03
    batch_size :          60
    best_batch_loss : 0.03341052802538318
    Epoch 0, Loss: 0.39604875044648313, fit: 0.85
    Epoch 0, Loss: 0.39604875044648313
    Best fit: 0.85
    Best loss: 0.39604875044648313
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_0.29626084219376725_fit_0.85_2024-01-03_194215
  self.fit : 0.85
  self.loss : 0.29626084219376725
  current_accuracy : 0.8766
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.03
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.03
    learning_rate : 0.03
    batch_size :          60
    best_batch_loss : 0.0001878494904123719
    Epoch 0, Loss: 0.20404273071667, fit: 0.8833333333333333
    Epoch 0, Loss: 0.20404273071667
    epoch : 1
      loss :              0.20135256961405165
      loss_factor :              1.095397545432909e-17
      loss adapted learning_rate :              3.286192636298727e-19
    learning_rate : 3.286192636298727e-19
    batch_size :          60
    best_batch_loss : 0.014607021229056483
    Epoch 1, Loss: 0.17801722254383412, fit: 0.7833333333333333
    epoch : 2
      loss :              0.3616782181786872
      loss_factor :              3.83021845627746e-15
      loss adapted learning_rate :              1.1490655368832378e-16
    learning_rate : 1.1490655368832378e-16
    batch_size :          60
    best_batch_loss : 4.116400780398862e-07
    Epoch 2, Loss: 0.17801722254383423, fit: 0.9
    epoch : 3
      loss :              0.19099329855704894
      loss_factor :              6.4592380863843084e-18
      loss adapted learning_rate :              1.9377714259152924e-19
    learning_rate : 1.9377714259152924e-19
    batch_size :          60
    best_batch_loss : 3.849222530373042e-06
    Epoch 3, Loss: 0.1780172225438341, fit: 0.8833333333333333
    epoch : 4
      loss :              0.216963132784596
      loss_factor :              2.311316561412563e-17
      loss adapted learning_rate :              6.9339496842376885e-19
    learning_rate : 6.9339496842376885e-19
    batch_size :          60
    best_batch_loss : 0.0004713919731967072
    Epoch 4, Loss: 0.1780172225438341, fit: 0.8833333333333333
    epoch : 5
      loss :              0.20545400975268485
      loss_factor :              1.340127798993271e-17
      loss adapted learning_rate :              4.0203833969798126e-19
    learning_rate : 4.0203833969798126e-19
    batch_size :          60
    best_batch_loss : 4.849319352091933e-10
    Epoch 5, Loss: 0.17801722254383426, fit: 0.9
    epoch : 6
      loss :              0.20299278547291594
      loss_factor :              1.1879715226460832e-17
      loss adapted learning_rate :              3.5639145679382494e-19
    learning_rate : 3.5639145679382494e-19
    batch_size :          60
    best_batch_loss : 8.849268583497385e-07
    Epoch 6, Loss: 0.1780172225438338, fit: 0.9333333333333333
    epoch : 7
      loss :              0.12318815935624512
      loss_factor :              8.048031390112232e-20
      loss adapted learning_rate :              2.4144094170336694e-21
    learning_rate : 2.4144094170336694e-21
    batch_size :          60
    best_batch_loss : 0.003087290701534762
    Epoch 7, Loss: 0.17801722254383423, fit: 0.8166666666666667
    epoch : 8
      loss :              0.31531657020069326
      loss_factor :              9.715562268202583e-16
      loss adapted learning_rate :              2.914668680460775e-17
    learning_rate : 2.914668680460775e-17
    batch_size :          60
    best_batch_loss : 1.9656817415174007e-05
    Epoch 8, Loss: 0.17801722254383412, fit: 0.8833333333333333
    Best fit: 0.9333333333333333
    Best loss: 0.1780172225438338
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.03_hidden_size_784_loss_0.22414616411408997_fit_0.8833333333333333_2024-01-03_194316
  self.fit : 0.8833333333333333
  self.loss : 0.22414616411408997
  current_accuracy : 0.9039
   Accuracy mean: 0.8766
   Accuracy mean: 0.9039
  LR: 0.1
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.1
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.1
    learning_rate : 0.1
    batch_size :          60
    best_batch_loss : 2.557826703524528e-06
    Epoch 0, Loss: 0.5742133557565354, fit: 0.38333333333333336
    Epoch 0, Loss: 0.5742133557565354
    Best fit: 0.38333333333333336
    Best loss: 0.5742133557565354
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.210899006577715_fit_0.38333333333333336_2024-01-03_194323
  self.fit : 0.38333333333333336
  self.loss : 1.210899006577715
  current_accuracy : 0.4813
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.1
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.1
    learning_rate : 0.1
    batch_size :          60
    best_batch_loss : 0.8239795831005214
    Epoch 0, Loss: 1.5917903494367365, fit: 0.08333333333333333
    Epoch 0, Loss: 1.5917903494367365
    epoch : 1
      loss :              1.8333324292094038
      loss_factor :              4.2895546575771476e-08
      loss adapted learning_rate :              4.2895546575771475e-09
    learning_rate : 4.2895546575771475e-09
    batch_size :          60
    best_batch_loss : 1.382287037425472
    Epoch 1, Loss: 1.7639102264727038, fit: 0.1
    epoch : 2
      loss :              1.801973748523704
      loss_factor :              3.609812114493032e-08
      loss adapted learning_rate :              3.609812114493032e-09
    learning_rate : 3.609812114493032e-09
    batch_size :          60
    best_batch_loss : 1.4666666667416843
    Epoch 2, Loss: 1.7639104488586865, fit: 0.15
    epoch : 3
      loss :              1.7
      loss_factor :              2.0159939004489983e-08
      loss adapted learning_rate :              2.0159939004489984e-09
    learning_rate : 2.0159939004489984e-09
    batch_size :          60
    best_batch_loss : 1.4488526265155985
    Epoch 3, Loss: 1.7639105687267989, fit: 0.11666666666666667
    epoch : 4
      loss :              1.7665726527920007
      loss_factor :              2.9601589456355313e-08
      loss adapted learning_rate :              2.9601589456355315e-09
    learning_rate : 2.9601589456355315e-09
    batch_size :          60
    best_batch_loss : 1.4845731727330154
    Epoch 4, Loss: 1.7639107136766612, fit: 0.13333333333333333
    epoch : 5
      loss :              1.7333286044888265
      loss_factor :              2.4479866259322404e-08
      loss adapted learning_rate :              2.4479866259322405e-09
    learning_rate : 2.4479866259322405e-09
    batch_size :          60
    best_batch_loss : 1.4666667047024788
    Epoch 5, Loss: 1.7639108196983349, fit: 0.08333333333333333
    epoch : 6
      loss :              1.833333333333332
      loss_factor :              4.289575811938196e-08
      loss adapted learning_rate :              4.289575811938196e-09
    learning_rate : 4.289575811938196e-09
    batch_size :          60
    best_batch_loss : 1.4385944830993391
    Epoch 6, Loss: 1.7639110171280232, fit: 0.05
    epoch : 7
      loss :              1.89874077511003
      loss_factor :              6.090553590538113e-08
      loss adapted learning_rate :              6.090553590538113e-09
    learning_rate : 6.090553590538113e-09
    batch_size :          60
    best_batch_loss : 1.433332159216016
    Epoch 7, Loss: 1.7639111955733688, fit: 0.08333333333333333
    epoch : 8
      loss :              1.8294132483085348
      loss_factor :              4.198732420202626e-08
      loss adapted learning_rate :              4.198732420202626e-09
    learning_rate : 4.198732420202626e-09
    batch_size :          60
    best_batch_loss : 1.4333338911329687
    Epoch 8, Loss: 1.7639115713203535, fit: 0.11666666666666667
    Best fit: 0.15
    Best loss: 1.5917903494367365
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.1_hidden_size_784_loss_1.766666659230173_fit_0.11666666666666667_2024-01-03_194425
  self.fit : 0.11666666666666667
  self.loss : 1.766666659230173
  current_accuracy : 0.1146
   Accuracy mean: 0.4813
   Accuracy mean: 0.1146
  LR: 0.9
  Hidden Size: 784
  accuracy_mean_list : [0.0, 0.0]
  Sample: 1/1
  Epochs: 1
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          1
    learning_rate :          0.9
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.9
    learning_rate : 0.9
    batch_size :          60
    best_batch_loss : 1.4262726819893743
    Epoch 0, Loss: 1.7950383887878452, fit: 0.13333333333333333
    Epoch 0, Loss: 1.7950383887878452
    Best fit: 0.13333333333333333
    Best loss: 1.7950383887878452
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.7333333333333334_fit_0.13333333333333333_2024-01-03_194432
  self.fit : 0.13333333333333333
  self.loss : 1.7333333333333334
  current_accuracy : 0.101
  Epochs: 10
    train
    hidden_activation_function :          tanh
    output_activation_function :          softmax
    X.shape :          (60000, 784)
    y_one_hot.shape :          (60000, 10)
    epochs :          9
    learning_rate :          0.9
    batch_rate :          0.001
    epoch : 0
      loss_factor :              1
      loss adapted learning_rate :              0.9
    learning_rate : 0.9
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 0, Loss: 1.7956333333333518, fit: 0.16666666666666666
    Epoch 0, Loss: 1.7956333333333518
    epoch : 1
      loss :              1.6666666666666667
      loss_factor :              1.653817168792022e-08
      loss adapted learning_rate :              1.48843545191282e-08
    learning_rate : 1.48843545191282e-08
    batch_size :          60
    best_batch_loss : 1.4666666666666666
    Epoch 1, Loss: 1.7956333333333518, fit: 0.13333333333333333
    epoch : 2
      loss :              1.7333333333333334
      loss_factor :              2.4480534124042247e-08
      loss adapted learning_rate :              2.2032480711638022e-08
    learning_rate : 2.2032480711638022e-08
    batch_size :          60
    best_batch_loss : 1.4666666666666666
    Epoch 2, Loss: 1.795633333333352, fit: 0.13333333333333333
    epoch : 3
      loss :              1.7333333333333334
      loss_factor :              2.4480534124042247e-08
      loss adapted learning_rate :              2.2032480711638022e-08
    learning_rate : 2.2032480711638022e-08
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 3, Loss: 1.7956333333333518, fit: 0.06666666666666667
    epoch : 4
      loss :              1.8666666666666667
      loss_factor :              5.1365050906215907e-08
      loss adapted learning_rate :              4.6228545815594314e-08
    learning_rate : 4.6228545815594314e-08
    batch_size :          60
    best_batch_loss : 1.4666666666666666
    Epoch 4, Loss: 1.795633333333352, fit: 0.1
    epoch : 5
      loss :              1.8
      loss_factor :              3.570467226623999e-08
      loss adapted learning_rate :              3.2134205039615987e-08
    learning_rate : 3.2134205039615987e-08
    batch_size :          60
    best_batch_loss : 1.5333333333333334
    Epoch 5, Loss: 1.795633333333352, fit: 0.13333333333333333
    epoch : 6
      loss :              1.7333333333333334
      loss_factor :              2.4480534124042247e-08
      loss adapted learning_rate :              2.2032480711638022e-08
    learning_rate : 2.2032480711638022e-08
    batch_size :          60
    best_batch_loss : 1.4333333333333333
    Epoch 6, Loss: 1.7956333333333525, fit: 0.13333333333333333
    epoch : 7
      loss :              1.7333333333333334
      loss_factor :              2.4480534124042247e-08
      loss adapted learning_rate :              2.2032480711638022e-08
    learning_rate : 2.2032480711638022e-08
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 7, Loss: 1.7956333333333523, fit: 0.1
    epoch : 8
      loss :              1.8
      loss_factor :              3.570467226623999e-08
      loss adapted learning_rate :              3.2134205039615987e-08
    learning_rate : 3.2134205039615987e-08
    batch_size :          60
    best_batch_loss : 1.5
    Epoch 8, Loss: 1.795633333333352, fit: 0.08333333333333333
    Best fit: 0.16666666666666666
    Best loss: 1.7956333333333518
      Weights saved to src/weights/weights_tanh_softmax_learning_rate_0.9_hidden_size_784_loss_1.8333333333333333_fit_0.08333333333333333_2024-01-03_194534
  self.fit : 0.08333333333333333
  self.loss : 1.8333333333333333
  current_accuracy : 0.101
   Accuracy mean: 0.101
   Accuracy mean: 0.101
  Error saving file: doc/out/test_combinations_results/20240103194534.
  normalized_accuracies :      [0.01364289 0.01364289 0.01184777 0.01184777 0.0295596  0.0295596
   0.         0.         0.03865486 0.03853518 0.14145524 0.21589277
   0.5351843  0.67663954 0.94255625 0.9877932  0.96744854 0.99497367
   0.96732887 1.         0.49425562 0.05540929 0.03913356 0.03913356]
